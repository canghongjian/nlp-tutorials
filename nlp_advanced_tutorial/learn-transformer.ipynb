{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ebacd7d",
   "metadata": {},
   "source": [
    "# 一课Transformer\n",
    "本文介绍hugging face提供的transformer库的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b729630",
   "metadata": {},
   "source": [
    "## 环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2dd7052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 1.6.0\n",
      "transformer: 4.18.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(f'torch: {torch.__version__}')\n",
    "print(f'transformer: {transformers.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61b86877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 所有大写字母路径需要改为自己的实际路径\n",
    "BASE_URL = '/root/transformers_data_and_model/'\n",
    "BERT_MODEL_NAME_OR_PATH = BASE_URL + 'bert-base-uncased'\n",
    "# 展示文件夹中的内容\n",
    "#!tree {BERT_MODEL_NAME_OR_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76fb29c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /root/transformers_data_and_model/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /root/transformers_data_and_model/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 初始化model和tokenizer\n",
    "# 所有model和tokenizer的初始化都是用from_pretrained方法，保存都使用save_pretrained的方法\n",
    "# 对from_pretrained:第一个参数是文件夹的路径/文件的路径/模型的short name等几种方法，这里推荐文件夹的方法\n",
    "# model初始化默认是eval模式，这里加载的是BERT的tokenizer和分类模型model\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(BERT_MODEL_NAME_OR_PATH)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(BERT_MODEL_NAME_OR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4816ee9",
   "metadata": {},
   "source": [
    "## tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ac74037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# tokenizer的作用：对于给定的文本，经过tokenizer处理成model可以接收的格式\n",
    "# tokenizer最重要的方法是__call__，这个方法可以将文本输出为模型要的格式\n",
    "# tokenzier还有其他方法，encode/decode,顾名思义，就是将文本转换成input_ids及将input_ids转换成文本\n",
    "# encode/decode与__call__其实无本质区别，只是__call__为了提供统一的处理接口\n",
    "inputs = tokenizer(\"We are very happy to show you the 🤗 Transformers library.\")\n",
    "# input_ids是文本每个词的index；token_type_id是表示文本是第一句/第二句；attention_mask是处理mask用的\n",
    "# 这里处理的是一个样本且只有一句话的例子，如果多句话，输入为一个文本list即可。\n",
    "#sentences = [[\"Gonna be ok.\", \"Ready perfectly.\"]]\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90671261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 7592, 102, 2204, 2851, 102], 'token_type_ids': [0, 0, 0, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# 一个样本且有两句话的例子，分别作为第一个和第二个参数输入\n",
    "# 如果是多个样本，每个样本都是两句话，则第一个参数是第一句话的文本list，第二个参数是第二句话的文本list\n",
    "#inputs2 = tokenizer([\"Gonna be ok.\", \"Ready perfectly.\", \"hello\"], [\"Gonna be ok.\", \"Ready perfectly.\", 'good morning'])\n",
    "inputs2 = tokenizer('hello', 'good morning')\n",
    "print(inputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dd95295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [[101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], [101, 2057, 3246, 2017, 2123, 1005, 1056, 5223, 2009, 1012, 102, 0, 0, 0]]\n",
      "token_type_ids: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# 更多时候，我们需要的model输入是成batch格式的\n",
    "# 第一个输入是文本list，padding设置成True，truncation设置成True 可以进行padding和truncation\n",
    "# return_tensors写明了返回格式，是一个pytorch的tensor\n",
    "pt_batch = tokenizer(\n",
    "    [\"We are very happy to show you the 🤗 Transformers library.\", \"We hope you don't hate it.\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "for key, value in pt_batch.items():\n",
    "    print(f'{key}: {value.numpy().tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f02673e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:tensor([[0.0497, 0.4243],\n",
      "        [0.0902, 0.4860]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "loss: 0.7168716788291931\n",
      "logits: tensor([[0.0497, 0.4243],\n",
      "        [0.0902, 0.4860]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 对于tokenzier处理后的文本，目标是送入model，用于分类、预测等任务\n",
    "# 上面的pt_batch就是一个batch，增加**直接输入模型\n",
    "# 根据transformers库的规则，所有model的输出都是元组\n",
    "# 如果只有每个batch的输入，元组输出的第一个是logits\n",
    "# 如果同时传入了labels的参数，则元组输出的第一个是loss，第二个是logits\n",
    "# 一般回归任务loss用的Mean-Square loss，分类任务则是Cross-Entry\n",
    "pt_outputs = model(**pt_batch)\n",
    "print(f'logits:{pt_outputs[0]}\\n')\n",
    "# 传入labels参数的情况\n",
    "pt_outputs = model(**pt_batch, labels=torch.LongTensor([1, 0]))\n",
    "print(f'loss: {pt_outputs[0]}\\nlogits: {pt_outputs[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e846a9",
   "metadata": {},
   "source": [
    "上面就是pytorch版transformers的基本输入逻辑：\n",
    "transformers中的所有model都是pytorch标准模型类torch.nn.Module.文本可以通过tokenizer调用转换为模型的输入，模型输入这些信息，得到logits，计算loss，进行误差回传backward，进行迭代，就完成了训练/fine-tune.模型输入的时候，如果传入labels的参数，也可以直接得到相应的loss，一帮backward，多次迭代，完成训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05887168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完成训练/微调后，可以将tokenizer和model保存到相同文件夹\n",
    "# 在transformers框架里，一个很好的习惯，将model、tokenizer参数、训练参数等所有保存到一个文件夹下。\n",
    "# 这里的model/tokenzier/config的初始化使用from_pretrained,保存使用save_pretrained\n",
    "# save_pretrained传入具体文件夹名即可\n",
    "SAVE_DIRECTORY = BASE_URL + 'bert_save_example'\n",
    "tokenizer.save_pretrained(SAVE_DIRECTORY)\n",
    "model.save_pretrained(SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3968ee89",
   "metadata": {},
   "source": [
    "### 总结\n",
    "1.在transformers框架里，提供了model/tokenizer/config通用化的加载和保存，也就是from_pretraiend/save_pretrained；\n",
    "2.tokenizer的作用在于，通过__call__将文本进行转换成模型接受的格式，model的输出都是元组，依据这些元组的内容进行计算loss；\n",
    "3.tokenizer包装了Byte-Pair Encoding、WordPiece、SentencePiece等不同的方式；\n",
    "4.model是包装了BERT、GPT2、ALBERT等不同的模型，并且提供标准化的类。等下细说。\n",
    "5.对于我们来讲，去复现、实验、研究更容易。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac92be9b",
   "metadata": {},
   "source": [
    "## 概念与说明\n",
    "### 主要的类\n",
    "主要的类有Model、Configuration、Tokenizer这三个类，下面分别介绍。\n",
    "Model类，比如BertModel，均从pytorch models(torch.nn.Module)或者keras models(tf.keras.Model)继承而来，用于处理预训练权重。\n",
    "\n",
    "Configuration类，比如BertConfig，里面保存着建立模型所需要的所有参数。并不是总是我们手动去初始化这个类，尤其是当你使用没有做任何更改的预训练时，model会自动处理好这个类。也就是说，如果自己重新预训练的模型且架构不一致时，是允许我们去初始化这个类的。\n",
    "\n",
    "Tokenizer类，比如BERTTokenizer，为每个模型保存词典，并将文本进行编码/解码成模型需要的格式——token嵌入的索引。\n",
    "\n",
    "上面的类，都有下面两个方法去实例化类和保存至本地：\n",
    "\n",
    "from_pretrained() 允许我们加载预训练模型，可以使用short_name，也可以使用本地的模型，作为第一个参数model_name_or_path传入，可以是文件夹、文件、short_name等。其中文件夹的话，会默认寻找文件夹中的pytorh_model.bin。\n",
    "\n",
    "save_pretrained() 允许我们将模型保存至本地，保存的参数是可以是文件夹。\n",
    "\n",
    "### AutoModels\n",
    "以BERT为例，每个模型都有一个Config（BertConfig）；有1-2个tokenizer，分别是基于rust的快速tokenizer（BertTokenizerFast），一个是基于python原版的tokenizer（BertTokenizer），部分没有提供rust的快速tokenizer；有多个皆有不同head的Model，比如最原始的模型，不含head的BertModel、预训练MLM和NSP的BertForPreTraining、MLM head的BertForMaskedLM，NSP的BertForNextSentencePrediction，用于句子分类的BertForSequenceClassification，用于多选的BertForMultipleChoice，用单词分类的BertForTokenClassification，用于问答的BertForQuestionAnswering。\n",
    "\n",
    "不同的模型，会稍有不同。但是config类都继承自PretrainedConfig；tokenizer都继承自PreTrainedTokenizer或PreTrainedTokenizerFast；model都继承自PreTrainedModel。\n",
    "\n",
    "为了使用方便，AutoConfig、AutoTokenizer、AutoModel、AutoModelForPreTraining、AutoModelWithLMHead、AutoModelForSequenceClassification、AutoModelForQuestionAnswering、AutoModelForTokenClassification等可以用于自动查找模型。\n",
    "\n",
    "### Trainer类\n",
    "Trainer类提供了一个完整的标准训练的API，目前支持语言模型、文本分类、单词分类（NER）等任务。对于前面的config、tokenizer、model，我们可以认为，帮助我们简化的是写模型的这一步，正常生成dataset、dataloader，然后再每个epoch、batch进行训练，得到最终的结果。正常写的话，Trainer类可以不用，Trainer其实是简化的是我们训练的这一步。\n",
    "\n",
    "对于通常的训练过程，写法大致是这样的（下面是伪代码）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a6e68d88",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-35995ac0a14b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 通常训练过程代码\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 加载数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# 转换成features，获得dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_data' is not defined"
     ]
    }
   ],
   "source": [
    "## 伪代码 不要执行！\n",
    "\n",
    "# 通常训练过程代码\n",
    "# 加载数据\n",
    "train_data, test_dat = get_data()\n",
    "# 转换成features，获得dataset\n",
    "train_dataset = MyDataset(train_data, args)\n",
    "test_dataset = MyDataset(test_data, args)\n",
    "\n",
    "# 转换成dataloader，用于生成batch\n",
    "# sampler 定义取batch的方法，是一个迭代器， 每次生成一个key 用于读取dataset中的值\n",
    "train_sampler, test_sampler = ...\n",
    "# collate_fn函数会将batch_size个样本整理成一个batch样本，便于批量训练。\n",
    "train_dataloader = Dataloader(train_dataset, sampler=train_sampler, batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_dataloader = Dataloader(test_dataset, sampler=test_sampler, batch_size=batch_size, collate_fn=collate_fn)\n",
    "# 初始化tensorboard， tensorboard是一个展示训练过程各张量变化的工具\n",
    "tb_writer = SummaryWriter(log_dir=None)\n",
    "# 加载optimizer\n",
    "optimizer = ...\n",
    "model.to(GPU)\n",
    "# 开始训练\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_dataloader:\n",
    "        # 转换为train\n",
    "        model.train()\n",
    "        # 传入GPU\n",
    "        batch.to(GPU)\n",
    "        # 计算每一步的loss，然后回传\n",
    "        tr_loss = train_step(model, inputs, optimizer)\n",
    "        tr_loss.backward()\n",
    "        model.zero_grad()\n",
    "for batch in test_dataloader:\n",
    "    ...\n",
    "model.save_pretrained(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebe35ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 伪代码 不要执行！！\n",
    "# 下面是使用了Trainer的训练过程\n",
    "\n",
    "# 加载数据\n",
    "train_data, test_data = get_data()\n",
    "# 转换成features， 获得dataset\n",
    "train_dataset = MyDataset(train_data, args)\n",
    "test_dataset = MyDataset(test_data, args)\n",
    "# 读入train_args\n",
    "train_args = **\n",
    "\n",
    "# 初始化本Trainer\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=build_compute_metrics_fn(data_args.task_name),\n",
    ")\n",
    "# 训练\n",
    "if training_args.do_train:\n",
    "    trainer.train(\n",
    "        model_path=model_args.model_name_or_path if os.pash.isdir(model_args.model_name_or_path) else None\n",
    "    )\n",
    "    # trainer保存模型，里面调用的还是save_pretrained方法\n",
    "    trainer.save_model()\n",
    "    # 保存tokenizer到同一个文件夹，方便使用\n",
    "    if trainer.is_world_master():\n",
    "        tokenizer.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddddc16c",
   "metadata": {},
   "source": [
    "## GLUE/MRPC数据集进行文本分类的示例\n",
    "### 4.1本节说明\n",
    "以简单的例子，说明文本分类的fine-tune详细过程，线上部署代码\n",
    "4.2节详细实现了文本分类任务的详细代码，4.3节实现了基于4.2节训练好的文本分类模型的线上预测代码，4.4节总结了文本分类的基本经验和transformers使用的基本经验。\n",
    "本节涉及到的模型是bert-base-uncased，涉及到的数据集是glue数据集下的MRPC数据集。\n",
    "\n",
    "glue数据集共有9个任务，其中STS-B是一个回归任务，MNLI是三分类任务，剩余7类均是二分类任务。九个任务之一的MRPC（The Microsoft Research Paraphrase Corpus，微软研究院释义语料库），相似性和释义任务，是从在线新闻源中自动抽取句子对语料库，并人工注释句子对中的句子是否在语义上等效。类别并不平衡，其中68%的正样本，所以遵循常规的做法，报告准确率（accuracy）和F1值。样本个数：训练集3, 668个，开发集408个，测试集1, 725个。任务：是否释义二分类，是释义，不是释义两类。评价准则：准确率（accuracy）和F1值。标签为1（正样本，互为释义）的样例（每个样例是两句话，中间用tab隔开）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f2dac",
   "metadata": {},
   "source": [
    "### 4.2模型训练的详细过程\n",
    "transformers提供了config、tokenizer、model等类简化了分词、模型等步骤，同时又有Trainer类简化了训练过程。那么更详细的训练过程是什么呢？本节主要的内容就是实现和讲解模型分类的详细过程。\n",
    "\n",
    "简单的讲，主要分为几个步骤：\n",
    "1.加载参数，文本处理成Dataset；\n",
    "2.写collate_fn，用于处理padding；\n",
    "3.加载config、tokenizer、model等；\n",
    "4.写metrics，用于评估效果；\n",
    "5.将以上参数送入Trainer初始化类，然后调用Trainer的train方法训练。\n",
    "\n",
    "加载参数的参数分为三类：一类是model相关参数，用于记录模型的位置等信息，用于初始化模型；一类是数据参数，数据的位置，任务名称等用于提供模型输入前的参数；一类是训练参数，这些是模型在训练过程中的参数，比如learning_rate、epochs等。\n",
    "\n",
    "文本最开始需要载入，可以通过写明一个Processor类，这个类用于提供几个方法：获得训练样本、获得开发样本、获得标签list等。这里的获得样本是一个list，每个元素都是一个example，example里包含文本和对应的标签（有的不含，比如测试集）。这个Processor提供的方法主要是为了Dataset类使用，Dataset实现单个输入的样本。\n",
    "\n",
    "collate_fn是Dataloader类的输入，对于处理好的恒定长度的feature，可以不输入collate_fn，使用默认的collate_fn，对于长度不定涉及到padding的文本，需要自己写此参数。\n",
    "\n",
    "config、tokenizer、model的加载我们已基本熟悉。\n",
    "\n",
    "metrics写法可以参考下面方法。\n",
    "\n",
    "然后将这些参数送入Trainer，就可以训练和评估了。\n",
    "\n",
    "请看下面代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "919685bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import enum\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, Dict, Optional, List, Union, NamedTuple\n",
    "\n",
    "import filelock\n",
    "import torch\n",
    "import numpy as np\n",
    "import transformers\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "214db17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载好的预训练模型位置\n",
    "BASE_URL = '/root/transformers_data_and_model/'\n",
    "BERT_MODEL_NAME_OR_PATH = BASE_URL + 'bert-base-uncased'\n",
    "\n",
    "# 下载好的glue数据集中的MRPC数据集的位置\n",
    "MRPC_DATA_DIR = BASE_URL + 'MRPC'\n",
    "# finetune好的model、tokenizer等各种参数存放的位置\n",
    "FINETUNED_MRPC = BASE_URL + 'finetuned-mrpc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbdd402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日志文件\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41090d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型参数，包含model、config、tokenzier、cache_dir等\n",
    "# 有三类参数：\n",
    "# 一个是模型参数，这个就是下面的定义；\n",
    "# 一个是模型训练参数，可以参考transformers/src/train_args.py 文件，主要是epoch、batch_size等常见的训练参数，也包含device这种设备参数\n",
    "# 一个数据参数，决定数据处理任务的参数，使用什么数据，数据名称，是否覆盖数据的cache，最长长度，这个长度是生成features使用的\n",
    "# 下面这个就是模型参数：\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    '''\n",
    "    Arguments pertaining to which model/config/tokenzier we are going to fine-tune from\n",
    "    '''\n",
    "    \n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "        \n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "        \n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    \n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "\n",
    "        \n",
    "# 数据（训练使用的）参数\n",
    "# 这些参数用于送入Dataset、processor等，用于方便加载数据，完成从开始到模型输入前的参数\n",
    "@dataclass\n",
    "class GlueDataTrainingArguments:\n",
    "    '''\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \n",
    "    Using 'HfArgumentParser' we can turn this classs\n",
    "    into argparse arguments to be able to specify them on\n",
    "    the command line.\n",
    "    '''\n",
    "    \n",
    "    task_name: str = field(metadata={\"help\": \"The name of the task to train on: MRPC\"})\n",
    "    data_dir: str = field(\n",
    "        metadata={\"help\": \"The input data dir. Should contain the .tsv files (or other data files) for the tast.\"}\n",
    "    )\n",
    "    max_seq_length: int = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequence longer \"\n",
    "            \"than this will be truncated, sequence shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        self.task_name = self.task_name.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73daae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_args = ['--model_name_or_path', BERT_MODEL_NAME_OR_PATH,\n",
    "             '--task_name', 'MRPC',\n",
    "              '--do_train',\n",
    "              '--do_eval',\n",
    "              '--data_dir', MRPC_DATA_DIR,\n",
    "              '--max_seq_length', '128',\n",
    "              '--per_device_train_batch_size', '32',\n",
    "              '--learning_rate', '3e-5',\n",
    "              '--num_train_epochs', '3.0',\n",
    "              '--output_dir', FINETUNED_MRPC,\n",
    "              '--overwrite_cache',\n",
    "              '--overwrite_output_dir']\n",
    "\n",
    "\n",
    "# 以后新的实验，需要实现上面的模型参数和数据参数，基本上按照上面格式去写，使用下面的方法转换成参数空间即可\n",
    "# transformers里，有一个HfArgumentParser用于解析上面格式的参数，为标准的python参数\n",
    "parser = transformers.HfArgumentParser((ModelArguments, transformers.GlueDataTrainingArguments, transformers.TrainingArguments))\n",
    "# 将三类参数分别解析为对应空间\n",
    "# 模型本身的参数，数据的参数，训练的参数\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(input_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df5a5228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArguments(model_name_or_path='/root/transformers_data_and_model/bert-base-uncased', config_name=None, tokenizer_name=None, cache_dir=None)\n",
      "\n",
      "GlueDataTrainingArguments(task_name='mrpc', data_dir='/root/transformers_data_and_model/MRPC', max_seq_length=128, overwrite_cache=True)\n",
      "\n",
      "TrainingArguments(\n",
      "_n_gpu=4,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/root/transformers_data_and_model/finetuned-mrpc/runs/Apr25_03-52-24_e41dfb6b4fe0,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=/root/transformers_data_and_model/finetuned-mrpc,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/root/transformers_data_and_model/finetuned-mrpc,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 展示一下所有参数\n",
    "print(f'{model_args}\\n\\n{data_args}\\n\\n{training_args}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa4bba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确保output_dir可用\n",
    "if (\n",
    "    os.path.exists(training_args.output_dir)\n",
    "    and os.listdir(training_args.output_dir)\n",
    "    and training_args.do_train\n",
    "    and not training_args.overwrite_output_dir\n",
    "):\n",
    "    raise ValueError(\n",
    "        f'Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67c459b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04252022 03:52:29 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 4, distributed training: False, 16-bits training:False\n",
      "04252022 03:52:29 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=4,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/root/transformers_data_and_model/finetuned-mrpc/runs/Apr25_03-52-24_e41dfb6b4fe0,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=/root/transformers_data_and_model/finetuned-mrpc,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/root/transformers_data_and_model/finetuned-mrpc,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 设计日志格式，记录一些关键参数，并且把训练参数打印出来\n",
    "# 一个很重要的感受：使用logger打印中间变量很重要\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m%d%Y %H:%M:%S\",\n",
    "    level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    ")\n",
    "\n",
    "logger.warning(\n",
    "    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training:%s\",\n",
    "    training_args.local_rank,\n",
    "    training_args.device,\n",
    "    training_args.n_gpu,\n",
    "    bool(training_args.local_rank != -1),\n",
    "    training_args.fp16,\n",
    ")\n",
    "logger.info(\"Training/evaluation parameters %s\", training_args)\n",
    "\n",
    "# 设定种子\n",
    "transformers.set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45a17a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得标签的个数\n",
    "# 输出的模式，这里是classification与regression两种\n",
    "# 如果适配新任务，我们要的不是去按照这种格式，而是要得到这两个参数\n",
    "num_labels = 2\n",
    "output_mode = \"classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c442fd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /root/transformers_data_and_model/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /root/transformers_data_and_model/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 加载model、tokenizer、model这三个\n",
    "# config是包含层数、dropout参数、head个数、finetune任务等模型相关内容的参数，这个参数加载后只是为了model使用。\n",
    "# config内写入标签的个数num_labels，决定model后面分类使用的全连接的输出个数\n",
    "config = transformers.AutoConfig.from_pretrained(\n",
    "    model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    finetuning_task=data_args.task_name,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "    config=config,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cea67072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers.DataProcessor是一个基类，需要实现get_train_examples, get_dev_examples, get_test_examples, get_labels等几个函数\n",
    "# 分别用于提供的InputExample的集和(list)和标签的集和\n",
    "class MrpcProcessor(transformers.DataProcessor):\n",
    "    \"Processor for the MRPC data set (GLUE version).\"\n",
    "    \n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return InputExample(\n",
    "            tensor_dict['idx'].numpy(),\n",
    "            tensor_dict['sentence1'].numpy().decode('utf-8'),\n",
    "            tensor_dict['sentence2'].numpy().decode('utf-8'),\n",
    "            str(tensor_dict['label'].numpy()),\n",
    "        )\n",
    "    \n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.tsv\")))\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "    \n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "    \n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "    \n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "    \n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Create examples for the training, dev and test sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[3]\n",
    "            text_b = line[4]\n",
    "            label = None if set_type == \"test\" else line[0]\n",
    "            examples.append(transformers.InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3b8fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将examples转换成features\n",
    "def glue_convert_examples_to_features(\n",
    "    examples: List[transformers.InputExample],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    max_length: Optional[int] = None,\n",
    "    task=None,\n",
    "    label_list=None,\n",
    "    output_mode=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads a data file into a list of ``InputFeatures``\n",
    "\n",
    "    Args:\n",
    "        examples: List of ``InputExamples`` or ``tf.data.Dataset`` containing the examples.\n",
    "        tokenizer: Instance of a tokenizer that will tokenize the examples\n",
    "        max_length: Maximum example length. Defaults to the tokenizer's max_len\n",
    "        task: GLUE task\n",
    "        label_list: List of labels. Can be obtained from the processor using the ``processor.get_labels()`` method\n",
    "        output_mode: String indicating the output mode. Either ``regression`` or ``classification``\n",
    "\n",
    "    Returns:\n",
    "        If the ``examples`` input is a ``tf.data.Dataset``, will return a ``tf.data.Dataset``\n",
    "        containing the task-specific features. If the input is a list of ``InputExamples``, will return\n",
    "        a list of task-specific ``InputFeatures`` which can be fed to the model.\n",
    "\n",
    "    \"\"\"\n",
    "    if max_length is None:\n",
    "        max_length = tokenizer.max_len\n",
    "        \n",
    "    if task is not None:\n",
    "        processor = MrpcProcessor()\n",
    "        if label_list is None:\n",
    "            label_list = processor.get_labels()\n",
    "            logger.info(\"Using label list %s for task %s\" % (label_list, task))\n",
    "        if output_mode is None:\n",
    "            output_mode = glue_output_modes[task]\n",
    "            logger.info(\"Using output mode %s for task %s\" % (output_mode, task))\n",
    "    \n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "    \n",
    "    def label_from_example(example: transformers.InputExample) -> Union[int, float, None]:\n",
    "        if example.label is None:\n",
    "            return None\n",
    "        if output_mode == \"classification\":\n",
    "            return label_map[example.label]\n",
    "        elif output_mode == \"regression\":\n",
    "            return float(example.label)\n",
    "        raise KeyError(output_mode)\n",
    "    \n",
    "    labels = [label_from_example(example) for example in examples]\n",
    "    \n",
    "    batch_encoding = tokenizer(\n",
    "        [(example.text_a, example.text_b) for example in examples],\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    \n",
    "    features = []\n",
    "    for i in range(len(examples)):\n",
    "        inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
    "        \n",
    "        feature = transformers.InputFeatures(**inputs, label=labels[i])\n",
    "        features.append(feature)\n",
    "    \n",
    "    for i, example in enumerate(examples[:5]):\n",
    "        logger.info(\"*** Example ***\")\n",
    "        logger.info(\"guid: %s\" % (example.guid))\n",
    "        logger.info(\"features: %s\" % features[i])\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8acaaa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Split(enum.Enum):\n",
    "    train = 'train'\n",
    "    dev = 'dev'\n",
    "    test = 'test'\n",
    "    \n",
    "class GlueDataset(torch.utils.data.dataset.Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach soon.\n",
    "    \"\"\"\n",
    "    args: GlueDataTrainingArguments\n",
    "    output_mode: str\n",
    "    features: List[transformers.InputFeatures]\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        args: transformers.GlueDataTrainingArguments,\n",
    "        tokenizer: transformers.PreTrainedTokenizer,\n",
    "        limit_length: Optional[int] = None,\n",
    "        mode: Union[str, Split] = Split.train,\n",
    "        cache_dir: Optional[str] = None,\n",
    "        output_mode = \"classification\",\n",
    "    ):\n",
    "        self.args = args\n",
    "        self.processor = MrpcProcessor()\n",
    "        self.output_mode = output_mode\n",
    "        if isinstance(mode, str):\n",
    "            try:\n",
    "                mode =Split[mode]\n",
    "            except KeyError:\n",
    "                raise KeyError(\"mode is not a valid split name\")\n",
    "        # load data features from cache or dataset file\n",
    "        cached_features_file = os.path.join(\n",
    "            cache_dir if cache_dir is not None else args.data_dir,\n",
    "            \"cached_{}_{}_{}_{}\".format(\n",
    "                mode.value, tokenizer.__class__.__name__, str(args.max_seq_length), args.task_name,\n",
    "            ),\n",
    "        )\n",
    "        label_list = self.processor.get_labels()\n",
    "        self.label_list = label_list\n",
    "        \n",
    "        # Make sure only the first process in distributed training processes the dataset,\n",
    "        # and the others will use the cache.\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "        with filelock.FileLock(lock_path):\n",
    "            if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "                start = time.time()\n",
    "                self.features = torch.load(cached_features_file)\n",
    "                logger.info(\n",
    "                    f\"Loading features from cached file {cached_features_file} [took %3.f s]\", time.time() - start\n",
    "                )\n",
    "            else:\n",
    "                logger.info(f\"Creating features from dataset file at {args.data_dir}\")\n",
    "                \n",
    "                if mode == Split.dev:\n",
    "                    examples = self.processor.get_dev_examples(args.data_dir)\n",
    "                elif mode == Split.test:\n",
    "                    examples = self.processor.get_test_examples(args.data_dir)\n",
    "                else:\n",
    "                    examples = self.processor.get_train_examples(args.data_dir)\n",
    "                if limit_length is not None:\n",
    "                    examples = examples[:limit_length]\n",
    "                self.features = glue_convert_examples_to_features(\n",
    "                    examples,\n",
    "                    tokenizer,\n",
    "                    max_length=args.max_seq_length,\n",
    "                    label_list=label_list,\n",
    "                    output_mode=self.output_mode,\n",
    "                )\n",
    "                start = time.time()\n",
    "                torch.save(self.features, cached_features_file)\n",
    "                # ^ This seems to take a lot of time so I want to investigate why and how we can improve.\n",
    "                logger.info(\n",
    "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
    "                )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, i) -> transformers.InputFeatures:\n",
    "        return self.features[i]\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return self.label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a80fca8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04252022 03:54:49 - INFO - filelock - Lock 140540602682128 acquired on /root/transformers_data_and_model/MRPC/cached_train_BertTokenizerFast_128_mrpc.lock\n",
      "04252022 03:54:49 - INFO - __main__ - Creating features from dataset file at /root/transformers_data_and_model/MRPC\n",
      "04252022 03:54:49 - INFO - __main__ - LOOKING AT /root/transformers_data_and_model/MRPC/train.tsv\n",
      "04252022 03:54:50 - INFO - __main__ - *** Example ***\n",
      "04252022 03:54:50 - INFO - __main__ - guid: train-1\n",
      "04252022 03:54:50 - INFO - __main__ - features: InputFeatures(input_ids=[101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 7727, 2000, 2032, 2004, 2069, 1000, 1996, 7409, 1000, 1010, 2572, 3217, 5831, 5496, 2010, 2567, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "04252022 03:54:50 - INFO - __main__ - *** Example ***\n",
      "04252022 03:54:50 - INFO - __main__ - guid: train-2\n",
      "04252022 03:54:50 - INFO - __main__ - features: InputFeatures(input_ids=[101, 9805, 3540, 11514, 2050, 3079, 11282, 2243, 1005, 1055, 2077, 4855, 1996, 4677, 2000, 3647, 4576, 1999, 2687, 2005, 1002, 1016, 1012, 1019, 4551, 1012, 102, 9805, 3540, 11514, 2050, 4149, 11282, 2243, 1005, 1055, 1999, 2786, 2005, 1002, 6353, 2509, 2454, 1998, 2853, 2009, 2000, 3647, 4576, 2005, 1002, 1015, 1012, 1022, 4551, 1999, 2687, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "04252022 03:54:50 - INFO - __main__ - *** Example ***\n",
      "04252022 03:54:50 - INFO - __main__ - guid: train-3\n",
      "04252022 03:54:50 - INFO - __main__ - features: InputFeatures(input_ids=[101, 2027, 2018, 2405, 2019, 15147, 2006, 1996, 4274, 2006, 2238, 2184, 1010, 5378, 1996, 6636, 2005, 5096, 1010, 2002, 2794, 1012, 102, 2006, 2238, 2184, 1010, 1996, 2911, 1005, 1055, 5608, 2018, 2405, 2019, 15147, 2006, 1996, 4274, 1010, 5378, 1996, 14792, 2005, 5096, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "04252022 03:54:50 - INFO - __main__ - *** Example ***\n",
      "04252022 03:54:50 - INFO - __main__ - guid: train-4\n",
      "04252022 03:54:50 - INFO - __main__ - features: InputFeatures(input_ids=[101, 2105, 6021, 19481, 13938, 2102, 1010, 21628, 6661, 2020, 2039, 2539, 16653, 1010, 2030, 1018, 1012, 1018, 1003, 1010, 2012, 1037, 1002, 1018, 1012, 5179, 1010, 2383, 3041, 2275, 1037, 2501, 2152, 1997, 1037, 1002, 1018, 1012, 5401, 1012, 102, 21628, 6661, 5598, 2322, 16653, 1010, 2030, 1018, 1012, 1020, 1003, 1010, 2000, 2275, 1037, 2501, 5494, 2152, 2012, 1037, 1002, 1018, 1012, 5401, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "04252022 03:54:50 - INFO - __main__ - *** Example ***\n",
      "04252022 03:54:50 - INFO - __main__ - guid: train-5\n",
      "04252022 03:54:50 - INFO - __main__ - features: InputFeatures(input_ids=[101, 1996, 4518, 3123, 1002, 1016, 1012, 2340, 1010, 2030, 2055, 2340, 3867, 1010, 2000, 2485, 5958, 2012, 1002, 2538, 1012, 4868, 2006, 1996, 2047, 2259, 4518, 3863, 1012, 102, 18720, 1004, 1041, 13058, 1012, 6661, 5598, 1002, 1015, 1012, 6191, 2030, 1022, 3867, 2000, 1002, 2538, 1012, 6021, 2006, 1996, 2047, 2259, 4518, 3863, 2006, 5958, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "04252022 03:54:51 - INFO - __main__ - Saving features into cached file /root/transformers_data_and_model/MRPC/cached_train_BertTokenizerFast_128_mrpc [took 0.878 s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04252022 03:54:51 - INFO - filelock - Lock 140540602682128 released on /root/transformers_data_and_model/MRPC/cached_train_BertTokenizerFast_128_mrpc.lock\n",
      "04252022 03:54:51 - INFO - filelock - Lock 140540607032720 acquired on /root/transformers_data_and_model/MRPC/cached_dev_BertTokenizerFast_128_mrpc.lock\n",
      "04252022 03:54:51 - INFO - __main__ - Creating features from dataset file at /root/transformers_data_and_model/MRPC\n",
      "04252022 03:54:51 - INFO - __main__ - *** Example ***\n",
      "04252022 03:54:51 - INFO - __main__ - guid: dev-1\n",
      "04252022 03:54:51 - INFO - __main__ - features: InputFeatures(input_ids=[101, 2002, 2056, 1996, 9440, 2121, 7903, 2063, 11345, 2449, 2987, 1005, 1056, 4906, 1996, 2194, 1005, 1055, 2146, 1011, 2744, 3930, 5656, 1012, 102, 1000, 1996, 9440, 2121, 7903, 2063, 11345, 2449, 2515, 2025, 4906, 2256, 2146, 1011, 2744, 3930, 5656, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "04252022 03:54:51 - INFO - __main__ - *** Example ***\n",
      "04252022 03:54:51 - INFO - __main__ - guid: dev-2\n",
      "04252022 03:54:51 - INFO - __main__ - features: InputFeatures(input_ids=[101, 20201, 22948, 2056, 10958, 19053, 4140, 6283, 1996, 8956, 6939, 1998, 2246, 2830, 2000, 2478, 2010, 2146, 2086, 1997, 2731, 1999, 1996, 2162, 1012, 102, 2010, 2564, 2056, 2002, 2001, 1000, 2531, 3867, 2369, 2577, 5747, 1000, 1998, 2246, 2830, 2000, 2478, 2010, 2086, 1997, 2731, 1999, 1996, 2162, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "04252022 03:54:51 - INFO - __main__ - *** Example ***\n",
      "04252022 03:54:51 - INFO - __main__ - guid: dev-3\n",
      "04252022 03:54:51 - INFO - __main__ - features: InputFeatures(input_ids=[101, 1996, 7922, 2001, 2012, 12904, 1012, 6227, 18371, 2114, 1996, 18371, 1010, 4257, 2006, 1996, 5219, 1010, 1998, 2012, 1015, 1012, 27054, 2487, 2114, 1996, 5364, 23151, 2278, 1010, 2036, 4257, 1012, 102, 1996, 7922, 2001, 2012, 12904, 1012, 6275, 18371, 16545, 2100, 1027, 1010, 8990, 4257, 2006, 1996, 5219, 1010, 1998, 2012, 1015, 1012, 23090, 2487, 2114, 1996, 5364, 23151, 2278, 10381, 2546, 1027, 1010, 2091, 1014, 1012, 1015, 3867, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "04252022 03:54:51 - INFO - __main__ - *** Example ***\n",
      "04252022 03:54:51 - INFO - __main__ - guid: dev-4\n",
      "04252022 03:54:51 - INFO - __main__ - features: InputFeatures(input_ids=[101, 1996, 10028, 1011, 25022, 2080, 2003, 3403, 2127, 2255, 2000, 5630, 2065, 2009, 2097, 2203, 5668, 2063, 1037, 4018, 1012, 102, 1996, 10028, 1011, 25022, 2080, 2623, 9317, 2008, 2009, 2097, 5630, 1999, 2255, 3251, 2000, 2203, 5668, 2063, 1037, 4018, 2077, 1996, 27419, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "04252022 03:54:51 - INFO - __main__ - *** Example ***\n",
      "04252022 03:54:51 - INFO - __main__ - guid: dev-5\n",
      "04252022 03:54:51 - INFO - __main__ - features: InputFeatures(input_ids=[101, 2053, 5246, 2031, 2042, 2275, 2005, 1996, 2942, 2030, 1996, 4735, 3979, 1012, 102, 2053, 5246, 2031, 2042, 2275, 2005, 1996, 4735, 2030, 2942, 3572, 1010, 2021, 17137, 3051, 2038, 12254, 2025, 5905, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "04252022 03:54:51 - INFO - __main__ - Saving features into cached file /root/transformers_data_and_model/MRPC/cached_dev_BertTokenizerFast_128_mrpc [took 0.110 s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04252022 03:54:51 - INFO - filelock - Lock 140540607032720 released on /root/transformers_data_and_model/MRPC/cached_dev_BertTokenizerFast_128_mrpc.lock\n"
     ]
    }
   ],
   "source": [
    "# 获得dataset\n",
    "train_dataset, eval_dataset, test_dataset = None, None, None\n",
    "if training_args.do_train:\n",
    "    train_dataset = GlueDataset(data_args, tokenizer=tokenizer, cache_dir=model_args.cache_dir, output_mode=output_mode)\n",
    "if training_args.do_eval:\n",
    "    eval_dataset = GlueDataset(data_args, tokenizer=tokenizer, mode=\"dev\", cache_dir=model_args.cache_dir)\n",
    "if training_args.do_predict:\n",
    "    test_dataset = GlueDataset(data_args, tokenizer=tokenizer, mode=\"test\", cache_dir=model_args.cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d7f2cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到计算结果的函数\n",
    "# 这里计算的是acc和f1\n",
    "# EvalPrediction是预测的结果的格式，prediction是预测的，labels_ids是正确的\n",
    "class EvalPrediction(NamedTuple):\n",
    "    \"\"\"\n",
    "    Evaluation output (always contains labels), to be used to compute metrics.\n",
    "    \n",
    "    Parameters:\n",
    "        predictions (:obj:`np.ndarray`): Predictions of the model.\n",
    "        label_ids (:obj:`np.ndarray`): Targets to be matched.\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions:np.ndarray\n",
    "    label_ids:np.ndarray\n",
    "        \n",
    "    \n",
    "# 得到计算函数\n",
    "def compute_metrics_fn(p: EvalPrediction):\n",
    "    if output_mode == \"classification\":\n",
    "        # 预测的结果\n",
    "        preds = np.argmax(p.predictions, axis=1)\n",
    "    # 正确的结果\n",
    "    labels = p.label_ids\n",
    "\n",
    "    # acc和f1\n",
    "    acc = (preds == labels).mean()\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds)\n",
    "    return {\"acc\": acc, \"f1\":f1, \"acc_and_f1\": (acc+f1) / 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc9a08a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化本Trainer\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ed8d3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1145: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "Loading model from /root/transformers_data_and_model/bert-base-uncased).\n",
      "There were missing keys in the checkpoint model loaded: ['bert.embeddings.position_ids', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'classifier.weight', 'classifier.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.gamma', 'cls.predictions.transform.LayerNorm.beta', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.embeddings.LayerNorm.gamma', 'bert.embeddings.LayerNorm.beta', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma', 'bert.encoder.layer.0.attention.output.LayerNorm.beta', 'bert.encoder.layer.0.output.LayerNorm.gamma', 'bert.encoder.layer.0.output.LayerNorm.beta', 'bert.encoder.layer.1.attention.output.LayerNorm.gamma', 'bert.encoder.layer.1.attention.output.LayerNorm.beta', 'bert.encoder.layer.1.output.LayerNorm.gamma', 'bert.encoder.layer.1.output.LayerNorm.beta', 'bert.encoder.layer.2.attention.output.LayerNorm.gamma', 'bert.encoder.layer.2.attention.output.LayerNorm.beta', 'bert.encoder.layer.2.output.LayerNorm.gamma', 'bert.encoder.layer.2.output.LayerNorm.beta', 'bert.encoder.layer.3.attention.output.LayerNorm.gamma', 'bert.encoder.layer.3.attention.output.LayerNorm.beta', 'bert.encoder.layer.3.output.LayerNorm.gamma', 'bert.encoder.layer.3.output.LayerNorm.beta', 'bert.encoder.layer.4.attention.output.LayerNorm.gamma', 'bert.encoder.layer.4.attention.output.LayerNorm.beta', 'bert.encoder.layer.4.output.LayerNorm.gamma', 'bert.encoder.layer.4.output.LayerNorm.beta', 'bert.encoder.layer.5.attention.output.LayerNorm.gamma', 'bert.encoder.layer.5.attention.output.LayerNorm.beta', 'bert.encoder.layer.5.output.LayerNorm.gamma', 'bert.encoder.layer.5.output.LayerNorm.beta', 'bert.encoder.layer.6.attention.output.LayerNorm.gamma', 'bert.encoder.layer.6.attention.output.LayerNorm.beta', 'bert.encoder.layer.6.output.LayerNorm.gamma', 'bert.encoder.layer.6.output.LayerNorm.beta', 'bert.encoder.layer.7.attention.output.LayerNorm.gamma', 'bert.encoder.layer.7.attention.output.LayerNorm.beta', 'bert.encoder.layer.7.output.LayerNorm.gamma', 'bert.encoder.layer.7.output.LayerNorm.beta', 'bert.encoder.layer.8.attention.output.LayerNorm.gamma', 'bert.encoder.layer.8.attention.output.LayerNorm.beta', 'bert.encoder.layer.8.output.LayerNorm.gamma', 'bert.encoder.layer.8.output.LayerNorm.beta', 'bert.encoder.layer.9.attention.output.LayerNorm.gamma', 'bert.encoder.layer.9.attention.output.LayerNorm.beta', 'bert.encoder.layer.9.output.LayerNorm.gamma', 'bert.encoder.layer.9.output.LayerNorm.beta', 'bert.encoder.layer.10.attention.output.LayerNorm.gamma', 'bert.encoder.layer.10.attention.output.LayerNorm.beta', 'bert.encoder.layer.10.output.LayerNorm.gamma', 'bert.encoder.layer.10.output.LayerNorm.beta', 'bert.encoder.layer.11.attention.output.LayerNorm.gamma', 'bert.encoder.layer.11.attention.output.LayerNorm.beta', 'bert.encoder.layer.11.output.LayerNorm.gamma', 'bert.encoder.layer.11.output.LayerNorm.beta'].\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py:26: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 3 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "***** Running training *****\n",
      "  Num examples = 3668\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 87\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='87' max='87' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [87/87 00:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to /root/transformers_data_and_model/finetuned-mrpc\n",
      "Configuration saved in /root/transformers_data_and_model/finetuned-mrpc/config.json\n",
      "Model weights saved in /root/transformers_data_and_model/finetuned-mrpc/pytorch_model.bin\n",
      "tokenizer config file saved in /root/transformers_data_and_model/finetuned-mrpc/tokenizer_config.json\n",
      "Special tokens file saved in /root/transformers_data_and_model/finetuned-mrpc/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "if training_args.do_train:\n",
    "    # 此时传入model_path是为了加载optimiztor，进行继续训练，对于通常的fine-tune来说，model_path可以不传入\n",
    "    # 对于初始化后的Trainer，调用train方法就可以训练了，简化了训练的过程\n",
    "    trainer.train(\n",
    "        model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n",
    "    )\n",
    "    # Trainer保存模型调用此方法\n",
    "    trainer.save_model()\n",
    "    \n",
    "    # 为了方便使用起见，将tokenizer的模型参数也存入model同目录(原方法已经弃用)\n",
    "    if trainer.is_world_process_zero():\n",
    "        tokenizer.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d1c7932",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04252022 03:57:48 - INFO - __main__ - *** Evaluate ***\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py:26: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 3 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "04252022 03:57:50 - INFO - __main__ - **** Eval results mrpc *****\n",
      "04252022 03:57:50 - INFO - __main__ -   eval_loss = 0.5169751048088074\n",
      "04252022 03:57:50 - INFO - __main__ -   eval_acc = 0.7843137254901961\n",
      "04252022 03:57:50 - INFO - __main__ -   eval_f1 = 0.8594249201277956\n",
      "04252022 03:57:50 - INFO - __main__ -   eval_acc_and_f1 = 0.8218693228089958\n",
      "04252022 03:57:50 - INFO - __main__ -   eval_runtime = 1.5105\n",
      "04252022 03:57:50 - INFO - __main__ -   eval_samples_per_second = 270.117\n",
      "04252022 03:57:50 - INFO - __main__ -   eval_steps_per_second = 8.607\n",
      "04252022 03:57:50 - INFO - __main__ -   epoch = 3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "key: eval_loss  value: 0.5169751048088074\n",
      "key: eval_acc  value: 0.7843137254901961\n",
      "key: eval_f1  value: 0.8594249201277956\n",
      "key: eval_acc_and_f1  value: 0.8218693228089958\n",
      "key: eval_runtime  value: 1.5105\n",
      "key: eval_samples_per_second  value: 270.117\n",
      "key: eval_steps_per_second  value: 8.607\n",
      "key: epoch  value: 3.0\n"
     ]
    }
   ],
   "source": [
    "# 评估结果\n",
    "eval_results = {}\n",
    "if training_args.do_eval:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "    \n",
    "    # 传入metrics，对于前面初始化已经传入的，此时如果没变化，可以省略此步骤\n",
    "    trainer.compute_metrics = compute_metrics_fn\n",
    "    # 传入评估的dataset\n",
    "    eval_results = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "    output_eval_file = os.path.join(\n",
    "        training_args.output_dir, f\"eval_results_{eval_dataset.args.task_name}.txt\"\n",
    "    )\n",
    "    \n",
    "    if trainer.is_world_process_zero():\n",
    "        # 写入本地文件\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"**** Eval results {} *****\".format(eval_dataset.args.task_name))\n",
    "            for key, value in eval_results.items():\n",
    "                print(\"key:\", key, \" value:\", value)\n",
    "                logger.info(\"  %s = %s\", key, value)\n",
    "                writer.write(\"%s = %s\\n\" % (key, value))\n",
    "                \n",
    "        eval_results.update(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04c0a023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# 测试集的评估\n",
    "print(training_args.do_predict)\n",
    "if training_args.do_predict:\n",
    "    logging.info(\"*** Test ***\")\n",
    "\n",
    "    predictions = trainer.predict(test_dataset=test_dataset).predictions\n",
    "    if output_mode == \"classification\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    output_test_file = os.path.join(\n",
    "        training_args.output_dir, f\"test_results_{test_dataset.args.task_name}.txt\"\n",
    "    )\n",
    "    if trainer.is_world_master():\n",
    "        with open(output_test_file, \"w\") as writer:\n",
    "            logger.info(\"***** Test results {} *****\".format(test_dataset.args.task_name))\n",
    "            writer.write(\"index\\tprediction\\n\")\n",
    "            for index, item in enumerate(predictions):\n",
    "                item = test_dataset.get_labels()[item]\n",
    "                writer.write(\"%d\\t%s\\n\" % (index, item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a0bdd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5169751048088074, 'eval_acc': 0.7843137254901961, 'eval_f1': 0.8594249201277956, 'eval_acc_and_f1': 0.8218693228089958, 'eval_runtime': 1.6126, 'eval_samples_per_second': 253.008, 'eval_steps_per_second': 8.062, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d697f706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/transformers_data_and_model/finetuned-mrpc'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FINETUNED_MRPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5371da22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/root/transformers_data_and_model/finetuned-mrpc\u001b[00m\r\n",
      "├── config.json\r\n",
      "├── eval_results_mrpc.txt\r\n",
      "├── pytorch_model.bin\r\n",
      "├── special_tokens_map.json\r\n",
      "├── tokenizer.json\r\n",
      "├── tokenizer_config.json\r\n",
      "├── training_args.bin\r\n",
      "└── vocab.txt\r\n",
      "\r\n",
      "0 directories, 8 files\r\n"
     ]
    }
   ],
   "source": [
    "# 查看训练好的模型文件\n",
    "!tree {FINETUNED_MRPC}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa5ccd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
