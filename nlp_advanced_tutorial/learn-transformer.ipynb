{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ebacd7d",
   "metadata": {},
   "source": [
    "# ä¸€è¯¾Transformer\n",
    "æœ¬æ–‡ä»‹ç»hugging faceæä¾›çš„transformeråº“çš„ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b729630",
   "metadata": {},
   "source": [
    "## ç¯å¢ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2dd7052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 1.6.0\n",
      "transformer: 4.18.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(f'torch: {torch.__version__}')\n",
    "print(f'transformer: {transformers.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61b86877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰€æœ‰å¤§å†™å­—æ¯è·¯å¾„éœ€è¦æ”¹ä¸ºè‡ªå·±çš„å®é™…è·¯å¾„\n",
    "BASE_URL = '/root/transformers_data_and_model/'\n",
    "BERT_MODEL_NAME_OR_PATH = BASE_URL + 'bert-base-uncased'\n",
    "# å±•ç¤ºæ–‡ä»¶å¤¹ä¸­çš„å†…å®¹\n",
    "#!tree {BERT_MODEL_NAME_OR_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76fb29c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /root/transformers_data_and_model/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /root/transformers_data_and_model/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# åˆå§‹åŒ–modelå’Œtokenizer\n",
    "# æ‰€æœ‰modelå’Œtokenizerçš„åˆå§‹åŒ–éƒ½æ˜¯ç”¨from_pretrainedæ–¹æ³•ï¼Œä¿å­˜éƒ½ä½¿ç”¨save_pretrainedçš„æ–¹æ³•\n",
    "# å¯¹from_pretrained:ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯æ–‡ä»¶å¤¹çš„è·¯å¾„/æ–‡ä»¶çš„è·¯å¾„/æ¨¡å‹çš„short nameç­‰å‡ ç§æ–¹æ³•ï¼Œè¿™é‡Œæ¨èæ–‡ä»¶å¤¹çš„æ–¹æ³•\n",
    "# modelåˆå§‹åŒ–é»˜è®¤æ˜¯evalæ¨¡å¼ï¼Œè¿™é‡ŒåŠ è½½çš„æ˜¯BERTçš„tokenizerå’Œåˆ†ç±»æ¨¡å‹model\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(BERT_MODEL_NAME_OR_PATH)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(BERT_MODEL_NAME_OR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4816ee9",
   "metadata": {},
   "source": [
    "## tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ac74037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# tokenizerçš„ä½œç”¨ï¼šå¯¹äºç»™å®šçš„æ–‡æœ¬ï¼Œç»è¿‡tokenizerå¤„ç†æˆmodelå¯ä»¥æ¥æ”¶çš„æ ¼å¼\n",
    "# tokenizeræœ€é‡è¦çš„æ–¹æ³•æ˜¯__call__ï¼Œè¿™ä¸ªæ–¹æ³•å¯ä»¥å°†æ–‡æœ¬è¾“å‡ºä¸ºæ¨¡å‹è¦çš„æ ¼å¼\n",
    "# tokenzierè¿˜æœ‰å…¶ä»–æ–¹æ³•ï¼Œencode/decode,é¡¾åæ€ä¹‰ï¼Œå°±æ˜¯å°†æ–‡æœ¬è½¬æ¢æˆinput_idsåŠå°†input_idsè½¬æ¢æˆæ–‡æœ¬\n",
    "# encode/decodeä¸__call__å…¶å®æ— æœ¬è´¨åŒºåˆ«ï¼Œåªæ˜¯__call__ä¸ºäº†æä¾›ç»Ÿä¸€çš„å¤„ç†æ¥å£\n",
    "inputs = tokenizer(\"We are very happy to show you the ğŸ¤— Transformers library.\")\n",
    "# input_idsæ˜¯æ–‡æœ¬æ¯ä¸ªè¯çš„indexï¼›token_type_idæ˜¯è¡¨ç¤ºæ–‡æœ¬æ˜¯ç¬¬ä¸€å¥/ç¬¬äºŒå¥ï¼›attention_maskæ˜¯å¤„ç†maskç”¨çš„\n",
    "# è¿™é‡Œå¤„ç†çš„æ˜¯ä¸€ä¸ªæ ·æœ¬ä¸”åªæœ‰ä¸€å¥è¯çš„ä¾‹å­ï¼Œå¦‚æœå¤šå¥è¯ï¼Œè¾“å…¥ä¸ºä¸€ä¸ªæ–‡æœ¬listå³å¯ã€‚\n",
    "#sentences = [[\"Gonna be ok.\", \"Ready perfectly.\"]]\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90671261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 7592, 102, 2204, 2851, 102], 'token_type_ids': [0, 0, 0, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# ä¸€ä¸ªæ ·æœ¬ä¸”æœ‰ä¸¤å¥è¯çš„ä¾‹å­ï¼Œåˆ†åˆ«ä½œä¸ºç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªå‚æ•°è¾“å…¥\n",
    "# å¦‚æœæ˜¯å¤šä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½æ˜¯ä¸¤å¥è¯ï¼Œåˆ™ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯ç¬¬ä¸€å¥è¯çš„æ–‡æœ¬listï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯ç¬¬äºŒå¥è¯çš„æ–‡æœ¬list\n",
    "#inputs2 = tokenizer([\"Gonna be ok.\", \"Ready perfectly.\", \"hello\"], [\"Gonna be ok.\", \"Ready perfectly.\", 'good morning'])\n",
    "inputs2 = tokenizer('hello', 'good morning')\n",
    "print(inputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dd95295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [[101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], [101, 2057, 3246, 2017, 2123, 1005, 1056, 5223, 2009, 1012, 102, 0, 0, 0]]\n",
      "token_type_ids: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# æ›´å¤šæ—¶å€™ï¼Œæˆ‘ä»¬éœ€è¦çš„modelè¾“å…¥æ˜¯æˆbatchæ ¼å¼çš„\n",
    "# ç¬¬ä¸€ä¸ªè¾“å…¥æ˜¯æ–‡æœ¬listï¼Œpaddingè®¾ç½®æˆTrueï¼Œtruncationè®¾ç½®æˆTrue å¯ä»¥è¿›è¡Œpaddingå’Œtruncation\n",
    "# return_tensorså†™æ˜äº†è¿”å›æ ¼å¼ï¼Œæ˜¯ä¸€ä¸ªpytorchçš„tensor\n",
    "pt_batch = tokenizer(\n",
    "    [\"We are very happy to show you the ğŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "for key, value in pt_batch.items():\n",
    "    print(f'{key}: {value.numpy().tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f02673e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:tensor([[0.0497, 0.4243],\n",
      "        [0.0902, 0.4860]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "loss: 0.7168716788291931\n",
      "logits: tensor([[0.0497, 0.4243],\n",
      "        [0.0902, 0.4860]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# å¯¹äºtokenzierå¤„ç†åçš„æ–‡æœ¬ï¼Œç›®æ ‡æ˜¯é€å…¥modelï¼Œç”¨äºåˆ†ç±»ã€é¢„æµ‹ç­‰ä»»åŠ¡\n",
    "# ä¸Šé¢çš„pt_batchå°±æ˜¯ä¸€ä¸ªbatchï¼Œå¢åŠ **ç›´æ¥è¾“å…¥æ¨¡å‹\n",
    "# æ ¹æ®transformersåº“çš„è§„åˆ™ï¼Œæ‰€æœ‰modelçš„è¾“å‡ºéƒ½æ˜¯å…ƒç»„\n",
    "# å¦‚æœåªæœ‰æ¯ä¸ªbatchçš„è¾“å…¥ï¼Œå…ƒç»„è¾“å‡ºçš„ç¬¬ä¸€ä¸ªæ˜¯logits\n",
    "# å¦‚æœåŒæ—¶ä¼ å…¥äº†labelsçš„å‚æ•°ï¼Œåˆ™å…ƒç»„è¾“å‡ºçš„ç¬¬ä¸€ä¸ªæ˜¯lossï¼Œç¬¬äºŒä¸ªæ˜¯logits\n",
    "# ä¸€èˆ¬å›å½’ä»»åŠ¡lossç”¨çš„Mean-Square lossï¼Œåˆ†ç±»ä»»åŠ¡åˆ™æ˜¯Cross-Entry\n",
    "pt_outputs = model(**pt_batch)\n",
    "print(f'logits:{pt_outputs[0]}\\n')\n",
    "# ä¼ å…¥labelså‚æ•°çš„æƒ…å†µ\n",
    "pt_outputs = model(**pt_batch, labels=torch.LongTensor([1, 0]))\n",
    "print(f'loss: {pt_outputs[0]}\\nlogits: {pt_outputs[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e846a9",
   "metadata": {},
   "source": [
    "ä¸Šé¢å°±æ˜¯pytorchç‰ˆtransformersçš„åŸºæœ¬è¾“å…¥é€»è¾‘ï¼š\n",
    "transformersä¸­çš„æ‰€æœ‰modeléƒ½æ˜¯pytorchæ ‡å‡†æ¨¡å‹ç±»torch.nn.Module.æ–‡æœ¬å¯ä»¥é€šè¿‡tokenizerè°ƒç”¨è½¬æ¢ä¸ºæ¨¡å‹çš„è¾“å…¥ï¼Œæ¨¡å‹è¾“å…¥è¿™äº›ä¿¡æ¯ï¼Œå¾—åˆ°logitsï¼Œè®¡ç®—lossï¼Œè¿›è¡Œè¯¯å·®å›ä¼ backwardï¼Œè¿›è¡Œè¿­ä»£ï¼Œå°±å®Œæˆäº†è®­ç»ƒ/fine-tune.æ¨¡å‹è¾“å…¥çš„æ—¶å€™ï¼Œå¦‚æœä¼ å…¥labelsçš„å‚æ•°ï¼Œä¹Ÿå¯ä»¥ç›´æ¥å¾—åˆ°ç›¸åº”çš„lossï¼Œä¸€å¸®backwardï¼Œå¤šæ¬¡è¿­ä»£ï¼Œå®Œæˆè®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05887168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Œæˆè®­ç»ƒ/å¾®è°ƒåï¼Œå¯ä»¥å°†tokenizerå’Œmodelä¿å­˜åˆ°ç›¸åŒæ–‡ä»¶å¤¹\n",
    "# åœ¨transformersæ¡†æ¶é‡Œï¼Œä¸€ä¸ªå¾ˆå¥½çš„ä¹ æƒ¯ï¼Œå°†modelã€tokenizerå‚æ•°ã€è®­ç»ƒå‚æ•°ç­‰æ‰€æœ‰ä¿å­˜åˆ°ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸‹ã€‚\n",
    "# è¿™é‡Œçš„model/tokenzier/configçš„åˆå§‹åŒ–ä½¿ç”¨from_pretrained,ä¿å­˜ä½¿ç”¨save_pretrained\n",
    "# save_pretrainedä¼ å…¥å…·ä½“æ–‡ä»¶å¤¹åå³å¯\n",
    "SAVE_DIRECTORY = BASE_URL + 'bert_save_example'\n",
    "tokenizer.save_pretrained(SAVE_DIRECTORY)\n",
    "model.save_pretrained(SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3968ee89",
   "metadata": {},
   "source": [
    "### æ€»ç»“\n",
    "1.åœ¨transformersæ¡†æ¶é‡Œï¼Œæä¾›äº†model/tokenizer/configé€šç”¨åŒ–çš„åŠ è½½å’Œä¿å­˜ï¼Œä¹Ÿå°±æ˜¯from_pretraiend/save_pretrainedï¼›\n",
    "2.tokenizerçš„ä½œç”¨åœ¨äºï¼Œé€šè¿‡__call__å°†æ–‡æœ¬è¿›è¡Œè½¬æ¢æˆæ¨¡å‹æ¥å—çš„æ ¼å¼ï¼Œmodelçš„è¾“å‡ºéƒ½æ˜¯å…ƒç»„ï¼Œä¾æ®è¿™äº›å…ƒç»„çš„å†…å®¹è¿›è¡Œè®¡ç®—lossï¼›\n",
    "3.tokenizeråŒ…è£…äº†Byte-Pair Encodingã€WordPieceã€SentencePieceç­‰ä¸åŒçš„æ–¹å¼ï¼›\n",
    "4.modelæ˜¯åŒ…è£…äº†BERTã€GPT2ã€ALBERTç­‰ä¸åŒçš„æ¨¡å‹ï¼Œå¹¶ä¸”æä¾›æ ‡å‡†åŒ–çš„ç±»ã€‚ç­‰ä¸‹ç»†è¯´ã€‚\n",
    "5.å¯¹äºæˆ‘ä»¬æ¥è®²ï¼Œå»å¤ç°ã€å®éªŒã€ç ”ç©¶æ›´å®¹æ˜“ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac92be9b",
   "metadata": {},
   "source": [
    "## æ¦‚å¿µä¸è¯´æ˜\n",
    "### ä¸»è¦çš„ç±»\n",
    "ä¸»è¦çš„ç±»æœ‰Modelã€Configurationã€Tokenizerè¿™ä¸‰ä¸ªç±»ï¼Œä¸‹é¢åˆ†åˆ«ä»‹ç»ã€‚\n",
    "Modelç±»ï¼Œæ¯”å¦‚BertModelï¼Œå‡ä»pytorch models(torch.nn.Module)æˆ–è€…keras models(tf.keras.Model)ç»§æ‰¿è€Œæ¥ï¼Œç”¨äºå¤„ç†é¢„è®­ç»ƒæƒé‡ã€‚\n",
    "\n",
    "Configurationç±»ï¼Œæ¯”å¦‚BertConfigï¼Œé‡Œé¢ä¿å­˜ç€å»ºç«‹æ¨¡å‹æ‰€éœ€è¦çš„æ‰€æœ‰å‚æ•°ã€‚å¹¶ä¸æ˜¯æ€»æ˜¯æˆ‘ä»¬æ‰‹åŠ¨å»åˆå§‹åŒ–è¿™ä¸ªç±»ï¼Œå°¤å…¶æ˜¯å½“ä½ ä½¿ç”¨æ²¡æœ‰åšä»»ä½•æ›´æ”¹çš„é¢„è®­ç»ƒæ—¶ï¼Œmodelä¼šè‡ªåŠ¨å¤„ç†å¥½è¿™ä¸ªç±»ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœè‡ªå·±é‡æ–°é¢„è®­ç»ƒçš„æ¨¡å‹ä¸”æ¶æ„ä¸ä¸€è‡´æ—¶ï¼Œæ˜¯å…è®¸æˆ‘ä»¬å»åˆå§‹åŒ–è¿™ä¸ªç±»çš„ã€‚\n",
    "\n",
    "Tokenizerç±»ï¼Œæ¯”å¦‚BERTTokenizerï¼Œä¸ºæ¯ä¸ªæ¨¡å‹ä¿å­˜è¯å…¸ï¼Œå¹¶å°†æ–‡æœ¬è¿›è¡Œç¼–ç /è§£ç æˆæ¨¡å‹éœ€è¦çš„æ ¼å¼â€”â€”tokenåµŒå…¥çš„ç´¢å¼•ã€‚\n",
    "\n",
    "ä¸Šé¢çš„ç±»ï¼Œéƒ½æœ‰ä¸‹é¢ä¸¤ä¸ªæ–¹æ³•å»å®ä¾‹åŒ–ç±»å’Œä¿å­˜è‡³æœ¬åœ°ï¼š\n",
    "\n",
    "from_pretrained() å…è®¸æˆ‘ä»¬åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨short_nameï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨æœ¬åœ°çš„æ¨¡å‹ï¼Œä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°model_name_or_pathä¼ å…¥ï¼Œå¯ä»¥æ˜¯æ–‡ä»¶å¤¹ã€æ–‡ä»¶ã€short_nameç­‰ã€‚å…¶ä¸­æ–‡ä»¶å¤¹çš„è¯ï¼Œä¼šé»˜è®¤å¯»æ‰¾æ–‡ä»¶å¤¹ä¸­çš„pytorh_model.binã€‚\n",
    "\n",
    "save_pretrained() å…è®¸æˆ‘ä»¬å°†æ¨¡å‹ä¿å­˜è‡³æœ¬åœ°ï¼Œä¿å­˜çš„å‚æ•°æ˜¯å¯ä»¥æ˜¯æ–‡ä»¶å¤¹ã€‚\n",
    "\n",
    "### AutoModels\n",
    "ä»¥BERTä¸ºä¾‹ï¼Œæ¯ä¸ªæ¨¡å‹éƒ½æœ‰ä¸€ä¸ªConfigï¼ˆBertConfigï¼‰ï¼›æœ‰1-2ä¸ªtokenizerï¼Œåˆ†åˆ«æ˜¯åŸºäºrustçš„å¿«é€Ÿtokenizerï¼ˆBertTokenizerFastï¼‰ï¼Œä¸€ä¸ªæ˜¯åŸºäºpythonåŸç‰ˆçš„tokenizerï¼ˆBertTokenizerï¼‰ï¼Œéƒ¨åˆ†æ²¡æœ‰æä¾›rustçš„å¿«é€Ÿtokenizerï¼›æœ‰å¤šä¸ªçš†æœ‰ä¸åŒheadçš„Modelï¼Œæ¯”å¦‚æœ€åŸå§‹çš„æ¨¡å‹ï¼Œä¸å«headçš„BertModelã€é¢„è®­ç»ƒMLMå’ŒNSPçš„BertForPreTrainingã€MLM headçš„BertForMaskedLMï¼ŒNSPçš„BertForNextSentencePredictionï¼Œç”¨äºå¥å­åˆ†ç±»çš„BertForSequenceClassificationï¼Œç”¨äºå¤šé€‰çš„BertForMultipleChoiceï¼Œç”¨å•è¯åˆ†ç±»çš„BertForTokenClassificationï¼Œç”¨äºé—®ç­”çš„BertForQuestionAnsweringã€‚\n",
    "\n",
    "ä¸åŒçš„æ¨¡å‹ï¼Œä¼šç¨æœ‰ä¸åŒã€‚ä½†æ˜¯configç±»éƒ½ç»§æ‰¿è‡ªPretrainedConfigï¼›tokenizeréƒ½ç»§æ‰¿è‡ªPreTrainedTokenizeræˆ–PreTrainedTokenizerFastï¼›modeléƒ½ç»§æ‰¿è‡ªPreTrainedModelã€‚\n",
    "\n",
    "ä¸ºäº†ä½¿ç”¨æ–¹ä¾¿ï¼ŒAutoConfigã€AutoTokenizerã€AutoModelã€AutoModelForPreTrainingã€AutoModelWithLMHeadã€AutoModelForSequenceClassificationã€AutoModelForQuestionAnsweringã€AutoModelForTokenClassificationç­‰å¯ä»¥ç”¨äºè‡ªåŠ¨æŸ¥æ‰¾æ¨¡å‹ã€‚\n",
    "\n",
    "### Trainerç±»\n",
    "Trainerç±»æä¾›äº†ä¸€ä¸ªå®Œæ•´çš„æ ‡å‡†è®­ç»ƒçš„APIï¼Œç›®å‰æ”¯æŒè¯­è¨€æ¨¡å‹ã€æ–‡æœ¬åˆ†ç±»ã€å•è¯åˆ†ç±»ï¼ˆNERï¼‰ç­‰ä»»åŠ¡ã€‚å¯¹äºå‰é¢çš„configã€tokenizerã€modelï¼Œæˆ‘ä»¬å¯ä»¥è®¤ä¸ºï¼Œå¸®åŠ©æˆ‘ä»¬ç®€åŒ–çš„æ˜¯å†™æ¨¡å‹çš„è¿™ä¸€æ­¥ï¼Œæ­£å¸¸ç”Ÿæˆdatasetã€dataloaderï¼Œç„¶åå†æ¯ä¸ªepochã€batchè¿›è¡Œè®­ç»ƒï¼Œå¾—åˆ°æœ€ç»ˆçš„ç»“æœã€‚æ­£å¸¸å†™çš„è¯ï¼ŒTrainerç±»å¯ä»¥ä¸ç”¨ï¼ŒTrainerå…¶å®æ˜¯ç®€åŒ–çš„æ˜¯æˆ‘ä»¬è®­ç»ƒçš„è¿™ä¸€æ­¥ã€‚\n",
    "\n",
    "å¯¹äºé€šå¸¸çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå†™æ³•å¤§è‡´æ˜¯è¿™æ ·çš„ï¼ˆä¸‹é¢æ˜¯ä¼ªä»£ç ï¼‰ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a6e68d88",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-35995ac0a14b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# é€šå¸¸è®­ç»ƒè¿‡ç¨‹ä»£ç \u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# åŠ è½½æ•°æ®\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# è½¬æ¢æˆfeaturesï¼Œè·å¾—dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_data' is not defined"
     ]
    }
   ],
   "source": [
    "## ä¼ªä»£ç  ä¸è¦æ‰§è¡Œï¼\n",
    "\n",
    "# é€šå¸¸è®­ç»ƒè¿‡ç¨‹ä»£ç \n",
    "# åŠ è½½æ•°æ®\n",
    "train_data, test_dat = get_data()\n",
    "# è½¬æ¢æˆfeaturesï¼Œè·å¾—dataset\n",
    "train_dataset = MyDataset(train_data, args)\n",
    "test_dataset = MyDataset(test_data, args)\n",
    "\n",
    "# è½¬æ¢æˆdataloaderï¼Œç”¨äºç”Ÿæˆbatch\n",
    "# sampler å®šä¹‰å–batchçš„æ–¹æ³•ï¼Œæ˜¯ä¸€ä¸ªè¿­ä»£å™¨ï¼Œ æ¯æ¬¡ç”Ÿæˆä¸€ä¸ªkey ç”¨äºè¯»å–datasetä¸­çš„å€¼\n",
    "train_sampler, test_sampler = ...\n",
    "# collate_fnå‡½æ•°ä¼šå°†batch_sizeä¸ªæ ·æœ¬æ•´ç†æˆä¸€ä¸ªbatchæ ·æœ¬ï¼Œä¾¿äºæ‰¹é‡è®­ç»ƒã€‚\n",
    "train_dataloader = Dataloader(train_dataset, sampler=train_sampler, batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_dataloader = Dataloader(test_dataset, sampler=test_sampler, batch_size=batch_size, collate_fn=collate_fn)\n",
    "# åˆå§‹åŒ–tensorboardï¼Œ tensorboardæ˜¯ä¸€ä¸ªå±•ç¤ºè®­ç»ƒè¿‡ç¨‹å„å¼ é‡å˜åŒ–çš„å·¥å…·\n",
    "tb_writer = SummaryWriter(log_dir=None)\n",
    "# åŠ è½½optimizer\n",
    "optimizer = ...\n",
    "model.to(GPU)\n",
    "# å¼€å§‹è®­ç»ƒ\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_dataloader:\n",
    "        # è½¬æ¢ä¸ºtrain\n",
    "        model.train()\n",
    "        # ä¼ å…¥GPU\n",
    "        batch.to(GPU)\n",
    "        # è®¡ç®—æ¯ä¸€æ­¥çš„lossï¼Œç„¶åå›ä¼ \n",
    "        tr_loss = train_step(model, inputs, optimizer)\n",
    "        tr_loss.backward()\n",
    "        model.zero_grad()\n",
    "for batch in test_dataloader:\n",
    "    ...\n",
    "model.save_pretrained(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebe35ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¼ªä»£ç  ä¸è¦æ‰§è¡Œï¼ï¼\n",
    "# ä¸‹é¢æ˜¯ä½¿ç”¨äº†Trainerçš„è®­ç»ƒè¿‡ç¨‹\n",
    "\n",
    "# åŠ è½½æ•°æ®\n",
    "train_data, test_data = get_data()\n",
    "# è½¬æ¢æˆfeaturesï¼Œ è·å¾—dataset\n",
    "train_dataset = MyDataset(train_data, args)\n",
    "test_dataset = MyDataset(test_data, args)\n",
    "# è¯»å…¥train_args\n",
    "train_args = **\n",
    "\n",
    "# åˆå§‹åŒ–æœ¬Trainer\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=build_compute_metrics_fn(data_args.task_name),\n",
    ")\n",
    "# è®­ç»ƒ\n",
    "if training_args.do_train:\n",
    "    trainer.train(\n",
    "        model_path=model_args.model_name_or_path if os.pash.isdir(model_args.model_name_or_path) else None\n",
    "    )\n",
    "    # trainerä¿å­˜æ¨¡å‹ï¼Œé‡Œé¢è°ƒç”¨çš„è¿˜æ˜¯save_pretrainedæ–¹æ³•\n",
    "    trainer.save_model()\n",
    "    # ä¿å­˜tokenizeråˆ°åŒä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œæ–¹ä¾¿ä½¿ç”¨\n",
    "    if trainer.is_world_master():\n",
    "        tokenizer.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddddc16c",
   "metadata": {},
   "source": [
    "## GLUE/MRPCæ•°æ®é›†è¿›è¡Œæ–‡æœ¬åˆ†ç±»çš„ç¤ºä¾‹\n",
    "### 4.1æœ¬èŠ‚è¯´æ˜\n",
    "ä»¥ç®€å•çš„ä¾‹å­ï¼Œè¯´æ˜æ–‡æœ¬åˆ†ç±»çš„fine-tuneè¯¦ç»†è¿‡ç¨‹ï¼Œçº¿ä¸Šéƒ¨ç½²ä»£ç \n",
    "4.2èŠ‚è¯¦ç»†å®ç°äº†æ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„è¯¦ç»†ä»£ç ï¼Œ4.3èŠ‚å®ç°äº†åŸºäº4.2èŠ‚è®­ç»ƒå¥½çš„æ–‡æœ¬åˆ†ç±»æ¨¡å‹çš„çº¿ä¸Šé¢„æµ‹ä»£ç ï¼Œ4.4èŠ‚æ€»ç»“äº†æ–‡æœ¬åˆ†ç±»çš„åŸºæœ¬ç»éªŒå’Œtransformersä½¿ç”¨çš„åŸºæœ¬ç»éªŒã€‚\n",
    "æœ¬èŠ‚æ¶‰åŠåˆ°çš„æ¨¡å‹æ˜¯bert-base-uncasedï¼Œæ¶‰åŠåˆ°çš„æ•°æ®é›†æ˜¯glueæ•°æ®é›†ä¸‹çš„MRPCæ•°æ®é›†ã€‚\n",
    "\n",
    "glueæ•°æ®é›†å…±æœ‰9ä¸ªä»»åŠ¡ï¼Œå…¶ä¸­STS-Bæ˜¯ä¸€ä¸ªå›å½’ä»»åŠ¡ï¼ŒMNLIæ˜¯ä¸‰åˆ†ç±»ä»»åŠ¡ï¼Œå‰©ä½™7ç±»å‡æ˜¯äºŒåˆ†ç±»ä»»åŠ¡ã€‚ä¹ä¸ªä»»åŠ¡ä¹‹ä¸€çš„MRPCï¼ˆThe Microsoft Research Paraphrase Corpusï¼Œå¾®è½¯ç ”ç©¶é™¢é‡Šä¹‰è¯­æ–™åº“ï¼‰ï¼Œç›¸ä¼¼æ€§å’Œé‡Šä¹‰ä»»åŠ¡ï¼Œæ˜¯ä»åœ¨çº¿æ–°é—»æºä¸­è‡ªåŠ¨æŠ½å–å¥å­å¯¹è¯­æ–™åº“ï¼Œå¹¶äººå·¥æ³¨é‡Šå¥å­å¯¹ä¸­çš„å¥å­æ˜¯å¦åœ¨è¯­ä¹‰ä¸Šç­‰æ•ˆã€‚ç±»åˆ«å¹¶ä¸å¹³è¡¡ï¼Œå…¶ä¸­68%çš„æ­£æ ·æœ¬ï¼Œæ‰€ä»¥éµå¾ªå¸¸è§„çš„åšæ³•ï¼ŒæŠ¥å‘Šå‡†ç¡®ç‡ï¼ˆaccuracyï¼‰å’ŒF1å€¼ã€‚æ ·æœ¬ä¸ªæ•°ï¼šè®­ç»ƒé›†3, 668ä¸ªï¼Œå¼€å‘é›†408ä¸ªï¼Œæµ‹è¯•é›†1, 725ä¸ªã€‚ä»»åŠ¡ï¼šæ˜¯å¦é‡Šä¹‰äºŒåˆ†ç±»ï¼Œæ˜¯é‡Šä¹‰ï¼Œä¸æ˜¯é‡Šä¹‰ä¸¤ç±»ã€‚è¯„ä»·å‡†åˆ™ï¼šå‡†ç¡®ç‡ï¼ˆaccuracyï¼‰å’ŒF1å€¼ã€‚æ ‡ç­¾ä¸º1ï¼ˆæ­£æ ·æœ¬ï¼Œäº’ä¸ºé‡Šä¹‰ï¼‰çš„æ ·ä¾‹ï¼ˆæ¯ä¸ªæ ·ä¾‹æ˜¯ä¸¤å¥è¯ï¼Œä¸­é—´ç”¨tabéš”å¼€ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f2dac",
   "metadata": {},
   "source": [
    "### 4.2æ¨¡å‹è®­ç»ƒçš„è¯¦ç»†è¿‡ç¨‹\n",
    "transformersæä¾›äº†configã€tokenizerã€modelç­‰ç±»ç®€åŒ–äº†åˆ†è¯ã€æ¨¡å‹ç­‰æ­¥éª¤ï¼ŒåŒæ—¶åˆæœ‰Trainerç±»ç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ã€‚é‚£ä¹ˆæ›´è¯¦ç»†çš„è®­ç»ƒè¿‡ç¨‹æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿæœ¬èŠ‚ä¸»è¦çš„å†…å®¹å°±æ˜¯å®ç°å’Œè®²è§£æ¨¡å‹åˆ†ç±»çš„è¯¦ç»†è¿‡ç¨‹ã€‚\n",
    "\n",
    "ç®€å•çš„è®²ï¼Œä¸»è¦åˆ†ä¸ºå‡ ä¸ªæ­¥éª¤ï¼š\n",
    "1.åŠ è½½å‚æ•°ï¼Œæ–‡æœ¬å¤„ç†æˆDatasetï¼›\n",
    "2.å†™collate_fnï¼Œç”¨äºå¤„ç†paddingï¼›\n",
    "3.åŠ è½½configã€tokenizerã€modelç­‰ï¼›\n",
    "4.å†™metricsï¼Œç”¨äºè¯„ä¼°æ•ˆæœï¼›\n",
    "5.å°†ä»¥ä¸Šå‚æ•°é€å…¥Traineråˆå§‹åŒ–ç±»ï¼Œç„¶åè°ƒç”¨Trainerçš„trainæ–¹æ³•è®­ç»ƒã€‚\n",
    "\n",
    "åŠ è½½å‚æ•°çš„å‚æ•°åˆ†ä¸ºä¸‰ç±»ï¼šä¸€ç±»æ˜¯modelç›¸å…³å‚æ•°ï¼Œç”¨äºè®°å½•æ¨¡å‹çš„ä½ç½®ç­‰ä¿¡æ¯ï¼Œç”¨äºåˆå§‹åŒ–æ¨¡å‹ï¼›ä¸€ç±»æ˜¯æ•°æ®å‚æ•°ï¼Œæ•°æ®çš„ä½ç½®ï¼Œä»»åŠ¡åç§°ç­‰ç”¨äºæä¾›æ¨¡å‹è¾“å…¥å‰çš„å‚æ•°ï¼›ä¸€ç±»æ˜¯è®­ç»ƒå‚æ•°ï¼Œè¿™äº›æ˜¯æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å‚æ•°ï¼Œæ¯”å¦‚learning_rateã€epochsç­‰ã€‚\n",
    "\n",
    "æ–‡æœ¬æœ€å¼€å§‹éœ€è¦è½½å…¥ï¼Œå¯ä»¥é€šè¿‡å†™æ˜ä¸€ä¸ªProcessorç±»ï¼Œè¿™ä¸ªç±»ç”¨äºæä¾›å‡ ä¸ªæ–¹æ³•ï¼šè·å¾—è®­ç»ƒæ ·æœ¬ã€è·å¾—å¼€å‘æ ·æœ¬ã€è·å¾—æ ‡ç­¾listç­‰ã€‚è¿™é‡Œçš„è·å¾—æ ·æœ¬æ˜¯ä¸€ä¸ªlistï¼Œæ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªexampleï¼Œexampleé‡ŒåŒ…å«æ–‡æœ¬å’Œå¯¹åº”çš„æ ‡ç­¾ï¼ˆæœ‰çš„ä¸å«ï¼Œæ¯”å¦‚æµ‹è¯•é›†ï¼‰ã€‚è¿™ä¸ªProcessoræä¾›çš„æ–¹æ³•ä¸»è¦æ˜¯ä¸ºäº†Datasetç±»ä½¿ç”¨ï¼ŒDatasetå®ç°å•ä¸ªè¾“å…¥çš„æ ·æœ¬ã€‚\n",
    "\n",
    "collate_fnæ˜¯Dataloaderç±»çš„è¾“å…¥ï¼Œå¯¹äºå¤„ç†å¥½çš„æ’å®šé•¿åº¦çš„featureï¼Œå¯ä»¥ä¸è¾“å…¥collate_fnï¼Œä½¿ç”¨é»˜è®¤çš„collate_fnï¼Œå¯¹äºé•¿åº¦ä¸å®šæ¶‰åŠåˆ°paddingçš„æ–‡æœ¬ï¼Œéœ€è¦è‡ªå·±å†™æ­¤å‚æ•°ã€‚\n",
    "\n",
    "configã€tokenizerã€modelçš„åŠ è½½æˆ‘ä»¬å·²åŸºæœ¬ç†Ÿæ‚‰ã€‚\n",
    "\n",
    "metricså†™æ³•å¯ä»¥å‚è€ƒä¸‹é¢æ–¹æ³•ã€‚\n",
    "\n",
    "ç„¶åå°†è¿™äº›å‚æ•°é€å…¥Trainerï¼Œå°±å¯ä»¥è®­ç»ƒå’Œè¯„ä¼°äº†ã€‚\n",
    "\n",
    "è¯·çœ‹ä¸‹é¢ä»£ç ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "919685bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import enum\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, Dict, Optional, List, Union, NamedTuple\n",
    "\n",
    "import filelock\n",
    "import torch\n",
    "import numpy as np\n",
    "import transformers\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "214db17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸‹è½½å¥½çš„é¢„è®­ç»ƒæ¨¡å‹ä½ç½®\n",
    "BASE_URL = '/root/transformers_data_and_model/'\n",
    "BERT_MODEL_NAME_OR_PATH = BASE_URL + 'bert-base-uncased'\n",
    "\n",
    "# ä¸‹è½½å¥½çš„glueæ•°æ®é›†ä¸­çš„MRPCæ•°æ®é›†çš„ä½ç½®\n",
    "MRPC_DATA_DIR = BASE_URL + 'MRPC'\n",
    "# finetuneå¥½çš„modelã€tokenizerç­‰å„ç§å‚æ•°å­˜æ”¾çš„ä½ç½®\n",
    "FINETUNED_MRPC = BASE_URL + 'finetuned-mrpc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbdd402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ—¥å¿—æ–‡ä»¶\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41090d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰æ¨¡å‹å‚æ•°ï¼ŒåŒ…å«modelã€configã€tokenzierã€cache_dirç­‰\n",
    "# æœ‰ä¸‰ç±»å‚æ•°ï¼š\n",
    "# ä¸€ä¸ªæ˜¯æ¨¡å‹å‚æ•°ï¼Œè¿™ä¸ªå°±æ˜¯ä¸‹é¢çš„å®šä¹‰ï¼›\n",
    "# ä¸€ä¸ªæ˜¯æ¨¡å‹è®­ç»ƒå‚æ•°ï¼Œå¯ä»¥å‚è€ƒtransformers/src/train_args.py æ–‡ä»¶ï¼Œä¸»è¦æ˜¯epochã€batch_sizeç­‰å¸¸è§çš„è®­ç»ƒå‚æ•°ï¼Œä¹ŸåŒ…å«deviceè¿™ç§è®¾å¤‡å‚æ•°\n",
    "# ä¸€ä¸ªæ•°æ®å‚æ•°ï¼Œå†³å®šæ•°æ®å¤„ç†ä»»åŠ¡çš„å‚æ•°ï¼Œä½¿ç”¨ä»€ä¹ˆæ•°æ®ï¼Œæ•°æ®åç§°ï¼Œæ˜¯å¦è¦†ç›–æ•°æ®çš„cacheï¼Œæœ€é•¿é•¿åº¦ï¼Œè¿™ä¸ªé•¿åº¦æ˜¯ç”Ÿæˆfeaturesä½¿ç”¨çš„\n",
    "# ä¸‹é¢è¿™ä¸ªå°±æ˜¯æ¨¡å‹å‚æ•°ï¼š\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    '''\n",
    "    Arguments pertaining to which model/config/tokenzier we are going to fine-tune from\n",
    "    '''\n",
    "    \n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "        \n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "        \n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    \n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "\n",
    "        \n",
    "# æ•°æ®ï¼ˆè®­ç»ƒä½¿ç”¨çš„ï¼‰å‚æ•°\n",
    "# è¿™äº›å‚æ•°ç”¨äºé€å…¥Datasetã€processorç­‰ï¼Œç”¨äºæ–¹ä¾¿åŠ è½½æ•°æ®ï¼Œå®Œæˆä»å¼€å§‹åˆ°æ¨¡å‹è¾“å…¥å‰çš„å‚æ•°\n",
    "@dataclass\n",
    "class GlueDataTrainingArguments:\n",
    "    '''\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \n",
    "    Using 'HfArgumentParser' we can turn this classs\n",
    "    into argparse arguments to be able to specify them on\n",
    "    the command line.\n",
    "    '''\n",
    "    \n",
    "    task_name: str = field(metadata={\"help\": \"The name of the task to train on: MRPC\"})\n",
    "    data_dir: str = field(\n",
    "        metadata={\"help\": \"The input data dir. Should contain the .tsv files (or other data files) for the tast.\"}\n",
    "    )\n",
    "    max_seq_length: int = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequence longer \"\n",
    "            \"than this will be truncated, sequence shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        self.task_name = self.task_name.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73daae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_args = ['--model_name_or_path', BERT_MODEL_NAME_OR_PATH,\n",
    "             '--task_name', 'MRPC',\n",
    "              '--do_train',\n",
    "              '--do_eval',\n",
    "              '--data_dir', MRPC_DATA_DIR,\n",
    "              '--max_seq_length', '128',\n",
    "              '--per_device_train_batch_size', '32',\n",
    "              '--learning_rate', '3e-5',\n",
    "              '--num_train_epochs', '3.0',\n",
    "              '--output_dir', FINETUNED_MRPC,\n",
    "              '--overwrite_cache',\n",
    "              '--overwrite_output_dir']\n",
    "\n",
    "\n",
    "# ä»¥åæ–°çš„å®éªŒï¼Œéœ€è¦å®ç°ä¸Šé¢çš„æ¨¡å‹å‚æ•°å’Œæ•°æ®å‚æ•°ï¼ŒåŸºæœ¬ä¸ŠæŒ‰ç…§ä¸Šé¢æ ¼å¼å»å†™ï¼Œä½¿ç”¨ä¸‹é¢çš„æ–¹æ³•è½¬æ¢æˆå‚æ•°ç©ºé—´å³å¯\n",
    "# transformersé‡Œï¼Œæœ‰ä¸€ä¸ªHfArgumentParserç”¨äºè§£æä¸Šé¢æ ¼å¼çš„å‚æ•°ï¼Œä¸ºæ ‡å‡†çš„pythonå‚æ•°\n",
    "parser = transformers.HfArgumentParser((ModelArguments, transformers.GlueDataTrainingArguments, transformers.TrainingArguments))\n",
    "# å°†ä¸‰ç±»å‚æ•°åˆ†åˆ«è§£æä¸ºå¯¹åº”ç©ºé—´\n",
    "# æ¨¡å‹æœ¬èº«çš„å‚æ•°ï¼Œæ•°æ®çš„å‚æ•°ï¼Œè®­ç»ƒçš„å‚æ•°\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(input_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df5a5228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArguments(model_name_or_path='/root/transformers_data_and_model/bert-base-uncased', config_name=None, tokenizer_name=None, cache_dir=None)\n",
      "\n",
      "GlueDataTrainingArguments(task_name='mrpc', data_dir='/root/transformers_data_and_model/MRPC', max_seq_length=128, overwrite_cache=True)\n",
      "\n",
      "TrainingArguments(\n",
      "_n_gpu=4,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/root/transformers_data_and_model/finetuned-mrpc/runs/Apr25_03-52-24_e41dfb6b4fe0,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=/root/transformers_data_and_model/finetuned-mrpc,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/root/transformers_data_and_model/finetuned-mrpc,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# å±•ç¤ºä¸€ä¸‹æ‰€æœ‰å‚æ•°\n",
    "print(f'{model_args}\\n\\n{data_args}\\n\\n{training_args}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa4bba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¡®ä¿output_dirå¯ç”¨\n",
    "if (\n",
    "    os.path.exists(training_args.output_dir)\n",
    "    and os.listdir(training_args.output_dir)\n",
    "    and training_args.do_train\n",
    "    and not training_args.overwrite_output_dir\n",
    "):\n",
    "    raise ValueError(\n",
    "        f'Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67c459b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04252022 03:52:29 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 4, distributed training: False, 16-bits training:False\n",
      "04252022 03:52:29 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=4,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/root/transformers_data_and_model/finetuned-mrpc/runs/Apr25_03-52-24_e41dfb6b4fe0,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=/root/transformers_data_and_model/finetuned-mrpc,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/root/transformers_data_and_model/finetuned-mrpc,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# è®¾è®¡æ—¥å¿—æ ¼å¼ï¼Œè®°å½•ä¸€äº›å…³é”®å‚æ•°ï¼Œå¹¶ä¸”æŠŠè®­ç»ƒå‚æ•°æ‰“å°å‡ºæ¥\n",
    "# ä¸€ä¸ªå¾ˆé‡è¦çš„æ„Ÿå—ï¼šä½¿ç”¨loggeræ‰“å°ä¸­é—´å˜é‡å¾ˆé‡è¦\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m%d%Y %H:%M:%S\",\n",
    "    level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    ")\n",
    "\n",
    "logger.warning(\n",
    "    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training:%s\",\n",
    "    training_args.local_rank,\n",
    "    training_args.device,\n",
    "    training_args.n_gpu,\n",
    "    bool(training_args.local_rank != -1),\n",
    "    training_args.fp16,\n",
    ")\n",
    "logger.info(\"Training/evaluation parameters %s\", training_args)\n",
    "\n",
    "# è®¾å®šç§å­\n",
    "transformers.set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45a17a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è·å¾—æ ‡ç­¾çš„ä¸ªæ•°\n",
    "# è¾“å‡ºçš„æ¨¡å¼ï¼Œè¿™é‡Œæ˜¯classificationä¸regressionä¸¤ç§\n",
    "# å¦‚æœé€‚é…æ–°ä»»åŠ¡ï¼Œæˆ‘ä»¬è¦çš„ä¸æ˜¯å»æŒ‰ç…§è¿™ç§æ ¼å¼ï¼Œè€Œæ˜¯è¦å¾—åˆ°è¿™ä¸¤ä¸ªå‚æ•°\n",
    "num_labels = 2\n",
    "output_mode = \"classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c442fd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /root/transformers_data_and_model/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /root/transformers_data_and_model/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½modelã€tokenizerã€modelè¿™ä¸‰ä¸ª\n",
    "# configæ˜¯åŒ…å«å±‚æ•°ã€dropoutå‚æ•°ã€headä¸ªæ•°ã€finetuneä»»åŠ¡ç­‰æ¨¡å‹ç›¸å…³å†…å®¹çš„å‚æ•°ï¼Œè¿™ä¸ªå‚æ•°åŠ è½½ååªæ˜¯ä¸ºäº†modelä½¿ç”¨ã€‚\n",
    "# configå†…å†™å…¥æ ‡ç­¾çš„ä¸ªæ•°num_labelsï¼Œå†³å®šmodelåé¢åˆ†ç±»ä½¿ç”¨çš„å…¨è¿æ¥çš„è¾“å‡ºä¸ªæ•°\n",
    "config = transformers.AutoConfig.from_pretrained(\n",
    "    model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    finetuning_task=data_args.task_name,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "    config=config,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cea67072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers.DataProcessoræ˜¯ä¸€ä¸ªåŸºç±»ï¼Œéœ€è¦å®ç°get_train_examples, get_dev_examples, get_test_examples, get_labelsç­‰å‡ ä¸ªå‡½æ•°\n",
    "# åˆ†åˆ«ç”¨äºæä¾›çš„InputExampleçš„é›†å’Œ(list)å’Œæ ‡ç­¾çš„é›†å’Œ\n",
    "class MrpcProcessor(transformers.DataProcessor):\n",
    "    \"Processor for the MRPC data set (GLUE version).\"\n",
    "    \n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return InputExample(\n",
    "            tensor_dict['idx'].numpy(),\n",
    "            tensor_dict['sentence1'].numpy().decode('utf-8'),\n",
    "            tensor_dict['sentence2'].numpy().decode('utf-8'),\n",
    "            str(tensor_dict['label'].numpy()),\n",
    "        )\n",
    "    \n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.tsv\")))\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "    \n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "    \n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "    \n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "    \n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Create examples for the training, dev and test sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[3]\n",
    "            text_b = line[4]\n",
    "            label = None if set_type == \"test\" else line[0]\n",
    "            examples.append(transformers.InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3b8fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°†examplesè½¬æ¢æˆfeatures\n",
    "def glue_convert_examples_to_features(\n",
    "    examples: List[transformers.InputExample],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    max_length: Optional[int] = None,\n",
    "    task=None,\n",
    "    label_list=None,\n",
    "    output_mode=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads a data file into a list of ``InputFeatures``\n",
    "\n",
    "    Args:\n",
    "        examples: List of ``InputExamples`` or ``tf.data.Dataset`` containing the examples.\n",
    "        tokenizer: Instance of a tokenizer that will tokenize the examples\n",
    "        max_length: Maximum example length. Defaults to the tokenizer's max_len\n",
    "        task: GLUE task\n",
    "        label_list: List of labels. Can be obtained from the processor using the ``processor.get_labels()`` method\n",
    "        output_mode: String indicating the output mode. Either ``regression`` or ``classification``\n",
    "\n",
    "    Returns:\n",
    "        If the ``examples`` input is a ``tf.data.Dataset``, will return a ``tf.data.Dataset``\n",
    "        containing the task-specific features. If the input is a list of ``InputExamples``, will return\n",
    "        a list of task-specific ``InputFeatures`` which can be fed to the model.\n",
    "\n",
    "    \"\"\"\n",
    "    if max_length is None:\n",
    "        max_length = tokenizer.max_len\n",
    "        \n",
    "    if task is not None:\n",
    "        processor = MrpcProcessor()\n",
    "        if label_list is None:\n",
    "            label_list = processor.get_labels()\n",
    "            logger.info(\"Using label list %s for task %s\" % (label_list, task))\n",
    "        if output_mode is None:\n",
    "            output_mode = glue_output_modes[task]\n",
    "            logger.info(\"Using output mode %s for task %s\" % (output_mode, task))\n",
    "    \n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "    \n",
    "    def label_from_example(example: transformers.InputExample) -> Union[int, float, None]:\n",
    "        if example.label is None:\n",
    "            return None\n",
    "        if output_mode == \"classification\":\n",
    "            return label_map[example.label]\n",
    "        elif output_mode == \"regression\":\n",
    "            return float(example.label)\n",
    "        raise KeyError(output_mode)\n",
    "    \n",
    "    labels = [label_from_example(example) for example in examples]\n",
    "    \n",
    "    batch_encoding = tokenizer(\n",
    "        [(example.text_a, example.text_b) for example in examples],\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    \n",
    "    features = []\n",
    "    for i in range(len(examples)):\n",
    "        inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
    "        \n",
    "        feature = transformers.InputFeatures(**inputs, label=labels[i])\n",
    "        features.append(feature)\n",
    "    \n",
    "    for i, example in enumerate(examples[:5]):\n",
    "        logger.info(\"*** Example ***\")\n",
    "        logger.info(\"guid: %s\" % (example.guid))\n",
    "        logger.info(\"features: %s\" % features[i])\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8acaaa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Split(enum.Enum):\n",
    "    train = 'train'\n",
    "    dev = 'dev'\n",
    "    test = 'test'\n",
    "    \n",
    "class GlueDataset(torch.utils.data.dataset.Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach soon.\n",
    "    \"\"\"\n",
    "    args: GlueDataTrainingArguments\n",
    "    output_mode: str\n",
    "    features: List[transformers.InputFeatures]\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        args: transformers.GlueDataTrainingArguments,\n",
    "        tokenizer: transformers.PreTrainedTokenizer,\n",
    "        limit_length: Optional[int] = None,\n",
    "        mode: Union[str, Split] = Split.train,\n",
    "        cache_dir: Optional[str] = None,\n",
    "        output_mode = \"classification\",\n",
    "    ):\n",
    "        self.args = args\n",
    "        self.processor = MrpcProcessor()\n",
    "        self.output_mode = output_mode\n",
    "        if isinstance(mode, str):\n",
    "            try:\n",
    "                mode =Split[mode]\n",
    "            except KeyError:\n",
    "                raise KeyError(\"mode is not a valid split name\")\n",
    "        # load data features from cache or dataset file\n",
    "        cached_features_file = os.path.join(\n",
    "            cache_dir if cache_dir is not None else args.data_dir,\n",
    "            \"cached_{}_{}_{}_{}\".format(\n",
    "                mode.value, tokenizer.__class__.__name__, str(args.max_seq_length), args.task_name,\n",
    "            ),\n",
    "        )\n",
    "        label_list = self.processor.get_labels()\n",
    "        self.label_list = label_list\n",
    "        \n",
    "        # Make sure only the first process in distributed training processes the dataset,\n",
    "        # and the others will use the cache.\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "        with filelock.FileLock(lock_path):\n",
    "            if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "                start = time.time()\n",
    "                self.features = torch.load(cached_features_file)\n",
    "                logger.info(\n",
    "                    f\"Loading features from cached file {cached_features_file} [took %3.f s]\", time.time() - start\n",
    "                )\n",
    "            else:\n",
    "                logger.info(f\"Creating features from dataset file at {args.data_dir}\")\n",
    "                \n",
    "                if mode == Split.dev:\n",
    "                    examples = self.processor.get_dev_examples(args.data_dir)\n",
    "                elif mode == Split.test:\n",
    "                    examples = self.processor.get_test_examples(args.data_dir)\n",
    "                else:\n",
    "                    examples = self.processor.get_train_examples(args.data_dir)\n",
    "                if limit_length is not None:\n",
    "                    examples = examples[:limit_length]\n",
    "                self.features = glue_convert_examples_to_features(\n",
    "                    examples,\n",
    "                    tokenizer,\n",
    "                    max_length=args.max_seq_length,\n",
    "                    label_list=label_list,\n",
    "                    output_mode=self.output_mode,\n",
    "                )\n",
    "                start = time.time()\n",
    "                torch.save(self.features, cached_features_file)\n",
    "                # ^ This seems to take a lot of time so I want to investigate why and how we can improve.\n",
    "                logger.info(\n",
    "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
    "                )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, i) -> transformers.InputFeatures:\n",
    "        return self.features[i]\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return self.label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a80fca8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04252022 03:54:49 - INFO - filelock - Lock 140540602682128 acquired on /root/transformers_data_and_model/MRPC/cached_train_BertTokenizerFast_128_mrpc.lock\n",
      "04252022 03:54:49 - INFO - __main__ - Creating features from dataset file at /root/transformers_data_and_model/MRPC\n",
      "04252022 03:54:49 - INFO - __main__ - LOOKING AT /root/transformers_data_and_model/MRPC/train.tsv\n",
      "04252022 03:54:50 - INFO - __main__ - *** Example ***\n",
      "04252022 03:54:50 - INFO - __main__ - guid: train-1\n",
      "04252022 03:54:50 - INFO - __main__ - features: InputFeatures(input_ids=[101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 7727, 2000, 2032, 2004, 2069, 1000, 1996, 7409, 1000, 1010, 2572, 3217, 5831, 5496, 2010, 2567, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "04252022 03:54:50 - INFO - __main__ - *** Example ***\n",
      "04252022 03:54:50 - INFO - __main__ - guid: train-2\n",
      "04252022 03:54:50 - INFO - __main__ - features: InputFeatures(input_ids=[101, 9805, 3540, 11514, 2050, 3079, 11282, 2243, 1005, 1055, 2077, 4855, 1996, 4677, 2000, 3647, 4576, 1999, 2687, 2005, 1002, 1016, 1012, 1019, 4551, 1012, 102, 9805, 3540, 11514, 2050, 4149, 11282, 2243, 1005, 1055, 1999, 2786, 2005, 1002, 6353, 2509, 2454, 1998, 2853, 2009, 2000, 3647, 4576, 2005, 1002, 1015, 1012, 1022, 4551, 1999, 2687, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "04252022 03:54:50 - INFO - __main__ - *** Example ***\n",
      "04252022 03:54:50 - INFO - __main__ - guid: train-3\n",
      "04252022 03:54:50 - INFO - __main__ - features: InputFeatures(input_ids=[101, 2027, 2018, 2405, 2019, 15147, 2006, 1996, 4274, 2006, 2238, 2184, 1010, 5378, 1996, 6636, 2005, 5096, 1010, 2002, 2794, 1012, 102, 2006, 2238, 2184, 1010, 1996, 2911, 1005, 1055, 5608, 2018, 2405, 2019, 15147, 2006, 1996, 4274, 1010, 5378, 1996, 14792, 2005, 5096, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "04252022 03:54:50 - INFO - __main__ - *** Example ***\n",
      "04252022 03:54:50 - INFO - __main__ - guid: train-4\n",
      "04252022 03:54:50 - INFO - __main__ - features: InputFeatures(input_ids=[101, 2105, 6021, 19481, 13938, 2102, 1010, 21628, 6661, 2020, 2039, 2539, 16653, 1010, 2030, 1018, 1012, 1018, 1003, 1010, 2012, 1037, 1002, 1018, 1012, 5179, 1010, 2383, 3041, 2275, 1037, 2501, 2152, 1997, 1037, 1002, 1018, 1012, 5401, 1012, 102, 21628, 6661, 5598, 2322, 16653, 1010, 2030, 1018, 1012, 1020, 1003, 1010, 2000, 2275, 1037, 2501, 5494, 2152, 2012, 1037, 1002, 1018, 1012, 5401, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "04252022 03:54:50 - INFO - __main__ - *** Example ***\n",
      "04252022 03:54:50 - INFO - __main__ - guid: train-5\n",
      "04252022 03:54:50 - INFO - __main__ - features: InputFeatures(input_ids=[101, 1996, 4518, 3123, 1002, 1016, 1012, 2340, 1010, 2030, 2055, 2340, 3867, 1010, 2000, 2485, 5958, 2012, 1002, 2538, 1012, 4868, 2006, 1996, 2047, 2259, 4518, 3863, 1012, 102, 18720, 1004, 1041, 13058, 1012, 6661, 5598, 1002, 1015, 1012, 6191, 2030, 1022, 3867, 2000, 1002, 2538, 1012, 6021, 2006, 1996, 2047, 2259, 4518, 3863, 2006, 5958, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "04252022 03:54:51 - INFO - __main__ - Saving features into cached file /root/transformers_data_and_model/MRPC/cached_train_BertTokenizerFast_128_mrpc [took 0.878 s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04252022 03:54:51 - INFO - filelock - Lock 140540602682128 released on /root/transformers_data_and_model/MRPC/cached_train_BertTokenizerFast_128_mrpc.lock\n",
      "04252022 03:54:51 - INFO - filelock - Lock 140540607032720 acquired on /root/transformers_data_and_model/MRPC/cached_dev_BertTokenizerFast_128_mrpc.lock\n",
      "04252022 03:54:51 - INFO - __main__ - Creating features from dataset file at /root/transformers_data_and_model/MRPC\n",
      "04252022 03:54:51 - INFO - __main__ - *** Example ***\n",
      "04252022 03:54:51 - INFO - __main__ - guid: dev-1\n",
      "04252022 03:54:51 - INFO - __main__ - features: InputFeatures(input_ids=[101, 2002, 2056, 1996, 9440, 2121, 7903, 2063, 11345, 2449, 2987, 1005, 1056, 4906, 1996, 2194, 1005, 1055, 2146, 1011, 2744, 3930, 5656, 1012, 102, 1000, 1996, 9440, 2121, 7903, 2063, 11345, 2449, 2515, 2025, 4906, 2256, 2146, 1011, 2744, 3930, 5656, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "04252022 03:54:51 - INFO - __main__ - *** Example ***\n",
      "04252022 03:54:51 - INFO - __main__ - guid: dev-2\n",
      "04252022 03:54:51 - INFO - __main__ - features: InputFeatures(input_ids=[101, 20201, 22948, 2056, 10958, 19053, 4140, 6283, 1996, 8956, 6939, 1998, 2246, 2830, 2000, 2478, 2010, 2146, 2086, 1997, 2731, 1999, 1996, 2162, 1012, 102, 2010, 2564, 2056, 2002, 2001, 1000, 2531, 3867, 2369, 2577, 5747, 1000, 1998, 2246, 2830, 2000, 2478, 2010, 2086, 1997, 2731, 1999, 1996, 2162, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "04252022 03:54:51 - INFO - __main__ - *** Example ***\n",
      "04252022 03:54:51 - INFO - __main__ - guid: dev-3\n",
      "04252022 03:54:51 - INFO - __main__ - features: InputFeatures(input_ids=[101, 1996, 7922, 2001, 2012, 12904, 1012, 6227, 18371, 2114, 1996, 18371, 1010, 4257, 2006, 1996, 5219, 1010, 1998, 2012, 1015, 1012, 27054, 2487, 2114, 1996, 5364, 23151, 2278, 1010, 2036, 4257, 1012, 102, 1996, 7922, 2001, 2012, 12904, 1012, 6275, 18371, 16545, 2100, 1027, 1010, 8990, 4257, 2006, 1996, 5219, 1010, 1998, 2012, 1015, 1012, 23090, 2487, 2114, 1996, 5364, 23151, 2278, 10381, 2546, 1027, 1010, 2091, 1014, 1012, 1015, 3867, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "04252022 03:54:51 - INFO - __main__ - *** Example ***\n",
      "04252022 03:54:51 - INFO - __main__ - guid: dev-4\n",
      "04252022 03:54:51 - INFO - __main__ - features: InputFeatures(input_ids=[101, 1996, 10028, 1011, 25022, 2080, 2003, 3403, 2127, 2255, 2000, 5630, 2065, 2009, 2097, 2203, 5668, 2063, 1037, 4018, 1012, 102, 1996, 10028, 1011, 25022, 2080, 2623, 9317, 2008, 2009, 2097, 5630, 1999, 2255, 3251, 2000, 2203, 5668, 2063, 1037, 4018, 2077, 1996, 27419, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "04252022 03:54:51 - INFO - __main__ - *** Example ***\n",
      "04252022 03:54:51 - INFO - __main__ - guid: dev-5\n",
      "04252022 03:54:51 - INFO - __main__ - features: InputFeatures(input_ids=[101, 2053, 5246, 2031, 2042, 2275, 2005, 1996, 2942, 2030, 1996, 4735, 3979, 1012, 102, 2053, 5246, 2031, 2042, 2275, 2005, 1996, 4735, 2030, 2942, 3572, 1010, 2021, 17137, 3051, 2038, 12254, 2025, 5905, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "04252022 03:54:51 - INFO - __main__ - Saving features into cached file /root/transformers_data_and_model/MRPC/cached_dev_BertTokenizerFast_128_mrpc [took 0.110 s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04252022 03:54:51 - INFO - filelock - Lock 140540607032720 released on /root/transformers_data_and_model/MRPC/cached_dev_BertTokenizerFast_128_mrpc.lock\n"
     ]
    }
   ],
   "source": [
    "# è·å¾—dataset\n",
    "train_dataset, eval_dataset, test_dataset = None, None, None\n",
    "if training_args.do_train:\n",
    "    train_dataset = GlueDataset(data_args, tokenizer=tokenizer, cache_dir=model_args.cache_dir, output_mode=output_mode)\n",
    "if training_args.do_eval:\n",
    "    eval_dataset = GlueDataset(data_args, tokenizer=tokenizer, mode=\"dev\", cache_dir=model_args.cache_dir)\n",
    "if training_args.do_predict:\n",
    "    test_dataset = GlueDataset(data_args, tokenizer=tokenizer, mode=\"test\", cache_dir=model_args.cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d7f2cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¾—åˆ°è®¡ç®—ç»“æœçš„å‡½æ•°\n",
    "# è¿™é‡Œè®¡ç®—çš„æ˜¯accå’Œf1\n",
    "# EvalPredictionæ˜¯é¢„æµ‹çš„ç»“æœçš„æ ¼å¼ï¼Œpredictionæ˜¯é¢„æµ‹çš„ï¼Œlabels_idsæ˜¯æ­£ç¡®çš„\n",
    "class EvalPrediction(NamedTuple):\n",
    "    \"\"\"\n",
    "    Evaluation output (always contains labels), to be used to compute metrics.\n",
    "    \n",
    "    Parameters:\n",
    "        predictions (:obj:`np.ndarray`): Predictions of the model.\n",
    "        label_ids (:obj:`np.ndarray`): Targets to be matched.\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions:np.ndarray\n",
    "    label_ids:np.ndarray\n",
    "        \n",
    "    \n",
    "# å¾—åˆ°è®¡ç®—å‡½æ•°\n",
    "def compute_metrics_fn(p: EvalPrediction):\n",
    "    if output_mode == \"classification\":\n",
    "        # é¢„æµ‹çš„ç»“æœ\n",
    "        preds = np.argmax(p.predictions, axis=1)\n",
    "    # æ­£ç¡®çš„ç»“æœ\n",
    "    labels = p.label_ids\n",
    "\n",
    "    # accå’Œf1\n",
    "    acc = (preds == labels).mean()\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds)\n",
    "    return {\"acc\": acc, \"f1\":f1, \"acc_and_f1\": (acc+f1) / 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc9a08a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ–æœ¬Trainer\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ed8d3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1145: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "Loading model from /root/transformers_data_and_model/bert-base-uncased).\n",
      "There were missing keys in the checkpoint model loaded: ['bert.embeddings.position_ids', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'classifier.weight', 'classifier.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.gamma', 'cls.predictions.transform.LayerNorm.beta', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.embeddings.LayerNorm.gamma', 'bert.embeddings.LayerNorm.beta', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma', 'bert.encoder.layer.0.attention.output.LayerNorm.beta', 'bert.encoder.layer.0.output.LayerNorm.gamma', 'bert.encoder.layer.0.output.LayerNorm.beta', 'bert.encoder.layer.1.attention.output.LayerNorm.gamma', 'bert.encoder.layer.1.attention.output.LayerNorm.beta', 'bert.encoder.layer.1.output.LayerNorm.gamma', 'bert.encoder.layer.1.output.LayerNorm.beta', 'bert.encoder.layer.2.attention.output.LayerNorm.gamma', 'bert.encoder.layer.2.attention.output.LayerNorm.beta', 'bert.encoder.layer.2.output.LayerNorm.gamma', 'bert.encoder.layer.2.output.LayerNorm.beta', 'bert.encoder.layer.3.attention.output.LayerNorm.gamma', 'bert.encoder.layer.3.attention.output.LayerNorm.beta', 'bert.encoder.layer.3.output.LayerNorm.gamma', 'bert.encoder.layer.3.output.LayerNorm.beta', 'bert.encoder.layer.4.attention.output.LayerNorm.gamma', 'bert.encoder.layer.4.attention.output.LayerNorm.beta', 'bert.encoder.layer.4.output.LayerNorm.gamma', 'bert.encoder.layer.4.output.LayerNorm.beta', 'bert.encoder.layer.5.attention.output.LayerNorm.gamma', 'bert.encoder.layer.5.attention.output.LayerNorm.beta', 'bert.encoder.layer.5.output.LayerNorm.gamma', 'bert.encoder.layer.5.output.LayerNorm.beta', 'bert.encoder.layer.6.attention.output.LayerNorm.gamma', 'bert.encoder.layer.6.attention.output.LayerNorm.beta', 'bert.encoder.layer.6.output.LayerNorm.gamma', 'bert.encoder.layer.6.output.LayerNorm.beta', 'bert.encoder.layer.7.attention.output.LayerNorm.gamma', 'bert.encoder.layer.7.attention.output.LayerNorm.beta', 'bert.encoder.layer.7.output.LayerNorm.gamma', 'bert.encoder.layer.7.output.LayerNorm.beta', 'bert.encoder.layer.8.attention.output.LayerNorm.gamma', 'bert.encoder.layer.8.attention.output.LayerNorm.beta', 'bert.encoder.layer.8.output.LayerNorm.gamma', 'bert.encoder.layer.8.output.LayerNorm.beta', 'bert.encoder.layer.9.attention.output.LayerNorm.gamma', 'bert.encoder.layer.9.attention.output.LayerNorm.beta', 'bert.encoder.layer.9.output.LayerNorm.gamma', 'bert.encoder.layer.9.output.LayerNorm.beta', 'bert.encoder.layer.10.attention.output.LayerNorm.gamma', 'bert.encoder.layer.10.attention.output.LayerNorm.beta', 'bert.encoder.layer.10.output.LayerNorm.gamma', 'bert.encoder.layer.10.output.LayerNorm.beta', 'bert.encoder.layer.11.attention.output.LayerNorm.gamma', 'bert.encoder.layer.11.attention.output.LayerNorm.beta', 'bert.encoder.layer.11.output.LayerNorm.gamma', 'bert.encoder.layer.11.output.LayerNorm.beta'].\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py:26: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 3 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "***** Running training *****\n",
      "  Num examples = 3668\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 87\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='87' max='87' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [87/87 00:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to /root/transformers_data_and_model/finetuned-mrpc\n",
      "Configuration saved in /root/transformers_data_and_model/finetuned-mrpc/config.json\n",
      "Model weights saved in /root/transformers_data_and_model/finetuned-mrpc/pytorch_model.bin\n",
      "tokenizer config file saved in /root/transformers_data_and_model/finetuned-mrpc/tokenizer_config.json\n",
      "Special tokens file saved in /root/transformers_data_and_model/finetuned-mrpc/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# è®­ç»ƒ\n",
    "if training_args.do_train:\n",
    "    # æ­¤æ—¶ä¼ å…¥model_pathæ˜¯ä¸ºäº†åŠ è½½optimiztorï¼Œè¿›è¡Œç»§ç»­è®­ç»ƒï¼Œå¯¹äºé€šå¸¸çš„fine-tuneæ¥è¯´ï¼Œmodel_pathå¯ä»¥ä¸ä¼ å…¥\n",
    "    # å¯¹äºåˆå§‹åŒ–åçš„Trainerï¼Œè°ƒç”¨trainæ–¹æ³•å°±å¯ä»¥è®­ç»ƒäº†ï¼Œç®€åŒ–äº†è®­ç»ƒçš„è¿‡ç¨‹\n",
    "    trainer.train(\n",
    "        model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n",
    "    )\n",
    "    # Trainerä¿å­˜æ¨¡å‹è°ƒç”¨æ­¤æ–¹æ³•\n",
    "    trainer.save_model()\n",
    "    \n",
    "    # ä¸ºäº†æ–¹ä¾¿ä½¿ç”¨èµ·è§ï¼Œå°†tokenizerçš„æ¨¡å‹å‚æ•°ä¹Ÿå­˜å…¥modelåŒç›®å½•(åŸæ–¹æ³•å·²ç»å¼ƒç”¨)\n",
    "    if trainer.is_world_process_zero():\n",
    "        tokenizer.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d1c7932",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04252022 03:57:48 - INFO - __main__ - *** Evaluate ***\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py:26: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 3 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "04252022 03:57:50 - INFO - __main__ - **** Eval results mrpc *****\n",
      "04252022 03:57:50 - INFO - __main__ -   eval_loss = 0.5169751048088074\n",
      "04252022 03:57:50 - INFO - __main__ -   eval_acc = 0.7843137254901961\n",
      "04252022 03:57:50 - INFO - __main__ -   eval_f1 = 0.8594249201277956\n",
      "04252022 03:57:50 - INFO - __main__ -   eval_acc_and_f1 = 0.8218693228089958\n",
      "04252022 03:57:50 - INFO - __main__ -   eval_runtime = 1.5105\n",
      "04252022 03:57:50 - INFO - __main__ -   eval_samples_per_second = 270.117\n",
      "04252022 03:57:50 - INFO - __main__ -   eval_steps_per_second = 8.607\n",
      "04252022 03:57:50 - INFO - __main__ -   epoch = 3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "key: eval_loss  value: 0.5169751048088074\n",
      "key: eval_acc  value: 0.7843137254901961\n",
      "key: eval_f1  value: 0.8594249201277956\n",
      "key: eval_acc_and_f1  value: 0.8218693228089958\n",
      "key: eval_runtime  value: 1.5105\n",
      "key: eval_samples_per_second  value: 270.117\n",
      "key: eval_steps_per_second  value: 8.607\n",
      "key: epoch  value: 3.0\n"
     ]
    }
   ],
   "source": [
    "# è¯„ä¼°ç»“æœ\n",
    "eval_results = {}\n",
    "if training_args.do_eval:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "    \n",
    "    # ä¼ å…¥metricsï¼Œå¯¹äºå‰é¢åˆå§‹åŒ–å·²ç»ä¼ å…¥çš„ï¼Œæ­¤æ—¶å¦‚æœæ²¡å˜åŒ–ï¼Œå¯ä»¥çœç•¥æ­¤æ­¥éª¤\n",
    "    trainer.compute_metrics = compute_metrics_fn\n",
    "    # ä¼ å…¥è¯„ä¼°çš„dataset\n",
    "    eval_results = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "    output_eval_file = os.path.join(\n",
    "        training_args.output_dir, f\"eval_results_{eval_dataset.args.task_name}.txt\"\n",
    "    )\n",
    "    \n",
    "    if trainer.is_world_process_zero():\n",
    "        # å†™å…¥æœ¬åœ°æ–‡ä»¶\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"**** Eval results {} *****\".format(eval_dataset.args.task_name))\n",
    "            for key, value in eval_results.items():\n",
    "                print(\"key:\", key, \" value:\", value)\n",
    "                logger.info(\"  %s = %s\", key, value)\n",
    "                writer.write(\"%s = %s\\n\" % (key, value))\n",
    "                \n",
    "        eval_results.update(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04c0a023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯•é›†çš„è¯„ä¼°\n",
    "print(training_args.do_predict)\n",
    "if training_args.do_predict:\n",
    "    logging.info(\"*** Test ***\")\n",
    "\n",
    "    predictions = trainer.predict(test_dataset=test_dataset).predictions\n",
    "    if output_mode == \"classification\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    output_test_file = os.path.join(\n",
    "        training_args.output_dir, f\"test_results_{test_dataset.args.task_name}.txt\"\n",
    "    )\n",
    "    if trainer.is_world_master():\n",
    "        with open(output_test_file, \"w\") as writer:\n",
    "            logger.info(\"***** Test results {} *****\".format(test_dataset.args.task_name))\n",
    "            writer.write(\"index\\tprediction\\n\")\n",
    "            for index, item in enumerate(predictions):\n",
    "                item = test_dataset.get_labels()[item]\n",
    "                writer.write(\"%d\\t%s\\n\" % (index, item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a0bdd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5169751048088074, 'eval_acc': 0.7843137254901961, 'eval_f1': 0.8594249201277956, 'eval_acc_and_f1': 0.8218693228089958, 'eval_runtime': 1.6126, 'eval_samples_per_second': 253.008, 'eval_steps_per_second': 8.062, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d697f706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/transformers_data_and_model/finetuned-mrpc'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FINETUNED_MRPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5371da22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/root/transformers_data_and_model/finetuned-mrpc\u001b[00m\r\n",
      "â”œâ”€â”€ config.json\r\n",
      "â”œâ”€â”€ eval_results_mrpc.txt\r\n",
      "â”œâ”€â”€ pytorch_model.bin\r\n",
      "â”œâ”€â”€ special_tokens_map.json\r\n",
      "â”œâ”€â”€ tokenizer.json\r\n",
      "â”œâ”€â”€ tokenizer_config.json\r\n",
      "â”œâ”€â”€ training_args.bin\r\n",
      "â””â”€â”€ vocab.txt\r\n",
      "\r\n",
      "0 directories, 8 files\r\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥çœ‹è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶\n",
    "!tree {FINETUNED_MRPC}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa5ccd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
