{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fc8f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本文使用openprompt库 地址https://github.com/thunlp/OpenPrompt\n",
    "# 包括入门范例、\n",
    "# 以情感分类为例\n",
    "from openprompt.data_utils import InputExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fcab286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 情感分类共两个classes: positive, negative\n",
    "classes = [\n",
    "    \"positive\",\n",
    "    \"negative\"\n",
    "]\n",
    "\n",
    "# 实例数据集共两个数据\n",
    "dataset = [\n",
    "    # text_a 是数据的输入文本，其他任务的数据可能有多条输入文本\n",
    "    InputExample(\n",
    "        guid = 0,\n",
    "        text_a = \"Albert Einstein was one of the greatest intellects of his time.\",\n",
    "    ),\n",
    "    InputExample(\n",
    "        guid = 1,\n",
    "        text_a = \"The film was badly made.\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f3f6626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/cryptography/hazmat/backends/openssl/x509.py:17: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  utils.DeprecatedIn35,\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 定义预训练模型，这里用bert，这是因为根据prompt设计，想让模型输出[mask]位置短语，属于填空问题\n",
    "from openprompt.plms import load_plm\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a294727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义prompt模板，这里是手动设计模板\n",
    "from openprompt.prompts import ManualTemplate\n",
    "promptTemplate = ManualTemplate(\n",
    "    text = '{\"placeholder\": \"text_a\"} It was {\"mask\"}',\n",
    "    tokenizer = tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61a1b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义输出-label映射\n",
    "# It was {mask} mask的位置输出是一个单词，我们还要将这个单词映射为\"positive\",\"negative\"标签，这个过程称之为\"Verbalizer\"\n",
    "from openprompt.prompts import ManualVerbalizer\n",
    "promptVerbalizer = ManualVerbalizer(\n",
    "    classes = classes,\n",
    "    label_words = {\n",
    "        \"negative\": [\"bad\"],\n",
    "        \"positive\": [\"good\", \"wonderful\", \"great\"],\n",
    "    },\n",
    "    tokenizer = tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc2ddca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将前面的plm，promptTemplate，promptVerbalizer组合成promptModel\n",
    "from openprompt import PromptForClassification\n",
    "promptModel = PromptForClassification(\n",
    "    template = promptTemplate,\n",
    "    plm = plm,\n",
    "    verbalizer = promptVerbalizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38285016",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 2it [00:00, 401.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# 定义dataloader\n",
    "from openprompt import PromptDataLoader\n",
    "data_loader = PromptDataLoader(\n",
    "    dataset = dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    template = promptTemplate,\n",
    "    tokenizer_wrapper_class=WrapperClass,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "029769db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:\n",
      "{\"input_ids\": [[101, 3986, 16127, 1108, 1141, 1104, 1103, 4459, 1107, 7854, 18465, 1116, 1104, 1117, 1159, 119, 1135, 1108, 103, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], \"inputs_embeds\": null, \"attention_mask\": [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], \"token_type_ids\": null, \"label\": null, \"decoder_input_ids\": null, \"decoder_inputs_embeds\": null, \"soft_token_ids\": null, \"past_key_values\": null, \"loss_ids\": [[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], \"guid\": [0], \"tgt_text\": null, \"encoded_tgt_text\": null, \"input_ids_len\": null}\n",
      "\n",
      "logits:\n",
      "tensor([[-1.9090, -3.2924]])\n",
      "pred:\n",
      "tensor([0])\n",
      "pred labels:\n",
      "positive\n",
      "batch:\n",
      "{\"input_ids\": [[101, 1109, 1273, 1108, 6118, 1189, 119, 1135, 1108, 103, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], \"inputs_embeds\": null, \"attention_mask\": [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], \"token_type_ids\": null, \"label\": null, \"decoder_input_ids\": null, \"decoder_inputs_embeds\": null, \"soft_token_ids\": null, \"past_key_values\": null, \"loss_ids\": [[-100, 0, 0, 0, 0, 0, 0, 0, 0, 1, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], \"guid\": [1], \"tgt_text\": null, \"encoded_tgt_text\": null, \"input_ids_len\": null}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([[-2.2736, -1.4058]])\n",
      "pred:\n",
      "tensor([1])\n",
      "pred labels:\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "# 开始训练及测试。prompt的目的：统一预训练模型的训练模型与当前预测模型\n",
    "# 使用带prompt的预训练MLM执行零跳推理\n",
    "import torch\n",
    "promptModel.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "        #print(\"batch:\")\n",
    "        #print(batch)\n",
    "        logits = promptModel(batch)\n",
    "        #print(\"logits:\")\n",
    "        #print(logits)\n",
    "        preds = torch.argmax(logits, dim=-1) # 返回指定维度最大值的序号\n",
    "        print(\"pred:\")\n",
    "        print(preds)\n",
    "        print(\"pred labels:\")\n",
    "        print(classes[preds])\n",
    "        # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe384883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43c7b95229a4d858cb8c0f9c1467c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/9.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa8d54d851f493c91fc8c364c238191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/8.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset super_glue/cb (download: 73.71 KiB, generated: 198.02 KiB, post-processed: Unknown size, total: 271.73 KiB) to ../datasets/.cache/huggingface_datasets/super_glue/cb/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3061b36de2ce49afbc9016bb8c33f1d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/75.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/56 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset super_glue downloaded and prepared to ../datasets/.cache/huggingface_datasets/super_glue/cb/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e222036f88ae4e2ab618836c1c42566f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tutorial 0_basic https://github.com/thunlp/OpenPrompt/blob/main/tutorial/0_basic.py\n",
    "# load dataset\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "#print(datasets.list_datasets())\n",
    "raw_dataset = load_dataset('super_glue', 'cb', cache_dir=\"../datasets/.cache/huggingface_datasets\")\n",
    "# raw_dataset['train'][0]\n",
    "# from datasets import load_from_disk\n",
    "# raw_dataset = load_from_disk(\"/home/hushengding/huggingface_datasets/saved_to_disk/super_glue.cb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dd5aec82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premise': \"He's weird enough to have undressed me without thinking, according to some mad notion of the ``proper'' thing to do. Perhaps he thought I couldn't lie in bed with my clothes on.\", 'hypothesis': \"she couldn't lie in bed with her clothes on\", 'idx': 23, 'label': 1}\n",
      "{'premise': \"I should dearly have liked to know whether they were Europeans or Americans, but I couldn't hear the accents. They appeared to be arguing. I hoped the white men weren't telling him to eliminate all witnesses because I don't believe it would have needed much persuasion.\", 'hypothesis': 'eliminating all witnesses would have needed much persuasion', 'idx': 26, 'label': 1}\n",
      "{'premise': \"But the damage was done as far as my faith was concerned, which is probably why I went mad. So anyway, that Christmas Eve night confirmed my worst fears, it was like a kind of ``royal flush'' for the infant Jimbo. All three kings - Pa Santa and the King of Kings - all down the pan together... And to be honest I don't believe any of them stands a chance of ever making a comeback with me.\", 'hypothesis': 'any of the three kings stands a chance of ever making a comeback with him', 'idx': 27, 'label': 1}\n",
      "{'premise': \"He had seen something I should have, which was a car turning in from Soho Square and coming up behind me. My right foot hovered over the accelerator pedal and I balanced Armstrong on the clutch. I wasn't as convinced as Malpass that Nevil was out of harm's way.\", 'hypothesis': \"Nevil was out of harm's way\", 'idx': 28, 'label': 1}\n",
      "{'premise': \"Jed wondered. He 'd scarcely set eyes on him since the night they 'd had dinner together at the house in Westwood. Nobody had mentioned him either and Jed didn't feel he should ask.\", 'hypothesis': 'Jed should ask', 'idx': 29, 'label': 1}\n",
      "{'premise': \"``I hope you are settling down and the cat is well.'' This was a lie. She did not hope the cat was well.\", 'hypothesis': 'the cat was well', 'idx': 33, 'label': 2}\n",
      "{'premise': \"Most of them young, about his age, stood and talked and drank and laughed. The two girls he had noticed earlier were standing talking to some other girls. Graham hoped they all realised that just because he was standing talking to Slater that didn't mean he was gay too.\", 'hypothesis': 'Graham was gay too', 'idx': 41, 'label': 1}\n",
      "{'premise': \"He said that maybe I wasn't ready to join the Party just yet. Terry's passion for equality appealed to my purer mind, and his hatred of existing authority appealed to my resentments. But although I hated inequality it didn't mean I wanted to be treated like everyone else.\", 'hypothesis': 'he wanted to be treated like everyone else', 'idx': 42, 'label': 1}\n",
      "{'premise': \"``Oh, my poor Folly... We 've been together for five years, Lexy and I - she practically holds that company together. Of course I gave her an ``A''. But that doesn't mean I'm having an affair with her.\", 'hypothesis': 'he is having an affair with Lexy', 'idx': 43, 'label': 1}\n",
      "{'premise': \"Oh, I did, I did! I was lucky. I would have liked brothers and sisters but I don't remember that I was ever lonely.\", 'hypothesis': 'she was ever lonely', 'idx': 53, 'label': 1}\n",
      "{'premise': \"He pulled occasionally, his arms tiring. Conker slowed a little, but the branches were coming too fast, he had to lean right forward and couldn't use his hands. He remembered the gate at the end of the track had time to hope it was open because he didn't think Conker could jump it.\", 'hypothesis': 'Conker could jump the gate', 'idx': 57, 'label': 1}\n",
      "{'premise': \"But he ended up eating it himself. I was reluctant to kiss my mother, afraid that somehow her weakness and unhappiness would infect me. Naturally I didn't think for a minute that my life and spirit could stimulate her.\", 'hypothesis': 'her life and spirit could stimulate her mother', 'idx': 58, 'label': 1}\n",
      "{'premise': \"``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this.\", 'hypothesis': 'Ely had considered him wanting to be free', 'idx': 59, 'label': 1}\n",
      "{'premise': \"I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly.\", 'hypothesis': 'Morton is capable of interpreting this food for thought correctly', 'idx': 60, 'label': 1}\n",
      "{'premise': \"The big Norwegian shook his head, frowning. ``Jeg fonstAr ikke.'' I don't think he found Ward's accent at all easy and anyway like many foreigners he found it easier to speak English than to understand it.\", 'hypothesis': \"the big Norwegian found Ward's accent at all easy\", 'idx': 61, 'label': 1}\n",
      "{'premise': \"You really don't know anything about me, do you, despite all that wallowing in my mind? As it happens I don't think I'm the right person to lead humanity into the future no.\", 'hypothesis': 'she is the right person to lead humanity into the future', 'idx': 62, 'label': 1}\n",
      "{'premise': \"It's where the bands practise. I can't remember what band Petra's in, but I seen them practise once. They were OK but I didn't think they was brilliant.\", 'hypothesis': \"Petra's band was brilliant\", 'idx': 63, 'label': 1}\n",
      "{'premise': \"She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper?\", 'hypothesis': 'Tara was only the housekeeper', 'idx': 64, 'label': 1}\n",
      "{'premise': 'If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them?', 'hypothesis': 'people can have power over spirits', 'idx': 65, 'label': 1}\n",
      "{'premise': 'Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important?', 'hypothesis': 'Maelmuire is important', 'idx': 66, 'label': 1}\n",
      "{'premise': \"It is all very well, in these changing times, to adapt one's work to take in duties not traditionally within one's realm. But bantering is of another dimension altogether. For one thing how would one know for sure that at any given moment a response of the bantering sort is truly what is expected?\", 'hypothesis': 'at any given moment a response of the bantering sort is truly what is expected', 'idx': 67, 'label': 2}\n",
      "{'premise': \"``Clever''. Klug means ``clever''. Would you say that Abie was clever?\", 'hypothesis': 'Abie was clever', 'idx': 76, 'label': 2}\n",
      "{'premise': \"She didn't know if they had given themselves sufficient time to think things over before they married - that was the kind of question her sister Louise asked. Edward stayed in the Engineers for a bit, then came out and was not very successful in finding a job to suit him. That wasn't his fault and if anyone said that it was Nenna would still feel like poking a hole in them.\", 'hypothesis': \"it was Edward's fault\", 'idx': 79, 'label': 1}\n",
      "{'premise': \"Nicky approached her with the assumption that men are naturally right and it is the role of women to follow their lead. Constance, whose confidence was growing daily, was not prepared to give in to Nicky's wishes merely because of his sex. If she felt he was right then she agreed with him.\", 'hypothesis': 'Nicky was right', 'idx': 84, 'label': 2}\n",
      "{'premise': \"I can't afford to get bogged down in the weeds. But at least you know she did leave. Maybe a coincidence maybe the two girls talked on the phone decided they 'd both had enough.\", 'hypothesis': 'the two girls had both had enough', 'idx': 86, 'label': 2}\n",
      "{'premise': \"That evening Shannon steered well clear of Dane, all but tiptoeing around him, determined not to land in any more confrontations. From now till this snow siege ended, she would simply live under the same roof, but keep to her own side of an invisible barrier, she decided. She could only hope he 'd do the same.\", 'hypothesis': 'Dane would do the same', 'idx': 95, 'label': 2}\n",
      "{'premise': 'Then it cried. It was another girl. I was a little disappointed but I could only hope that Celia was still a bit hazy from the drugs.', 'hypothesis': 'Celia was still a bit hazy from the drugs', 'idx': 96, 'label': 2}\n",
      "{'premise': \"Meh ' Lindi did not develop the lower set of arms nor the bony, sinuous tail. Too much to expect a new pair of arms to grow out of her ribs, or her coccyx to elongate so enormously. Nor could Jaq imagine that she could attain the full strength of a purestrain Stealer - though her own strength was formidable even when unenhanced.\", 'hypothesis': \"Meh ' Lindi could attain the full strength of a purestrain Stealer\", 'idx': 99, 'label': 1}\n",
      "{'premise': \"A: How do you feel about gun control? B: Well, uh, I mean I don't think that guns should be outlawed\", 'hypothesis': 'guns should be outlawed', 'idx': 117, 'label': 1}\n",
      "{'premise': \"B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did.\", 'hypothesis': 'he and his wife have as much as his parents did', 'idx': 118, 'label': 1}\n",
      "{'premise': \"A: Well, how do you feel about the immigration laws? B: At, currently, I think they are a little restrictive. Uh, particularly for, uh, certain ethnic groups or from certain countries. Um, I think we should permit, uh, more immigration from eastern Europe, for example, uh, particularly uh, the Jewish, uh, uh, people from Russia. I think we could permit more of them in than we have permitted in the last, uh, several years. And, I think we have, uh, uh, too much restriction uh, on the Orientals also, but, of course, that's just my opinion. A: Yeah, well, I'm not real sure why I got this topic, because I don't think I checked it off on the list because I know very little about the current immigration laws.\", 'hypothesis': 'he checked the topic off on the list', 'idx': 119, 'label': 1}\n",
      "{'premise': \"B: boy, he's a big one. A: he's pretty big. That's why it really surprises me, you know, that he hasn't come back, because, like I said, he's never gone away like this before, and, I would think, you know, I mean, he might could get hurt by a car or something. I don't know that he could really get killed that easily because he is so big.\", 'hypothesis': 'he could really get killed that easily', 'idx': 120, 'label': 1}\n",
      "{'premise': \"A: and then once they do get elected, they don't have the power or the authority or the willingness to do those things that they promised, you know, beforehand. B: Right. A: You know, maybe it just wasn't possible at all in the first place, you know, like the no new taxes thing. You know, that's, uh, with the economy going the way it is and everything, that was nearly ridiculous thing to, even try to do. B: Yeah.  Yeah. Well, I don't think he's going to have to worry about that next year.\", 'hypothesis': \"he's going to have to worry about that next year\", 'idx': 121, 'label': 1}\n",
      "{'premise': \"B: And I don't think that rehabilitation is effective. A: Right. Have to agree with you, and I'm kind of in favor of capital punishment also. I just don't think that it acts much as a deterrent to these people because, uh, you still see them committing the same crimes,\", 'hypothesis': 'it acts much as a deterrent to these people', 'idx': 123, 'label': 1}\n",
      "{'premise': \"A: so you hear so much, you get a little tired of it, but then again, so many people you got to understand only catch it once a day, maybe B: Yep. A: but I think the quality of uh, our news is just, uh, I don't believe it could be better\", 'hypothesis': 'it could be better', 'idx': 124, 'label': 1}\n",
      "{'premise': \"B: And I've worked in the hospital for fifteen years and I've taken care of a few AIDS patients. A: Uh-huh. B: Uh, when they asked us did we want to, uh, keep it the same or, uh, spend more, spend less, uh, I think right now what they're spending is adequate. Uh, for my personal opinion. Uh, because I think it's something that's going to take them a while to come up with a, uh, vaccine for. A: Yeah.  Uh-huh.  Uh-huh. B: I don't think it's going to be that easy to come up with\", 'hypothesis': 'it is going to be that easy to come up with', 'idx': 125, 'label': 1}\n",
      "{'premise': \"A: Yeah, they did. They put a lot of pressure on him from the outside and from the inside. Uh, it's funny watching them play, he's probably like a lot of quarterbacks, uh, when the pressure is really on when it's down to the last few minutes of the game for the season is when the guys seem to really do their best. B: Uh-huh. A: And I haven't quite figured that out, if they figure they have got it won or if there's no real hurry because the first three quarters or, uh, uh, if something happens that that adrenalin starts flowing.\", 'hypothesis': 'they have got it won', 'idx': 126, 'label': 2}\n",
      "{'premise': \"A: Uh, well then you must know a lot more about this than I do. B: Uh, I think, uh, the system right now, you know, you know, is fine. I think it should be by a jury. I don't think the judge should have, I mean he's just there kind of like the referee. A: Uh-huh, Uh-huh. B: Uh, I don't even think that it should be unanimous either. Uh,\", 'hypothesis': 'it should be unanimous', 'idx': 127, 'label': 1}\n",
      "{'premise': \"B: Uh-huh. It, I mean, I don't know, I don't think George Bush will make the American people happy with ninety-seven cents a week. A: No, no, not at all. B: I just don't think it was a well thought out incentive.\", 'hypothesis': 'it was a well thought out incentive', 'idx': 128, 'label': 1}\n",
      "{'premise': \"B: but I found that, uh, it was made of some material which actually ended up rusting uh, after, A: Oh. B: even, despite, you know, diligent washing, it got rusty after about, uh, three weeks of use. And I don't think it was my fault because you know, I had made a point of like drying it off and cleaning it\", 'hypothesis': 'it was his fault', 'idx': 129, 'label': 1}\n",
      "{'premise': \"A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period\", 'hypothesis': 'it was that expensive', 'idx': 130, 'label': 1}\n",
      "{'premise': \"B: I did, too. A: I mean, it was just more for my money. B: Yeah. I didn't think it was too long at all.\", 'hypothesis': 'it was too long', 'idx': 131, 'label': 1}\n",
      "{'premise': \"B: That's true. A: So. B: Uh, the other argument is that the death penalty is a deterrent and I really don't, uh, agree with that. I don't think anyone who would commit uh, a crime that would get them the death penalty would stop at the moment and say, well, I was about to kill and dismember this person but, oh, if they catch me they're going to kill me so I better not do it. I just, don't think uh, that it works that way. A: Yeah.  I don't think it's done.\", 'hypothesis': \"it's done\", 'idx': 132, 'label': 1}\n",
      "{'premise': \"B: But, uh, if the wind comes basically from the south it can be really bad. A: Uh-huh. B: Uh, the State of Wisconsin, as a matter of fact, uh, started some litigation against Illinois because of the air pollution we were getting. A: Uh-huh. B: Uh, I don't think it's going to go very far,\", 'hypothesis': \"it's going to go very far\", 'idx': 133, 'label': 1}\n",
      "{'premise': \"B: Yeah. Well, that's the guy that counts. A: Yes. But, maybe we'll get your guy. B: Oh, I don't think Jim Kelly is about to be swayed away from the Bills any time.\", 'hypothesis': 'Jim Kelly is about to be swayed away from the Bills any time', 'idx': 134, 'label': 1}\n",
      "{'premise': \"B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones,\", 'hypothesis': 'Mister Rogers is making new Mister Rogers', 'idx': 135, 'label': 1}\n",
      "{'premise': \"A: Yeah, that's crazy. B: and then you come here in the Dallas area, um, I don't believe that people should be allowed to carry guns in their vehicles.\", 'hypothesis': 'people should be allowed to carry guns in their vehicles', 'idx': 137, 'label': 1}\n",
      "{'premise': \"B: but if they get the little tiny kids saving it now, in five years, when they get bigger, it'll work a little bit more, too. A: Yeah.  True. B: Because it's, we've all got to do it right now. I just, I really amazed to find out that, eighty per cent are filled now, in garbage fills. In five years we're supposed to be at max. A: Uh-huh. B: I don't think I can keep my own garbage.\", 'hypothesis': 'she can keep her own garbage', 'idx': 138, 'label': 1}\n",
      "{'premise': \"A: And, uh, I got to stay home with my kids, which I really wanted to do, but now I could not go back and do it. B: Yeah. A: I really couldn't, I don't think I could stay home all the time and do nothing.\", 'hypothesis': 'he could stay home all the time and do nothing', 'idx': 139, 'label': 1}\n",
      "{'premise': \"A:  nanny, sort of? Uh-huh. Uh-huh. B: and you know, I could envision a society where that would happen and make an interesting, uh, uh, story or whatever. A: Yeah. B: I don't think I have a philosophical problem with that.\", 'hypothesis': 'she has a philosophical problem with that', 'idx': 140, 'label': 1}\n",
      "{'premise': \"B: That was kind of a funny movie with, uh, Richard Dreyfuss and Bill Murray. A: Uh-huh. B: That was fun. A: Golly, I don't think that I've ever heard of that movie.\", 'hypothesis': 'he has heard of that movie', 'idx': 141, 'label': 1}\n",
      "{'premise': \"A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper,\", 'hypothesis': 'she has seen the Little Dipper', 'idx': 142, 'label': 1}\n",
      "{'premise': \"B: Yeah. It's, uh, I have modem night computer so I can log into the, uh, network at Georgia Tech and access my account through there, which can be useful. But, uh, yeah that and word processing is while I tend to, I mean, obviously, I do almost all of my report writing on my computer, uh, whether it's term papers or even some smaller homework assignments. A: Uh-huh. B: So it's I don't know, really become my mainstay I guess. I can't even remember, actually I don't think I've ever used a typewriter in my life to do a report. Because my family, when I was growing up, we got a basic computer. TRS Eighty when they first came out.\", 'hypothesis': 'he has used a typewriter to do a report', 'idx': 143, 'label': 1}\n",
      "{'premise': \"B: Now see I. A: I'm intrigued by it, but I'm not sure I want to go see it yet. B: Yeah, I don't think I want to see that either.\", 'hypothesis': 'she wants to see that', 'idx': 145, 'label': 1}\n",
      "{'premise': \"A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them.\", 'hypothesis': 'she went to some of them', 'idx': 146, 'label': 1}\n",
      "{'premise': \"B: I wouldn't be surprised. A: You know, because they don't want to send them to daycare. B: I doubt if they would say it was too long.\", 'hypothesis': 'it was too long', 'idx': 147, 'label': 1}\n",
      "{'premise': \"A: But, uh, uh, I don't understand, I guess, why the schools seem to have such a high dropout rate in the big cities. B: Uh, well, I don't pretend to understand that either. Uh, but I'm not quite sure that it's the kind of thing that ought to be blamed on the schools. But then, again, I'm not quite sure where the blame ought to be put. Uh, because the dropout rate is, in those areas, w-, it's high in those areas where also there's poverty and crime. And they all seem to go together. And it seems like if you could eliminate one of the parts of that circle, where you have the dropout rate and crime and, you know, general poverty kind of conditions, that things ought to get better. So, uh, the other two a-, they're all three social issues and could be addressed by the government in any ways. And clearly, to me, is a kind of government thing to fix but it's just like, I don't expect them to know which part is best to fix just like I don't know. it's a complicated issue. I still don't think I would blame it directly on the school.\", 'hypothesis': 'he would blame it directly on the school', 'idx': 148, 'label': 1}\n",
      "{'premise': \"B: She says that when her husband died oh, that my uncle had said that he would never put her in a rest home. So it's kind of, uh, I don't know. I mean, I don't think my parents would but she is getting pretty bad like she has to have like a little toilet right by her bed and, it's, A: Uh-huh. B: and my mom has to take care of her pretty much so it gets, I don't know. it's a hard decision, but I don't think I would do it to my parents personally.\", 'hypothesis': 'she would do it to her parents', 'idx': 149, 'label': 1}\n",
      "{'premise': \"A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself.\", 'hypothesis': 'he would go to work without a bulletproof vest', 'idx': 150, 'label': 1}\n",
      "{'premise': \"A: Well, presumably those who find out such information, if they are doing it, I would prefer to not to be known, and, I mean, you know, the classic, oh, I don't know C I  conspiracy theories or whatever, would have such parties trying to do it without your knowledge. So there's,, things that invade that second type of privacy where you do know about them and possibly things that invade that second type of privacy without you knowing about it, and I can't talk about the second one other than to generate paranoia. It's a surmise and, I'd like to think that it's quite low, at least in this country. B: to surmise. It is there. A: I don't think I'd like the KGB monitoring my phone or anything like that.\", 'hypothesis': 'he would like the KGB monitoring his phone', 'idx': 151, 'label': 1}\n",
      "{'premise': \"B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either.\", 'hypothesis': 'his mother would want to be there', 'idx': 152, 'label': 1}\n",
      "{'premise': \"B: I mean, you can't guarantee that they wouldn't escape from prison and do it again. A: Right.  And then do it all over again.  Uh-huh. B: And I don't think that rehabilitation is effective.\", 'hypothesis': 'rehabilitation is effective', 'idx': 153, 'label': 1}\n",
      "{'premise': \"B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust,\", 'hypothesis': 'she would adjust', 'idx': 154, 'label': 1}\n",
      "{'premise': \"B: What am I afraid of? A: Yes. B: Um, I don't know if I'm really afraid of spending too much. I just, uh, don't think that I need them, you know.\", 'hypothesis': 'she needs them', 'idx': 155, 'label': 1}\n",
      "{'premise': \"A: That's fairly interesting. B: I bet that would be, rather interesting. Uh, that's, uh, self improvement, well, that's kind of a hobby but it is self improvement from the standpoint of probably relaxing, uh. A: Yeah, I don't know that I read anything strictly labeled self improvement.\", 'hypothesis': 'she reads anything strictly labeled self improvement', 'idx': 156, 'label': 1}\n",
      "{'premise': \"A: But, uh, B: Okay.  Uh, uh, I've had one or two American cars I think, and they were okay. I had a Pontiac once and I never had a problem with it, but, uh, my mother had a Dodge at one point and I had driven it a few times and I really did not feel that I would buy a Dodge just from,\", 'hypothesis': 'she would buy a Dodge', 'idx': 158, 'label': 1}\n",
      "{'premise': \"B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked.\", 'hypothesis': 'some kid should be exempt from being spanked', 'idx': 159, 'label': 1}\n",
      "{'premise': \"B: I understand we are doing care of the elderly, right? A: Yes. B: And how do you feel about putting someone in the nursing home? A: Well, I don't think that uh, any of my relatives would really like to go there.\", 'hypothesis': 'some of her relatives would really like to go there', 'idx': 160, 'label': 1}\n",
      "{'premise': \"A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently\", 'hypothesis': 'somebody roots differently', 'idx': 161, 'label': 1}\n",
      "{'premise': \"B: if you get it on sale, A: Yeah, yeah, so we bought that or we bought the filets, and then the chicken, or turkey nuggets, and I don't think anybody in my house knows the difference, unless you tell them.\", 'hypothesis': 'someone in his house knows the difference', 'idx': 162, 'label': 1}\n",
      "{'premise': \"A: Have you followed that very much or, B: Uh, not really. I don't think anything will ever take over the NFL.\", 'hypothesis': 'something will take over the NFL', 'idx': 163, 'label': 1}\n",
      "{'premise': \"B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it.\", 'hypothesis': \"something's going to be done about it\", 'idx': 164, 'label': 1}\n",
      "{'premise': \"A: Oh, yes. Animals have a way of talking. B: Alfie did. I tell you if I could have gotten a hold of that cat that day. A: I don't know uh, that I'd trade my dog in for the world.\", 'hypothesis': 'he would trade his dog in for the world', 'idx': 165, 'label': 1}\n",
      "{'premise': \"A: but at the same time I think it would do them a world of good. B: Yeah. A: But there's a, B: I don't know that you could require everyone yeah, to do it for a whole year, or two years or something like that,\", 'hypothesis': 'speaker A could require everyone to do it for a whole year', 'idx': 166, 'label': 1}\n",
      "{'premise': \"A: They have to for international trade. B: Yeah. A: But, I guess it's easier to switch back and forth than it used to be, uh, because uh, of computers coming into everything. B: Uh-huh. Yeah, I don't think switching back and forth is that big a deal.\", 'hypothesis': 'switching back and forth is that big a deal', 'idx': 168, 'label': 1}\n",
      "{'premise': \"A: And now it's election time again so they're trying to lower them. B: Oh. A: So they're just talk about lowering them but they never do, they just keep raising them. B: I've never seen taxes really go down.\", 'hypothesis': 'taxes would really go down', 'idx': 169, 'label': 1}\n",
      "{'premise': \"B: they did things for people, you know, for their communities, for their, uh, families, for their friends, where now, I'm not sure they really do. A: Yes.  Yeah. Well, I think sometimes through groups and organizations, um, uh, when they asked the question I thought, well that sounds wonderful. And then, I wondered if people were unwilling but I think even if you went in with a negative attitude I don't think it would stay negative very long.\", 'hypothesis': 'that attitude would stay negative very long', 'idx': 170, 'label': 1}\n",
      "{'premise': \"B: And, you know, the whole electronic classroom idea. I don't know if I I'm yeah, in favor of, yeah, A: I don't think that's going to work.\", 'hypothesis': 'that is going to work', 'idx': 171, 'label': 1}\n",
      "{'premise': \"A: Uh, well, it would depend on when you go it's not excessively crowded on the weekends. B: See I'd want to be there in the mornings like from nine thirty to ten thirty. A: Oh, I don't think that would be bad at all.\", 'hypothesis': 'that would be bad', 'idx': 173, 'label': 1}\n",
      "{'premise': \"B: I think in s-, and it, just would depend upon the circumstances and the extent of the abuse and if another alternative was available. A: Uh-huh.  Uh-huh.  Um.  Uh-huh. You know, now, I wonder what you think about this and, uh, unfortunately, we don't get to do it, but, uh, it used to be a long time ago, I guess in Biblical times when they had punishment, if somebody did something, for example, to your family, then you had the right to administer the punishment. So if somebody killed somebody in your family, then uh, if that person was caught and found guilty, you had the right to, uh, execute that person. And I know that, uh, if somebody had done something to my family, I would feel that I had the right to get revenge on them uh, but, I don't think that's done much anywhere.\", 'hypothesis': \"that's done much anywhere\", 'idx': 174, 'label': 1}\n",
      "{'premise': \"A: I spend a lot of time reading about these things. I'm quite interested. I find it very exciting for the coverage we have now, today. B: Yes and I think we do get pretty good coverage. I don't feel that the American people is being shortchanged by uh, the news coverage.\", 'hypothesis': 'the American people are being shortchanged by the news coverage', 'idx': 175, 'label': 1}\n",
      "{'premise': \"B: Uh, uh, I've had one or two American cars I think, and they were okay. I had a Pontiac once and I never had a problem with it, but, uh, my mother had a Dodge at one point and I had driven it a few times and I really did not feel that I would buy a Dodge just from, A: Um. B: well, actually, I had uh, a Dodge Omni at one point A: Uh-huh. B: and that was, I think, what really prejudiced me against American cars because I did not feel that it was a very quality, uh, car.\", 'hypothesis': 'the Dodge Omni was a very quality car', 'idx': 176, 'label': 1}\n",
      "{'premise': \"A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down.\", 'hypothesis': 'the jury should be the ones that put the sentencings down', 'idx': 177, 'label': 1}\n",
      "{'premise': \"B: Oh, I see. A: um, and I think I'm getting a better caliber of student at the private school, because I think their parents pay more, and I think the kids are a little bit more challenged, because their parents are probably college educated, where at the public school, I don't think as many parents are college educated,\", 'hypothesis': 'as many parents are college educated at the public school', 'idx': 179, 'label': 1}\n",
      "{'premise': \"B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change.\", 'hypothesis': 'the law will change', 'idx': 180, 'label': 1}\n",
      "{'premise': \"B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way\", 'hypothesis': 'the one solution to school funding will be necessarily the best way', 'idx': 181, 'label': 1}\n",
      "{'premise': \"A: I think so, I think, B: I really do. Oh, yeah, it's going to take, uh, you know, the police, I don't think can do it alone, you know.\", 'hypothesis': 'the police can do it alone', 'idx': 182, 'label': 1}\n",
      "{'premise': \"A: How do you feel about that. B: I don't really, I more, I don't know about the government as much as, uh, the people, uh, I wouldn't consider to be a threat at all and I really don't feel much like the Soviet Union itself is a threat anymore.\", 'hypothesis': 'the Soviet Union itself is a threat still', 'idx': 183, 'label': 1}\n",
      "{'premise': \"B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it.\", 'hypothesis': 'the way she gets the news is the right way to get it', 'idx': 185, 'label': 1}\n",
      "{'premise': \"B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it.\", 'hypothesis': 'there should be that big of a restriction on it', 'idx': 186, 'label': 1}\n",
      "{'premise': \"B: Uh, with regard uh, to jury trials. I really feel as though, uh, uh, whatever system has been used historically, in particular jurisdiction, you know, is really the only kinds of things that you can use. Because the jurisprudence is, you know, based uh, you know, on accumulated body of law. And if you have a situation where you change that body of law, then all of sudden they could start going back and digging up all these cases uh, that would be handled differently were they judged by today's standards. So I really don't think they can really do much of anything to change it.\", 'hypothesis': 'they can really do something to change it', 'idx': 187, 'label': 1}\n",
      "{'premise': \"B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant,\", 'hypothesis': 'they do the packaging at this plant', 'idx': 188, 'label': 1}\n",
      "{'premise': \"A: The one thing I sometimes wonder about, um, in civil cases is, uh, whether, especially sort of in, uh, maybe like product liability, or medical malpractice, where there's, um, sort of a very technical decision to be made sometimes B: Yes. A: you know, it's not just a matter um, of, you know, did this guy rip off this guy, and it's just a matter of interpreting a contract, it's sort of a matter of, um, you know, sometimes getting into very technical issues, and I wonder um, if the system works adequately in educating the jurors about, uh, whatever, um, you know, issue is under discussion. B: I don't think that they educate them enough to really know what's going on.\", 'hypothesis': \"they educate the jurors enough to really know what's going on\", 'idx': 189, 'label': 1}\n",
      "{'premise': \"B: You know, back around, you know, in the twenties and thirties when they were growing up, uh, you know, they were all located together, in one small community. A: Right, right.  Right. B: And I mean when time went on the family grew and moved away and so forth. And now when they come together it's generally, you know, like say the kids of those people who are not, you know, anywhere near one another and I do not think they feel the closeness that they used to be there. Which is a shame\", 'hypothesis': 'they feel the closeness that they used to be there', 'idx': 191, 'label': 1}\n",
      "{'premise': \"A: it was so fattening. B: That sounds good. A: But, I don't think we gained any weight from it\", 'hypothesis': 'they gained any weight from it', 'idx': 192, 'label': 1}\n",
      "{'premise': 'A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement.', 'hypothesis': 'they have seen a really high improvement', 'idx': 193, 'label': 1}\n",
      "{'premise': \"B: And, uh, I think they've all developed kind of an interest in reading also. A: That's re-, yeah. B: I'm not saying they read all the right things\", 'hypothesis': 'they read all the right things', 'idx': 194, 'label': 1}\n",
      "{'premise': \"B: And they go down the line ten years and then on some little technicality they get out and on the streets again doing the same they did before. A: Uh-huh. B: And, you know, that's about the only thing. Like for theft and stuff like that or manslaughter, you know, I don't think they should do that.\", 'hypothesis': 'they should do that', 'idx': 196, 'label': 1}\n",
      "{'premise': \"A: Oh, wow! But maybe you shouldn't be held responsible for something you did several years ago. B: So,  I know. A: That's the other thing. I mean a lot of people as kids or, you know, young people get into some things that they get out of later on and I don't think they should really have to pay for that forever.\", 'hypothesis': 'they should really have to pay for that forever', 'idx': 197, 'label': 1}\n",
      "{'premise': \"B: Uh, I have, uh, I guess a lot of thoughts about the Vietnam War, um, I guess I feel like I was pretty young while it was going on and so there's probably a lot of things I remember and a lot of things that I really didn't have a clue as to what was happening. B: Yeah. A: Um, looking back, like maybe some of the things that I know now, I'm not sure I do believe it was worth the cost in dollars and lives. That was one of the questions that she asked us to think about, because we never went to war. I don't think we were committed to winning it and getting out\", 'hypothesis': 'they were committed to winning the Vietnam War and getting out', 'idx': 198, 'label': 1}\n",
      "{'premise': \"B: um, they try to encourage you to follow a specific curriculum, although you don't have to. A: Uh-huh. B: And then if you have particular religious beliefs they're kind of monitored. You know, they will allow you to, I can't think of any examples but certain religious groups don't want their children in public schools because the influence. And maybe they were a group of Mennonites or something like that. A: Uh-huh. B: I don't think they're were in this area\", 'hypothesis': 'they were in this area', 'idx': 199, 'label': 1}\n",
      "{'premise': \"A: I, that would have been stupid, B: Yeah. A: and I don't think we did it. Everything else we handled in this seemed to be perfectly right. I don't think they would have done that.\", 'hypothesis': 'they would have done that', 'idx': 200, 'label': 1}\n",
      "{'premise': \"A: I really don't. But no, when the time comes hopefully we'll really look around before I decide on one for my parents B: Uh-huh. A: really do, because I have been raised in one, you know, so there's lot of things I know to look for. B: Yeah. A: And I don't think they'd want to go where they used to work, either you know.\", 'hypothesis': 'they would want to go where they used to work', 'idx': 201, 'label': 1}\n",
      "{'premise': \"B: And as far as like them, uh, entertaining the rights that they should have. A: Uh-huh. B: I mean, we educate them, we feed them, we take care of them and they no sooner get out on the street and they're back in again. A: Uh-huh. B: I'm not saying they're all bad,\", 'hypothesis': \"they're all bad\", 'idx': 202, 'label': 1}\n",
      "{'premise': \"A: Yeah, you're probably right, two years might be a little too long. B: Yeah, and there will be a lot of rebellion in that and when you get people who have no desire to be there in the first place, I don't think that they're going to be serving anybody.\", 'hypothesis': \"they're going to be serving somebody\", 'idx': 203, 'label': 1}\n",
      "{'premise': \"A: How did Radio Shack work? B: If you go in and buy anything they want your phone number. And I don't think they're going to call me and ask me how it's functioning,\", 'hypothesis': \"they're going to call him\", 'idx': 204, 'label': 1}\n",
      "{'premise': \"B: So, let's talk about the, uh, wonderful abuses in the State of Pennsylvania of personal property taxes whereby you can purchase something mail order and after the fact, the State of Pennsylvania can find out about it and send you a bill for the sales tax appropriate to that item that you purchased as well as interest and penalties from the time that you bought it. What do you think? Is Pennsylvania kind of out of line there? A: Well, actually, I do n't think they're out of line.\", 'hypothesis': \"they're out of line\", 'idx': 205, 'label': 1}\n",
      "{'premise': \"A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy\", 'hypothesis': \"they're shoddy\", 'idx': 206, 'label': 1}\n",
      "{'premise': \"B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it,\", 'hypothesis': \"they've gone that far\", 'idx': 207, 'label': 1}\n",
      "{'premise': \"A: but, it was interesting to, I don't know if they're making them better or not, it's hard to say. I think they more of their parts plastic, which, you know, makes things break easier, but I think the technology is better. B: I think technology is better, I'm not sure. I think you've got a good point with the plastic and that. A: About quality. B: I don't think necessarily that, things are being made better uh, you know,\", 'hypothesis': 'things are being made better', 'idx': 208, 'label': 1}\n",
      "{'premise': \"A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles.\", 'hypothesis': 'women look good with muscles', 'idx': 209, 'label': 1}\n",
      "{'premise': \"B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. B: Right.  Uh, are you saying you don't think anything should be done in the short term?\", 'hypothesis': 'anything should be done in the short term', 'idx': 210, 'label': 2}\n",
      "{'premise': \"B: Right. And I'm sure that would make a big difference, too. You know, you've got, A: Yeah. Well, what about a voluntary program? Do you think that would be a good idea?\", 'hypothesis': 'a voluntary program would be a good idea', 'idx': 211, 'label': 2}\n",
      "{'premise': \"A: Well I, uh, when is your next one, uh, scheduled now. B: Well it's like, the last one was my high school graduation the next one was when I graduated from college, so I guess about two more years. A: Yes, well, and do you think you'll have a baby to take back with you.\", 'hypothesis': 'speaker B will have a baby to take back with her', 'idx': 216, 'label': 2}\n",
      "{'premise': 'B: and both of those seem very easy to use compared to D Base. A: Uh-huh. Do you think D Base is more flexible or allows you to do more. Or do you think the others are pretty much compatible these days?', 'hypothesis': 'the others are pretty much compatible these days', 'idx': 218, 'label': 2}\n",
      "{'premise': \"B: Uh-huh. So, yeah, that's the thing is just to look at the school system in the area that you move into before you. A: Uh-huh. Of course we have a slight problem in that, uh, the number of the illiterate in America is mushrooming at this point, and, uh, you know, where our kids might be in a great school, we're still paying an awful lot of taxes for people who are on welfare and unemployment because they can't read, you know. B: Uh-huh. A: So. B: But do you think that there should be, um, nationwide, um, curriculum?\", 'hypothesis': 'there should be a nationwide curriculum', 'idx': 219, 'label': 2}\n",
      "{'premise': \"A: Sometimes you hear things on the radio that, you know, could be true or couldn't be. B: Uh-huh. A: Uh, do you feel like this is, I guess they're spending a billion or so a year on this AIDS research. B: Uh-huh. A: Do you think they should spend more?\", 'hypothesis': 'they should spend more', 'idx': 221, 'label': 2}\n",
      "{'premise': \"B: when you've lost something or uh, uh, don't have what other people have that's when you tend to realize, you know, what's out there and you know, what you have and what you don't have. A: Yeah I agree. B: So the original question, do we think they're you know, a security threat?\", 'hypothesis': \"they're a security threat\", 'idx': 222, 'label': 2}\n",
      "{'premise': \"A: Yeah. The radio doesn't really have much news sometimes. The stations I listen to are just mainly music. B: Yeah, I think you pretty much have to listen to all news station to get any news at all. A: Yeah. Do you think that TV is, uh, pretty accurate.\", 'hypothesis': 'TV is pretty accurate', 'idx': 224, 'label': 2}\n",
      "{'premise': \"B: What you want. where do they get it?. A: Well, I don't know, I guess they don't have it at home, B: I can't imagine it would stay fresh long enough to,\", 'hypothesis': 'it would stay fresh long enough', 'idx': 227, 'label': 1}\n",
      "{'premise': \"B: so there's only been really one working. A: Uh-huh, same here. Uh-huh. B: And, uh, it works for me but I can't see that it would work for probably the majority of people.\", 'hypothesis': 'it would work for probably the majority of people', 'idx': 228, 'label': 1}\n",
      "{'premise': \"B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll,\", 'hypothesis': 'she likes a lot of the very modern rock and roll', 'idx': 229, 'label': 1}\n",
      "{'premise': \"A: so I watch the fish, you know. Whatever I can do to keep myself occupied. I like to have the TV on, because that usually keeps me, um, more occupied. It kind of takes the time away and I don't realize, that's really the only time I ever watch TV, is when I'm on the bike. and then usually after I'm done riding the bike, just to cool myself down, I usually take a walk, you know, and that just kind of uh, gets me, you know, to where I'm not quite as tired I guess. But it's definitely a task. B: You think so? A: I can't say that I really enjoy it.\", 'hypothesis': 'she really enjoys it', 'idx': 230, 'label': 1}\n",
      "{'premise': \"A: That is the reason, I don't play over there. B: Yeah. A: I like the course, but I don't play over there because, they don't, uh, you know don't allow you to pull a cart. B: Right. A: And, I don't think a cart damages the turf.\", 'hypothesis': 'a cart damages the turf', 'idx': 233, 'label': 1}\n",
      "{'premise': 'B: I do not know. I wonder where he gets it? You know, you must, I think TV is bad. Because they, uh, show all sorts of violence on, A: That and I do not think a lot of parents, I mean, I do not know how it is in the Air Force base. But, uh, I just do not think a lot of people, because of the economy, both need to work, you know. I just do not think a lot of parents are that involved any more.', 'hypothesis': 'a lot of parents are that involved', 'idx': 234, 'label': 1}\n",
      "{'premise': \"A: Highland Park's thinking about going that route. B: Uh-huh. A: So it, and then I think that's going to push teachers to dress a lot more professionally than they do. B: Yeah, I don't think a lot of teachers are very professional\", 'hypothesis': 'a lot of teachers are very professional', 'idx': 235, 'label': 1}\n",
      "{'premise': \"A: Yeah, I think that's what aggravates a lot of people, is somebody does get a life sentence in place of the death penalty, and they wind up back on the streets after five years or six years or like the kid on the news tonight out in Mesquite who was out in six months.. B: Uh-huh. Yeah, it's just our criminal system is just so, I guess, overloaded, but the problem is not so much with the prison system, you know, I mean, because the cops are out there doing their job enforcing the laws, and the prison system are just, you know, they're trying to cope with them, but, you know, the thing about capital punishment I, you know, a lot of people don't think it would be a deterrent, uh, to future crime,\", 'hypothesis': 'capital punishment would be a deterrent to future crimes', 'idx': 236, 'label': 1}\n",
      "{'premise': \"A: or you know, it doesn't seem that it's going to make much of a difference. B: Uh-huh. It, I mean, I don't know, I don't think George Bush will make the American people happy with ninety-seven cents a week.\", 'hypothesis': 'George Bush will make American people happy with ninety-seven cents a week', 'idx': 237, 'label': 1}\n",
      "{'premise': \"B: Well, I was never there for any sentencing. Uh, I finally got empaneled on one case, uh, on my next to the last day, and, uh, we got into the, uh, jury room to, uh, decide the case, and there was one guy on the jury who announced to everybody that he didn't need to deliberate, because he'd already decided that the guy was, uh, not guilty, and he would never vote for guilty. A: Huh. B: So, uh, they appointed me jury foreman and I, uh, didn't think that, uh, going in without deliberating allowed us to reach a verdict,\", 'hypothesis': 'going in without deliberating allowed them to reach a verdict', 'idx': 238, 'label': 1}\n",
      "{'premise': \"B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either.\", 'hypothesis': 'Gorbachev is going to be allowed to get away with doing some change', 'idx': 239, 'label': 1}\n",
      "{'premise': \"Robert Erwin, president of Biosource, called Plant Genetic's approach ``interesting'' and ``novel,'' and ``complementary rather than competitive.'' ``There is a large market out there hungry for hybrid seeds,'' he said. Mr. Robinson of Delta & Pine, the seed producer in Scott, Miss., said Plant Genetic's success in creating genetically engineered male steriles doesn't automatically mean it would be simple to create hybrids in all crops.\", 'hypothesis': 'it would be simple to create hybrids in all crops', 'idx': 240, 'label': 1}\n",
      "{'premise': \"The trend toward lower rents may seem surprising given that some communities in New York are bemoaning the loss of favorite local businesses to high rents.  But, despite the recent softening, for many of these retailers there's still been too big a jump from the rental rates of the late 1970s, when their leases were signed. Certainly, the recent drop in prices doesn't mean Manhattan comes cheap.\", 'hypothesis': 'Manhattan comes cheap', 'idx': 242, 'label': 1}\n",
      "{'premise': \"The South Korean government is signing a protocol today establishing formal diplomatic relations with Poland. The two are also signing a trade agreement. South Korean government officials said they don't expect that Seoul can loan money to Warsaw, but it can ``offer experience.''\", 'hypothesis': 'Seoul can loan money to Warsaw', 'idx': 243, 'label': 1}\n",
      "{'premise': \"In finding ``good news'' in Berkeley's new freshman admissions plan (``The Privileged Class,'' editorial, Sept. 20), you're reading the headline but not the story. The plan indeed raises from 40% to 50% the number of freshmen applicants admitted strictly by academic criteria. But that doesn't mean ``half of the students attending Berkeley'' will be admitted this way.\", 'hypothesis': 'half of the students attending Berkeley will be admitted this way', 'idx': 244, 'label': 1}\n",
      "{'premise': \"GM confirmed it received U.S. antitrust clearance to boost its holding. Sansui Electric agreed to sell a 51% stake to Polly Peck of Britain for $110 million. Still, analysts said the accord doesn't suggest Japan is opening up to more foreign takeovers.\", 'hypothesis': 'Japan is opening up to more foreign takeovers', 'idx': 245, 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "#raw_dataset['train'][8]\n",
    "for item in raw_dataset['train']:\n",
    "    if item['label'] != 0:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8d81eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"guid\": 0,\n",
      "  \"label\": 0,\n",
      "  \"meta\": {},\n",
      "  \"text_a\": \"It was a complex language. Not written down but handed down. One might say it was peeled down.\",\n",
      "  \"text_b\": \"the language was peeled down\",\n",
      "  \"tgt_text\": null\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openprompt.data_utils import InputExample\n",
    "\n",
    "# 将raw数据处理为InputExample形式\n",
    "dataset = {}\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    dataset[split] = []\n",
    "    for data in raw_dataset[split]:\n",
    "        input_example = InputExample(text_a = data['premise'], text_b = data['hypothesis'], label=int(data['label']), guid=data['idx'])\n",
    "        dataset[split].append(input_example)\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e806a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/cryptography/hazmat/backends/openssl/x509.py:17: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  utils.DeprecatedIn35,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e68305b52ae431a8cb348b4d2181b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e51b8cc980423b838c7851083c3dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/850M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee177e00d0d42c78405c65aee80bba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 使用t5 plm\n",
    "from openprompt.plms import load_plm\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"t5\", \"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c97729e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手动构造template\n",
    "from openprompt.prompts import ManualTemplate\n",
    "template_text = '{\"placeholder\":\"text_a\"} Question: {\"placeholder\":\"text_b\"}? Is it correct? {\"mask\"}.'\n",
    "mytemplate = ManualTemplate(tokenizer=tokenizer, text=template_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "760474b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'text': 'It was a complex language. Not written down but handed down. One might say it was peeled down.', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' Question:', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': ' the language was peeled down', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '? Is it correct?', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'label': 0}]\n"
     ]
    }
   ],
   "source": [
    "# 打印看一下template如何包装输入input的\n",
    "wrapped_example = mytemplate.wrap_one_example(dataset['train'][0])\n",
    "print(wrapped_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afb7a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在包装好的input准备好传给tokenizer\n",
    "# 可以使用你自己的tokenizer，不过我们推荐使用wrapped tokenizer\n",
    "# 如果你使用我们的 \"load_plm\"函数，wrapped tokneizer已经给出，否则需要选择`openprompt.plms.__init__.py`中合适的配置的wrapper\n",
    "# 注意使用t5作为plm时，我们只需要将<pad> <extra_id_0> <eos>传给decoder\n",
    "# 损失在<extra_id_0>计算，因此传递decoder_max_length=3可以节省空间\n",
    "\n",
    "#wrapped_t5tokenizer = WrapperClass(max_seq_length=128, decoder_max_length=3, tokenizer=tokenizer,truncate_method=\"head\")\n",
    "# or\n",
    "from openprompt.plms import T5TokenizerWrapper\n",
    "wrapped_t5tokenizer= T5TokenizerWrapper(max_seq_length=128, decoder_max_length=3, tokenizer=tokenizer,truncate_method=\"head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ea0d856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [94, 47, 3, 9, 1561, 1612, 5, 933, 1545, 323, 68, 14014, 323, 5, 555, 429, 497, 34, 47, 158, 400, 26, 323, 5, 11860, 10, 8, 1612, 47, 158, 400, 26, 323, 3, 58, 27, 7, 34, 2024, 58, 32099, 3, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'decoder_input_ids': [0, 32099, 0], 'loss_ids': [0, 1, 0]}\n",
      "['▁It', '▁was', '▁', 'a', '▁complex', '▁language', '.', '▁Not', '▁written', '▁down', '▁but', '▁handed', '▁down', '.', '▁One', '▁might', '▁say', '▁it', '▁was', '▁pe', 'ele', 'd', '▁down', '.', '▁Question', ':', '▁the', '▁language', '▁was', '▁pe', 'ele', 'd', '▁down', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<extra_id_0>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "# 可视化tokenized example\n",
    "tokenized_example = wrapped_t5tokenizer.tokenize_one_example(wrapped_example, teacher_forcing=False)\n",
    "print(tokenized_example)\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized_example['input_ids']))\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized_example['decoder_input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e1bbb1be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[94,\n",
       " 47,\n",
       " 3,\n",
       " 9,\n",
       " 1561,\n",
       " 1612,\n",
       " 5,\n",
       " 933,\n",
       " 1545,\n",
       " 323,\n",
       " 68,\n",
       " 14014,\n",
       " 323,\n",
       " 5,\n",
       " 555,\n",
       " 429,\n",
       " 497,\n",
       " 34,\n",
       " 47,\n",
       " 158,\n",
       " 400,\n",
       " 26,\n",
       " 323,\n",
       " 5,\n",
       " 11860,\n",
       " 10,\n",
       " 8,\n",
       " 1612,\n",
       " 47,\n",
       " 158,\n",
       " 400,\n",
       " 26,\n",
       " 323,\n",
       " 3,\n",
       " 58,\n",
       " 27,\n",
       " 7,\n",
       " 34,\n",
       " 2024,\n",
       " 58,\n",
       " 32099,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs['train'][0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5e923e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# 下面将整个数据集转化为input格式\n",
    "model_inputs = {}\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    model_inputs[split] = []\n",
    "    for sample in dataset[split]:\n",
    "        tokenized_example = wrapped_t5tokenizer.tokenize_one_example(mytemplate.wrap_one_example(sample), teacher_forcing=False)\n",
    "        model_inputs[split].append(tokenized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3ddd2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 250it [00:01, 155.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# 使用提供的`PromptDataLoader`可以将上述转化为`torch.DataLoader`格式\n",
    "from openprompt import PromptDataLoader\n",
    "\n",
    "train_dataloader = PromptDataLoader(dataset=dataset[\"train\"], template=mytemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,  # 这里根据max_seq_length,将最大长度由128扩展到256\n",
    "    batch_size=4,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")\n",
    "# next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ec5ff502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd718168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[4273]],\n",
      "\n",
      "        [[ 150]],\n",
      "\n",
      "        [[2087]]])\n",
      "tensor([[-1.8581, -1.0345, -0.7161],\n",
      "        [-0.2899, -2.4579, -1.7953]])\n"
     ]
    }
   ],
   "source": [
    "# 定义verbalizer，也就是将logits映射到最终label概率\n",
    "from openprompt.prompts import ManualVerbalizer\n",
    "import torch\n",
    "\n",
    "# for example the verbalizer contains multiple label words in each class\n",
    "myverbalizer = ManualVerbalizer(tokenizer, num_classes=3,\n",
    "                        label_words=[[\"yes\"], [\"no\"], [\"maybe\"]])\n",
    "\n",
    "print(myverbalizer.label_words_ids)\n",
    "logits = torch.randn(2,len(tokenizer)) # creating a pseudo output from the plm, and\n",
    "print(myverbalizer.process_logits(logits)) # see what the verbalizer do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6b5f02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用PromptForClassification组装\n",
    "from openprompt import PromptForClassification\n",
    "\n",
    "use_cuda = True\n",
    "prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\n",
    "if use_cuda:\n",
    "    prompt_model=  prompt_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a854c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_model.plm.shared.weight\n",
      "Parameter containing:\n",
      "tensor([[ -0.7539,   0.5977,  -2.4375,  ...,   1.2500,  -0.7891,   3.5156],\n",
      "        [ 11.3750,  -4.8750,   9.0625,  ...,   4.8438,  14.3750,  -5.7812],\n",
      "        [-16.6250,  11.0625, -20.8750,  ...,  10.6875,  22.2500,  25.0000],\n",
      "        ...,\n",
      "        [  2.2344,   6.7500, -11.0625,  ..., -11.3125,  13.5625,  16.6250],\n",
      "        [  4.2500,   5.1250, -12.2500,  ..., -11.9375,  13.5000,  17.0000],\n",
      "        [  4.0625,   6.9688, -12.2500,  ..., -11.3750,  11.9375,  16.6250]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.0.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0762, -0.0471,  0.0309,  ...,  0.0143, -0.0491, -0.0593],\n",
      "        [ 0.0381, -0.0075,  0.0003,  ..., -0.0131, -0.0308, -0.0157],\n",
      "        [-0.0047, -0.0262, -0.0298,  ..., -0.0193,  0.0520, -0.0500],\n",
      "        ...,\n",
      "        [ 0.0066, -0.0496,  0.0422,  ...,  0.0474,  0.0308, -0.0200],\n",
      "        [-0.0176,  0.0217, -0.0289,  ...,  0.0413, -0.0488, -0.0444],\n",
      "        [-0.0256,  0.0493,  0.0060,  ..., -0.0181,  0.0459, -0.0334]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.0.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.5430, -0.2090, -0.0192,  ...,  0.0864, -0.7383,  0.0938],\n",
      "        [ 0.3750,  0.1650,  0.1416,  ..., -0.3066, -0.1836, -0.3457],\n",
      "        [-0.0452, -0.1299,  0.0089,  ..., -0.0986,  0.2598, -0.5820],\n",
      "        ...,\n",
      "        [ 0.4941, -0.0562,  0.2578,  ...,  0.2402,  0.2080, -0.0082],\n",
      "        [-0.1895,  0.5742,  0.2266,  ..., -0.5352,  0.4785,  0.1270],\n",
      "        [ 0.0854,  0.2109,  0.1357,  ..., -0.0442,  0.0442, -0.2285]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.0.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.4277,  0.8828, -0.3574,  ...,  0.3398, -0.0098, -0.2412],\n",
      "        [ 0.2217,  0.0063,  0.0303,  ..., -0.2793,  0.2031,  0.1021],\n",
      "        [ 0.3203, -0.6953,  0.1377,  ...,  0.2139,  0.2930,  0.3320],\n",
      "        ...,\n",
      "        [-0.0781, -0.2617, -0.1484,  ...,  0.1118, -0.0033, -0.2715],\n",
      "        [-0.4141,  0.3516,  0.4863,  ...,  0.0659,  0.2773,  0.0386],\n",
      "        [-0.2734, -0.1040,  0.2988,  ...,  0.0120, -0.1748, -0.1196]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.0.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.5039, -0.1885, -0.5039,  ...,  0.2871,  0.1465, -0.3066],\n",
      "        [-0.9102, -0.0525,  0.6953,  ...,  0.2480, -0.2715, -0.4434],\n",
      "        [ 0.2988, -0.0442, -0.1895,  ...,  0.3574,  0.7070,  0.3535],\n",
      "        ...,\n",
      "        [-0.3320,  0.0525, -0.2539,  ..., -0.1641, -0.1982,  0.0062],\n",
      "        [-0.0601, -0.0242, -0.5781,  ...,  0.3066,  0.0432, -0.7344],\n",
      "        [ 0.3301, -0.3027, -0.4824,  ..., -0.0801,  0.0684,  0.2754]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "Parameter containing:\n",
      "tensor([[ 2.8906e+00, -3.1719e+00, -3.3594e-01, -3.9551e-02, -5.7188e+00,\n",
      "          3.4219e+00, -1.7125e+01, -8.0625e+00,  3.0664e-01, -2.2559e-01,\n",
      "          7.9688e-01, -8.6875e+00],\n",
      "        [ 5.6641e-01,  6.4375e+00,  3.4531e+00,  1.7422e+00,  3.0000e+00,\n",
      "          9.6484e-01, -5.2500e+00,  3.4219e+00,  6.0312e+00, -2.7125e+01,\n",
      "         -5.1875e+00, -8.3984e-01],\n",
      "        [ 5.3516e-01,  3.8125e+00,  3.1406e+00,  1.9766e+00,  3.0469e+00,\n",
      "          5.8594e-01,  1.3379e-01,  3.6250e+00,  5.1562e+00, -1.2625e+01,\n",
      "         -2.1562e+00,  1.6895e-01],\n",
      "        [ 4.8633e-01,  2.4062e+00,  2.9531e+00,  1.9297e+00,  2.9531e+00,\n",
      "          4.3945e-01,  9.4531e-01,  3.5469e+00,  4.4375e+00, -2.1750e+01,\n",
      "         -1.8047e+00,  7.3438e-01],\n",
      "        [ 4.5703e-01,  1.4844e+00,  2.7812e+00,  1.8281e+00,  2.8906e+00,\n",
      "          3.3594e-01,  1.2656e+00,  3.3906e+00,  3.8906e+00, -4.1875e+00,\n",
      "         -1.7109e+00,  8.9844e-01],\n",
      "        [ 4.1016e-01,  7.5000e-01,  2.5938e+00,  1.7812e+00,  2.8281e+00,\n",
      "          2.8906e-01,  1.4609e+00,  3.2500e+00,  3.4531e+00, -3.7344e+00,\n",
      "         -1.7344e+00,  1.0781e+00],\n",
      "        [ 3.6328e-01,  2.3828e-01,  2.5000e+00,  1.6406e+00,  2.7344e+00,\n",
      "          2.5977e-01,  1.5312e+00,  3.1562e+00,  3.0625e+00, -3.9531e+00,\n",
      "         -1.7188e+00,  1.1484e+00],\n",
      "        [ 3.5156e-01, -1.0010e-01,  2.4062e+00,  1.5859e+00,  2.6875e+00,\n",
      "          1.5137e-01,  1.6328e+00,  3.0625e+00,  2.7500e+00, -3.8906e+00,\n",
      "         -1.7969e+00,  1.2266e+00],\n",
      "        [ 2.9492e-01, -7.0703e-01,  2.1406e+00,  1.4219e+00,  2.5469e+00,\n",
      "          1.2402e-01,  1.7734e+00,  2.7344e+00,  2.0000e+00, -4.1250e+00,\n",
      "         -1.6953e+00,  1.3359e+00],\n",
      "        [ 1.7480e-01, -1.0625e+00,  1.8438e+00,  1.1953e+00,  2.3750e+00,\n",
      "          9.3384e-03,  1.8594e+00,  2.3438e+00,  1.0391e+00, -4.3438e+00,\n",
      "         -1.8047e+00,  1.3984e+00],\n",
      "        [ 5.3711e-02, -1.3047e+00,  1.5469e+00,  9.2969e-01,  2.1250e+00,\n",
      "         -1.0449e-01,  1.9297e+00,  1.9141e+00,  1.1328e-01, -4.5938e+00,\n",
      "         -1.7344e+00,  1.4609e+00],\n",
      "        [-3.4668e-02, -1.4688e+00,  1.2188e+00,  5.7422e-01,  1.8828e+00,\n",
      "         -1.9043e-01,  1.9453e+00,  1.3750e+00, -9.7656e-01, -4.9688e+00,\n",
      "         -1.8438e+00,  1.4688e+00],\n",
      "        [-1.4453e-01, -1.6953e+00,  7.8516e-01,  2.6172e-01,  1.5625e+00,\n",
      "         -3.1445e-01,  1.9297e+00,  7.8125e-01, -2.1562e+00, -5.1562e+00,\n",
      "         -1.9141e+00,  1.4219e+00],\n",
      "        [-2.2852e-01, -1.8203e+00,  4.0820e-01, -8.8867e-02,  1.2266e+00,\n",
      "         -4.2578e-01,  1.9141e+00,  1.3379e-01, -3.2969e+00, -5.4688e+00,\n",
      "         -1.9453e+00,  1.4141e+00],\n",
      "        [-3.7695e-01, -1.9453e+00,  1.8677e-02, -3.8867e-01,  8.9844e-01,\n",
      "         -5.5469e-01,  1.8281e+00, -5.5859e-01, -4.6250e+00, -5.5938e+00,\n",
      "         -2.1250e+00,  1.3203e+00],\n",
      "        [-6.8359e-01, -2.1719e+00, -2.8250e+01, -1.1797e+00, -4.1260e-02,\n",
      "         -7.7344e-01,  1.7812e+00, -1.9609e+00, -7.4375e+00, -6.5000e+00,\n",
      "         -2.3750e+00,  1.2422e+00],\n",
      "        [ 3.5742e-01, -2.5977e-01,  3.5352e-01,  9.7656e-02, -3.6328e-01,\n",
      "          2.5195e-01,  2.6367e-01, -2.6562e-01, -1.2695e-01, -1.2109e-01,\n",
      "         -3.5742e-01, -5.2734e-02],\n",
      "        [ 1.3047e+00, -1.6328e+00,  4.5625e+00,  7.1484e-01,  3.1406e+00,\n",
      "          2.7812e+00, -2.5625e+00, -6.3125e+00, -4.3438e+00,  5.5625e+00,\n",
      "          7.1875e+00, -9.9609e-01],\n",
      "        [ 1.5078e+00, -1.6641e+00,  4.0625e+00,  1.0938e+00,  3.2344e+00,\n",
      "          1.4141e+00,  5.5469e-01, -5.5000e+00, -4.0312e+00,  5.4062e+00,\n",
      "          5.3438e+00,  1.2695e-01],\n",
      "        [ 1.4844e+00, -1.5625e+00,  3.7188e+00,  1.2188e+00,  3.1562e+00,\n",
      "          8.7500e-01,  1.2031e+00, -5.5000e+00, -4.0625e+00,  5.0625e+00,\n",
      "          4.3125e+00,  6.4453e-01],\n",
      "        [ 1.4375e+00, -1.5234e+00,  3.5156e+00,  1.1875e+00,  3.0625e+00,\n",
      "          5.0781e-01,  1.5312e+00, -5.5312e+00, -3.9531e+00,  4.7188e+00,\n",
      "          3.5938e+00,  8.7891e-01],\n",
      "        [ 1.3750e+00, -1.6797e+00,  3.3906e+00,  1.1250e+00,  2.9219e+00,\n",
      "          3.3398e-01,  1.6719e+00, -5.5000e+00, -4.1250e+00,  4.4688e+00,\n",
      "          3.0469e+00,  1.0703e+00],\n",
      "        [ 1.3047e+00, -1.6484e+00,  3.2656e+00,  1.0859e+00,  2.8750e+00,\n",
      "          8.5938e-02,  1.7734e+00, -5.4688e+00, -3.9844e+00,  4.1875e+00,\n",
      "          2.6250e+00,  1.1328e+00],\n",
      "        [ 1.2109e+00, -1.6797e+00,  3.1875e+00,  1.0000e+00,  2.8125e+00,\n",
      "          6.9824e-02,  1.8750e+00, -5.5000e+00, -4.0625e+00,  3.9688e+00,\n",
      "          2.3125e+00,  1.1797e+00],\n",
      "        [ 1.1172e+00, -1.6719e+00,  2.9844e+00,  8.8281e-01,  2.7031e+00,\n",
      "         -2.3193e-02,  1.9609e+00, -5.4062e+00, -4.0000e+00,  3.4844e+00,\n",
      "          1.5547e+00,  1.2812e+00],\n",
      "        [ 9.4922e-01, -1.7109e+00,  2.7031e+00,  7.1484e-01,  2.4844e+00,\n",
      "         -2.2363e-01,  2.0625e+00, -5.2812e+00, -4.0312e+00,  2.8125e+00,\n",
      "          6.7578e-01,  1.3672e+00],\n",
      "        [ 7.8516e-01, -1.7500e+00,  2.4062e+00,  4.4531e-01,  2.2188e+00,\n",
      "         -3.1836e-01,  2.1094e+00, -5.2812e+00, -4.0938e+00,  2.1719e+00,\n",
      "         -4.3213e-02,  1.3750e+00],\n",
      "        [ 5.6250e-01, -1.7656e+00,  2.0625e+00,  1.5137e-01,  1.8984e+00,\n",
      "         -4.4531e-01,  2.0781e+00, -5.1250e+00, -3.9844e+00,  1.3672e+00,\n",
      "         -5.3516e-01,  1.3906e+00],\n",
      "        [ 3.4180e-01, -1.8281e+00,  1.6719e+00, -1.3867e-01,  1.5859e+00,\n",
      "         -5.8203e-01,  2.0781e+00, -5.0938e+00, -3.9375e+00,  5.8203e-01,\n",
      "         -1.0312e+00,  1.3672e+00],\n",
      "        [ 1.5723e-01, -1.8516e+00,  1.1562e+00, -3.7109e-01,  1.1953e+00,\n",
      "         -6.6016e-01,  2.0156e+00, -4.9688e+00, -4.0000e+00, -2.0215e-01,\n",
      "         -1.3438e+00,  1.2891e+00],\n",
      "        [-5.1758e-02, -1.8828e+00,  7.1094e-01, -6.6016e-01,  7.9297e-01,\n",
      "         -7.3047e-01,  1.9375e+00, -4.8750e+00, -4.0000e+00, -8.4766e-01,\n",
      "         -1.5938e+00,  1.2578e+00],\n",
      "        [-4.0234e-01, -1.9922e+00, -2.0605e-01, -1.2344e+00, -1.5918e-01,\n",
      "         -8.1641e-01,  1.8984e+00, -4.7500e+00, -3.9844e+00, -1.6250e+00,\n",
      "         -1.9062e+00,  1.1719e+00]], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.0.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 0.1079,  0.0972,  0.1182,  0.0908,  0.0991,  0.1040,  0.1348,  0.1240,\n",
      "         0.1177,  0.1025,  0.1011,  0.1289,  0.1328,  0.1245,  0.1001,  0.1074,\n",
      "         0.1279,  0.1118,  0.1396,  0.1050,  0.1025,  0.1040,  0.1143,  0.0938,\n",
      "         0.0962,  0.1279,  0.1050,  0.1416,  0.1240,  0.1064,  0.1113,  0.0767,\n",
      "         0.1113,  0.0991,  0.1270,  0.1123,  0.1455,  0.1064,  0.0977,  0.1367,\n",
      "         0.1245,  0.1318,  0.1157,  0.1221,  0.1187,  0.1074,  0.1592,  0.1089,\n",
      "         0.1152,  0.1348,  0.1426,  0.1104,  0.1279,  0.0996,  0.0583,  0.1143,\n",
      "         0.0889,  0.1133,  0.1157,  0.1045,  0.1157,  0.1045,  0.1245,  0.1045,\n",
      "         0.1099,  0.0703,  0.1147,  0.1079,  0.2656,  0.0952,  0.1240,  0.1289,\n",
      "         0.0991,  0.1143,  0.1006,  0.1025,  0.1001,  0.1064,  0.0913,  0.1250,\n",
      "         0.1079,  0.1108,  0.1079,  0.1562,  0.1084,  0.1074,  0.1494,  0.1162,\n",
      "         0.1069,  0.0981,  0.1060,  0.0938,  0.1025,  0.1167,  0.1030,  0.1084,\n",
      "         0.1162,  0.1138,  0.0972,  0.0991,  0.1328,  0.1094,  0.1187,  0.1001,\n",
      "         0.1084,  0.2227,  0.1084,  0.1094,  0.1030,  0.1406,  0.1187,  0.1074,\n",
      "         0.1074,  0.1074,  0.0996,  0.1060,  0.1021,  0.0835,  0.4707,  0.1050,\n",
      "         0.0903,  0.0957,  0.1797,  0.1045,  0.1318,  0.1094,  0.1133,  0.0986,\n",
      "         0.1113,  0.1279,  0.1270,  0.1260,  0.0933,  0.1040,  0.0972,  0.1543,\n",
      "         0.1143,  0.1108,  0.1177,  0.1040,  0.1035,  0.1260,  0.1426,  0.1885,\n",
      "         0.1133,  0.1201,  0.1025,  0.1338,  0.1069,  0.1074,  0.0991,  0.1069,\n",
      "         0.1206,  0.1719,  0.1060,  0.0981,  0.1309,  0.0947,  0.1406,  0.0996,\n",
      "         0.0938,  0.1147,  0.1328,  0.1309,  0.0840,  0.0771,  0.1055,  0.1426,\n",
      "         0.0903,  0.1396,  0.1895,  0.1045,  0.1108,  0.1572,  0.1089,  0.1270,\n",
      "         0.1084,  0.1475,  0.1338,  0.1250,  0.1123,  0.0874,  0.1055,  0.1001,\n",
      "         0.1348,  0.1523,  0.1055,  0.1128,  0.1035,  0.1055,  0.1016,  0.1055,\n",
      "         0.0962,  0.1064,  0.0898,  0.1074,  0.1021,  0.1235,  0.1138,  0.0967,\n",
      "         0.0947,  0.1074,  0.1206,  0.1060,  0.1143, -0.1406,  0.1084,  0.1099,\n",
      "         0.0996,  0.1064,  0.1338,  0.1030,  0.1152,  0.1270,  0.1011,  0.1270,\n",
      "         0.1089,  0.1069,  0.1206,  0.1543,  0.1187,  0.1079,  0.0874,  0.1030,\n",
      "         0.1113,  0.1055,  0.0903,  0.1426,  0.1074,  0.1270,  0.1118,  0.1226,\n",
      "         0.1328,  0.1021,  0.0986,  0.1167,  0.1348,  0.1226,  0.0942,  0.1396,\n",
      "         0.1367,  0.1279,  0.1050,  0.1118,  0.0933,  0.1069,  0.1113,  0.1206,\n",
      "         0.0840,  0.1235,  0.1309,  0.0991,  0.1011,  0.1021,  0.1040,  0.1069,\n",
      "         0.1104,  0.0806,  0.1279,  0.0942,  0.1436,  0.1445,  0.1064,  0.1138,\n",
      "         0.1099,  0.1069,  0.1001,  0.1021,  0.0981,  0.1089,  0.0962,  0.2012,\n",
      "         0.1318,  0.1187,  0.1514,  0.1191,  0.1128,  0.1289,  0.1123,  0.1104,\n",
      "        -0.1680,  0.1602,  0.1143,  0.1226,  0.1196,  0.1553,  0.1025,  0.1040,\n",
      "         0.0952,  0.1855,  0.0918,  0.1074,  0.0996,  0.1074,  0.1108,  0.0986,\n",
      "         0.1250,  0.1147,  0.0967,  0.1318,  0.1299,  0.1084,  0.2031,  0.1201,\n",
      "         0.1206,  0.1055,  0.1123,  0.1064,  0.1167,  0.1050,  0.0981,  0.1289,\n",
      "         0.0942,  0.1436,  0.1270,  0.1021,  0.1387,  0.1523,  0.1162,  0.1030,\n",
      "         0.0889,  0.1299,  0.1030,  0.0898,  0.1245,  0.0957,  0.1270,  0.1533,\n",
      "         0.1196,  0.1055,  0.1113,  0.1094,  0.1328,  0.1030,  0.1099,  0.1079,\n",
      "         0.1045,  0.1025,  0.1035,  0.1973,  0.1162,  0.1250,  0.1211,  0.1079,\n",
      "         0.1157,  0.1030,  0.1279,  0.1108,  0.0830,  0.1348,  0.1328,  0.0771,\n",
      "         0.1021,  0.1035,  0.1206,  0.1221,  0.1206,  0.2852,  0.1025,  0.1338,\n",
      "         0.1050,  0.1455,  0.0830,  0.0874,  0.0981,  0.1104,  0.1357,  0.1172,\n",
      "         0.1021,  0.1040,  0.1299,  0.1289,  0.1465,  0.1084,  0.1318,  0.0952,\n",
      "         0.1357,  0.0894,  0.1426,  0.1533,  0.1079,  0.1138,  0.1934,  0.1055,\n",
      "         0.1069,  0.1445,  0.0977,  0.1455,  0.0884,  0.1250,  0.1050,  0.1318,\n",
      "         0.1069,  0.2119,  0.1387,  0.1143,  0.1216,  0.1016,  0.1074,  0.1089,\n",
      "         0.0957,  0.1387,  0.1055,  0.1157,  0.0903,  0.1138,  0.1553,  0.0913,\n",
      "         0.0981,  0.0962,  0.1045,  0.1006,  0.1196,  0.1138,  0.1025,  0.1094,\n",
      "         0.1030,  0.0962,  0.0669,  0.1289,  0.1455,  0.1113,  0.0996,  0.2910,\n",
      "         0.1123,  0.1045,  0.1289,  0.1011,  0.1113,  0.1050,  0.1328,  0.1060,\n",
      "         0.1504,  0.0903,  0.1445,  0.1089,  0.1074,  0.1055,  0.1240,  0.1426,\n",
      "         0.1226,  0.1406,  0.1270,  0.1426,  0.1025,  0.1123,  0.0991,  0.1270,\n",
      "         0.2715,  0.1123, -0.1011,  0.1069,  0.1099,  0.1064,  0.1089,  0.1270,\n",
      "         0.1670,  0.0732,  0.1230,  0.0986,  0.0981,  0.0977,  0.1099,  0.1157,\n",
      "         0.1182,  0.1143,  0.1201,  0.1094,  0.0820,  0.0967,  0.0923,  0.1001,\n",
      "         0.0713,  0.0854,  0.1099,  0.1074,  0.1128,  0.1035,  0.1348,  0.1104,\n",
      "         0.1226,  0.1455,  0.1182,  0.0967,  0.2402,  0.0986,  0.0962,  0.1074,\n",
      "         0.1016,  0.1108,  0.1152,  0.1138,  0.1357,  0.1128,  0.1045,  0.1387,\n",
      "         0.1040,  0.1147,  0.0991,  0.0996,  0.0923,  0.1318,  0.1191,  0.1406,\n",
      "         0.1099,  0.1299,  0.1147,  0.1230,  0.0903,  0.1113,  0.1128,  0.1050,\n",
      "         0.1172,  0.0957,  0.1221,  0.1099,  0.1089,  0.1094,  0.2812,  0.1328,\n",
      "         0.1021,  0.1089,  0.0957,  0.1182,  0.1118,  0.1348,  0.1187,  0.1816,\n",
      "         0.1167,  0.1260,  0.1455,  0.1030,  0.1162,  0.1133,  0.1123,  0.1260,\n",
      "         0.1196,  0.1055,  0.1172,  0.1221,  0.1191,  0.1035,  0.1187,  0.1387,\n",
      "         0.1108,  0.1016,  0.1177,  0.1079,  0.1011,  0.1348,  0.1406,  0.1953,\n",
      "         0.0933,  0.1982,  0.0962,  0.1143,  0.1260,  0.1001,  0.1143,  0.1211,\n",
      "         0.1279,  0.0996,  0.1089,  0.1367,  0.1177,  0.1748,  0.1523,  0.1128,\n",
      "         0.1309,  0.0996,  0.1167,  0.1279,  0.0928,  0.1040,  0.1011,  0.1299,\n",
      "         0.1113,  0.1338,  0.0957,  0.3516,  0.1191,  0.0981,  0.1045,  0.1040,\n",
      "         0.0894,  0.0977,  0.0981,  0.0962,  0.1021,  0.0981,  0.1079,  0.1270,\n",
      "         0.1123,  0.1104,  0.1045,  0.1177,  0.1064,  0.1147,  0.1611,  0.1104,\n",
      "         0.1338,  0.1069,  0.1069,  0.1064,  0.1318,  0.0991,  0.0947, -0.0996,\n",
      "         0.1240,  0.1157,  0.1118,  0.0928,  0.1128,  0.1006,  0.0947,  0.1245,\n",
      "         0.0977,  0.0688,  0.1030,  0.1094,  0.1143,  0.0972,  0.1084,  0.1445,\n",
      "         0.1074,  0.0957,  0.0981,  0.1387,  0.1104,  0.1328,  0.1152,  0.1040,\n",
      "         0.1338,  0.1650,  0.1064,  0.1416,  0.1133,  0.1167,  0.1260,  0.1069,\n",
      "         0.1299,  0.1143,  0.0947,  0.1108,  0.1089,  0.1128,  0.1016,  0.0957,\n",
      "         0.0854,  0.1172,  0.0972,  0.1045,  0.1128,  0.1270,  0.1406,  0.1396,\n",
      "         0.1045,  0.1167,  0.1182,  0.1260,  0.1367,  0.1118,  0.1118,  0.1875,\n",
      "         0.1040,  0.1250,  0.1074,  0.1035,  0.1113,  0.1113,  0.1060,  0.1250,\n",
      "         0.1030,  0.0913,  0.1089,  0.1025,  0.1099,  0.1074,  0.1069,  0.1196,\n",
      "         0.1084,  0.1050,  0.1147,  0.0806,  0.1060,  0.1104,  0.1064,  0.1172,\n",
      "         0.0977,  0.1128,  0.1016,  0.1187,  0.1797,  0.1016,  0.1064,  0.1011,\n",
      "         0.1260,  0.1016,  0.1177,  0.1187,  0.1108,  0.0942,  0.1074,  0.0962,\n",
      "         0.1089,  0.1011,  0.1064,  0.1758,  0.2002,  0.1035,  0.1309,  0.1162,\n",
      "         0.0986,  0.0986,  0.1963,  0.1099,  0.1206,  0.1250,  0.1001,  0.1318,\n",
      "         0.0996,  0.1338,  0.1455,  0.0981,  0.1196,  0.1074,  0.1216,  0.0664,\n",
      "         0.1040,  0.1289,  0.1138,  0.1104,  0.1050,  0.0972,  0.1152,  0.1143,\n",
      "         0.1123,  0.1016,  0.1011,  0.1216,  0.1138,  0.1387,  0.1260,  0.1533,\n",
      "         0.1035,  0.1387,  0.1001,  0.0947,  0.1387,  0.1089,  0.1104,  0.0938,\n",
      "         0.1445,  0.1182,  0.1309,  0.1143,  0.1367,  0.0908,  0.1582,  0.1147,\n",
      "         0.0928,  0.0903,  0.1099,  0.0894,  0.1172,  0.1157,  0.1045,  0.1279],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.0.layer.1.DenseReluDense.wi.weight\n",
      "Parameter containing:\n",
      "tensor([[ 1.4258e-01,  1.9531e-01, -3.5645e-02,  ...,  4.9219e-01,\n",
      "         -1.7383e-01,  3.8300e-03],\n",
      "        [ 1.9531e-01, -1.7480e-01, -6.5234e-01,  ..., -6.2500e-01,\n",
      "          1.3379e-01, -4.4922e-01],\n",
      "        [-1.2891e-01,  1.0840e-01, -1.3770e-01,  ...,  2.9688e-01,\n",
      "         -1.1865e-01,  6.1951e-03],\n",
      "        ...,\n",
      "        [ 5.5078e-01, -5.0000e-01, -7.1335e-04,  ..., -3.7305e-01,\n",
      "          6.2891e-01, -3.8477e-01],\n",
      "        [ 3.0469e-01, -5.7861e-02,  2.6562e-01,  ...,  4.7266e-01,\n",
      "         -5.5078e-01, -7.9297e-01],\n",
      "        [ 1.1963e-01, -3.6328e-01,  1.5820e-01,  ...,  2.5781e-01,\n",
      "         -3.3594e-01, -1.6895e-01]], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.0.layer.1.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0106,  0.2275,  0.0757,  ..., -0.1611, -0.1963, -0.4082],\n",
      "        [-0.1465, -0.1816,  0.2139,  ..., -0.0586,  0.0127,  0.2461],\n",
      "        [-0.0282,  0.0229,  0.2451,  ...,  0.1279,  0.1455, -0.1973],\n",
      "        ...,\n",
      "        [-0.2500, -0.2422, -0.1475,  ...,  0.1553, -0.1245, -0.2500],\n",
      "        [-0.4551, -0.3594,  0.1865,  ...,  0.1279,  0.1943, -0.2002],\n",
      "        [ 0.0288,  0.1118, -0.0928,  ..., -0.1953,  0.2305,  0.0903]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.0.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 0.3125,  0.3906,  0.3652,  0.4824,  0.2305,  0.2969,  0.2559,  0.3125,\n",
      "         0.3867,  0.2637,  0.3242,  0.2852,  0.3516,  0.4004,  0.2158,  0.2520,\n",
      "         0.3457,  0.3320, -0.4316,  0.2402,  0.2578,  0.2637,  0.1216,  0.2559,\n",
      "         0.2266,  0.3652,  0.2236,  0.4082,  0.2539,  0.2236,  0.2480,  0.5508,\n",
      "         0.3066,  0.2207,  0.3320,  0.2617,  0.3301,  0.5898,  0.2197,  0.3359,\n",
      "         0.4043,  0.3516,  0.3398,  0.2734, -0.5508,  0.2832,  0.4258,  0.2559,\n",
      "         0.2832,  0.3418,  0.4258,  0.2793,  0.3262,  0.2324,  0.0605,  0.3145,\n",
      "         0.3535,  0.2715,  0.2461,  0.2441,  0.2812, -0.2041,  0.2910,  0.2246,\n",
      "         0.2197,  0.2080,  0.2451,  0.2598,  0.6016,  0.2480,  0.2480,  0.3262,\n",
      "         0.2139,  0.2793,  0.2354,  0.2314,  0.3926,  0.2852,  0.2383,  0.2891,\n",
      "         0.2500,  0.3047,  0.2656,  0.5117,  0.2734,  0.2578,  1.1406,  0.3027,\n",
      "         0.2539,  0.2334,  0.2930,  0.2324,  0.2539,  0.3242,  0.2256,  0.2441,\n",
      "         0.2334,  0.2373,  0.2148,  0.2520,  0.3652,  0.2676,  0.4297,  0.2100,\n",
      "         0.2441,  0.5586,  0.2695,  0.2432,  0.2314,  0.3457,  0.2910,  0.2871,\n",
      "         0.2852,  0.2598,  0.2520,  0.2314,  0.2871,  0.2539,  0.9883,  0.2432,\n",
      "         0.1904,  0.2314,  0.5000,  0.2520,  0.3340,  0.2793,  0.2754,  0.2988,\n",
      "         0.3086,  0.4375,  0.4648,  0.3047,  0.2324,  0.2227,  0.2832,  0.4355,\n",
      "         0.3164,  0.2773,  0.3008,  0.3262,  0.2812,  0.3359,  0.3848,  0.4746,\n",
      "         0.3184,  0.4316,  0.2363,  0.3711,  0.2383,  0.2969,  0.2441,  0.2754,\n",
      "         0.2930,  0.4922,  0.2188,  0.2178,  0.3301,  0.2021,  0.4395,  0.3027,\n",
      "         0.2207,  0.2754,  0.3418,  0.3125,  0.2383,  0.2695,  0.2451,  0.4297,\n",
      "         0.3242,  0.3379,  0.5156, -0.2617,  0.2422,  0.3594,  0.2598,  0.3105,\n",
      "         0.2852,  0.3926,  0.4336,  0.0981,  0.2324,  0.2373,  0.2617,  0.2100,\n",
      "         0.3848,  0.3906,  0.2490,  0.3887,  0.2793,  0.3184,  0.2578,  0.2393,\n",
      "         0.2334,  0.2559,  0.2314,  0.3359,  0.2539,  0.3105,  0.2871,  0.2539,\n",
      "         0.2148,  0.3203,  0.2773,  0.2520,  0.2490,  0.4355,  0.3047,  0.2852,\n",
      "         0.2598,  0.2715,  0.4570,  0.2891,  0.3125,  0.2930,  0.2637,  0.3027,\n",
      "         0.2734,  0.2412, -0.3105,  1.3516,  0.3730,  0.2832,  0.2197,  0.2988,\n",
      "         0.2930,  0.2617,  0.4102,  0.3457,  0.2715,  0.3301,  0.3125,  0.3047,\n",
      "         0.3027,  0.2031,  0.2158, -0.2539,  0.0952,  0.3418,  0.4336,  0.3301,\n",
      "         1.1250,  0.3242,  0.2373,  0.2598,  0.2402,  0.2578,  0.2578,  0.3145,\n",
      "         0.2080,  0.3594,  0.3477,  0.2344,  0.2490,  0.2168,  0.2139,  0.2656,\n",
      "         0.2363,  0.2910,  0.3711,  0.2656,  0.4883,  0.2539,  0.2393,  0.2539,\n",
      "         0.2598,  0.2715,  0.2559,  0.2656,  0.2539,  0.2539,  0.3711,  0.6133,\n",
      "         0.3613,  0.3438,  0.5469,  0.3066,  0.2471,  0.3750,  0.2441,  0.3281,\n",
      "        -0.8281,  0.4785,  0.2539,  0.3047,  0.3125,  0.3848,  0.2061,  0.2217,\n",
      "         0.2871,  0.5703,  0.2148,  0.2637,  0.2197,  0.2324,  0.2285,  0.2012,\n",
      "         0.4141,  0.2246,  0.2217,  0.3066,  0.3008,  0.2266,  0.6094,  0.3164,\n",
      "         0.2793,  0.2227,  0.2930,  0.2402,  0.3320,  0.3613,  0.2598,  0.3281,\n",
      "         0.2402,  0.3828,  0.3496,  0.2793,  0.3223,  0.6914,  0.2949,  0.2354,\n",
      "         0.2324,  0.3281,  0.3027,  0.2090,  0.3789,  0.2461,  0.3672,  0.3945,\n",
      "         0.3301,  0.2539,  0.2344,  0.3809,  0.3418,  0.2559,  0.2441,  0.2490,\n",
      "         0.2227,  0.2236,  0.3125,  0.6484,  0.2891,  0.2969,  0.2930,  0.2520,\n",
      "         0.2891,  0.2812,  0.3242,  0.3125,  0.3750,  0.4160,  0.4336,  0.2451,\n",
      "         0.2852,  0.2441,  0.3691,  0.2695,  0.2871,  0.6211,  0.2188,  0.3633,\n",
      "         0.2539,  0.4180,  0.5234,  0.2256,  0.2139,  0.2637,  0.3984,  0.2793,\n",
      "         0.2334,  0.2871,  0.2910,  0.3320,  0.1240,  0.2832,  0.3574,  0.2070,\n",
      "         0.3730,  0.2432,  0.3516,  0.4980,  0.2578,  0.2949,  0.5195,  0.2598,\n",
      "         0.2334,  0.3984,  0.2441,  0.3555,  0.2129,  0.3027,  0.2354,  0.3496,\n",
      "         0.3164,  0.5703,  0.3652,  0.2734,  0.3008,  0.2217,  0.2969,  0.2344,\n",
      "         0.2637,  0.0574,  0.2139,  0.2256,  0.2480,  0.2578,  0.4258,  0.2539,\n",
      "         0.2314,  0.2402,  0.2412,  0.2207,  0.3086,  0.3047,  0.2295,  0.2793,\n",
      "         0.2490,  0.2324,  0.2002,  0.3398,  1.3281,  0.2871,  0.2119,  0.3926,\n",
      "         0.2871,  0.2480,  0.3398,  0.2227,  0.3047,  0.3555,  0.3672,  0.2373,\n",
      "         0.3809,  0.2314,  0.3926,  0.2754,  0.2637,  0.2314,  0.2988,  0.3945,\n",
      "         0.2812,  0.3828,  0.3926,  0.3477,  0.2363,  0.3105,  0.2314,  0.3105,\n",
      "         0.6797,  0.2812,  0.3027,  0.2295,  0.2656,  0.2324,  0.2910,  0.3633,\n",
      "         0.5195,  0.3633,  0.3516,  0.2266,  0.2148,  0.2334,  0.2500,  0.2656,\n",
      "         0.2793,  0.2539,  0.2910,  0.2734,  0.2227,  0.2363,  0.2295,  0.2178,\n",
      "         0.2422,  0.2168,  0.2715,  0.2412,  0.2324,  0.2373,  0.3027,  0.2812,\n",
      "         0.3594,  0.3984,  0.2656,  0.2559,  1.0469,  0.2500,  0.2266,  0.3789,\n",
      "         0.2012,  0.2871,  0.2598,  0.2949,  0.3262,  0.2256,  0.2754,  1.2500,\n",
      "         0.2949,  0.2734,  0.2559,  0.2266,  0.2295,  0.4434,  0.3125,  0.3633,\n",
      "         0.2559,  0.3438,  0.2949,  0.3379,  0.2021,  0.3262,  0.3066,  0.2207,\n",
      "         0.3906,  0.2402,  0.3320,  0.2266,  0.2949,  0.2988,  0.6055,  0.2969,\n",
      "         0.2363,  0.2988,  0.2197,  0.2852,  0.2656,  0.3945,  0.3340,  0.4258,\n",
      "         0.2734,  0.3320,  0.4219,  0.2490,  0.3066,  0.2637,  0.3242,  0.2891,\n",
      "         0.3008,  0.2773,  0.3516,  0.3125,  0.3164,  0.2344,  0.3555,  0.3691,\n",
      "         0.2471,  0.2402,  0.3184,  0.2500,  0.2617,  0.3555,  0.4766,  0.4629,\n",
      "         0.2197,  0.8008,  0.2441,  0.2500,  0.3555,  0.2598,  0.3184,  0.3555,\n",
      "         0.3125,  0.2754,  0.5703,  0.9570,  0.2793,  0.3809,  0.0693,  0.3105,\n",
      "         0.3555,  0.2461,  0.3027,  0.1738,  0.2090,  0.2559,  0.2930,  0.3418,\n",
      "         0.2324,  0.3418,  0.2148,  0.7812,  0.3164,  0.2197,  0.2246,  0.2910,\n",
      "         0.2129,  0.2109,  0.2305,  0.2402,  0.2188,  0.2354,  0.2480,  0.2988,\n",
      "         0.2324,  0.2910,  0.2988,  0.2559,  0.3828,  0.2871,  0.5664,  0.2695,\n",
      "         0.4336,  0.2314,  0.3086,  0.3320,  0.3223,  0.2080,  0.2217,  0.4043,\n",
      "         0.3125,  0.2715, -0.2754,  0.2412,  0.3066,  0.2324,  0.2656,  0.3027,\n",
      "         0.2676,  0.3926,  0.2490,  0.2793,  0.2891,  0.2402,  0.2393,  0.4238,\n",
      "         0.2891,  0.2520,  0.2910,  0.3594,  0.3027,  0.3906,  0.3613,  0.2578,\n",
      "         0.3711,  1.4531,  0.2217,  0.4102,  0.3262,  0.2773,  0.3359,  0.2539,\n",
      "        -0.4180,  0.2969,  0.2451,  0.2637,  0.3887,  0.3281,  0.4043,  0.2158,\n",
      "         0.2520,  0.2256,  0.2988,  0.2539,  0.2793,  0.3008,  0.3262,  0.5352,\n",
      "         0.2617,  0.3340,  0.3223,  0.3438,  0.3262,  0.0854,  0.3066,  0.4863,\n",
      "         0.2695,  0.3555,  0.2656,  0.2148,  0.2812,  0.2402,  0.2656,  0.2910,\n",
      "         0.2676,  0.2559,  0.2930,  0.2197,  0.2949,  0.2539,  0.2539,  0.2598,\n",
      "         0.2969,  0.2471,  0.3223,  0.2266, -0.3125,  0.3555, -0.2754,  0.3477,\n",
      "         0.2109,  0.2891, -0.2188,  0.3223,  0.5117,  0.2373,  0.2500,  0.2910,\n",
      "         0.2734,  0.2051,  0.2656,  0.3125,  0.2871, -0.2285,  0.2812,  0.3184,\n",
      "         0.2754,  0.2109,  0.2432,  0.5586,  0.5273, -0.2246,  0.5312,  0.2832,\n",
      "         0.2539,  0.2461,  0.3418,  0.2969,  0.3105,  0.3496,  0.2988,  0.3711,\n",
      "         0.2461,  0.3262,  0.4473,  0.2305,  0.2852,  0.3828,  0.3340,  0.2539,\n",
      "         0.2275,  0.3359,  0.3047,  0.2334,  0.2383,  0.3066,  0.2793,  0.2637,\n",
      "         0.2617,  0.2676,  0.2734,  0.3066,  0.2871,  0.4180,  0.3359,  0.3965,\n",
      "         0.2471,  0.4121,  0.3086,  0.2148,  0.3145,  0.4082,  0.2910,  0.2539,\n",
      "         0.4688,  0.2910,  0.2949,  0.2422,  0.3398,  0.2393,  0.4199,  0.2910,\n",
      "         0.3340,  0.2461,  0.2969,  0.2275,  0.3027,  0.3301,  0.2578,  0.3691],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.1.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[ 8.3618e-03, -2.5757e-02, -1.1597e-02,  ...,  2.2705e-02,\n",
      "          5.8413e-05, -1.0681e-02],\n",
      "        [ 2.8564e-02,  4.6631e-02,  2.1240e-02,  ..., -2.4780e-02,\n",
      "         -6.1951e-03, -2.2949e-02],\n",
      "        [-6.6406e-02, -1.6113e-02, -7.9102e-02,  ..., -2.4292e-02,\n",
      "          4.8218e-03, -3.8086e-02],\n",
      "        ...,\n",
      "        [-4.6082e-03, -3.1982e-02,  4.3945e-03,  ...,  5.4932e-02,\n",
      "         -2.1729e-02,  1.7578e-02],\n",
      "        [-4.1504e-03, -4.8096e-02, -3.2959e-02,  ..., -3.9307e-02,\n",
      "          4.4678e-02, -2.4414e-02],\n",
      "        [-2.4414e-02, -1.7334e-02,  1.5259e-02,  ...,  1.4771e-02,\n",
      "         -8.5938e-02,  2.2278e-03]], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.1.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1631,  0.3965,  0.2930,  ...,  0.1787,  0.3633,  0.3711],\n",
      "        [ 0.2305,  0.8047,  0.2871,  ...,  0.1289, -0.4648, -0.3848],\n",
      "        [-0.2500,  0.1191, -0.4043,  ..., -0.2227, -0.2969, -0.7227],\n",
      "        ...,\n",
      "        [ 0.1270, -0.1543, -0.2363,  ...,  0.0430, -0.3320, -0.0315],\n",
      "        [ 0.0344, -0.1187, -0.6133,  ..., -0.0898,  0.5195,  0.1260],\n",
      "        [-0.0479, -0.1309, -0.1025,  ...,  0.2695, -0.5000, -0.1484]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.1.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.4004, -0.1592, -0.1660,  ...,  0.1050, -0.0104, -0.1455],\n",
      "        [-0.5781, -0.0547,  0.2305,  ..., -0.8633,  0.0688,  0.3184],\n",
      "        [-0.7031, -0.1074,  0.4062,  ..., -0.8047,  0.0342, -0.5234],\n",
      "        ...,\n",
      "        [ 0.1553, -0.4805,  0.7031,  ..., -0.3457,  0.1494,  0.1777],\n",
      "        [ 0.0199, -0.1758,  0.1934,  ..., -0.2480,  0.2441, -0.0713],\n",
      "        [-0.3008, -0.3418, -0.0461,  ..., -0.2363, -0.3086, -0.3340]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.1.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.1123,  0.0086,  0.0034,  ..., -0.0771, -0.2715,  0.4141],\n",
      "        [ 0.1650, -0.1396,  0.3496,  ...,  0.7500, -0.4980,  0.4844],\n",
      "        [ 0.0569, -0.2246, -0.5234,  ..., -0.3496,  0.0703,  0.6289],\n",
      "        ...,\n",
      "        [-0.2100,  0.1289,  0.5547,  ...,  0.0250, -0.0977, -0.1514],\n",
      "        [ 0.1245, -0.1289, -0.1680,  ..., -0.4844,  0.0259, -0.7383],\n",
      "        [ 0.2637, -0.3164,  0.3555,  ..., -0.9258, -0.5547, -0.3086]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.1.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 0.1572,  0.1631,  0.1699,  0.1787,  0.1235,  0.1436,  0.0503,  0.1729,\n",
      "         0.1221,  0.1426,  0.1572,  0.1689,  0.1211,  0.1738,  0.1196,  0.1436,\n",
      "         0.1719,  0.1641,  0.2178,  0.1328,  0.1387,  0.1416,  0.0537,  0.1206,\n",
      "         0.1235,  0.1748,  0.1206,  0.1533,  0.1631,  0.1230,  0.1426,  0.1260,\n",
      "         0.1533,  0.1182,  0.1816,  0.1514,  0.1738,  0.2451,  0.1113,  0.2031,\n",
      "         0.1797,  0.1787,  0.1895,  0.1631,  0.1074,  0.1387,  0.1719,  0.1377,\n",
      "         0.1611,  0.1660,  0.2129,  0.1562,  0.1709,  0.1230,  0.0391,  0.1387,\n",
      "         0.0767,  0.1592,  0.1387,  0.1348,  0.1592,  0.1177,  0.1543,  0.1270,\n",
      "         0.1328,  0.0530,  0.1416,  0.1299,  0.2637,  0.1279,  0.1484,  0.2051,\n",
      "         0.1221,  0.1465,  0.1426,  0.1309,  0.1816,  0.1475,  0.1201,  0.1709,\n",
      "         0.1289,  0.1523,  0.1387,  0.2061,  0.1426,  0.1494,  0.2578,  0.1631,\n",
      "         0.1484,  0.1289,  0.1592,  0.1177,  0.1260,  0.1602,  0.1260,  0.1182,\n",
      "         0.1348,  0.1260,  0.1328,  0.1226,  0.1875,  0.1494,  0.1816,  0.1201,\n",
      "         0.1445,  0.2559,  0.1436,  0.1328,  0.1201,  0.1768,  0.1621,  0.1475,\n",
      "         0.1387,  0.1299,  0.1377,  0.1245,  0.1465,  0.1226,  0.3164,  0.1299,\n",
      "         0.1128,  0.1299,  0.2129,  0.1387,  0.2012,  0.1426,  0.1484,  0.1426,\n",
      "         0.1494,  0.1787,  0.1465,  0.1719,  0.1182, -0.1167,  0.1377,  0.2041,\n",
      "         0.1699,  0.1357,  0.1562,  0.1650,  0.1455,  0.1846,  0.2021,  0.2100,\n",
      "         0.1611,  0.1709,  0.1245,  0.1787,  0.1328,  0.1562,  0.1396,  0.1484,\n",
      "         0.1514,  0.2314,  0.1260,  0.1309,  0.1709,  0.1206,  0.1807,  0.1357,\n",
      "         0.1128, -0.1436,  0.1348,  0.1060,  0.1152,  0.1133,  0.1396,  0.1738,\n",
      "         0.1455,  0.2139,  0.2402,  0.1475,  0.1406,  0.2002,  0.1553,  0.1680,\n",
      "         0.1553,  0.2158,  0.1846,  0.0089,  0.1348,  0.1260,  0.1455,  0.1211,\n",
      "         0.1768,  0.2041,  0.1279,  0.1904,  0.1494,  0.1553,  0.1289,  0.1328,\n",
      "         0.0996,  0.1475,  0.1206,  0.0645,  0.1406,  0.1807,  0.1572,  0.1167,\n",
      "         0.1211,  0.1592,  0.1494,  0.1387,  0.1387,  0.1973,  0.1426,  0.1426,\n",
      "         0.1348,  0.1475,  0.1953,  0.1367,  0.1689,  0.1719,  0.1357,  0.1709,\n",
      "         0.1504,  0.1357,  0.1719,  0.2793,  0.1387,  0.1533,  0.1079,  0.1494,\n",
      "         0.1533,  0.1367,  0.1855,  0.2100,  0.1426,  0.1768,  0.1445,  0.1553,\n",
      "         0.1050,  0.1201,  0.1211,  0.0923,  0.0679,  0.1797,  0.1504,  0.1943,\n",
      "         0.2441,  0.1816,  0.1406,  0.1504,  0.1177,  0.1309,  0.1465,  0.1553,\n",
      "         0.1089,  0.1582,  0.1221,  0.1240,  0.1211,  0.1191,  0.1147,  0.1299,\n",
      "         0.1377,  0.1230,  0.1572,  0.1387,  0.2158,  0.1865,  0.1309,  0.1426,\n",
      "         0.1377,  0.1377,  0.1338,  0.1328,  0.1416,  0.1387,  0.0688,  0.2500,\n",
      "         0.1895,  0.0718,  0.2314,  0.1621,  0.1348,  0.1953,  0.1436,  0.1543,\n",
      "         0.2754,  0.2129,  0.1387,  0.1621,  0.1699,  0.1875,  0.1138,  0.1328,\n",
      "         0.1387,  0.2158,  0.1157,  0.1445,  0.1221,  0.1377,  0.1367,  0.1128,\n",
      "         0.1982,  0.1348,  0.1245,  0.1846,  0.1777,  0.1387,  0.2812,  0.1641,\n",
      "         0.1553,  0.1240,  0.1445,  0.1279,  0.1650,  0.1670,  0.1338,  0.1875,\n",
      "         0.1289,  0.1357,  0.1904,  0.1553,  0.1240,  0.2891,  0.1465,  0.1270,\n",
      "         0.1162,  0.1924,  0.1494,  0.1187,  0.1807,  0.1235,  0.1973,  0.2256,\n",
      "         0.1631,  0.1484,  0.1328,  0.1709,  0.1670,  0.1299,  0.1377,  0.1475,\n",
      "         0.1191,  0.1367,  0.1494,  0.2617,  0.1699,  0.1758,  0.1621,  0.1309,\n",
      "         0.1533,  0.1455,  0.1680,  0.1504,  0.1611,  0.1826,  0.2080,  0.1211,\n",
      "         0.1426,  0.1328,  0.1729,  0.1660,  0.1641,  0.2656,  0.1338,  0.1982,\n",
      "         0.1270,  0.2158,  0.1787,  0.1045,  0.1221,  0.1357,  0.1826,  0.1504,\n",
      "         0.1196,  0.1504,  0.1641,  0.1826,  0.0293,  0.1211,  0.2021,  0.1191,\n",
      "         0.1963,  0.1328,  0.2119,  0.1895,  0.1387,  0.1602,  0.2354,  0.1426,\n",
      "         0.1328,  0.1602,  0.1455,  0.1992,  0.1079,  0.1787,  0.1299,  0.1904,\n",
      "         0.1562,  0.2422,  0.1895,  0.1494,  0.1621,  0.1196,  0.1562,  0.1270,\n",
      "         0.1216,  0.0396,  0.1138,  0.1475,  0.1235,  0.1455,  0.2070,  0.1289,\n",
      "         0.1309,  0.1338,  0.1299,  0.1211,  0.1426,  0.1631,  0.1216,  0.1426,\n",
      "         0.1216,  0.1299,  0.0562,  0.1758,  0.3027,  0.1533,  0.1162,  0.0762,\n",
      "         0.1592,  0.1357,  0.1689,  0.1235,  0.1494,  0.1562,  0.1777,  0.1235,\n",
      "         0.1953,  0.1211,  0.2197,  0.1318,  0.1416,  0.1250,  0.1836,  0.2100,\n",
      "         0.1670,  0.1670,  0.1846,  0.2061,  0.1270,  0.1592,  0.1221,  0.1621,\n",
      "         0.2520,  0.1436,  0.1533,  0.1318,  0.1494,  0.1289,  0.1631,  0.1738,\n",
      "         0.1924,  0.1562,  0.1709,  0.1182,  0.1182,  0.1245,  0.1299,  0.1543,\n",
      "         0.1494,  0.1602,  0.1177,  0.1357,  0.1079,  0.1221,  0.1133,  0.1187,\n",
      "         0.0991,  0.1152,  0.1426,  0.1504,  0.1377,  0.1289,  0.1650,  0.1650,\n",
      "         0.1768,  0.1992,  0.1553,  0.1289,  0.3457,  0.1299,  0.1270,  0.1865,\n",
      "         0.1167,  0.1514,  0.1514,  0.1748,  0.1475,  0.1309,  0.1523,  0.2266,\n",
      "         0.1621,  0.1465,  0.1406,  0.1201,  0.1245,  0.1973,  0.1719,  0.1016,\n",
      "         0.1455,  0.1846,  0.1621, -0.1738,  0.1084,  0.1602,  0.1631,  0.1270,\n",
      "         0.1025,  0.1245,  0.1729,  0.1318,  0.1533,  0.1377,  0.2422,  0.1768,\n",
      "         0.1191,  0.1494,  0.1123,  0.1406,  0.1484,  0.1934,  0.1729,  0.2246,\n",
      "         0.1445,  0.1768,  0.1973,  0.1416,  0.1455,  0.1436,  0.1533,  0.1641,\n",
      "         0.1582,  0.1416,  0.1914,  0.1582,  0.1758,  0.1299,  0.1641,  0.2051,\n",
      "         0.1328,  0.1396,  0.1768,  0.1465,  0.1357,  0.1719,  0.2207,  0.2324,\n",
      "         0.1128,  0.1924,  0.1216,  0.1523,  0.1689,  0.1309,  0.1543,  0.1895,\n",
      "         0.1738,  0.1211,  0.2080,  0.2656,  0.1650,  0.2041,  0.0295,  0.1455,\n",
      "         0.1777,  0.1309,  0.1758,  0.0491,  0.1147,  0.1377,  0.1445,  0.1738,\n",
      "         0.1348,  0.1943,  0.1260,  0.2988,  0.1416,  0.1270,  0.1338,  0.1377,\n",
      "         0.1133,  0.1260,  0.1240,  0.1328,  0.1201,  0.1270,  0.1377,  0.1807,\n",
      "         0.1348,  0.1523,  0.1562,  0.1416,  0.1611,  0.1416,  0.2129,  0.1328,\n",
      "         0.2002,  0.1406,  0.1562,  0.1035,  0.1826,  0.1177,  0.1270,  0.1611,\n",
      "         0.1895,  0.1436,  0.1553,  0.1211,  0.1641,  0.1348,  0.1416,  0.1797,\n",
      "         0.1309,  0.1553,  0.1357,  0.1523,  0.1611,  0.1211,  0.1299,  0.2051,\n",
      "         0.1387,  0.1270,  0.1582,  0.1826,  0.1494,  0.1709,  0.1719,  0.1387,\n",
      "         0.1807,  0.2715,  0.1309,  0.1924,  0.1553,  0.1514,  0.2021,  0.1338,\n",
      "         0.1738,  0.1582,  0.1187,  0.1455,  0.1660,  0.1768,  0.1729,  0.1211,\n",
      "         0.1201,  0.1260,  0.1299,  0.1406,  0.1426,  0.1484,  0.1914,  0.2031,\n",
      "         0.1367,  0.1621,  0.1768,  0.1719,  0.1855,  0.0388,  0.1387,  0.2676,\n",
      "         0.1387,  0.1797,  0.1396,  0.1289,  0.1611,  0.1348,  0.1406,  0.1611,\n",
      "         0.1406,  0.1318,  0.1611,  0.1299,  0.1396,  0.1396,  0.1406,  0.1465,\n",
      "         0.1543,  0.1387,  0.1592,  0.1113,  0.1494,  0.1387,  0.1406,  0.1729,\n",
      "         0.1226,  0.1660,  0.1182,  0.1650,  0.2412,  0.1216,  0.1318,  0.1309,\n",
      "         0.1758,  0.1230,  0.1436,  0.1670,  0.1641,  0.1094,  0.1475,  0.1377,\n",
      "         0.1455,  0.1216,  0.1279,  0.2051,  0.2695,  0.1328,  0.1318,  0.1523,\n",
      "         0.1396,  0.1338,  0.0439,  0.1592,  0.1660,  0.1943,  0.1455,  0.2061,\n",
      "         0.1318,  0.1396,  0.2051,  0.1309,  0.1650,  0.1699,  0.1787,  0.1167,\n",
      "         0.1406,  0.1865,  0.1660,  0.1367,  0.1309,  0.1123,  0.1455,  0.1416,\n",
      "         0.1504,  0.1494,  0.1436,  0.1680,  0.1553,  0.1943,  0.1787,  0.1875,\n",
      "         0.1328,  0.1787,  0.1562,  0.1196,  0.1855,  0.1855,  0.1484,  0.1338,\n",
      "         0.2168,  0.1543,  0.1758,  0.1328,  0.1719,  0.1226,  0.2031,  0.1660,\n",
      "         0.1377,  0.1191,  0.1494,  0.1128,  0.1621,  0.1768,  0.1416,  0.1982],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.1.layer.1.DenseReluDense.wi.weight\n",
      "Parameter containing:\n",
      "tensor([[ 2.9492e-01,  5.3906e-01,  9.8438e-01,  ..., -5.8984e-01,\n",
      "          6.7188e-01, -9.8828e-01],\n",
      "        [-7.9590e-02, -9.2285e-02,  6.0547e-01,  ..., -1.6016e-01,\n",
      "          4.2969e-01,  3.9673e-03],\n",
      "        [-4.6875e-02,  4.5508e-01, -5.0391e-01,  ..., -1.8555e-01,\n",
      "          4.1016e-01, -7.5684e-02],\n",
      "        ...,\n",
      "        [ 2.9492e-01, -2.8711e-01,  6.2561e-04,  ..., -4.8633e-01,\n",
      "          1.2891e-01, -4.6631e-02],\n",
      "        [-8.0078e-01,  3.1250e-01, -4.9316e-02,  ..., -1.7188e-01,\n",
      "          7.1777e-02, -5.6250e-01],\n",
      "        [-9.5215e-02,  7.6953e-01,  4.5508e-01,  ..., -4.3555e-01,\n",
      "          1.2817e-02,  5.7031e-01]], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.1.layer.1.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0732, -0.1777,  0.0303,  ..., -0.2266,  0.2344,  0.1445],\n",
      "        [-0.2715, -0.0791, -0.0369,  ..., -0.2090, -0.1245,  0.0432],\n",
      "        [-0.1836,  0.2100,  0.1768,  ..., -0.1074, -0.1250, -0.2041],\n",
      "        ...,\n",
      "        [-0.0146,  0.3711, -0.2402,  ..., -0.1953, -0.4668,  0.0781],\n",
      "        [-0.0215, -0.0347, -0.1079,  ...,  0.0972, -0.0674, -0.0012],\n",
      "        [-0.2461, -0.0815,  0.2217,  ...,  0.0122,  0.1484, -0.2988]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.1.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 0.5312,  0.6172,  0.5977,  0.6758,  0.3691,  0.5078,  0.2158,  0.5547,\n",
      "         1.1484,  0.4492,  0.5352,  0.5703,  0.5312,  0.6992,  0.3516,  0.4512,\n",
      "         0.5547,  0.5273,  0.7305,  0.3828,  0.4316,  0.4766,  0.3867,  0.4004,\n",
      "         0.3652,  0.6055,  0.3652,  0.6133,  0.4668,  0.3672, -0.4434,  0.5156,\n",
      "         0.5312,  0.3789,  0.6016,  0.4766,  0.7070,  0.8789,  0.3555,  0.6484,\n",
      "         0.6562,  0.6523,  0.6172,  0.4805,  1.2188,  0.4688,  0.7227,  0.4570,\n",
      "         0.4746,  0.6562,  0.6719,  0.4863,  0.6680,  0.4141,  0.4180,  0.4902,\n",
      "         0.8008,  0.5156,  0.3984,  0.4316,  0.4961,  0.3516,  0.4824,  0.3984,\n",
      "         0.3848,  0.4180,  0.4395,  0.4336,  0.9023,  0.4023,  0.4160,  0.6250,\n",
      "         0.3730,  0.4727,  0.4004,  0.4043,  0.7148,  0.4883,  0.3809,  0.5195,\n",
      "         0.4199,  0.5156,  0.4395,  0.8242,  0.4727,  0.4551,  1.1719,  0.5156,\n",
      "         0.4492,  0.3574,  0.4805,  0.3770,  0.4102,  0.5234,  0.3672,  0.3965,\n",
      "         0.4023,  0.3906,  0.3574,  0.3965,  0.6172,  0.4609,  0.7266,  0.3594,\n",
      "         0.4297,  0.9453,  0.4512,  0.4336,  0.3613,  0.6484,  0.5195,  0.4395,\n",
      "         0.4727,  0.4297,  0.4043,  0.3730,  0.4785,  0.4082,  1.3203,  0.4238,\n",
      "         0.3281,  0.3848,  0.8242,  0.3906,  0.6523,  0.4648,  0.4473,  0.4668,\n",
      "         0.4844,  0.6758,  0.8477,  0.5508,  0.3711,  0.3730,  0.4336,  0.7461,\n",
      "         0.5508,  0.4180,  0.5039,  0.5586,  0.4941,  0.6211,  0.7227,  0.8164,\n",
      "         0.5508,  0.6953,  0.3711,  0.6211,  0.4004,  0.5352,  0.4199,  0.4648,\n",
      "         0.4844,  0.8711,  0.3613,  0.3770,  0.6016,  0.3594,  0.7109,  0.4668,\n",
      "         0.3633, -0.4473,  0.5273,  0.9414,  0.3867,  0.3926,  0.4219,  0.6094,\n",
      "         0.5000,  0.6641,  0.8398,  0.4395,  0.4004,  0.7227,  0.4590,  0.5273,\n",
      "         0.5234,  0.6328,  0.7109,  0.1260,  0.4004,  0.3750,  0.4727,  0.3477,\n",
      "         0.5859,  0.6719,  0.3887,  0.6328,  0.4980,  0.5195,  0.4258,  0.3945,\n",
      "         0.3672,  0.4609,  0.3652,  0.2617,  0.4160,  0.5625,  0.4844,  0.3848,\n",
      "         0.3555,  0.5117,  0.4727,  0.4414,  0.4434,  0.7305,  0.5078,  0.4668,\n",
      "         0.4219,  0.4668,  0.6992,  0.4766,  0.5664,  0.5625,  0.4414,  0.5352,\n",
      "         0.4727,  0.4219,  0.5703,  1.8203,  0.5938,  0.4941,  0.3516,  0.4863,\n",
      "         0.5508,  0.4551,  0.7148,  0.6523,  0.4648,  0.5742,  0.4980,  0.5117,\n",
      "         0.4160,  0.3457,  0.3535,  0.3262,  0.1934,  0.5898,  0.6445,  0.6523,\n",
      "         1.0469,  0.5742,  0.4141,  0.4375,  0.4004,  0.4102,  0.4355,  0.5430,\n",
      "         0.3496,  0.5508,  0.8906,  0.3945,  0.4160,  0.3789,  0.3496,  0.4141,\n",
      "         0.3945,  0.4355,  0.7031,  0.4453,  0.7773,  0.2754,  0.3848,  0.4277,\n",
      "         0.4336,  0.4609,  0.4238,  0.4375,  0.4609,  0.4336,  0.6250,  0.9141,\n",
      "         0.7344,  0.7344,  0.8516,  0.5273,  0.3789,  0.7148,  0.4434,  0.5039,\n",
      "         1.0000,  0.8555,  0.4023,  0.5898,  0.5312,  0.7148,  0.3594,  0.3906,\n",
      "         0.5156,  0.9414,  0.3516,  0.4453,  0.3613,  0.4004,  0.4199,  0.3320,\n",
      "         0.6094,  0.4004,  0.4043,  0.5781,  0.4961,  0.4062,  0.9141,  0.5625,\n",
      "         0.4707,  0.3691,  0.4414,  0.4043,  0.5430,  0.5859,  0.4297,  0.6133,\n",
      "         0.3828,  0.5703,  0.6055,  0.4922,  0.5117,  0.8984,  0.5625,  0.3887,\n",
      "         0.3750,  0.6055,  0.4883,  0.3438,  0.6406,  0.3945,  0.6562,  0.7031,\n",
      "         0.5391,  0.4551,  0.3867,  0.6680,  0.6211,  0.4414,  0.4062,  0.4297,\n",
      "         0.3730,  0.3984,  0.5078,  0.9414,  0.5547,  0.5156,  0.4961,  0.4141,\n",
      "         0.5078,  0.5078,  0.6328,  0.5234,  0.6406,  0.8516,  0.7148,  0.3926,\n",
      "         0.4609,  0.3906,  0.5859,  0.4922,  0.5625,  0.8633,  0.3672,  0.6523,\n",
      "         0.4082,  0.7383,  0.9453, -0.3203,  0.3535,  0.4609,  0.7266,  0.5078,\n",
      "         0.3926,  0.4941,  0.5391,  0.5430,  0.0053,  0.4316,  0.6914,  0.3496,\n",
      "         0.6602,  0.4102,  0.6797,  0.8242,  0.4551,  0.5117,  0.8008,  0.4336,\n",
      "         0.4121,  0.7617,  0.4102,  0.6836,  0.3359,  0.5781,  0.4004,  0.7422,\n",
      "         0.5234,  0.8125,  0.6484,  0.5000,  0.5625,  0.3848,  0.5000,  0.3867,\n",
      "         0.3984,  0.2910,  0.3359,  0.4160,  0.4062,  0.4512,  0.7422,  0.3887,\n",
      "         0.3867,  0.3906,  0.4141,  0.3574,  0.5195,  0.5273,  0.3770,  0.4863,\n",
      "         0.4062,  0.3906,  0.2041,  0.6406,  1.4297,  0.4902,  0.3516,  0.3379,\n",
      "         0.4844,  0.4219,  0.5898,  0.3789,  0.5156,  0.5781,  0.6250,  0.4141,\n",
      "         0.7422,  0.3965,  0.7461,  0.4102,  0.4336,  0.3926,  0.5625,  0.6719,\n",
      "         0.5117,  0.6562,  0.6172,  0.6445,  0.3770,  0.5469,  0.3867,  0.5312,\n",
      "         0.9531,  0.4746,  0.4980,  0.3867,  0.4512,  0.4062,  0.5195,  0.5977,\n",
      "         0.6836,  0.5391,  0.6094,  0.3887,  0.3633, -0.3887,  0.4141,  0.4844,\n",
      "         0.4668,  0.4531, -0.4277,  0.4551,  0.3633,  0.3809,  0.3770,  0.3535,\n",
      "         0.3594,  0.3672,  0.4570,  0.4102,  0.3809,  0.4023,  0.5820,  0.4668,\n",
      "         0.6055,  0.8242,  0.4492,  0.3809,  1.2969,  0.4219,  0.3789,  0.6914,\n",
      "         0.3398,  0.4844,  0.4375,  0.5586,  0.6133,  0.3809,  0.4941,  1.0625,\n",
      "         0.4863,  0.4629,  0.4043,  0.3457,  0.3887,  0.6836,  0.5703,  0.5859,\n",
      "         0.4238,  0.6133,  0.5391,  0.6016,  0.3477,  0.5586,  0.5391,  0.3633,\n",
      "         0.9531,  0.3691,  0.5977,  0.3457,  0.5352,  0.5234,  1.0234,  0.5391,\n",
      "         0.3711,  0.5156,  0.3496,  0.4707,  0.4648,  0.7852,  0.5898,  0.7773,\n",
      "         0.4570,  0.5742,  0.6797,  0.4434,  0.5195,  0.4766,  0.5312,  0.5117,\n",
      "         0.5156,  0.4570,  0.6953,  0.5352,  0.5781,  0.3965,  0.5859,  0.6719,\n",
      "         0.4414,  0.3965,  0.5508,  0.4531,  0.4219,  0.6055,  0.8359,  0.7969,\n",
      "         0.3730,  1.4297,  0.4141,  0.4531,  0.6523,  0.3984,  0.5469,  0.6172,\n",
      "         0.5703,  0.3867,  0.8867,  0.9922,  0.4824,  0.6133,  0.1523,  0.4922,\n",
      "         0.6094,  0.4277,  0.5273,  0.2031,  0.3496,  0.4551,  0.4473,  0.5742,\n",
      "         0.4043,  0.6562,  0.3809,  1.1094,  0.5312,  0.3633,  0.3906,  0.4258,\n",
      "         0.3281,  0.3418,  0.4023,  0.4219,  0.3613,  0.3945,  0.4004,  0.5781,\n",
      "         0.3906,  0.4609,  0.4863,  0.4258,  0.5625,  0.4609,  0.7812,  0.4316,\n",
      "         0.6367,  0.3887,  0.5117,  0.4121,  0.5859,  0.3418,  0.3828,  0.5938,\n",
      "         0.6367,  0.4629,  0.4922,  0.4082,  0.5391,  0.3965,  0.4727,  0.5547,\n",
      "        -0.4160,  0.5703,  0.4375,  0.4805,  0.5078, -0.3984,  0.4316,  0.7031,\n",
      "         0.4941,  0.4375,  0.5117,  0.6562,  0.5000,  0.6953,  0.5859,  0.4551,\n",
      "         0.5898,  1.1953,  0.3789,  0.6836,  0.5195,  0.4863,  0.6133,  0.4043,\n",
      "         0.6523,  0.5078,  0.4121,  0.4648,  0.5703,  0.5820,  0.6328,  0.3594,\n",
      "         0.4141,  0.3984,  0.4277,  0.4512,  0.4629,  0.5273,  0.5742,  0.8516,\n",
      "         0.4395,  0.6172,  0.5430,  0.6133,  0.5859,  0.1226,  0.4824,  0.8359,\n",
      "         0.4668,  0.6406,  0.4531,  0.3613,  0.4805,  0.4141,  0.4766,  0.6172,\n",
      "         0.4375,  0.4258,  0.5000,  0.3867,  0.4277,  0.4316,  0.4531,  0.4551,\n",
      "         0.4863,  0.4238,  0.5430,  0.3457,  0.4688,  0.6992,  0.4707,  0.5703,\n",
      "         0.3750,  0.5273,  0.3789,  0.5234,  0.8750,  0.3652,  0.4277,  0.4434,\n",
      "         0.5586,  0.3828,  0.4746,  0.5859,  0.5000,  0.3594,  0.4941,  0.4766,\n",
      "         0.4883,  0.3613,  0.4023,  0.9570,  0.8281,  0.3965,  1.3125,  0.4629,\n",
      "         0.4746,  0.3867,  0.2656,  0.5156,  0.5664,  0.6406,  0.4746,  0.7266,\n",
      "         0.4082, -0.5391,  0.7031,  0.3828,  0.5000,  0.5742,  0.5820,  0.3887,\n",
      "         0.3945,  0.6250,  0.5312,  0.4180,  0.3828, -0.3809,  0.4629,  0.4336,\n",
      "         0.4609,  0.4551,  0.4707,  0.5469,  0.4883,  0.7070,  0.5898,  0.6484,\n",
      "         0.3984,  0.6797,  0.5117,  0.3672,  0.5469,  0.7109,  0.4688,  0.4375,\n",
      "         0.8320,  0.4844,  0.5000,  0.4375,  0.5547,  0.4141,  0.6094,  0.5352,\n",
      "         0.5195,  0.3672,  0.4961,  0.3594,  0.5117,  0.5508,  0.4238,  0.6562],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.2.layer.0.SelfAttention.q.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0081,  0.0093, -0.0042,  ..., -0.0635,  0.0486,  0.0649],\n",
      "        [-0.0640,  0.0266, -0.0029,  ..., -0.0366, -0.0471,  0.1138],\n",
      "        [-0.0684,  0.0540,  0.0427,  ...,  0.0327, -0.0293,  0.0040],\n",
      "        ...,\n",
      "        [ 0.0488, -0.0339, -0.0145,  ...,  0.0136,  0.0178, -0.0322],\n",
      "        [ 0.0400,  0.0079,  0.0005,  ..., -0.0104, -0.0108,  0.0337],\n",
      "        [-0.0713,  0.0142, -0.0432,  ..., -0.0454, -0.0062,  0.0150]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.2.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.2314,  0.5625, -1.0703,  ..., -0.0286,  0.7891,  0.0879],\n",
      "        [-0.2412,  0.3047,  0.5859,  ..., -0.0603, -0.2441,  0.4863],\n",
      "        [-0.3926, -0.6289,  0.1299,  ...,  0.7188,  0.1270, -0.1953],\n",
      "        ...,\n",
      "        [-0.1934,  0.1123,  0.2471,  ...,  0.0283,  0.1436,  0.2373],\n",
      "        [ 0.2656, -0.3965,  0.1836,  ..., -0.1436, -0.4648, -0.4023],\n",
      "        [-0.0942,  0.0664,  0.0535,  ...,  0.6172, -0.1167, -0.0522]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.2.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1030, -0.0515, -0.2207,  ..., -0.4160, -0.1777, -0.1914],\n",
      "        [ 0.1367,  0.2021,  0.2441,  ..., -0.3320,  0.2988, -0.2383],\n",
      "        [-0.3086, -0.1436,  0.1167,  ...,  0.4199, -0.3223, -0.1943],\n",
      "        ...,\n",
      "        [ 0.0801,  0.6133,  0.3652,  ...,  0.6094,  0.1074, -1.0625],\n",
      "        [ 0.4824,  0.2656, -0.5078,  ..., -0.4824, -0.6641,  0.3164],\n",
      "        [ 1.1562, -0.4297,  0.5352,  ...,  0.4453, -0.8320, -0.4863]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.2.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1143, -0.4453,  0.3125,  ...,  0.3496,  0.1553, -1.0469],\n",
      "        [-0.5781, -0.5273,  0.0237,  ..., -1.3203, -0.4121,  0.9492],\n",
      "        [-0.1875, -0.1309,  0.3457,  ..., -0.4531,  0.6914, -0.5977],\n",
      "        ...,\n",
      "        [ 0.3926,  0.5859,  0.2227,  ..., -0.0266,  0.2227, -0.0879],\n",
      "        [ 0.4688,  0.0981,  0.1973,  ...,  0.1982,  0.5000,  0.6250],\n",
      "        [ 0.9844,  0.2178,  0.0082,  ...,  1.3594, -1.0234,  1.3438]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.2.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 1.7578e-01,  1.6602e-01,  1.8066e-01,  2.0117e-01,  1.2061e-01,\n",
      "         1.4355e-01,  5.0049e-02,  1.8652e-01,  9.9121e-02,  1.4258e-01,\n",
      "         1.6992e-01,  1.6504e-01,  1.1621e-01,  1.8945e-01,  1.1621e-01,\n",
      "         1.5332e-01,  1.7383e-01,  1.7188e-01,  2.1875e-01,  1.3184e-01,\n",
      "         1.2451e-01,  1.4551e-01, -1.7548e-04,  1.3184e-01,  1.2354e-01,\n",
      "         1.7871e-01,  1.2305e-01,  1.5332e-01,  1.5625e-01,  1.1963e-01,\n",
      "         1.4160e-01,  1.4453e-01,  1.5918e-01,  1.2500e-01,  1.8945e-01,\n",
      "         1.5430e-01,  1.4844e-01,  2.6953e-01,  1.1768e-01,  2.1680e-01,\n",
      "         1.7383e-01,  1.7578e-01,  1.9824e-01,  1.6406e-01,  9.4727e-02,\n",
      "         1.4160e-01,  1.5820e-01,  1.4258e-01,  1.5723e-01,  1.5625e-01,\n",
      "         2.3145e-01,  1.6309e-01,  1.5430e-01,  1.2354e-01,  4.1504e-02,\n",
      "         1.3867e-01,  4.8096e-02,  1.4941e-01,  1.5137e-01,  1.3281e-01,\n",
      "         1.6797e-01,  1.1230e-01,  1.6113e-01,  1.2988e-01,  1.3477e-01,\n",
      "         5.3223e-02,  1.3965e-01,  1.3281e-01,  2.4414e-01,  1.3379e-01,\n",
      "         1.4551e-01,  2.0215e-01,  1.2061e-01,  1.5527e-01,  1.3281e-01,\n",
      "         1.1914e-01,  1.6504e-01,  1.4844e-01,  1.2158e-01,  1.7090e-01,\n",
      "         1.2891e-01,  1.6797e-01,  1.3965e-01,  2.0215e-01,  1.4453e-01,\n",
      "         1.5527e-01,  2.5000e-01,  1.6797e-01,  1.4453e-01,  1.2891e-01,\n",
      "         1.5918e-01,  1.1182e-01,  1.2793e-01,  1.6113e-01,  1.2354e-01,\n",
      "         1.1963e-01,  1.4355e-01,  1.3281e-01,  1.2695e-01,  1.2793e-01,\n",
      "         1.9922e-01,  1.4648e-01,  1.7285e-01,  1.1768e-01,  1.4062e-01,\n",
      "         2.3145e-01,  1.4551e-01,  1.4160e-01,  1.2695e-01,  1.6309e-01,\n",
      "         1.6211e-01,  1.4941e-01,  1.4062e-01,  1.1816e-01,  1.4160e-01,\n",
      "         1.2207e-01,  1.5625e-01,  1.2500e-01,  2.8711e-01,  1.3574e-01,\n",
      "         1.1230e-01,  1.4062e-01,  2.0898e-01,  1.3184e-01,  2.0605e-01,\n",
      "         1.5234e-01,  1.5820e-01,  1.5723e-01,  1.5332e-01,  1.9531e-01,\n",
      "         1.2256e-01,  1.8066e-01,  1.1914e-01,  1.1670e-01,  1.4355e-01,\n",
      "         1.9531e-01,  1.7480e-01,  1.4258e-01,  1.6406e-01,  1.6992e-01,\n",
      "         1.4844e-01,  1.9727e-01,  2.0117e-01,  1.9629e-01,  1.6113e-01,\n",
      "         1.7676e-01,  1.2891e-01,  1.9141e-01,  1.3281e-01,  1.6699e-01,\n",
      "         1.3574e-01,  1.4453e-01,  1.5820e-01,  2.4512e-01,  1.2695e-01,\n",
      "         1.2598e-01,  1.7871e-01,  1.1475e-01,  1.7383e-01,  1.2891e-01,\n",
      "         1.1719e-01,  1.3770e-01,  1.2451e-01,  9.6680e-02,  1.1914e-01,\n",
      "         1.2207e-01,  1.3867e-01,  1.8457e-01,  1.6602e-01,  2.3047e-01,\n",
      "         2.3730e-01,  1.4941e-01,  1.4355e-01,  1.8359e-01,  1.5625e-01,\n",
      "         1.5527e-01,  1.7383e-01,  2.1387e-01,  1.7383e-01, -4.7302e-04,\n",
      "         1.3770e-01,  1.2891e-01,  1.5625e-01,  1.2207e-01,  1.9141e-01,\n",
      "         2.0410e-01,  1.2891e-01,  2.0410e-01,  1.5137e-01,  1.5723e-01,\n",
      "         1.1816e-01,  1.3770e-01,  9.8145e-02,  1.4062e-01,  1.2061e-01,\n",
      "         6.4941e-02,  1.3770e-01,  1.8945e-01,  1.5527e-01,  1.0645e-01,\n",
      "         1.1230e-01,  1.6211e-01,  1.4941e-01,  1.3965e-01,  1.4746e-01,\n",
      "         2.1973e-01,  1.4258e-01,  1.4648e-01,  1.3184e-01,  1.5332e-01,\n",
      "         1.7871e-01,  1.3672e-01,  1.7090e-01,  1.7383e-01,  1.4355e-01,\n",
      "         1.7871e-01,  1.6113e-01,  1.4355e-01,  1.7871e-01,  2.4805e-01,\n",
      "         1.2402e-01,  1.5625e-01,  1.1475e-01,  1.5820e-01,  1.5039e-01,\n",
      "         1.4551e-01,  1.8945e-01,  2.2852e-01,  1.5039e-01,  1.9141e-01,\n",
      "         1.2793e-01,  1.5430e-01,  9.2773e-02,  1.1865e-01,  1.1133e-01,\n",
      "         8.4473e-02,  5.6885e-02,  1.8359e-01,  1.6211e-01,  2.0898e-01,\n",
      "         2.4023e-01,  1.9141e-01,  1.3672e-01,  1.5723e-01,  1.1182e-01,\n",
      "         1.2695e-01,  1.5039e-01,  1.7676e-01,  1.0400e-01,  1.5820e-01,\n",
      "         1.1279e-01,  1.2500e-01,  1.2109e-01,  1.2305e-01,  1.1621e-01,\n",
      "         1.1914e-01,  1.3574e-01,  1.1621e-01,  1.4648e-01,  1.4453e-01,\n",
      "         2.0410e-01,  1.5723e-01,  1.2354e-01,  1.4551e-01,  1.3574e-01,\n",
      "         1.3086e-01,  1.4355e-01,  1.3770e-01,  1.4160e-01,  1.3574e-01,\n",
      "         6.3965e-02,  2.5195e-01,  1.9629e-01,  5.2246e-02,  2.4121e-01,\n",
      "         1.6211e-01,  1.3965e-01,  1.9238e-01,  1.5430e-01,  1.4551e-01,\n",
      "         2.6562e-01,  2.1973e-01,  1.3965e-01,  1.5723e-01,  1.7578e-01,\n",
      "         1.8359e-01,  1.1572e-01,  1.3184e-01,  1.4551e-01,  2.0117e-01,\n",
      "         1.1621e-01,  1.4160e-01,  1.1963e-01,  1.3379e-01,  1.3477e-01,\n",
      "         1.1475e-01,  2.0312e-01,  1.4551e-01,  1.3281e-01,  1.7773e-01,\n",
      "         1.8848e-01,  1.4355e-01,  2.9297e-01,  1.7090e-01,  1.6113e-01,\n",
      "         1.1621e-01,  1.5137e-01,  1.3770e-01,  1.6699e-01,  1.6211e-01,\n",
      "         1.5625e-01,  1.9336e-01,  1.3281e-01,  1.2451e-01,  1.9043e-01,\n",
      "         1.5625e-01,  1.1279e-01,  2.8711e-01,  1.3379e-01,  1.2598e-01,\n",
      "         1.1621e-01,  1.8945e-01,  1.5137e-01,  1.1963e-01,  1.7676e-01,\n",
      "         1.3086e-01,  1.9434e-01,  2.3828e-01,  1.5918e-01,  1.5234e-01,\n",
      "         1.3086e-01,  1.7969e-01,  1.7090e-01,  1.3770e-01,  1.2891e-01,\n",
      "         1.4258e-01,  1.1182e-01,  1.3379e-01,  1.5723e-01,  2.6953e-01,\n",
      "         1.6602e-01,  1.8164e-01,  1.6016e-01,  1.3281e-01,  1.6406e-01,\n",
      "         1.5430e-01,  1.6895e-01,  1.5234e-01,  1.8457e-01,  1.5332e-01,\n",
      "         2.1777e-01,  1.2109e-01,  1.4355e-01,  1.3574e-01,  1.7480e-01,\n",
      "         1.6504e-01,  1.7578e-01,  2.4316e-01,  1.2451e-01,  2.0508e-01,\n",
      "         1.2793e-01,  2.3438e-01,  1.6504e-01,  9.6191e-02,  1.2061e-01,\n",
      "         1.3867e-01,  1.7383e-01,  1.5234e-01,  1.1914e-01,  1.5723e-01,\n",
      "         1.6113e-01,  1.8848e-01, -7.7820e-04,  1.2109e-01,  2.1582e-01,\n",
      "         1.2061e-01,  1.8945e-01,  1.3477e-01,  2.2852e-01,  1.6406e-01,\n",
      "         1.3965e-01,  1.6406e-01,  2.5000e-01,  1.5430e-01,  1.4551e-01,\n",
      "         1.3574e-01,  1.4160e-01,  2.1875e-01,  9.9609e-02,  1.9238e-01,\n",
      "         1.3184e-01,  1.7969e-01,  1.5723e-01,  2.3730e-01,  1.9922e-01,\n",
      "         1.5527e-01,  1.7480e-01,  1.2891e-01,  1.5723e-01,  1.2793e-01,\n",
      "         1.2109e-01,  3.1738e-02,  1.1475e-01,  1.5625e-01,  1.2598e-01,\n",
      "         1.4551e-01,  2.1387e-01,  1.2891e-01,  1.3086e-01,  1.2793e-01,\n",
      "         1.2988e-01,  1.1377e-01,  1.5137e-01,  1.4941e-01,  1.2695e-01,\n",
      "         1.5723e-01,  1.1719e-01,  1.1963e-01,  5.5664e-02,  1.7480e-01,\n",
      "         2.6367e-01,  1.5723e-01,  1.1279e-01,  5.9326e-02,  1.6602e-01,\n",
      "         1.3086e-01,  1.6895e-01,  1.1914e-01,  1.5918e-01,  1.6895e-01,\n",
      "         1.8848e-01,  1.3770e-01,  1.8359e-01,  1.2793e-01,  2.2559e-01,\n",
      "         1.2988e-01,  1.3867e-01,  1.2891e-01,  1.8750e-01,  2.1582e-01,\n",
      "         1.6895e-01,  1.6602e-01,  2.0215e-01,  2.1289e-01,  1.2891e-01,\n",
      "         1.6895e-01,  1.2109e-01,  1.6406e-01,  2.4121e-01,  1.5918e-01,\n",
      "         1.5234e-01,  1.3281e-01,  1.4844e-01,  1.3477e-01,  1.7383e-01,\n",
      "         1.8359e-01,  1.9238e-01,  1.6602e-01,  1.7090e-01,  1.1963e-01,\n",
      "         1.2402e-01,  1.2305e-01,  1.3574e-01,  1.5723e-01,  1.5820e-01,\n",
      "         1.6699e-01,  1.0449e-01,  1.4258e-01,  1.0840e-01,  1.3281e-01,\n",
      "         1.1377e-01,  1.1768e-01,  1.0596e-01,  1.2402e-01,  1.6113e-01,\n",
      "         1.4746e-01,  1.3574e-01,  1.4160e-01,  1.5430e-01,  1.6309e-01,\n",
      "         1.8457e-01,  1.5527e-01,  1.5820e-01,  1.3281e-01,  3.2422e-01,\n",
      "         1.3281e-01,  1.2793e-01,  1.9043e-01,  1.1279e-01,  1.6113e-01,\n",
      "         1.4453e-01,  1.6406e-01,  1.2354e-01,  1.2988e-01,  1.4941e-01,\n",
      "         2.0508e-01,  1.6309e-01,  1.5039e-01,  1.4062e-01,  1.1670e-01,\n",
      "         1.2793e-01,  2.0605e-01,  1.6699e-01,  6.4941e-02,  1.4160e-01,\n",
      "         1.8750e-01,  1.7188e-01,  1.8652e-01,  1.0693e-01,  1.6309e-01,\n",
      "         1.6992e-01,  1.2793e-01,  9.0332e-02,  1.2598e-01,  1.8359e-01,\n",
      "         1.2891e-01,  1.4941e-01,  1.2891e-01,  2.0117e-01,  1.8359e-01,\n",
      "         1.1914e-01,  1.4355e-01,  1.0938e-01,  1.4355e-01,  1.5234e-01,\n",
      "         1.6895e-01,  1.9043e-01,  2.5195e-01,  1.4258e-01,  1.8555e-01,\n",
      "         2.0508e-01,  1.4844e-01,  1.5723e-01,  1.4453e-01,  1.5430e-01,\n",
      "         1.6504e-01,  1.5430e-01,  1.4941e-01,  2.0605e-01,  1.4844e-01,\n",
      "         1.8750e-01,  1.2988e-01,  1.6797e-01,  2.1289e-01,  1.4453e-01,\n",
      "         1.4551e-01,  1.7871e-01,  1.4941e-01,  1.2891e-01,  1.5527e-01,\n",
      "         2.1680e-01,  2.3340e-01,  1.0400e-01,  1.7285e-01,  1.2695e-01,\n",
      "         1.5723e-01,  1.6602e-01,  1.3477e-01,  1.5918e-01,  1.7871e-01,\n",
      "         1.7871e-01,  1.1816e-01,  2.1973e-01,  2.6562e-01,  1.7578e-01,\n",
      "         2.0020e-01,  2.9785e-02,  1.4258e-01,  1.8066e-01,  1.3574e-01,\n",
      "         1.6895e-01,  5.5908e-02,  1.0889e-01,  1.4551e-01,  1.4941e-01,\n",
      "         1.7383e-01,  1.3965e-01,  2.2168e-01,  1.2695e-01,  2.7344e-01,\n",
      "         1.5527e-01,  1.2695e-01,  1.3281e-01,  1.4062e-01,  1.0938e-01,\n",
      "         1.2256e-01,  1.2598e-01,  1.2695e-01,  1.2305e-01,  1.2256e-01,\n",
      "         1.4160e-01,  1.8555e-01,  1.3574e-01,  1.5039e-01,  1.4941e-01,\n",
      "         1.5625e-01,  1.5430e-01,  1.4453e-01,  2.3340e-01,  1.4453e-01,\n",
      "         2.0020e-01,  1.4258e-01,  1.6113e-01,  9.0332e-02,  1.9043e-01,\n",
      "         1.1768e-01,  1.2500e-01,  1.6992e-01,  1.9336e-01,  1.4648e-01,\n",
      "         1.5039e-01,  1.2598e-01,  1.6211e-01,  1.3281e-01,  1.4355e-01,\n",
      "         1.8848e-01,  1.2256e-01,  1.7188e-01,  1.4746e-01,  1.4453e-01,\n",
      "         1.6016e-01,  1.1865e-01,  1.3477e-01,  2.1777e-01,  1.4258e-01,\n",
      "         1.2793e-01,  1.5820e-01,  1.6113e-01,  1.3770e-01,  1.6992e-01,\n",
      "         1.8066e-01,  1.4355e-01,  1.8164e-01,  2.8320e-01,  1.2598e-01,\n",
      "         1.8945e-01,  1.6309e-01,  1.5625e-01,  2.1680e-01,  1.3184e-01,\n",
      "         1.7090e-01,  1.6309e-01,  1.2354e-01,  1.5137e-01,  1.8652e-01,\n",
      "         1.7773e-01,  1.8262e-01,  1.2158e-01,  1.1963e-01,  1.1963e-01,\n",
      "         1.3770e-01,  1.4355e-01,  1.4551e-01,  1.4258e-01,  1.8457e-01,\n",
      "         2.0605e-01,  1.4453e-01,  1.5430e-01,  1.8750e-01,  1.7969e-01,\n",
      "         1.8945e-01,  8.1253e-04,  1.4160e-01,  2.7930e-01,  1.4648e-01,\n",
      "         1.8555e-01,  1.3086e-01,  1.2109e-01,  1.7188e-01,  1.4355e-01,\n",
      "         1.5039e-01,  1.4062e-01,  1.3770e-01,  1.3086e-01,  1.6504e-01,\n",
      "         1.2598e-01,  1.4551e-01,  1.3672e-01,  1.3379e-01,  1.4648e-01,\n",
      "         1.5332e-01,  1.3770e-01,  1.6113e-01,  1.1084e-01,  1.4746e-01,\n",
      "         1.1426e-01,  1.2500e-01,  1.8164e-01,  1.2402e-01,  1.6602e-01,\n",
      "         1.1621e-01,  1.8066e-01,  2.4121e-01,  1.2891e-01,  1.4062e-01,\n",
      "         1.2354e-01,  1.8750e-01,  1.2158e-01,  1.5430e-01,  1.6992e-01,\n",
      "         1.6211e-01,  9.7168e-02,  1.5527e-01,  1.3574e-01,  1.4844e-01,\n",
      "         1.1914e-01,  1.3477e-01,  1.7969e-01,  2.5195e-01,  1.2012e-01,\n",
      "         1.0742e-01,  1.5625e-01,  1.4746e-01,  1.3184e-01,  4.8828e-02,\n",
      "         1.6699e-01,  1.7383e-01,  1.9141e-01,  1.4160e-01,  2.1582e-01,\n",
      "         1.3281e-01,  1.3184e-01,  2.1680e-01,  1.3086e-01,  1.6699e-01,\n",
      "         1.7480e-01,  1.8750e-01,  1.1328e-01,  1.3965e-01,  1.8555e-01,\n",
      "         1.6992e-01,  1.3965e-01,  1.2695e-01,  1.0791e-01,  1.3965e-01,\n",
      "         1.5430e-01,  1.5820e-01,  1.5332e-01,  1.4258e-01,  1.7773e-01,\n",
      "         1.5820e-01,  2.0508e-01,  1.9531e-01,  1.9141e-01,  1.4551e-01,\n",
      "         1.8848e-01,  1.6211e-01,  1.1719e-01,  1.9141e-01,  2.0410e-01,\n",
      "         1.5332e-01,  1.3672e-01,  2.3047e-01,  1.5234e-01,  1.7578e-01,\n",
      "         1.4258e-01,  1.6602e-01,  1.2695e-01,  1.9434e-01,  1.7871e-01,\n",
      "         1.5234e-01,  1.1621e-01,  1.5332e-01,  1.1572e-01,  1.7676e-01,\n",
      "         1.7871e-01,  1.4160e-01,  2.1680e-01], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.encoder.block.2.layer.1.DenseReluDense.wi.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0008,  0.3516, -0.6641,  ..., -0.0898, -0.5430, -0.1299],\n",
      "        [-0.3398,  0.0243, -0.4297,  ...,  0.8047, -0.2949, -0.0486],\n",
      "        [-0.3730,  0.4570, -0.4863,  ...,  0.1523, -0.2832, -0.4004],\n",
      "        ...,\n",
      "        [ 0.2637, -0.7539,  0.5625,  ..., -0.1108,  0.7734, -0.0791],\n",
      "        [ 0.0500, -0.3965, -0.3242,  ...,  0.3516,  0.0654, -0.0564],\n",
      "        [-0.2656,  0.5547, -0.5586,  ...,  0.3281, -0.0991,  0.0016]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.2.layer.1.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.2051, -0.1113,  0.2217,  ..., -0.0454, -0.1592,  0.4277],\n",
      "        [-0.0723,  0.3047,  0.0625,  ...,  0.0049,  0.1709, -0.1260],\n",
      "        [ 0.2080, -0.3652,  0.0613,  ..., -0.0581, -0.2559, -0.4590],\n",
      "        ...,\n",
      "        [ 0.2832, -0.0359,  0.0747,  ...,  0.2812,  0.1523,  0.1006],\n",
      "        [-0.0518,  0.0593,  0.0796,  ...,  0.0361, -0.0420, -0.3809],\n",
      "        [ 0.1123, -0.2188,  0.2754,  ..., -0.0835,  0.1504,  0.2832]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.2.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 0.8281,  0.9141,  0.9102,  1.0312,  0.5977,  0.7930,  0.3477,  0.8320,\n",
      "         2.1875,  0.6875,  0.8203,  0.8750,  0.7188,  1.0391,  0.5742,  0.7109,\n",
      "         0.9180,  0.8633,  1.0547,  0.6484,  0.7305,  0.7305,  0.6328,  0.6172,\n",
      "         0.6055,  0.9141,  0.5898,  0.8242,  0.7930,  0.5820, -0.6953,  0.7148,\n",
      "         0.7891,  0.5898,  0.9492,  0.7344,  1.1875,  1.2812,  0.5703,  1.0703,\n",
      "         0.9180,  1.0781,  0.9453,  0.6953,  2.1875,  0.6914,  1.0156,  0.7109,\n",
      "         0.7422,  1.0469,  1.0234,  0.7188,  1.1328,  0.6289,  0.7812,  0.7500,\n",
      "         1.1328,  0.8125,  0.6641,  0.6562,  0.8008,  0.5547,  0.7656,  0.6445,\n",
      "         0.6328,  0.7148,  0.6875,  0.6875,  1.2344,  0.6523,  0.6992,  1.0234,\n",
      "         0.5781,  0.7344,  0.6758,  0.6133,  1.0156,  0.7617,  0.6211,  0.8008,\n",
      "         0.6875,  0.7891,  0.6992,  1.2109,  0.7344,  0.7305,  1.4766,  0.8320,\n",
      "         0.7266,  0.6016,  0.7852,  0.6016,  0.6172,  0.8086,  0.6289,  0.6133,\n",
      "         0.6133,  0.6328,  0.5664,  0.6445,  1.0234,  0.7031,  1.0469,  0.5781,\n",
      "         0.7227,  1.2812,  0.6953,  0.6797,  0.5859,  1.0391,  0.8398,  0.7031,\n",
      "         0.7031,  0.7227,  0.6641,  0.5938,  0.7930,  0.6328,  1.6406,  0.6523,\n",
      "         0.5391,  0.6055,  1.1641,  0.6289,  1.0078,  0.7305,  0.7109,  0.7305,\n",
      "         0.7891,  0.9258,  1.2969,  0.8828,  0.5703,  0.5938,  0.6602,  1.1094,\n",
      "         0.8828,  0.6797,  0.8516,  0.8672,  0.7109,  0.9922,  1.0312,  1.1797,\n",
      "         0.7930,  1.0234,  0.6328,  0.8984,  0.6680,  0.7656,  0.6836,  0.7617,\n",
      "         0.7812,  1.1953,  0.5938,  0.5898,  0.9180,  0.5664,  0.9727, -0.7500,\n",
      "         0.6172,  0.7188,  0.7422,  1.9375,  0.6094,  0.5859,  0.6914,  0.9102,\n",
      "         0.7383,  1.0703,  1.1328,  0.6797,  0.6523,  1.0938,  0.6953,  0.7578,\n",
      "         0.7930,  0.8711,  1.0625,  0.2012,  0.6406,  0.6172,  0.7422, -0.5586,\n",
      "         0.8320,  1.0000,  0.6133,  0.9570,  0.8008,  0.8516,  0.6719,  0.6016,\n",
      "         0.5586,  0.7227,  0.5859,  0.3047,  0.6602,  0.9219,  0.7344,  0.6094,\n",
      "         0.5898,  0.8008,  0.7383,  0.6914,  0.6484,  1.0391,  0.7266,  0.7188,\n",
      "         0.6797,  0.7109,  1.0312,  0.7070,  0.9023,  0.8555,  0.7070,  0.8672,\n",
      "         0.7383,  0.6914,  0.8672,  2.4062,  0.8203,  0.7578,  0.5703,  0.7539,\n",
      "         0.8594,  0.6875,  1.0625,  1.0391,  0.7227,  0.8750,  0.7266,  0.8125,\n",
      "         0.5156,  0.5586,  0.5898,  0.4707,  0.2305,  0.9062,  0.9375,  1.0312,\n",
      "         1.2812,  0.8867,  0.6484,  0.6992,  0.6055,  0.6367,  0.6914,  0.8750,\n",
      "         0.5508,  0.8594,  1.6719,  0.6055,  0.6367,  0.6484,  0.5703,  0.6680,\n",
      "         0.6250,  0.6836,  1.0391,  0.6797,  1.0859,  0.3066,  0.6445,  0.6719,\n",
      "         0.6562,  0.7148,  0.6836,  0.6992,  0.7031,  0.6562,  0.9219,  1.2734,\n",
      "         1.1406,  1.2109,  1.1328,  0.7969,  0.6367,  1.0547,  0.7266,  0.8008,\n",
      "         1.2891,  1.2031,  0.6562,  0.9531,  0.8281,  1.0391,  0.5859,  0.6367,\n",
      "         0.7500,  1.3906,  0.5469,  0.7305,  0.6016,  0.6055,  0.6719,  0.5508,\n",
      "         0.9453,  0.6211,  0.6328,  0.8789,  0.8516,  0.6641,  1.2812,  0.8672,\n",
      "         0.7422,  0.6328,  0.6953,  0.6328,  0.8438,  0.8203,  0.6758,  0.9297,\n",
      "         0.6562,  0.7422,  0.9180,  0.7891,  0.7070,  1.1328,  0.9336,  0.6328,\n",
      "         0.5820,  0.9805,  0.7891,  0.5508,  0.9844,  0.6094,  0.9922,  1.0547,\n",
      "         0.8125,  0.7148,  0.6094,  0.9961,  0.9141,  0.6680,  0.6250,  0.6914,\n",
      "         0.6094,  0.6328,  0.8047,  1.2422,  0.8555,  0.8047,  0.7539,  0.6719,\n",
      "         0.7617,  0.8047,  0.9727,  0.8008,  0.9141,  1.2969,  1.0625,  0.5977,\n",
      "         0.7070,  0.6289,  0.8281,  0.7773,  0.8047,  1.2344,  0.6133,  1.0547,\n",
      "         0.6562,  1.0703,  1.3047,  0.5078,  0.5664,  0.6328,  1.0469,  0.8438,\n",
      "         0.6406,  0.8438,  0.8633,  0.8594,  0.0171,  0.6211,  1.0781,  0.5742,\n",
      "         1.0000,  0.6719,  1.0156,  1.1406,  0.7070,  0.8203,  1.1328,  0.7031,\n",
      "         0.6406,  1.1484,  0.6719,  1.0078,  0.5078,  0.9062,  0.6367,  1.2031,\n",
      "         0.8125,  1.1094,  0.9648,  0.8164,  0.8555,  0.6445,  0.8008,  0.6211,\n",
      "         0.6133,  0.6289,  0.5586,  0.6641,  0.6680,  0.6641,  1.1172,  0.6602,\n",
      "         0.6133,  0.6641,  0.6250,  0.5508,  0.7969,  0.7969,  0.6133,  0.7695,\n",
      "         0.6055,  0.6289,  0.2451,  1.0547,  1.6719,  0.7773,  0.5742,  0.4004,\n",
      "         0.7773,  0.6211,  0.8711,  0.5977,  0.7891,  0.8711,  0.9570,  0.6328,\n",
      "         1.1406,  0.6250,  1.0859,  0.6250,  0.7422,  0.6289,  0.9062,  0.9336,\n",
      "         0.8008,  0.8633,  0.9180,  1.0078,  0.5898,  0.8555,  0.5977,  0.7578,\n",
      "         1.2812,  0.7461,  0.7188,  0.6406,  0.6914,  0.6211,  0.8281,  0.9648,\n",
      "         0.9570,  0.8086,  0.8984,  0.5938,  0.5703,  0.6055,  0.6367,  0.7539,\n",
      "         0.7578,  0.7227,  0.5742,  0.7227,  0.5664,  0.6133,  0.5977,  0.5781,\n",
      "         0.5742,  0.5586,  0.7812,  0.6094,  0.6172,  0.6523,  0.8906,  0.7773,\n",
      "         0.8828,  1.3828,  0.7344,  0.6016,  1.4531,  0.6484,  0.6055,  0.9648,\n",
      "         0.5820,  0.7656,  0.6992,  0.8398,  0.9609,  0.5742,  0.7656,  1.2500,\n",
      "         0.7812,  0.7031,  0.6641,  0.5625,  0.6172,  0.9336,  0.9375,  0.9570,\n",
      "         0.6758,  0.9297,  0.8086,  0.8867,  0.5430,  0.8633,  0.8242,  0.6094,\n",
      "         1.7031,  0.5898,  0.8711,  0.5781,  0.7500,  0.7773,  1.3281,  0.8281,\n",
      "         0.5703,  0.8359,  0.5859,  0.7422,  0.7656,  1.3359,  0.8672,  1.1406,\n",
      "         0.6719,  0.8711,  1.0234,  0.7031,  0.8086,  0.7031,  0.8047,  0.7852,\n",
      "         0.7695,  0.6680,  1.0859,  0.7773,  0.8906,  0.6523,  0.8906,  1.0078,\n",
      "         0.6875,  0.6719,  0.8125,  0.6953,  0.6875,  0.7734,  1.1797,  1.1094,\n",
      "         0.5703,  1.8672,  0.6094,  0.7578,  0.9180,  0.6445,  0.8398,  1.0000,\n",
      "         0.8711,  0.6094,  1.2734,  1.3047,  0.7383,  0.9258,  0.2275,  0.7383,\n",
      "         0.8828,  0.6836,  0.8047,  0.2832,  0.5898,  0.6953,  0.6992,  0.8516,\n",
      "         0.6797,  0.9727,  0.5625,  1.4688,  0.7930,  0.5703,  0.6406,  0.7070,\n",
      "         0.5430,  0.5781,  0.6055,  0.6641,  0.6172,  0.6289,  0.6680,  0.8867,\n",
      "         0.5938,  0.7461,  0.7773,  0.6758,  0.7852,  0.7188,  1.1719,  0.7227,\n",
      "         0.9648,  0.6250,  0.7539,  0.5430,  0.8359,  0.5703,  0.5781,  0.8086,\n",
      "         0.9805,  0.7305,  0.7500,  0.6328,  0.8789,  0.6562,  0.7109,  0.8984,\n",
      "         0.6641,  0.8203,  0.7070,  0.7383,  0.8125,  0.6367,  0.6758,  1.0469,\n",
      "         0.7500,  0.6641,  0.8281,  1.0781,  0.7344,  0.9414,  0.9062,  0.6953,\n",
      "         0.8633,  1.4375,  0.5820,  0.9688,  0.8242,  0.7656,  1.0000,  0.6328,\n",
      "         0.9297,  0.7695,  0.6406,  0.7812,  0.8438,  0.8008,  0.9141,  0.5820,\n",
      "         0.6328,  0.6211,  0.6680,  0.6875,  0.7031,  0.7812,  0.8633,  1.1328,\n",
      "         0.6797,  0.8320,  0.8320,  0.8984,  0.9023,  0.1328,  0.6914,  1.2344,\n",
      "         0.7109,  0.9961,  0.7148, -0.5898,  0.7422,  0.6289,  0.7539,  1.0312,\n",
      "         0.6641,  0.6719,  0.7617,  0.6367,  0.6836,  0.6758,  0.6992,  0.7109,\n",
      "         0.7461,  0.6953,  0.8281,  0.5977,  0.7422,  1.0625,  0.7148,  0.9219,\n",
      "         0.6016,  0.7969,  0.6094,  0.8164,  1.3047,  0.6094,  0.6602,  0.7188,\n",
      "         0.9180,  0.6055,  0.7461,  0.8477,  0.7891, -0.5430,  0.7656,  0.7227,\n",
      "         0.7695,  0.5859,  0.6328,  1.2891,  1.1641,  0.5859,  2.0938,  0.7422,\n",
      "         0.6953,  0.6289,  0.2988,  0.8281,  0.8906,  0.9531,  0.7578,  1.0781,\n",
      "         0.6094,  0.8242,  1.0234,  0.6523,  0.7969,  0.8281,  0.8906,  0.6016,\n",
      "         0.6172,  0.9883,  0.8125,  0.6758,  0.6055,  0.5547,  0.7148,  0.7070,\n",
      "         0.7266,  0.7383,  0.7031,  0.8359,  0.7656,  1.0547,  0.9375,  0.9336,\n",
      "         0.6484,  0.9805,  0.7773,  0.5625,  0.8398,  1.0391,  0.7539,  0.6836,\n",
      "         1.1875,  0.7305,  0.8047,  0.6836,  0.8047,  0.5977,  0.8477,  0.8750,\n",
      "         0.8633,  0.5781,  0.8047,  0.5898,  0.8086,  0.8438,  0.6875,  0.9805],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.3.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0082, -0.0126, -0.0077,  ...,  0.0366,  0.0393, -0.0052],\n",
      "        [ 0.0393,  0.0162, -0.0486,  ...,  0.0011, -0.0084,  0.0151],\n",
      "        [ 0.0221, -0.0197, -0.0339,  ..., -0.0016, -0.0356, -0.0391],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0156, -0.0166,  ...,  0.0352, -0.0097,  0.0113],\n",
      "        [-0.0026,  0.0228, -0.0150,  ..., -0.0500,  0.0031, -0.0154],\n",
      "        [ 0.0757,  0.0148,  0.0182,  ...,  0.1035,  0.0162, -0.0165]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.3.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.3613, -0.2441, -0.2422,  ..., -0.2451, -0.0408, -0.0396],\n",
      "        [-0.0356, -0.0059,  0.0928,  ...,  0.1113, -0.7422,  0.0586],\n",
      "        [ 0.1230,  0.2910,  0.1006,  ..., -0.4219,  0.0098,  0.1738],\n",
      "        ...,\n",
      "        [-0.2080,  0.4219,  0.0615,  ..., -0.3984, -0.0869,  0.1445],\n",
      "        [-0.1660,  0.4434, -0.2754,  ..., -0.4570, -0.1226, -0.1309],\n",
      "        [ 0.5078, -0.0488, -0.1260,  ...,  0.5117, -0.0211,  0.0361]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.3.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0039, -0.0298,  0.5117,  ..., -0.0889,  0.0776, -0.1064],\n",
      "        [ 0.7461, -0.6484, -0.1445,  ..., -0.1465, -0.5820, -0.0304],\n",
      "        [ 0.7812,  0.7773, -0.6836,  ...,  0.1416,  0.2500,  0.0471],\n",
      "        ...,\n",
      "        [ 0.4707,  0.5039, -0.5234,  ...,  0.4141,  0.0625,  0.0300],\n",
      "        [-0.3672,  0.2314, -0.3320,  ..., -0.4629, -0.1670, -0.2793],\n",
      "        [-0.1611, -0.5039, -0.7031,  ...,  0.7578,  0.2402,  0.0918]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.3.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0225, -0.2539, -0.8711,  ...,  0.4473,  0.8125, -0.3711],\n",
      "        [ 0.2598,  0.6875, -0.7227,  ...,  0.1377, -0.1729,  0.5000],\n",
      "        [-0.7734, -0.0208,  1.1172,  ..., -0.1455,  0.5977, -0.1953],\n",
      "        ...,\n",
      "        [-0.1328, -0.1133, -0.2227,  ..., -0.3633,  0.5781, -0.0330],\n",
      "        [ 0.1377,  0.3086, -0.7461,  ..., -0.6055, -0.3789, -0.0918],\n",
      "        [-0.2832, -0.3008, -0.1895,  ..., -0.1099, -0.3203,  0.6992]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.3.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 0.1982,  0.1943,  0.2070,  0.2285,  0.1357,  0.1582,  0.0564,  0.2109,\n",
      "         0.1475,  0.1650,  0.1992,  0.1826,  0.1250,  0.1895,  0.1299,  0.1787,\n",
      "         0.1914,  0.1846,  0.2412,  0.1533,  0.1426,  0.1719,  0.0245,  0.1455,\n",
      "         0.1494,  0.1973,  0.1387,  0.1797,  0.1865,  0.1377,  0.1572,  0.1602,\n",
      "         0.1826,  0.1426,  0.2012,  0.1670,  0.1631,  0.2910,  0.1328,  0.2373,\n",
      "         0.1924,  0.2021,  0.2129,  0.1826,  0.1191,  0.1592,  0.1670,  0.1592,\n",
      "         0.1924,  0.1396,  0.2383,  0.1865,  0.1641,  0.1426,  0.0010,  0.1650,\n",
      "         0.0630,  0.1631,  0.1709,  0.1543,  0.1895,  0.1299,  0.1748,  0.1465,\n",
      "         0.1504,  0.0664,  0.1592,  0.1494,  0.2480,  0.1445,  0.1689,  0.2227,\n",
      "         0.1348,  0.1816,  0.1523,  0.1455,  0.1758,  0.1748,  0.1367,  0.1992,\n",
      "         0.1553,  0.1846,  0.1680,  0.2324,  0.1738,  0.1836,  0.2793,  0.1963,\n",
      "         0.1699,  0.1484,  0.1846,  0.1348,  0.1387,  0.1777,  0.1436,  0.1475,\n",
      "         0.1582,  0.1553,  0.1504,  0.1348,  0.2207,  0.1719,  0.1934,  0.1377,\n",
      "         0.1582,  0.2266,  0.1641,  0.1680,  0.1367,  0.1504,  0.1816,  0.1680,\n",
      "         0.1582,  0.1396,  0.1523,  0.1465,  0.1719,  0.1367,  0.3027,  0.1582,\n",
      "         0.1270,  0.1523,  0.2109,  0.1611,  0.2158,  0.1729,  0.1709,  0.1719,\n",
      "         0.1699,  0.2090,  0.1309,  0.1963,  0.1426,  0.1250,  0.1562,  0.2021,\n",
      "         0.2021,  0.1533,  0.1768,  0.2021,  0.1572,  0.2256,  0.2227,  0.2002,\n",
      "         0.1768,  0.2158,  0.1504,  0.2021,  0.1436,  0.1807,  0.1582,  0.1787,\n",
      "         0.1816,  0.2598,  0.1436,  0.1377,  0.1992,  0.1387,  0.1904,  0.1504,\n",
      "         0.1357,  0.1523,  0.1357,  0.1328,  0.1377,  0.1426,  0.1543,  0.2090,\n",
      "         0.1885,  0.2773,  0.2480,  0.1816,  0.1660,  0.1816,  0.1670,  0.1689,\n",
      "         0.1885,  0.1943,  0.1924,  0.0320,  0.1582,  0.1455,  0.1709,  0.1426,\n",
      "         0.2031,  0.2363,  0.1465,  0.2256,  0.1797,  0.1758,  0.1406,  0.1514,\n",
      "         0.1069,  0.1680,  0.1377,  0.0698,  0.1592,  0.2129,  0.1689,  0.1108,\n",
      "         0.1357,  0.1777,  0.1670,  0.1514,  0.1699,  0.2354,  0.1650,  0.1602,\n",
      "         0.1504,  0.1719,  0.2002,  0.1484,  0.1973,  0.1914,  0.1572,  0.2061,\n",
      "         0.1807,  0.1592,  0.2031,  0.2344,  0.1279,  0.1816,  0.1235,  0.1836,\n",
      "         0.1621,  0.1592,  0.2246,  0.2480,  0.1816,  0.2168,  0.1436,  0.1758,\n",
      "         0.0986,  0.1348,  0.1396,  0.0972,  0.0688,  0.1982,  0.1924,  0.2227,\n",
      "         0.2676,  0.2090,  0.1582,  0.1689,  0.1357,  0.1416,  0.1621,  0.1953,\n",
      "         0.1260,  0.1904,  0.1416,  0.1357,  0.1406,  0.1377,  0.1309,  0.1396,\n",
      "         0.1504,  0.1309,  0.1377,  0.1543,  0.2354,  0.1475,  0.1484,  0.1660,\n",
      "         0.1514,  0.1582,  0.1602,  0.1562,  0.1709,  0.1543,  0.0747,  0.2695,\n",
      "         0.2109,  0.0640,  0.2559,  0.1895,  0.1670,  0.2119,  0.1689,  0.1602,\n",
      "         0.2852,  0.2412,  0.1631,  0.1738,  0.1992,  0.1855,  0.1387,  0.1543,\n",
      "         0.1611,  0.2139,  0.1318,  0.1670,  0.1387,  0.1592,  0.1543,  0.1279,\n",
      "         0.2207,  0.1621,  0.1514,  0.2002,  0.2168,  0.1592,  0.2832,  0.1855,\n",
      "         0.1768,  0.1367,  0.1709,  0.1465,  0.1768,  0.1855,  0.1719,  0.2246,\n",
      "         0.1494,  0.1270,  0.2178,  0.1807,  0.1182,  0.3008,  0.1416,  0.1484,\n",
      "         0.1377,  0.1982,  0.1709,  0.1396,  0.2012,  0.1367,  0.2227,  0.2578,\n",
      "         0.1787,  0.1836,  0.1543,  0.1914,  0.1836,  0.1621,  0.1562,  0.1553,\n",
      "         0.1455,  0.1533,  0.1807,  0.2852,  0.2031,  0.2051,  0.1846,  0.1484,\n",
      "         0.1885,  0.1836,  0.1680,  0.1738,  0.2021,  0.1641,  0.2285,  0.1426,\n",
      "         0.1650,  0.1572,  0.1924,  0.1836,  0.1924,  0.2500,  0.1465,  0.2207,\n",
      "         0.1553,  0.2412,  0.1855,  0.1060,  0.1377,  0.1592,  0.1631,  0.1641,\n",
      "         0.1377,  0.1816,  0.1807,  0.2070,  0.0304,  0.1318,  0.2295,  0.1387,\n",
      "         0.1973,  0.1553,  0.2500,  0.1689,  0.1592,  0.1846,  0.2354,  0.1592,\n",
      "         0.1553,  0.1250,  0.1572,  0.2314,  0.1074,  0.2148,  0.1504,  0.1709,\n",
      "         0.1797,  0.2451,  0.2266,  0.1719,  0.1982,  0.1426,  0.1807,  0.1514,\n",
      "         0.1348,  0.0447,  0.1318,  0.1650,  0.1455,  0.1572,  0.2197,  0.1553,\n",
      "         0.1484,  0.1406,  0.1611,  0.1348,  0.1689,  0.1650,  0.1455,  0.1719,\n",
      "         0.1309,  0.1367,  0.0684,  0.1875,  0.2852,  0.1748,  0.1328,  0.0618,\n",
      "         0.1846,  0.1543,  0.1914,  0.1445,  0.1787,  0.1807,  0.2119,  0.1572,\n",
      "         0.1777,  0.1367,  0.2363,  0.1504,  0.1660,  0.1523,  0.2129,  0.2217,\n",
      "         0.1943,  0.1660,  0.2139,  0.2480,  0.1455,  0.1904,  0.1357,  0.1953,\n",
      "         0.2471,  0.1748,  0.1738,  0.1533,  0.1719,  0.1475,  0.1924,  0.2002,\n",
      "         0.2139,  0.1738,  0.1943,  0.1387,  0.1396,  0.1426,  0.1553,  0.1797,\n",
      "         0.1797,  0.1875,  0.1187,  0.1533,  0.1172,  0.1504,  0.1279,  0.1357,\n",
      "         0.1299,  0.1299,  0.1807,  0.1582,  0.1504,  0.1582,  0.1484,  0.1826,\n",
      "         0.2168,  0.1650,  0.1650,  0.1465,  0.3203,  0.1523,  0.1416,  0.2168,\n",
      "        -0.1338,  0.1816,  0.1777,  0.1914,  0.1230,  0.1533,  0.1611,  0.2207,\n",
      "         0.1807,  0.1729,  0.1553,  0.1357,  0.1455,  0.2158,  0.1816,  0.0693,\n",
      "         0.1611,  0.2061,  0.1982,  0.2080,  0.1328,  0.1846,  0.1924,  0.1494,\n",
      "         0.1211,  0.1514,  0.2012,  0.1475,  0.1660,  0.1465,  0.2100,  0.1982,\n",
      "         0.1387,  0.1455,  0.1211,  0.1641,  0.1816,  0.1758,  0.2100,  0.2676,\n",
      "         0.1562,  0.2109,  0.2168,  0.1650,  0.1807,  0.1660,  0.1738,  0.1738,\n",
      "         0.1797,  0.1650,  0.2246,  0.1768,  0.2061,  0.1494,  0.1943,  0.2471,\n",
      "         0.1553,  0.1553,  0.2080,  0.1699,  0.1504,  0.1377,  0.2344,  0.2441,\n",
      "         0.1250,  0.2119,  0.1475,  0.1621,  0.1846,  0.1553,  0.1855,  0.1895,\n",
      "         0.1943,  0.1250,  0.2520,  0.2793,  0.1924,  0.2148,  0.0415,  0.1709,\n",
      "         0.1885,  0.1602,  0.2012,  0.0884,  0.1216,  0.1572,  0.1709,  0.1885,\n",
      "         0.1592,  0.2324,  0.1504,  0.2637,  0.1689,  0.1484,  0.1533,  0.1582,\n",
      "         0.1289,  0.1338,  0.1416,  0.1494,  0.1426,  0.1504,  0.1660,  0.2109,\n",
      "         0.1533,  0.1807,  0.1865,  0.1797,  0.1768,  0.1758,  0.2383,  0.1660,\n",
      "         0.2305,  0.1621,  0.1895,  0.0918,  0.2051,  0.1279,  0.1367,  0.1807,\n",
      "         0.2100,  0.1689,  0.1768,  0.1387,  0.1846,  0.1475,  0.1592,  0.2109,\n",
      "         0.1416,  0.1846,  0.1689,  0.1689,  0.1816,  0.1299,  0.1582,  0.2422,\n",
      "         0.1758,  0.1562,  0.1680,  0.1768,  0.1650,  0.1680,  0.2031,  0.1670,\n",
      "         0.1826,  0.3008,  0.1445,  0.1924,  0.1797,  0.1807,  0.2578,  0.1475,\n",
      "         0.1895,  0.1816,  0.1406,  0.1719,  0.2119,  0.1963,  0.1924,  0.1377,\n",
      "         0.1377,  0.1406,  0.1455,  0.1660,  0.1592,  0.1621,  0.1982,  0.2217,\n",
      "         0.1592,  0.1660,  0.2012,  0.2070,  0.1963,  0.0310,  0.1621,  0.2773,\n",
      "         0.1689,  0.1963,  0.1553,  0.1465,  0.1904,  0.1572,  0.1768,  0.1641,\n",
      "         0.1562,  0.1436,  0.1855,  0.1543,  0.1621,  0.1562,  0.1514,  0.1670,\n",
      "         0.1689,  0.1602,  0.1885,  0.1226,  0.1689,  0.1226,  0.1533,  0.2109,\n",
      "         0.1465,  0.1953,  0.1377,  0.2012,  0.2578,  0.1426,  0.1611,  0.1465,\n",
      "         0.2168,  0.1328,  0.1729,  0.1846,  0.1816,  0.1108,  0.1836,  0.1562,\n",
      "         0.1738,  0.1367,  0.1514,  0.1973,  0.2637,  0.1406,  0.1260,  0.1719,\n",
      "         0.1660,  0.1562,  0.0547,  0.1875,  0.1836,  0.2227,  0.1738,  0.2393,\n",
      "         0.1572,  0.1436,  0.2373,  0.1445,  0.1992,  0.1914,  0.2031,  0.1270,\n",
      "         0.1533,  0.1963,  0.1992,  0.1641,  0.1504,  0.1157,  0.1562,  0.1660,\n",
      "         0.1699,  0.1729,  0.1709,  0.2021,  0.1777,  0.2090,  0.2227,  0.2139,\n",
      "         0.1621,  0.2139,  0.1846,  0.1357,  0.2188,  0.2236,  0.1719,  0.1611,\n",
      "         0.2471,  0.1670,  0.1934,  0.1650,  0.1709,  0.1484,  0.2070,  0.2051,\n",
      "         0.1787,  0.1206,  0.1816,  0.1289,  0.1895,  0.1943,  0.1631,  0.2334],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.3.layer.1.DenseReluDense.wi.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.2676, -0.5156,  0.9336,  ..., -0.4141, -0.6328, -0.6797],\n",
      "        [-0.1719, -0.3164, -0.2471,  ...,  0.6875, -0.0471, -0.0222],\n",
      "        [-0.3574,  0.0825,  0.2363,  ..., -0.5938,  0.5898,  0.0364],\n",
      "        ...,\n",
      "        [ 0.1387, -0.6133, -0.4395,  ...,  0.2676, -0.4473,  0.1787],\n",
      "        [-0.2793,  0.0145,  0.5469,  ...,  0.2383, -0.2070, -0.2578],\n",
      "        [ 0.7969, -0.2197, -0.4141,  ...,  0.6133,  0.2559, -0.3574]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.3.layer.1.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.2451,  0.0415,  0.0457,  ..., -0.1367,  0.0820,  0.1904],\n",
      "        [-0.0366,  0.1953, -0.1631,  ..., -0.0898,  0.2090, -0.2598],\n",
      "        [-0.1875, -0.1475,  0.1602,  ..., -0.3945,  0.3184,  0.0166],\n",
      "        ...,\n",
      "        [ 0.4062,  0.3613,  0.0087,  ...,  0.2051, -0.3730,  0.0049],\n",
      "        [ 0.3633,  0.2930, -0.1289,  ..., -0.0884,  0.1143, -0.3574],\n",
      "        [ 0.0081, -0.1338, -0.0771,  ..., -0.2656,  0.1260, -0.2139]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.3.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 1.0391e+00,  1.1250e+00,  1.0234e+00,  1.2500e+00,  8.0078e-01,\n",
      "         1.0625e+00,  5.4297e-01,  1.1172e+00,  3.2031e+00,  9.8438e-01,\n",
      "         1.0781e+00,  1.0938e+00,  9.3750e-01,  1.3281e+00,  8.0469e-01,\n",
      "         9.5703e-01,  1.0938e+00,  1.0391e+00,  1.2500e+00,  9.1016e-01,\n",
      "         9.7656e-01,  1.0234e+00,  9.8828e-01,  8.3203e-01,  8.0859e-01,\n",
      "         1.1719e+00,  7.7344e-01,  1.0234e+00,  1.0312e+00,  8.0859e-01,\n",
      "         9.1406e-01,  8.9062e-01,  1.0391e+00,  8.1250e-01,  1.2188e+00,\n",
      "         9.6094e-01,  1.5625e+00,  1.3984e+00,  7.8125e-01,  1.3750e+00,\n",
      "         1.1328e+00,  1.3438e+00,  1.2109e+00,  9.8047e-01,  3.2031e+00,\n",
      "         9.4531e-01,  1.1953e+00,  9.3750e-01,  9.8828e-01,  1.2500e+00,\n",
      "         1.3047e+00,  9.8438e-01,  1.5547e+00,  8.2031e-01,  1.1406e+00,\n",
      "         9.5703e-01,  1.4844e+00,  1.0000e+00,  8.9453e-01,  9.1797e-01,\n",
      "         1.0391e+00,  7.6562e-01,  9.9609e-01,  8.5547e-01,  8.5156e-01,\n",
      "         1.0781e+00,  9.1016e-01,  9.0625e-01,  1.4297e+00,  8.8672e-01,\n",
      "         9.4922e-01,  1.3984e+00,  7.9688e-01,  9.5312e-01,  8.9062e-01,\n",
      "         8.7891e-01,  1.3125e+00,  1.0156e+00,  8.2812e-01,  1.0000e+00,\n",
      "         8.6328e-01,  1.0312e+00,  9.4922e-01,  1.4453e+00,  1.0000e+00,\n",
      "         9.5703e-01,  1.5938e+00,  1.0312e+00,  9.6094e-01,  8.4766e-01,\n",
      "         9.8828e-01,  8.4375e-01,  8.0469e-01,  1.0625e+00,  8.4766e-01,\n",
      "         8.2031e-01,  8.7109e-01,  8.1641e-01,  7.9688e-01,  8.4766e-01,\n",
      "         1.2500e+00,  1.0234e+00,  1.2969e+00,  7.8906e-01,  9.3359e-01,\n",
      "         1.5938e+00,  9.1016e-01,  9.6484e-01,  8.0078e-01,  1.4922e+00,\n",
      "         1.1172e+00,  9.1016e-01,  9.6484e-01,  9.6484e-01,  8.9844e-01,\n",
      "         8.2031e-01,  1.0000e+00,  8.5156e-01,  1.8281e+00,  8.9062e-01,\n",
      "         7.4609e-01,  8.2422e-01,  1.3984e+00,  8.6328e-01,  1.2422e+00,\n",
      "         9.9219e-01,  9.2969e-01,  1.0078e+00,  1.0156e+00,  1.1172e+00,\n",
      "         1.6797e+00,  1.0703e+00,  8.1250e-01,  7.6172e-01,  9.1406e-01,\n",
      "         1.2969e+00,  1.1484e+00,  8.9062e-01,  1.1172e+00,  1.1016e+00,\n",
      "         9.7656e-01,  1.3203e+00,  1.3438e+00,  1.3203e+00,  1.0000e+00,\n",
      "         1.3359e+00,  8.3984e-01,  1.1250e+00,  8.9062e-01,  1.0391e+00,\n",
      "         9.0234e-01,  9.9609e-01,  1.0078e+00,  1.4688e+00,  8.0469e-01,\n",
      "         8.1641e-01,  1.1328e+00,  8.1641e-01,  1.2031e+00,  9.6875e-01,\n",
      "         7.9297e-01,  9.4922e-01,  9.2969e-01,  2.9688e+00,  8.1641e-01,\n",
      "         7.8906e-01,  9.0234e-01,  1.1641e+00,  9.4531e-01,  1.3203e+00,\n",
      "         1.3125e+00,  9.5312e-01,  8.8672e-01,  1.4062e+00,  8.9453e-01,\n",
      "         9.8438e-01,  1.0547e+00,  9.8438e-01,  1.3203e+00,  3.1250e-01,\n",
      "         9.1016e-01,  8.5156e-01,  9.8047e-01,  7.7344e-01,  1.0547e+00,\n",
      "         1.2422e+00,  8.4766e-01,  1.2188e+00,  1.0859e+00,  1.0547e+00,\n",
      "         8.5156e-01,  8.2812e-01,  7.3047e-01,  9.2969e-01,  7.8906e-01,\n",
      "         3.8281e-01,  9.1797e-01,  1.2656e+00,  1.0234e+00,  8.0859e-01,\n",
      "         8.3594e-01,  1.0469e+00,  9.4141e-01,  9.4922e-01,  8.9453e-01,\n",
      "         1.2891e+00,  9.9219e-01,  9.6484e-01,  8.8672e-01,  9.6094e-01,\n",
      "         1.2812e+00,  9.5703e-01,  1.1641e+00,  1.1641e+00,  8.9844e-01,\n",
      "         1.1484e+00,  9.8438e-01,  8.6328e-01,  1.1016e+00,  2.7969e+00,\n",
      "         1.0234e+00,  1.0234e+00,  7.8125e-01,  1.0391e+00,  1.1797e+00,\n",
      "         8.9844e-01,  1.2891e+00,  1.2891e+00,  9.6094e-01,  1.1094e+00,\n",
      "         8.8672e-01,  1.0703e+00, -6.1328e-01,  7.7344e-01,  7.8516e-01,\n",
      "         6.4453e-01,  4.5312e-01,  1.1875e+00,  1.2188e+00,  1.3125e+00,\n",
      "         1.4766e+00,  1.2031e+00,  8.9453e-01,  9.4531e-01,  8.1641e-01,\n",
      "         8.7891e-01,  9.5703e-01,  1.1172e+00,  7.4609e-01,  1.0781e+00,\n",
      "         2.6094e+00,  8.4375e-01,  8.6328e-01,  8.8281e-01,  7.9297e-01,\n",
      "         8.7109e-01,  8.6328e-01,  8.8281e-01,  1.2812e+00,  9.3750e-01,\n",
      "         1.3516e+00,  2.6758e-01,  8.7500e-01,  9.0234e-01,  9.0234e-01,\n",
      "         9.6094e-01,  8.9453e-01,  8.8672e-01,  9.6094e-01,  9.0625e-01,\n",
      "         1.1719e+00,  1.4453e+00,  1.4844e+00,  1.7500e+00,  1.2812e+00,\n",
      "         1.1094e+00,  9.4922e-01,  1.2812e+00,  9.4531e-01,  9.7266e-01,\n",
      "         1.4688e+00,  1.4062e+00,  9.1797e-01,  1.4062e+00,  1.0234e+00,\n",
      "         1.2656e+00,  7.8516e-01,  8.6328e-01,  9.3359e-01,  1.8203e+00,\n",
      "         7.8516e-01,  9.8828e-01,  8.1641e-01,  8.3203e-01,  8.9453e-01,\n",
      "         7.8906e-01,  1.1875e+00,  9.2578e-01,  8.5547e-01,  1.1953e+00,\n",
      "         1.1250e+00,  8.3984e-01,  1.5000e+00,  1.1016e+00,  1.0000e+00,\n",
      "         7.9297e-01,  9.4141e-01,  8.9062e-01,  1.0469e+00,  1.1016e+00,\n",
      "         9.5312e-01,  1.1875e+00,  8.7109e-01, -9.0625e-01, -1.1641e+00,\n",
      "         1.0625e+00,  8.7500e-01,  1.3906e+00,  1.3125e+00,  9.0625e-01,\n",
      "         7.8516e-01,  1.2812e+00,  1.0000e+00,  7.6953e-01,  1.2734e+00,\n",
      "         8.2422e-01,  1.2578e+00,  1.3516e+00,  1.0938e+00,  9.7656e-01,\n",
      "         8.2812e-01,  1.3047e+00,  1.1641e+00,  9.2969e-01,  9.0234e-01,\n",
      "         9.8438e-01,  8.2422e-01,  8.0859e-01,  9.7656e-01,  1.4297e+00,\n",
      "         1.1094e+00,  1.0156e+00,  1.0625e+00,  9.0234e-01,  9.9609e-01,\n",
      "         1.0703e+00,  1.3047e+00,  1.0234e+00,  1.1875e+00,  1.8281e+00,\n",
      "         1.2969e+00,  7.9297e-01,  9.5312e-01,  8.6719e-01,  1.0781e+00,\n",
      "         9.8828e-01,  1.1250e+00,  1.3359e+00,  8.7891e-01,  1.2891e+00,\n",
      "         8.8672e-01,  1.3359e+00,  1.5703e+00,  6.5234e-01,  8.2812e-01,\n",
      "         9.0625e-01,  1.1875e+00,  1.0938e+00,  8.5547e-01,  1.0781e+00,\n",
      "         1.1641e+00,  1.1094e+00,  9.4891e-05,  8.4766e-01,  1.3047e+00,\n",
      "         7.9297e-01,  1.2812e+00,  8.8672e-01,  1.3828e+00,  1.3984e+00,\n",
      "         8.9062e-01,  1.0547e+00,  1.3594e+00,  9.3359e-01,  9.1016e-01,\n",
      "         1.3906e+00,  8.6719e-01,  1.2266e+00,  6.7969e-01,  1.2109e+00,\n",
      "         8.8281e-01,  1.7656e+00,  1.0547e+00,  1.2969e+00,  1.1953e+00,\n",
      "         1.0391e+00,  1.1484e+00,  8.2422e-01,  1.0469e+00,  8.7891e-01,\n",
      "         8.2031e-01,  1.0547e+00,  7.4609e-01,  8.7891e-01,  8.6328e-01,\n",
      "         9.1406e-01,  1.2812e+00,  8.5156e-01,  8.6328e-01,  8.7500e-01,\n",
      "         8.5156e-01,  8.5938e-01,  1.0312e+00,  1.0391e+00,  8.3203e-01,\n",
      "         9.9609e-01,  8.5547e-01,  8.3594e-01,  3.2812e-01,  1.3516e+00,\n",
      "         1.9062e+00,  1.0234e+00,  7.8906e-01,  4.3359e-01,  1.0234e+00,\n",
      "         8.3203e-01,  1.1094e+00,  8.2812e-01,  1.0234e+00,  1.0547e+00,\n",
      "         1.1953e+00,  8.7109e-01,  1.4844e+00,  7.8906e-01,  1.4219e+00,\n",
      "         8.6328e-01,  1.0078e+00,  8.2422e-01,  1.1562e+00,  1.1797e+00,\n",
      "         1.0781e+00,  1.0469e+00,  1.1875e+00,  1.2891e+00,  8.2422e-01,\n",
      "         1.0391e+00,  8.2812e-01,  1.0625e+00,  1.3750e+00,  9.8438e-01,\n",
      "         9.6484e-01,  7.9688e-01,  9.8047e-01,  8.5156e-01,  1.1094e+00,\n",
      "         1.2266e+00,  1.1562e+00,  1.0234e+00,  1.1562e+00,  7.8125e-01,\n",
      "         7.6562e-01,  8.5156e-01,  8.7109e-01,  1.0000e+00,  1.0156e+00,\n",
      "         9.6875e-01,  7.4609e-01,  9.2969e-01,  7.7734e-01,  8.2812e-01,\n",
      "         8.0859e-01,  7.5391e-01,  7.3828e-01,  8.1250e-01,  1.0312e+00,\n",
      "         8.5938e-01,  8.5156e-01,  8.7891e-01,  1.1875e+00,  9.9219e-01,\n",
      "         1.1875e+00,  1.9219e+00,  9.9609e-01,  8.6719e-01,  1.6016e+00,\n",
      "         8.6719e-01,  8.5547e-01,  1.1875e+00,  7.9688e-01,  1.0000e+00,\n",
      "         9.7656e-01,  1.1016e+00,  1.3203e+00,  8.6328e-01,  1.0312e+00,\n",
      "         1.4453e+00,  1.0703e+00,  9.1797e-01,  9.2578e-01,  7.9297e-01,\n",
      "         8.6719e-01,  1.1094e+00,  1.2109e+00,  1.3359e+00,  8.7891e-01,\n",
      "         1.1328e+00,  1.0156e+00,  1.1172e+00,  7.2266e-01,  1.1250e+00,\n",
      "         1.0625e+00,  8.4766e-01,  2.5469e+00,  8.2422e-01,  1.1016e+00,\n",
      "         8.1250e-01,  9.6484e-01,  1.0078e+00,  1.5938e+00,  1.0703e+00,\n",
      "         7.6953e-01,  1.0938e+00,  7.5000e-01,  9.9609e-01,  9.8047e-01,\n",
      "         1.8281e+00,  1.1250e+00,  1.4375e+00,  9.0625e-01,  1.0938e+00,\n",
      "         1.3125e+00,  9.4141e-01,  1.0547e+00,  9.8047e-01,  1.0547e+00,\n",
      "         1.0234e+00,  9.6094e-01,  9.3750e-01,  1.3125e+00,  1.0625e+00,\n",
      "         1.1562e+00,  8.9844e-01,  1.1094e+00,  1.2500e+00,  9.4531e-01,\n",
      "         9.1016e-01,  1.0938e+00,  9.8438e-01,  9.3359e-01,  8.3203e-01,\n",
      "         1.4453e+00,  1.3672e+00,  7.5391e-01,  1.6484e+00,  8.5156e-01,\n",
      "         9.8828e-01,  1.1875e+00,  8.8672e-01,  1.0859e+00,  1.2266e+00,\n",
      "         1.1484e+00,  7.9297e-01,  1.5312e+00,  1.4609e+00,  9.9219e-01,\n",
      "         1.1953e+00, -3.4943e-03,  9.5312e-01,  1.0938e+00,  9.2578e-01,\n",
      "         1.1016e+00,  2.9297e-01,  7.5781e-01,  9.4922e-01,  9.6875e-01,\n",
      "         1.0859e+00,  9.4922e-01,  1.2422e+00,  8.2031e-01,  1.6406e+00,\n",
      "         9.9609e-01,  8.5547e-01,  8.7109e-01,  8.9453e-01,  7.8516e-01,\n",
      "         7.7344e-01,  8.0078e-01,  8.7109e-01,  8.5938e-01,  8.4375e-01,\n",
      "         8.8672e-01,  1.1172e+00,  8.3203e-01,  9.3359e-01,  9.2578e-01,\n",
      "         8.9844e-01,  9.8438e-01,  9.5312e-01,  1.3672e+00,  9.0625e-01,\n",
      "         1.2812e+00,  8.7891e-01,  9.8047e-01,  6.7578e-01,  1.0781e+00,\n",
      "         7.7344e-01,  8.3203e-01,  1.0391e+00,  1.3281e+00,  9.2578e-01,\n",
      "         1.0078e+00,  8.3203e-01,  1.1406e+00,  8.3594e-01,  9.8828e-01,\n",
      "         1.2031e+00,  8.9453e-01,  1.0078e+00,  9.5703e-01,  9.4922e-01,\n",
      "         1.0781e+00,  8.3984e-01,  8.8672e-01,  1.3125e+00,  1.0312e+00,\n",
      "         9.2188e-01,  1.0234e+00,  1.3984e+00,  9.8438e-01,  1.1953e+00,\n",
      "         1.1016e+00,  9.6094e-01,  1.1094e+00,  1.6172e+00,  8.0469e-01,\n",
      "         1.1250e+00,  1.0469e+00,  1.0078e+00,  1.2031e+00,  8.4375e-01,\n",
      "         1.1562e+00,  1.0234e+00,  8.8281e-01,  1.0703e+00,  1.0547e+00,\n",
      "         1.0391e+00,  1.1016e+00,  7.8516e-01,  8.4375e-01,  8.6328e-01,\n",
      "         8.4766e-01,  9.3750e-01,  9.3750e-01,  9.6094e-01,  1.0625e+00,\n",
      "         1.3516e+00,  8.7109e-01,  1.0703e+00,  1.0312e+00,  1.1953e+00,\n",
      "         1.0938e+00,  9.4727e-02,  9.0234e-01,  1.4062e+00,  9.4531e-01,\n",
      "         1.2578e+00,  8.9844e-01,  8.4766e-01,  9.7656e-01,  8.4766e-01,\n",
      "         1.0000e+00,  1.4688e+00,  9.5312e-01,  8.7891e-01,  9.8047e-01,\n",
      "         8.6719e-01,  8.8281e-01,  9.1016e-01,  9.1797e-01,  9.3359e-01,\n",
      "         9.5703e-01,  9.0625e-01,  1.0859e+00,  7.8516e-01,  9.6484e-01,\n",
      "         1.4609e+00,  9.0625e-01,  1.1641e+00,  8.5547e-01,  1.0547e+00,\n",
      "         8.5156e-01,  1.0625e+00,  1.5469e+00,  8.4766e-01,  9.1016e-01,\n",
      "         9.0234e-01,  1.2031e+00,  8.0078e-01,  8.9844e-01,  1.0547e+00,\n",
      "         1.0625e+00,  7.6562e-01,  9.7656e-01,  9.6094e-01,  9.9219e-01,\n",
      "         8.3203e-01,  8.9062e-01,  1.6562e+00,  1.4062e+00,  7.7734e-01,\n",
      "         2.9219e+00,  9.8438e-01,  9.2969e-01,  8.6328e-01,  4.0039e-01,\n",
      "         1.0391e+00,  1.1797e+00,  1.2109e+00,  9.5703e-01,  1.3828e+00,\n",
      "         8.9453e-01,  1.0156e+00,  1.2422e+00,  8.6328e-01,  1.0078e+00,\n",
      "         9.6484e-01,  1.0625e+00,  8.2812e-01,  8.5156e-01,  1.2188e+00,\n",
      "         1.0859e+00,  8.9453e-01,  8.4375e-01,  7.3828e-01,  9.2188e-01,\n",
      "         9.4141e-01,  9.5312e-01,  9.6484e-01,  9.3750e-01,  9.9609e-01,\n",
      "         1.0000e+00,  1.3047e+00,  1.1562e+00,  1.1797e+00,  9.2578e-01,\n",
      "         1.2500e+00,  1.0469e+00,  8.0859e-01,  1.1172e+00,  1.3906e+00,\n",
      "         9.9609e-01,  9.2578e-01,  1.4688e+00,  9.6875e-01,  1.0469e+00,\n",
      "         8.7109e-01,  1.0234e+00,  8.5938e-01,  1.0156e+00,  1.0781e+00,\n",
      "         1.0312e+00,  6.9922e-01,  1.0000e+00,  8.1250e-01,  1.0391e+00,\n",
      "         1.0625e+00,  9.0625e-01,  1.2422e+00], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.encoder.block.4.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0125, -0.0437,  0.0220,  ...,  0.0469,  0.0398,  0.0272],\n",
      "        [ 0.0049,  0.0009, -0.0618,  ..., -0.0210,  0.0508, -0.0284],\n",
      "        [ 0.0674, -0.0098,  0.0205,  ..., -0.0053, -0.0344, -0.0405],\n",
      "        ...,\n",
      "        [ 0.0417, -0.0471,  0.0148,  ..., -0.0046, -0.0649, -0.0178],\n",
      "        [ 0.0356, -0.0193, -0.0491,  ...,  0.0060, -0.0141, -0.0032],\n",
      "        [-0.0630,  0.0103,  0.0157,  ...,  0.0102,  0.0034,  0.0179]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.4.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1001,  0.2422,  0.0771,  ...,  0.0179,  0.0381, -0.1318],\n",
      "        [ 0.1357,  0.6250,  0.0776,  ..., -0.1904, -0.1807,  0.0991],\n",
      "        [-0.0090, -0.0549, -0.1904,  ..., -0.0693,  0.0669, -0.0942],\n",
      "        ...,\n",
      "        [ 0.3047, -0.3379,  0.1157,  ...,  0.4434, -0.6289, -0.0801],\n",
      "        [ 0.3848, -0.1797, -0.5938,  ..., -0.0679, -0.1309, -0.0101],\n",
      "        [ 0.1631,  0.0210, -0.3184,  ...,  0.0199, -0.0131,  0.0320]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.4.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0339,  0.2930,  0.0281,  ..., -0.0122, -0.1885,  0.5469],\n",
      "        [ 0.3066, -0.2314,  0.4648,  ...,  0.0388,  0.0388, -0.4219],\n",
      "        [-0.2949, -0.0281,  0.3125,  ..., -1.0859, -0.1758,  0.1094],\n",
      "        ...,\n",
      "        [-0.1592, -0.1787,  0.2773,  ...,  0.1963, -0.2910, -0.0359],\n",
      "        [ 0.1196,  0.2578,  1.0312,  ...,  0.3691,  0.7461, -0.5078],\n",
      "        [-0.7969,  0.7422, -0.3359,  ..., -0.2451,  0.3242, -0.1816]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.4.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.2227, -0.3418,  0.3867,  ...,  0.7891,  0.7344,  0.1484],\n",
      "        [-0.7188, -0.0525,  0.1416,  ...,  0.2275, -0.4688,  0.2246],\n",
      "        [ 0.2256, -0.3379, -0.2988,  ...,  1.1250,  0.1582, -0.3086],\n",
      "        ...,\n",
      "        [-0.2617,  0.2266,  0.7305,  ...,  0.6875, -0.1113,  0.4824],\n",
      "        [ 0.2314, -0.2451,  0.3672,  ..., -0.4258,  0.6445,  0.2197],\n",
      "        [-0.1836,  0.1953,  0.2305,  ...,  0.3105, -0.6406, -0.0366]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.4.layer.0.layer_norm.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 1.8164e-01,  1.7676e-01,  1.8066e-01,  2.1484e-01,  1.3965e-01,\n",
      "         1.4746e-01,  5.1270e-02,  2.0996e-01,  9.5215e-02,  1.5625e-01,\n",
      "         1.9727e-01,  1.8359e-01,  1.1279e-01,  1.7480e-01,  1.3379e-01,\n",
      "         1.7188e-01,  1.7578e-01,  1.7676e-01,  2.3145e-01,  1.5039e-01,\n",
      "         1.3086e-01,  1.6504e-01,  6.0547e-02,  1.4258e-01,  1.4648e-01,\n",
      "         1.8848e-01,  1.3574e-01,  1.6797e-01,  1.7480e-01,  1.3184e-01,\n",
      "         1.5332e-01,  1.5332e-01,  1.7480e-01,  1.3281e-01,  1.8652e-01,\n",
      "         1.6016e-01,  1.4746e-01,  2.5391e-01,  1.2891e-01,  2.2070e-01,\n",
      "         1.7090e-01,  1.7578e-01,  1.9629e-01,  1.7383e-01,  9.7168e-02,\n",
      "         1.4746e-01,  1.4453e-01,  1.5332e-01,  1.8066e-01,  8.8379e-02,\n",
      "         2.2363e-01,  1.7676e-01,  1.3965e-01,  1.3281e-01, -9.7156e-06,\n",
      "         1.5332e-01,  5.8350e-02,  1.6797e-01,  1.7285e-01,  1.5625e-01,\n",
      "         1.8945e-01,  1.3184e-01,  1.7676e-01,  1.4551e-01,  1.4453e-01,\n",
      "         5.7373e-02,  1.5332e-01,  1.4258e-01,  2.2949e-01,  1.4258e-01,\n",
      "         1.6406e-01,  2.1289e-01,  1.4648e-01,  1.6504e-01,  1.5625e-01,\n",
      "         1.4746e-01,  1.5625e-01,  1.7871e-01,  1.3281e-01,  1.9531e-01,\n",
      "         1.4160e-01,  1.7969e-01,  1.6211e-01,  2.1289e-01,  1.6406e-01,\n",
      "         1.8066e-01,  2.4902e-01,  1.8262e-01,  1.6699e-01,  1.4648e-01,\n",
      "         1.7480e-01,  1.2988e-01,  1.3965e-01,  1.6406e-01,  1.4355e-01,\n",
      "         1.3770e-01,  1.5918e-01,  1.4844e-01,  1.4258e-01,  1.3184e-01,\n",
      "         2.0605e-01,  1.6699e-01,  1.6016e-01,  1.3965e-01,  1.6113e-01,\n",
      "         1.8164e-01,  1.6016e-01,  1.4941e-01,  1.4062e-01,  1.1279e-01,\n",
      "         1.7383e-01,  1.6016e-01,  1.6113e-01,  1.3086e-01,  1.5918e-01,\n",
      "         1.3574e-01,  1.5918e-01,  1.2988e-01,  2.2852e-01,  1.5430e-01,\n",
      "         1.2988e-01,  1.5332e-01,  1.9434e-01,  1.5430e-01,  1.8945e-01,\n",
      "         1.6406e-01,  1.8750e-01,  1.7188e-01,  1.5918e-01,  2.0020e-01,\n",
      "         1.0596e-01,  1.8652e-01,  1.4551e-01,  1.2158e-01,  1.5430e-01,\n",
      "         1.6895e-01,  1.9824e-01,  1.5430e-01,  1.7188e-01,  1.8652e-01,\n",
      "         1.4941e-01,  2.2363e-01,  1.9336e-01,  1.7773e-01,  1.7383e-01,\n",
      "         2.0117e-01,  1.5332e-01,  1.9824e-01,  1.4062e-01,  1.7383e-01,\n",
      "         1.5137e-01,  1.7480e-01,  1.7578e-01,  2.3047e-01,  1.4355e-01,\n",
      "         1.3965e-01,  1.7969e-01,  1.3379e-01,  1.7090e-01,  1.4551e-01,\n",
      "         1.2109e-01,  1.4258e-01,  1.1475e-01,  7.0801e-02,  1.3379e-01,\n",
      "         1.2988e-01,  1.5918e-01,  1.9043e-01,  1.9531e-01,  2.5977e-01,\n",
      "         2.2754e-01,  1.7871e-01,  1.7676e-01,  1.5234e-01,  1.6504e-01,\n",
      "         1.4844e-01,  1.8359e-01,  1.4551e-01,  1.6699e-01, -4.2725e-04,\n",
      "         1.4941e-01,  1.4160e-01,  1.7480e-01,  1.3770e-01,  1.9727e-01,\n",
      "         2.1094e-01,  1.4355e-01,  2.0410e-01,  1.7383e-01,  1.6504e-01,\n",
      "         1.3867e-01,  1.4941e-01,  1.0352e-01,  1.6309e-01,  1.3672e-01,\n",
      "         6.1035e-02,  1.6211e-01,  2.0117e-01,  1.6992e-01,  1.2402e-01,\n",
      "         1.2988e-01,  1.7383e-01,  1.6309e-01,  1.5820e-01,  1.5918e-01,\n",
      "         2.1777e-01,  1.4941e-01,  1.6113e-01,  1.5039e-01,  1.6406e-01,\n",
      "         1.7871e-01,  1.4551e-01,  1.8750e-01,  1.9531e-01,  1.5332e-01,\n",
      "         1.8652e-01,  1.7285e-01,  1.6211e-01,  1.8750e-01,  1.8945e-01,\n",
      "         1.1377e-01,  1.7383e-01,  1.3477e-01,  1.7383e-01,  1.4062e-01,\n",
      "         1.6211e-01,  2.1387e-01,  2.3242e-01,  1.6211e-01,  2.0020e-01,\n",
      "         1.3281e-01,  1.6797e-01,  9.3262e-02,  1.2695e-01,  1.3477e-01,\n",
      "         9.1797e-02,  7.6660e-02,  1.9434e-01,  1.8457e-01,  1.9238e-01,\n",
      "         2.2363e-01,  1.9434e-01,  1.5430e-01,  1.6602e-01,  1.2012e-01,\n",
      "         1.4648e-01,  1.5918e-01,  1.7578e-01,  1.2891e-01,  1.7773e-01,\n",
      "         1.2256e-01,  1.3184e-01,  1.3770e-01,  1.3574e-01,  1.2451e-01,\n",
      "         1.3672e-01,  1.4258e-01,  1.2695e-01,  1.1670e-01,  1.5234e-01,\n",
      "         2.1582e-01,  1.1523e-01,  1.4355e-01,  1.6797e-01,  1.4258e-01,\n",
      "         1.4844e-01,  1.5918e-01,  1.5625e-01,  1.5430e-01,  1.5625e-01,\n",
      "         6.9336e-02,  2.4316e-01,  1.7969e-01,  4.7607e-02,  2.2852e-01,\n",
      "         1.8652e-01,  1.5234e-01,  1.8066e-01,  1.6992e-01,  1.5332e-01,\n",
      "         2.5195e-01,  2.3340e-01,  1.6113e-01,  1.5332e-01,  1.8457e-01,\n",
      "         1.6113e-01,  1.3379e-01,  1.5039e-01,  1.4062e-01,  1.7188e-01,\n",
      "         1.2695e-01,  1.5625e-01,  1.3477e-01,  1.4258e-01,  1.5918e-01,\n",
      "         1.2012e-01,  1.9824e-01,  1.6309e-01,  1.4160e-01,  1.8945e-01,\n",
      "         2.1094e-01,  1.5723e-01,  2.5391e-01,  1.7969e-01,  1.6992e-01,\n",
      "         1.2891e-01,  1.6113e-01,  1.4844e-01,  1.6797e-01,  1.6406e-01,\n",
      "         1.7090e-01,  2.0703e-01,  1.4551e-01,  1.1035e-01,  1.9824e-01,\n",
      "         1.7773e-01,  1.1475e-01,  2.5977e-01,  1.2891e-01,  1.4844e-01,\n",
      "         1.4258e-01,  1.8652e-01,  1.5918e-01,  1.3965e-01,  1.8750e-01,\n",
      "         1.3672e-01,  2.0020e-01,  2.4023e-01,  1.7578e-01,  1.7383e-01,\n",
      "         1.4941e-01,  1.6016e-01,  1.6602e-01,  1.5820e-01,  1.5918e-01,\n",
      "         1.5918e-01,  1.3184e-01,  1.4062e-01,  1.6309e-01,  2.4316e-01,\n",
      "         2.0117e-01,  1.9727e-01,  1.6895e-01,  1.4941e-01,  1.7969e-01,\n",
      "         1.7285e-01,  1.5332e-01,  1.7188e-01,  1.8262e-01,  1.3965e-01,\n",
      "         2.0508e-01,  1.3574e-01,  1.6113e-01,  1.6211e-01,  1.8750e-01,\n",
      "         1.8164e-01,  1.8066e-01,  2.2754e-01,  1.4160e-01,  2.0020e-01,\n",
      "         1.4355e-01,  2.1094e-01,  1.8066e-01,  1.0059e-01,  1.3672e-01,\n",
      "         1.5137e-01,  1.2891e-01,  1.5430e-01,  1.3477e-01,  1.7480e-01,\n",
      "         1.6895e-01,  1.9141e-01,  2.9175e-02,  1.2695e-01,  2.1191e-01,\n",
      "         1.2891e-01,  1.6895e-01,  1.5137e-01,  2.3535e-01,  1.4160e-01,\n",
      "         1.6895e-01,  1.7871e-01,  2.0996e-01,  1.6602e-01,  1.5332e-01,\n",
      "         9.6680e-02,  1.5723e-01,  1.8652e-01,  1.0400e-01,  2.0605e-01,\n",
      "         1.4844e-01,  1.3867e-01,  1.7383e-01,  2.2754e-01,  2.0898e-01,\n",
      "         1.6504e-01,  1.8750e-01,  1.4648e-01,  1.7578e-01,  1.4355e-01,\n",
      "         1.3379e-01, -6.4850e-04,  1.2061e-01,  1.7480e-01,  1.4453e-01,\n",
      "         1.5723e-01,  2.0020e-01,  1.4160e-01,  1.4648e-01,  1.2988e-01,\n",
      "         1.4551e-01,  1.3770e-01,  1.4746e-01,  1.5918e-01,  1.4355e-01,\n",
      "         1.6406e-01,  1.2012e-01,  1.3867e-01,  6.0303e-02,  1.8262e-01,\n",
      "         2.2949e-01,  1.6992e-01,  1.3184e-01,  6.2988e-02,  1.7090e-01,\n",
      "         1.4941e-01,  1.8164e-01,  1.3770e-01,  1.8457e-01,  1.7773e-01,\n",
      "         1.9824e-01,  1.5332e-01,  1.5332e-01,  1.3574e-01,  2.0215e-01,\n",
      "         1.4160e-01,  1.6211e-01,  1.4258e-01,  1.9629e-01,  2.2070e-01,\n",
      "         1.7969e-01,  1.4355e-01,  2.0215e-01,  2.3926e-01,  1.4160e-01,\n",
      "         1.8652e-01,  1.3281e-01,  1.7871e-01,  2.2949e-01,  1.8457e-01,\n",
      "         1.5918e-01,  1.4844e-01,  1.6113e-01,  1.4941e-01,  1.9238e-01,\n",
      "         1.9238e-01,  1.9629e-01,  1.6113e-01,  1.7383e-01,  1.2988e-01,\n",
      "         1.3379e-01,  1.3477e-01,  1.4355e-01,  1.7383e-01,  1.7871e-01,\n",
      "         1.8066e-01,  1.0742e-01,  1.4453e-01,  1.2061e-01,  1.5332e-01,\n",
      "         1.2988e-01,  1.3477e-01,  1.3086e-01,  1.3574e-01,  1.7773e-01,\n",
      "         1.6113e-01,  1.4355e-01,  1.4746e-01,  1.1279e-01,  1.7676e-01,\n",
      "         1.9629e-01,  1.3379e-01,  1.5918e-01,  1.4355e-01,  2.7734e-01,\n",
      "         1.4453e-01,  1.3672e-01,  1.9922e-01,  1.2598e-01,  1.6797e-01,\n",
      "         1.7285e-01,  1.8262e-01,  9.8145e-02,  1.5137e-01,  1.5723e-01,\n",
      "         1.9824e-01,  1.7285e-01,  1.5527e-01,  1.5723e-01,  1.2891e-01,\n",
      "         1.4551e-01,  2.0117e-01,  1.6699e-01,  6.0547e-02,  1.6211e-01,\n",
      "         2.0117e-01,  1.9336e-01,  1.9141e-01,  1.2598e-01,  1.7773e-01,\n",
      "         2.0215e-01,  1.4160e-01,  9.2285e-02,  1.4551e-01,  2.0117e-01,\n",
      "         1.4258e-01,  1.4551e-01,  1.3477e-01,  1.7090e-01,  2.0605e-01,\n",
      "         1.3867e-01,  1.4746e-01,  1.1963e-01,  1.4941e-01,  1.7676e-01,\n",
      "         1.4941e-01,  1.9824e-01,  2.3730e-01,  1.4453e-01,  2.0020e-01,\n",
      "         2.1387e-01,  1.6895e-01,  1.6895e-01,  1.6016e-01,  1.5820e-01,\n",
      "         1.7285e-01,  1.7578e-01,  1.6504e-01,  2.0020e-01,  1.6211e-01,\n",
      "         1.9336e-01,  1.4844e-01,  1.8359e-01,  2.1875e-01,  1.5137e-01,\n",
      "         1.5430e-01,  1.9141e-01,  1.6797e-01,  1.5137e-01,  9.5703e-02,\n",
      "         2.0508e-01,  2.1484e-01,  1.2354e-01,  2.3047e-01,  1.3867e-01,\n",
      "         1.6504e-01,  1.6406e-01,  1.5625e-01,  1.6895e-01,  1.7480e-01,\n",
      "         1.7871e-01,  1.1816e-01,  2.0996e-01,  2.3926e-01,  1.8457e-01,\n",
      "         1.9824e-01,  3.7354e-02,  1.6406e-01,  1.8164e-01,  1.5820e-01,\n",
      "         1.9434e-01,  6.8359e-02,  1.2500e-01,  1.5918e-01,  1.6504e-01,\n",
      "         1.7969e-01,  1.6211e-01,  2.1875e-01,  1.4648e-01,  2.2461e-01,\n",
      "         1.6309e-01,  1.4062e-01,  1.5820e-01,  1.4355e-01,  1.2793e-01,\n",
      "         1.2500e-01,  1.3867e-01,  1.3477e-01,  1.3184e-01,  1.4453e-01,\n",
      "         1.5918e-01,  1.9629e-01,  1.4941e-01,  1.7188e-01,  1.5625e-01,\n",
      "         1.7383e-01,  1.6211e-01,  1.6504e-01,  2.1387e-01,  1.5527e-01,\n",
      "         2.1777e-01,  1.5527e-01,  1.8457e-01,  1.0010e-01,  1.9238e-01,\n",
      "         1.2891e-01,  1.4355e-01,  1.6992e-01,  1.9434e-01,  1.5918e-01,\n",
      "         1.7871e-01,  1.3965e-01,  1.7578e-01,  1.4844e-01,  1.6211e-01,\n",
      "         2.1777e-01,  1.3574e-01,  1.8457e-01,  1.6602e-01,  1.6406e-01,\n",
      "         1.8652e-01,  1.2695e-01,  1.5723e-01,  2.2754e-01,  1.6602e-01,\n",
      "         1.5527e-01,  1.7676e-01,  1.5918e-01,  1.5430e-01,  1.4062e-01,\n",
      "         1.8359e-01,  1.5137e-01,  1.7383e-01,  2.6367e-01,  1.3477e-01,\n",
      "         1.6113e-01,  1.6992e-01,  1.7871e-01,  2.5781e-01,  1.5332e-01,\n",
      "         1.7188e-01,  1.7090e-01,  1.3477e-01,  1.5527e-01,  2.0703e-01,\n",
      "         1.8262e-01,  1.8555e-01,  1.2988e-01,  1.3477e-01,  1.3184e-01,\n",
      "         1.3477e-01,  1.5234e-01,  1.6895e-01,  1.4062e-01,  1.7773e-01,\n",
      "         2.0312e-01,  1.5820e-01,  1.5625e-01,  1.9336e-01,  1.9727e-01,\n",
      "         1.6895e-01,  3.0396e-02,  1.5039e-01,  2.4414e-01,  1.5234e-01,\n",
      "         1.7578e-01,  1.4258e-01,  1.4062e-01,  1.9141e-01,  1.5625e-01,\n",
      "         1.7871e-01,  1.4453e-01,  1.5039e-01,  1.3965e-01,  1.7285e-01,\n",
      "         1.4941e-01,  1.6602e-01,  1.4941e-01,  1.4453e-01,  1.6797e-01,\n",
      "         1.6406e-01,  1.5430e-01,  1.7871e-01,  1.2354e-01,  1.5723e-01,\n",
      "         1.2012e-01,  1.4062e-01,  1.9824e-01,  1.4453e-01,  1.8945e-01,\n",
      "         1.3574e-01,  1.8164e-01,  2.1777e-01,  1.3574e-01,  1.4844e-01,\n",
      "         1.4551e-01,  2.1191e-01,  1.3086e-01,  1.7285e-01,  1.8848e-01,\n",
      "         1.7578e-01,  1.0645e-01,  1.7090e-01,  1.4844e-01,  1.5430e-01,\n",
      "         1.4258e-01,  1.5332e-01,  1.6797e-01,  2.3047e-01,  1.3379e-01,\n",
      "         7.2754e-02,  1.6602e-01,  1.6602e-01,  1.4160e-01,  5.4199e-02,\n",
      "         1.7773e-01,  1.6895e-01,  2.0996e-01,  1.7090e-01,  2.0898e-01,\n",
      "         1.5234e-01,  1.1914e-01,  2.1289e-01,  1.4258e-01,  1.8262e-01,\n",
      "         1.7383e-01,  1.8945e-01,  1.2305e-01,  1.5625e-01,  1.8848e-01,\n",
      "         1.6992e-01,  1.5527e-01,  1.3672e-01,  1.0693e-01,  1.4551e-01,\n",
      "         1.6602e-01,  1.7285e-01,  1.6992e-01,  1.6016e-01,  1.8457e-01,\n",
      "         1.7773e-01,  1.9043e-01,  2.1191e-01,  2.1094e-01,  1.5820e-01,\n",
      "         1.9629e-01,  1.7871e-01,  1.3184e-01,  2.1289e-01,  1.9922e-01,\n",
      "         1.6992e-01,  1.4941e-01,  2.2168e-01,  1.6504e-01,  1.7969e-01,\n",
      "         1.5527e-01,  1.6797e-01,  1.4941e-01,  1.8457e-01,  2.0215e-01,\n",
      "         1.5918e-01,  1.1621e-01,  1.7285e-01,  1.2598e-01,  1.8359e-01,\n",
      "         1.8750e-01,  1.5625e-01,  2.2168e-01], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.encoder.block.4.layer.1.DenseReluDense.wi.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0120,  0.1504, -0.1641,  ...,  0.1318, -0.3906, -0.1816],\n",
      "        [ 1.0781,  0.1885, -0.4316,  ..., -0.4141, -0.2227, -0.0034],\n",
      "        [ 0.1494, -0.3730,  0.1157,  ...,  0.1475,  0.3496,  0.0625],\n",
      "        ...,\n",
      "        [ 0.1138,  0.2266, -0.6836,  ...,  0.5430,  0.1396,  0.4141],\n",
      "        [ 0.0654, -0.5430,  0.2500,  ...,  1.2188, -0.7773, -0.1660],\n",
      "        [ 0.1445, -0.5156,  0.5703,  ...,  0.3066,  0.3633, -0.2520]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.4.layer.1.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0332,  0.0942, -0.2285,  ...,  0.1953,  0.1475, -0.0496],\n",
      "        [ 0.0374,  0.2793, -0.3984,  ..., -0.0757, -0.2773,  0.0269],\n",
      "        [ 0.0996, -0.1914, -0.0400,  ..., -0.1836, -0.0898, -0.0967],\n",
      "        ...,\n",
      "        [ 0.2412, -0.0747,  0.0430,  ..., -0.0557, -0.2617, -0.1367],\n",
      "        [ 0.3477, -0.4219,  0.0388,  ...,  0.3223, -0.0079, -0.1543],\n",
      "        [ 0.0060, -0.3125, -0.2480,  ..., -0.3145,  0.2480,  0.0109]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.4.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([1.1484, 1.1484, 1.0938, 1.2422, 0.9141, 1.1172, 0.5469, 1.1953, 2.8750,\n",
      "        0.9961, 1.2109, 1.1875, 0.9609, 1.2422, 0.8789, 1.0469, 1.2031, 1.1406,\n",
      "        1.2578, 0.9883, 1.0547, 1.0703, 1.5703, 0.9414, 0.9336, 1.2109, 0.8789,\n",
      "        1.1172, 1.1016, 0.8633, 0.9688, 0.9883, 1.1250, 0.9258, 1.1953, 1.0000,\n",
      "        1.5781, 1.4453, 0.8867, 1.5625, 1.1484, 1.4062, 1.2500, 1.1016, 3.3438,\n",
      "        1.0469, 1.1328, 0.9961, 1.0859, 1.1875, 1.3828, 1.0938, 1.7109, 0.9023,\n",
      "        1.1250, 1.0234, 1.4922, 1.0469, 0.9453, 1.0234, 1.1719, 0.8594, 1.0859,\n",
      "        0.9844, 0.9531, 1.3984, 0.9883, 0.9844, 1.4375, 0.9648, 1.0469, 1.5625,\n",
      "        0.8867, 1.0859, 0.9648, 0.9453, 1.3281, 1.1094, 0.9297, 1.1094, 0.9688,\n",
      "        1.1016, 1.0234, 1.3906, 1.0234, 1.0938, 1.5625, 1.1250, 1.0547, 0.9414,\n",
      "        1.1250, 0.9258, 0.9219, 1.1719, 0.9609, 0.9219, 0.9766, 0.9258, 0.9297,\n",
      "        0.9297, 1.2188, 1.0312, 1.2578, 0.9141, 1.0312, 1.6562, 0.9805, 1.0625,\n",
      "        0.9180, 1.7734, 1.2109, 1.0391, 0.9961, 1.0312, 0.9844, 0.8867, 1.0391,\n",
      "        0.9062, 1.7578, 1.0234, 0.8594, 0.9414, 1.4922, 0.9648, 1.2891, 0.9766,\n",
      "        1.0156, 1.0703, 1.0703, 1.2109, 1.5859, 1.1250, 0.8672, 0.8750, 0.9375,\n",
      "        1.3594, 1.2734, 0.9414, 1.1641, 1.1719, 1.0391, 1.3438, 1.4141, 1.2812,\n",
      "        1.0391, 1.3359, 0.9922, 1.1328, 0.9375, 1.1328, 1.0000, 1.1172, 1.0859,\n",
      "        1.3828, 0.9414, 0.9023, 1.1328, 0.8906, 1.2422, 1.0625, 0.8359, 0.9766,\n",
      "        0.9648, 3.0625, 0.9219, 0.8828, 1.0078, 1.2266, 1.0312, 1.3359, 1.3359,\n",
      "        0.9766, 0.9688, 1.4219, 1.0312, 1.1406, 1.0859, 0.8516, 1.3203, 0.3535,\n",
      "        0.9648, 0.9531, 1.0391, 0.8320, 1.1406, 1.3047, 0.9453, 1.2266, 1.1562,\n",
      "        1.1562, 0.9258, 0.9453, 0.8633, 1.0078, 0.9141, 0.4453, 1.0312, 1.3047,\n",
      "        1.0391, 0.8828, 0.9297, 1.1250, 1.0234, 1.0547, 0.9531, 1.2812, 1.0391,\n",
      "        1.0312, 0.9453, 1.0312, 1.2578, 1.0156, 1.1875, 1.2734, 0.9961, 1.1641,\n",
      "        1.0469, 1.0312, 1.1953, 2.8281, 1.0625, 1.0938, 0.8711, 1.0547, 1.2734,\n",
      "        1.0547, 1.3281, 1.3672, 1.0547, 1.1641, 0.9805, 1.1172, 0.7070, 0.8711,\n",
      "        0.8711, 0.7070, 0.4922, 1.2422, 1.2578, 1.2266, 1.4609, 1.2344, 0.9766,\n",
      "        1.0312, 0.8516, 0.8750, 0.9961, 1.1328, 0.8594, 1.0703, 3.0781, 0.9102,\n",
      "        1.0000, 0.9531, 0.8203, 0.9297, 0.9258, 0.9375, 1.1719, 1.0000, 1.3203,\n",
      "        0.2256, 0.9258, 1.0156, 0.9688, 0.9492, 0.9414, 0.9648, 1.0000, 0.9570,\n",
      "        1.0000, 1.4219, 1.5391, 1.7812, 1.2891, 1.1953, 0.9766, 1.2656, 1.0078,\n",
      "        1.0000, 1.4766, 1.3750, 0.9844, 1.4766, 1.1875, 1.3516, 0.8594, 0.9766,\n",
      "        1.0078, 1.9688, 0.8438, 1.0547, 0.9023, 0.8828, 0.9805, 0.9102, 1.2188,\n",
      "        0.9492, 0.9531, 1.2578, 1.1562, 0.9570, 1.4531, 1.1016, 1.0781, 0.8867,\n",
      "        0.9688, 0.9492, 1.1406, 1.1641, 1.0000, 1.2891, 0.9609, 0.9375, 1.2109,\n",
      "        1.1094, 0.9297, 1.4297, 1.3750, 0.9727, 0.8867, 1.2969, 1.0781, 0.8789,\n",
      "        1.2578, 0.9258, 1.3281, 1.4062, 1.0625, 1.0078, 0.9648, 1.3516, 1.1562,\n",
      "        0.9727, 0.9609, 1.0312, 0.9336, 0.9141, 1.0469, 1.4141, 1.1484, 1.1094,\n",
      "        1.1250, 1.0156, 1.0703, 1.0781, 1.3516, 1.0938, 1.2188, 1.9609, 1.3047,\n",
      "        0.8945, 1.0859, 1.0078, 1.1875, 1.0703, 1.0938, 1.3438, 0.9102, 1.3359,\n",
      "        0.9414, 1.3906, 1.4062, 0.7539, 0.9141, 1.0234, 1.1484, 1.1484, 0.9180,\n",
      "        1.1562, 1.1797, 1.1797, 0.1836, 0.9141, 1.3516, 0.8828, 1.3594, 0.9453,\n",
      "        1.4219, 1.4062, 0.9922, 1.1406, 1.3750, 1.0078, 0.9688, 1.4062, 1.0078,\n",
      "        1.2188, 0.7734, 1.2656, 1.0234, 2.0938, 1.0547, 1.3203, 1.2344, 1.1094,\n",
      "        1.1953, 0.8789, 1.1094, 0.9453, 0.8984, 0.9141, 0.8438, 0.9922, 0.9219,\n",
      "        1.0156, 1.2812, 0.8906, 0.8750, 0.9102, 0.9336, 0.9297, 1.1250, 1.0625,\n",
      "        0.8828, 1.1172, 0.9375, 0.9336, 0.3281, 1.4141, 1.7891, 1.1250, 0.8555,\n",
      "        0.4727, 1.0781, 0.9453, 1.1016, 0.9453, 1.0938, 1.1406, 1.2734, 0.9883,\n",
      "        1.5000, 0.8633, 1.4375, 0.9453, 1.0391, 0.8945, 1.1953, 1.2969, 1.1328,\n",
      "        1.0703, 1.2656, 1.3516, 0.9102, 1.1328, 0.8906, 1.0781, 1.3359, 1.0547,\n",
      "        1.0469, 0.8906, 1.0391, 0.9180, 1.2031, 1.2891, 1.2578, 1.1094, 1.2031,\n",
      "        0.8633, 0.8555, 0.9297, 0.9961, 1.0625, 1.0078, 1.0469, 0.8398, 1.0000,\n",
      "        0.8594, 0.8906, 0.8984, 0.8750, 0.8516, 0.8516, 1.0625, 0.9062, 0.9297,\n",
      "        0.9375, 1.2031, 1.0938, 1.2031, 2.1094, 1.0156, 0.9609, 1.5625, 0.9961,\n",
      "        0.9336, 1.2656, 0.8516, 1.0625, 1.0391, 1.1719, 1.2578, 0.9453, 1.1406,\n",
      "        1.3516, 1.1094, 0.9922, 0.9883, 0.8672, 0.9766, 1.1641, 1.2656, 1.6406,\n",
      "        1.0469, 1.1562, 1.1172, 1.1250, 0.8438, 1.1250, 1.1250, 0.8984, 2.5781,\n",
      "        0.9062, 1.1719, 0.9219, 1.0312, 1.0625, 1.5391, 1.1875, 0.8672, 1.1172,\n",
      "        0.8594, 1.0625, 1.0859, 2.1719, 1.1953, 1.5234, 1.0000, 1.2188, 1.2734,\n",
      "        1.0391, 1.0938, 1.0938, 1.1172, 1.0938, 1.0234, 1.0156, 1.3750, 1.0781,\n",
      "        1.2344, 0.9531, 1.1875, 1.3828, 0.9805, 0.9336, 1.0859, 1.0469, 0.9961,\n",
      "        0.6367, 1.4297, 1.3594, 0.7617, 1.4531, 0.9492, 1.0547, 1.2578, 0.9023,\n",
      "        1.1250, 1.1797, 1.2500, 0.8672, 1.4766, 1.4531, 1.0234, 1.1797, 0.0151,\n",
      "        1.0859, 1.2031, 0.9961, 1.1172, 0.2949, 0.8633, 1.0000, 1.0391, 1.1172,\n",
      "        0.9883, 1.2812, 0.8828, 1.5859, 1.0078, 0.9102, 1.0312, 0.9922, 0.8398,\n",
      "        0.8555, 0.9102, 0.9141, 0.9492, 0.9180, 0.9922, 1.1797, 0.8945, 0.9922,\n",
      "        1.0312, 0.9883, 1.0703, 1.0547, 1.3906, 1.0000, 1.3203, 0.9531, 1.0703,\n",
      "        0.6445, 1.1250, 0.8711, 0.8594, 1.1172, 1.3594, 1.0312, 1.0859, 0.9336,\n",
      "        1.2031, 0.9375, 1.0312, 1.3203, 0.9844, 1.0859, 1.0156, 1.0234, 1.1797,\n",
      "        0.9297, 1.0234, 1.3203, 1.0625, 0.9453, 1.1016, 1.3672, 1.0156, 1.2578,\n",
      "        1.2266, 1.0000, 1.2031, 1.5781, 0.8555, 0.9922, 1.1406, 1.0312, 1.2812,\n",
      "        0.9062, 1.1875, 1.0859, 0.9922, 1.1562, 1.1875, 1.1094, 1.1172, 0.8828,\n",
      "        0.9727, 0.9531, 0.9609, 1.0625, 1.0547, 1.0625, 1.1172, 1.3516, 0.9375,\n",
      "        1.1172, 1.1328, 1.2891, 1.0859, 0.1128, 0.9961, 1.4297, 1.0703, 1.3047,\n",
      "        1.0469, 0.9766, 1.0156, 0.9883, 1.0859, 1.6094, 1.0156, 0.9844, 1.0391,\n",
      "        0.9648, 0.9688, 0.9883, 0.9648, 0.9727, 1.1016, 1.0469, 1.1172, 0.8672,\n",
      "        1.0156, 1.4609, 0.9609, 1.1875, 0.9102, 1.1250, 0.9531, 1.1484, 1.4922,\n",
      "        0.8711, 1.0469, 1.0391, 1.2031, 0.8672, 0.9805, 1.1250, 1.1797, 0.8633,\n",
      "        1.0469, 1.0469, 1.1172, 0.9180, 0.9766, 1.5312, 1.4141, 0.9297, 2.9062,\n",
      "        1.0234, 0.9922, 0.9297, 0.4023, 1.1562, 1.2188, 1.2734, 1.0234, 1.3125,\n",
      "        0.8984, 1.0859, 1.3047, 0.9414, 1.1484, 1.0781, 1.1094, 0.9414, 0.9492,\n",
      "        1.2969, 1.1484, 1.0156, 0.9258, 0.7773, 0.9961, 1.0156, 1.0312, 1.0703,\n",
      "        1.0156, 1.1016, 1.1016, 1.3438, 1.2344, 1.1953, 1.0312, 1.2188, 1.1094,\n",
      "        0.8828, 1.2500, 1.4062, 1.1016, 1.0391, 1.4531, 1.0625, 1.1016, 1.0312,\n",
      "        1.0938, 0.9531, 1.1094, 1.2109, 1.0859, 0.7930, 1.1406, 0.9062, 1.1016,\n",
      "        1.1562, 1.0156, 1.3125], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.5.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0391, -0.0430,  0.0217,  ...,  0.0216,  0.0030,  0.0291],\n",
      "        [ 0.0203,  0.0654, -0.0172,  ..., -0.0579,  0.0137,  0.0581],\n",
      "        [ 0.0129,  0.0347, -0.0170,  ...,  0.0186,  0.0505,  0.0297],\n",
      "        ...,\n",
      "        [-0.0486, -0.0182, -0.0118,  ...,  0.0275,  0.0503, -0.0011],\n",
      "        [-0.0059, -0.0398,  0.0356,  ...,  0.0635,  0.0167, -0.0034],\n",
      "        [ 0.0267,  0.0349, -0.0483,  ..., -0.0369, -0.0425, -0.0291]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.5.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.3730,  0.0991, -0.2227,  ...,  0.2402,  0.4766,  0.4160],\n",
      "        [ 0.3125,  0.0282, -0.4766,  ...,  0.0427,  0.0640,  0.3672],\n",
      "        [-0.0476, -0.2715, -0.2832,  ...,  0.0825,  0.5156,  0.2275],\n",
      "        ...,\n",
      "        [-0.2471, -0.1807,  0.2070,  ...,  0.2754,  0.2246,  0.0376],\n",
      "        [ 0.0913,  0.1602, -0.1533,  ..., -0.0435,  0.1348, -0.0236],\n",
      "        [-0.0277,  0.1177, -0.0791,  ...,  0.0825, -0.4355, -0.2148]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.5.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0181,  0.6562,  0.0874,  ...,  0.0522,  0.1895, -0.0356],\n",
      "        [ 0.2158, -0.0354, -0.5625,  ..., -0.3164, -0.0388,  0.1904],\n",
      "        [ 0.1816, -1.0469, -0.0310,  ...,  0.2949,  0.9609, -0.6445],\n",
      "        ...,\n",
      "        [ 0.1338, -0.6328,  0.2373,  ...,  0.2930, -0.6133, -0.7539],\n",
      "        [ 0.0165,  0.0674,  0.6641,  ...,  0.1377, -0.4102, -0.0260],\n",
      "        [-0.5117,  0.5312, -0.2969,  ..., -0.3906,  0.1631, -0.0255]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.5.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.2441, -0.4141, -0.7383,  ..., -0.6680, -0.6250, -0.2236],\n",
      "        [-0.2734,  0.0732,  1.8984,  ..., -0.2676,  0.4023,  0.3203],\n",
      "        [-0.3750,  0.7539,  0.1592,  ...,  0.4941,  0.6133, -0.2246],\n",
      "        ...,\n",
      "        [ 0.0603, -0.2734, -0.5156,  ..., -0.2754,  0.0811, -0.1167],\n",
      "        [ 0.2520, -0.2617, -0.4434,  ...,  0.9883, -0.3848, -0.3574],\n",
      "        [ 0.3086,  0.0153,  0.4531,  ...,  0.6445,  0.3926,  0.6445]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.5.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 0.1963,  0.1914,  0.1924,  0.2197,  0.1494,  0.1650,  0.0515,  0.2275,\n",
      "         0.0908,  0.1768,  0.2100,  0.1943,  0.1147,  0.1816,  0.1396,  0.1846,\n",
      "         0.1768,  0.1904,  0.2139,  0.1699,  0.1436,  0.1787,  0.0728,  0.1592,\n",
      "         0.1582,  0.1992,  0.1475,  0.1699,  0.1846,  0.1484,  0.1494,  0.1680,\n",
      "         0.1924,  0.1426,  0.2002,  0.1768,  0.1514,  0.2539,  0.1357,  0.2236,\n",
      "         0.1846,  0.1855,  0.1953,  0.1875, -0.0732,  0.1729,  0.1426,  0.1631,\n",
      "         0.1982,  0.0598,  0.2236,  0.1904,  0.1426,  0.1543, -0.0006,  0.1621,\n",
      "         0.0669,  0.1719,  0.1846,  0.1699,  0.2002,  0.1416,  0.1904,  0.1543,\n",
      "         0.1621,  0.0645,  0.1621,  0.1504,  0.2256,  0.1592,  0.1680,  0.2148,\n",
      "         0.1533,  0.1787,  0.1650,  0.1611,  0.1592,  0.1973,  0.1416,  0.2080,\n",
      "         0.1465,  0.1943,  0.1729,  0.2266,  0.1807,  0.1865,  0.2334,  0.2031,\n",
      "         0.1797,  0.1455,  0.1953,  0.1426,  0.1484,  0.1689,  0.1553,  0.1504,\n",
      "         0.1826,  0.1670,  0.1523,  0.1465,  0.2178,  0.1768,  0.1562,  0.1426,\n",
      "         0.1777,  0.1855,  0.1758,  0.1572,  0.1475,  0.1035,  0.1787,  0.1709,\n",
      "         0.1621,  0.1377,  0.1689,  0.1475,  0.1875,  0.1475,  0.2256,  0.1572,\n",
      "         0.1309,  0.1650,  0.2061,  0.1572,  0.1855,  0.1865,  0.1943,  0.1836,\n",
      "         0.1680,  0.2031,  0.1104,  0.2002,  0.1543,  0.1279,  0.1777,  0.1797,\n",
      "         0.2070,  0.1729,  0.1846,  0.2002,  0.1592,  0.2285,  0.2090,  0.1816,\n",
      "         0.1807,  0.2100,  0.1592,  0.2041,  0.1562,  0.1924,  0.1738,  0.1836,\n",
      "         0.1797,  0.2344,  0.1455,  0.1533,  0.1885, -0.1445,  0.1748,  0.1650,\n",
      "         0.1455,  0.1650,  0.1211,  0.0791,  0.1553,  0.1426,  0.1699,  0.1973,\n",
      "         0.2002,  0.2461,  0.2305,  0.1787,  0.1865,  0.1523,  0.1768,  0.1602,\n",
      "         0.2012,  0.1108,  0.1709,  0.0269,  0.1660,  0.1553,  0.1729,  0.1543,\n",
      "         0.2080,  0.2168,  0.1504,  0.2158,  0.1885,  0.1875,  0.1504,  0.1602,\n",
      "         0.1099,  0.1768,  0.1533,  0.0674,  0.1689,  0.2109,  0.1787,  0.1162,\n",
      "         0.1396,  0.1895,  0.1719,  0.1641,  0.1709,  0.2207,  0.1602,  0.1650,\n",
      "         0.1572,  0.1787,  0.1904,  0.1533,  0.2070,  0.1953,  0.1631,  0.2051,\n",
      "         0.1865,  0.1738,  0.1973,  0.1807,  0.1133,  0.1836,  0.1465,  0.1807,\n",
      "         0.1543,  0.1729,  0.2275,  0.2393,  0.1758,  0.2139,  0.1396,  0.1816,\n",
      "         0.1064,  0.1396,  0.1445,  0.1035,  0.0913,  0.2012,  0.2041,  0.1895,\n",
      "         0.2295,  0.2109,  0.1631,  0.1729,  0.1367,  0.1602,  0.1729,  0.2002,\n",
      "         0.1328,  0.1768,  0.1289,  0.1445,  0.1514, -0.1436,  0.1348,  0.1455,\n",
      "         0.1611,  0.1309,  0.0991,  0.1660,  0.2227,  0.1099,  0.1543,  0.1816,\n",
      "         0.1572,  0.1592,  0.1602,  0.1621,  0.1738,  0.1660,  0.0708,  0.2520,\n",
      "         0.1777,  0.0491,  0.2354,  0.1914,  0.1787,  0.1797,  0.1748,  0.1699,\n",
      "         0.2559,  0.2471,  0.1631,  0.1650,  0.1875,  0.1602,  0.1426,  0.1641,\n",
      "         0.1543,  0.1631,  0.1357,  0.1729,  0.1475,  0.1621,  0.1660,  0.1338,\n",
      "         0.2041,  0.1699,  0.1582,  0.1934,  0.2168,  0.1611,  0.2539,  0.1943,\n",
      "         0.1816,  0.1455,  0.1826,  0.1602,  0.1777,  0.1689,  0.1855,  0.2227,\n",
      "         0.1514,  0.1201,  0.2100,  0.1777,  0.1182,  0.2598,  0.1396,  0.1680,\n",
      "         0.1406,  0.1943,  0.1748,  0.1523,  0.2070,  0.1445,  0.1992,  0.2432,\n",
      "         0.1885,  0.1875,  0.1562,  0.1582,  0.1699,  0.1562,  0.1709,  0.1777,\n",
      "         0.1396,  0.1523,  0.1738,  0.2373,  0.2129,  0.2051,  0.1797,  0.1631,\n",
      "         0.1914,  0.1875,  0.1689,  0.1895,  0.1816,  0.1328,  0.2109,  0.1514,\n",
      "         0.1689,  0.1729,  0.1865,  0.1924,  0.1885,  0.2227,  0.1592,  0.2021,\n",
      "         0.1562,  0.2139,  0.1953,  0.1064,  0.1504,  0.1611,  0.1108,  0.1631,\n",
      "         0.1484,  0.1865,  0.1689,  0.2061,  0.0364,  0.1406,  0.2197,  0.1504,\n",
      "         0.1621,  0.1650,  0.2402,  0.1445,  0.1729,  0.1826,  0.2236,  0.1797,\n",
      "         0.1699,  0.0825,  0.1689,  0.1758,  0.1191,  0.2236,  0.1631,  0.1289,\n",
      "         0.1904,  0.2188,  0.2178,  0.1660,  0.1943,  0.1611,  0.1855,  0.1533,\n",
      "         0.1533,  0.0063,  0.1328,  0.1826,  0.1602,  0.1592,  0.2021,  0.1631,\n",
      "         0.1602,  0.1475,  0.1602,  0.1387,  0.1660,  0.1709,  0.1582,  0.1689,\n",
      "         0.1279,  0.1416,  0.0635,  0.1816,  0.2266,  0.1807,  0.1396,  0.0591,\n",
      "         0.1836,  0.1719,  0.1934,  0.1533,  0.1875,  0.1836,  0.2080,  0.1729,\n",
      "         0.1475,  0.1445,  0.2021,  0.1484,  0.1699,  0.1543,  0.2051,  0.2217,\n",
      "         0.1943,  0.1357,  0.2148,  0.2324,  0.1533,  0.1963,  0.1475,  0.1963,\n",
      "         0.2217,  0.1904,  0.1592,  0.1680,  0.1787,  0.1592,  0.2002,  0.2109,\n",
      "         0.2070,  0.1855,  0.1914,  0.1377,  0.1436,  0.1543,  0.1602,  0.1953,\n",
      "         0.2002,  0.1924,  0.1196,  0.1572,  0.1235,  0.1650,  0.1357,  0.1484,\n",
      "         0.1416,  0.1436,  0.1807,  0.1748,  0.1689,  0.1611,  0.1060,  0.1816,\n",
      "         0.2090,  0.1299,  0.1748,  0.1562,  0.2754,  0.1543,  0.1523,  0.2021,\n",
      "         0.1445,  0.1807,  0.1934,  0.1875,  0.0786,  0.1660,  0.1611,  0.2061,\n",
      "         0.1885,  0.1738,  0.1641,  0.1406,  0.1553,  0.2139,  0.1777,  0.0605,\n",
      "         0.1699,  0.2061,  0.1963,  0.1973,  0.1318,  0.1953,  0.2080,  0.1543,\n",
      "         0.0986,  0.1611,  0.2031,  0.1572,  0.1562,  0.1514,  0.1738,  0.2090,\n",
      "         0.1445,  0.1621,  0.1328,  0.1543,  0.1895,  0.1523,  0.2070,  0.2334,\n",
      "         0.1602,  0.2148,  0.2158,  0.1699,  0.1855,  0.1719,  0.1826,  0.1787,\n",
      "         0.1816,  0.1816,  0.2207,  0.1758,  0.2021,  0.1562,  0.1973,  0.2354,\n",
      "         0.1572,  0.1729,  0.1953,  0.1836,  0.1582,  0.0845,  0.2148,  0.2363,\n",
      "         0.1230,  0.2461,  0.1592,  0.1768,  0.1758,  0.1660,  0.1826,  0.1719,\n",
      "         0.1914,  0.1260,  0.2217,  0.2314,  0.1963,  0.2051,  0.0347,  0.1768,\n",
      "         0.1934,  0.1729,  0.2031,  0.0869,  0.1318,  0.1670,  0.1836,  0.1943,\n",
      "         0.1748,  0.2334,  0.1562,  0.2061,  0.1729,  0.1455,  0.1680,  0.1514,\n",
      "         0.1426,  0.1377,  0.1465,  0.1533,  0.1494,  0.1611,  0.1777,  0.2070,\n",
      "         0.1641,  0.1826,  0.1729,  0.1885,  0.1680,  0.1621,  0.2090,  0.1631,\n",
      "         0.2314,  0.1680,  0.1982,  0.0986,  0.2109,  0.1416,  0.1484,  0.1787,\n",
      "         0.1885,  0.1689,  0.1953,  0.1445,  0.1924,  0.1611,  0.1729,  0.2246,\n",
      "         0.1523,  0.1973,  0.1846,  0.1719,  0.2041,  0.1338,  0.1641,  0.2314,\n",
      "         0.1885,  0.1650,  0.1748,  0.1611,  0.1738,  0.1396,  0.1992,  0.1650,\n",
      "         0.1904,  0.2695,  0.1533,  0.1523,  0.1865,  0.1738,  0.2598,  0.1748,\n",
      "         0.1738,  0.1904,  0.1416,  0.1641,  0.2090,  0.1973,  0.1992,  0.1465,\n",
      "         0.1416,  0.1416,  0.1484,  0.1729,  0.1729,  0.1572,  0.1768,  0.2061,\n",
      "         0.1748,  0.1670,  0.1855,  0.2100,  0.1650,  0.0306,  0.1631,  0.2490,\n",
      "         0.1680,  0.1846,  0.1621,  0.1523,  0.2061,  0.1729,  0.1826,  0.1504,\n",
      "         0.1699,  0.1504,  0.1963,  0.1641,  0.1611,  0.1582,  0.1533,  0.1777,\n",
      "         0.1826,  0.1582,  0.1953,  0.1367,  0.1748,  0.1084,  0.1465,  0.2109,\n",
      "         0.1592,  0.2051,  0.1445,  0.1943,  0.2217,  0.1426,  0.1562,  0.1504,\n",
      "         0.2197,  0.1445,  0.1855,  0.1904,  0.1865,  0.1216,  0.1797,  0.1582,\n",
      "         0.1846,  0.1484,  0.1670,  0.1904,  0.2402,  0.1445,  0.0791,  0.1787,\n",
      "         0.1787,  0.1562,  0.0620,  0.1924,  0.1777,  0.2031,  0.1885,  0.2207,\n",
      "         0.1621,  0.1270,  0.2139,  0.1562,  0.1973,  0.1904,  0.2002,  0.1260,\n",
      "         0.1699,  0.2012,  0.1875,  0.1650,  0.1445,  0.1167,  0.1660,  0.1758,\n",
      "         0.1816,  0.1865,  0.1787,  0.2012,  0.1865,  0.2012,  0.2197,  0.2178,\n",
      "         0.1699,  0.1992,  0.1846,  0.1455,  0.2129,  0.2188,  0.1758,  0.1680,\n",
      "         0.2344,  0.1709,  0.1934,  0.1709,  0.1797,  0.1660,  0.1914,  0.2178,\n",
      "         0.1689,  0.1240,  0.1895,  0.1377,  0.2012,  0.1992,  0.1709,  0.2275],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.5.layer.1.DenseReluDense.wi.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.3145,  0.8398,  0.3438,  ..., -0.7148, -0.1543,  0.1094],\n",
      "        [-0.6016,  0.4023,  0.3223,  ..., -0.0471, -1.0469,  0.2832],\n",
      "        [ 0.2412, -0.4961, -0.5391,  ...,  0.5820, -0.2969,  0.9531],\n",
      "        ...,\n",
      "        [-0.2100, -0.0256, -0.0505,  ..., -0.3809, -0.6758, -0.1768],\n",
      "        [ 0.2871, -0.0498, -0.4238,  ...,  0.0100, -0.6875,  0.5977],\n",
      "        [-0.1680, -0.2930,  0.0718,  ..., -0.3887,  0.2773,  0.0518]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.5.layer.1.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0339, -0.1738, -0.1660,  ..., -0.0471, -0.2197, -0.3359],\n",
      "        [ 0.3027, -0.3223, -0.6875,  ...,  0.1436, -0.0287,  0.0327],\n",
      "        [ 0.2334, -0.4336,  0.0535,  ...,  0.3457, -0.0371,  0.1396],\n",
      "        ...,\n",
      "        [-0.3594, -0.1895,  0.1562,  ..., -0.0339,  0.0703,  0.0117],\n",
      "        [ 0.3027,  0.2578,  0.3066,  ...,  0.3320,  0.0422, -0.1338],\n",
      "        [ 0.1270,  0.1387,  0.1494,  ..., -0.1206, -0.0155,  0.1865]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.5.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 1.1562e+00,  1.1406e+00,  1.0703e+00,  1.1328e+00,  9.2188e-01,\n",
      "         1.1016e+00,  5.1172e-01,  1.2578e+00,  2.4219e+00,  1.0469e+00,\n",
      "         1.2031e+00,  1.1641e+00,  1.0000e+00,  1.2500e+00,  9.2969e-01,\n",
      "         1.0703e+00,  1.1797e+00,  1.1406e+00,  1.1875e+00,  1.0078e+00,\n",
      "         1.0547e+00,  1.0938e+00,  1.8516e+00,  9.6875e-01,  9.4531e-01,\n",
      "         1.2266e+00,  9.4922e-01,  1.0469e+00,  1.1641e+00,  9.3750e-01,\n",
      "         1.0234e+00,  9.8828e-01,  1.1172e+00,  9.7266e-01,  1.1797e+00,\n",
      "         1.0625e+00,  1.4688e+00,  1.3125e+00,  9.2188e-01,  1.6094e+00,\n",
      "         1.0859e+00,  1.4219e+00,  1.1953e+00,  1.1406e+00,  2.9844e+00,\n",
      "         1.0703e+00,  1.1016e+00,  1.0625e+00,  1.0781e+00,  9.6875e-01,\n",
      "         1.2812e+00,  1.1250e+00,  1.6484e+00,  9.2969e-01,  1.0312e+00,\n",
      "         1.0312e+00,  1.4688e+00,  1.0547e+00,  1.0625e+00,  1.0312e+00,\n",
      "         1.1484e+00,  9.2578e-01,  1.1094e+00,  1.0078e+00,  9.4922e-01,\n",
      "         1.4766e+00,  1.0000e+00,  1.0156e+00,  1.3516e+00,  9.7656e-01,\n",
      "         1.0469e+00,  1.5391e+00,  9.2969e-01,  1.1562e+00,  9.8828e-01,\n",
      "         9.8047e-01,  1.2969e+00,  1.0625e+00,  9.6875e-01,  1.0703e+00,\n",
      "         9.7656e-01,  1.0391e+00,  1.0312e+00,  1.3203e+00,  1.0469e+00,\n",
      "         1.1016e+00,  1.4141e+00,  1.1562e+00,  1.0547e+00,  9.8828e-01,\n",
      "         1.0859e+00,  9.4922e-01,  9.4141e-01,  1.1719e+00,  9.8828e-01,\n",
      "         9.0625e-01,  9.4922e-01,  1.0000e+00,  9.4922e-01,  9.4531e-01,\n",
      "         1.2422e+00,  1.0469e+00,  1.1406e+00,  9.6094e-01,  1.0938e+00,\n",
      "         1.6016e+00,  9.9219e-01,  1.0469e+00,  9.6484e-01,  1.7812e+00,\n",
      "         1.1719e+00,  1.0156e+00,  9.8828e-01,  1.0156e+00,  1.0000e+00,\n",
      "         9.5312e-01,  1.0391e+00,  9.2188e-01,  1.5625e+00,  9.8828e-01,\n",
      "         9.0625e-01,  9.6875e-01,  1.3906e+00,  9.7656e-01,  1.1875e+00,\n",
      "         1.0547e+00,  1.0781e+00,  1.0703e+00,  1.0547e+00,  1.1719e+00,\n",
      "         1.4453e+00,  1.1016e+00,  9.6094e-01,  9.3750e-01,  9.6094e-01,\n",
      "         1.2656e+00,  1.3047e+00,  9.9609e-01,  1.1953e+00,  1.1172e+00,\n",
      "         9.8438e-01,  1.3047e+00,  1.2969e+00,  1.2812e+00,  1.0781e+00,\n",
      "         1.3281e+00,  9.7656e-01,  1.1641e+00,  9.8438e-01,  1.0859e+00,\n",
      "         1.0234e+00,  1.1406e+00,  1.0547e+00,  1.3594e+00,  9.6484e-01,\n",
      "         9.3359e-01,  1.1562e+00,  9.3359e-01,  1.1562e+00,  1.0391e+00,\n",
      "         9.5703e-01,  1.0078e+00,  9.6094e-01,  2.9844e+00,  9.8828e-01,\n",
      "         8.9844e-01,  1.0078e+00,  1.1797e+00,  1.0156e+00,  1.2266e+00,\n",
      "         1.2422e+00,  1.0156e+00,  1.0000e+00,  1.3594e+00,  1.0547e+00,\n",
      "         1.0547e+00,  1.0703e+00,  6.7188e-01,  1.2031e+00,  4.3750e-01,\n",
      "         1.0000e+00,  9.6484e-01,  1.0703e+00,  9.1797e-01,  1.1250e+00,\n",
      "         1.2969e+00,  9.6484e-01,  1.2031e+00,  1.1562e+00,  1.0938e+00,\n",
      "         9.5312e-01,  9.9219e-01,  8.9844e-01,  1.0391e+00,  9.4531e-01,\n",
      "         4.2969e-01,  1.0156e+00,  1.2656e+00,  1.0781e+00,  9.6484e-01,\n",
      "         9.1016e-01,  1.1250e+00,  1.0547e+00,  1.1016e+00,  1.0000e+00,\n",
      "         1.2734e+00,  1.0625e+00,  1.0312e+00,  1.0156e+00,  1.0781e+00,\n",
      "         1.1875e+00,  9.9609e-01,  1.1328e+00,  1.2031e+00,  1.0469e+00,\n",
      "         1.1875e+00,  1.0781e+00,  1.0078e+00,  1.1875e+00,  2.5156e+00,\n",
      "         1.0312e+00,  1.0469e+00,  9.3359e-01,  1.0547e+00,  1.2344e+00,\n",
      "         1.0234e+00,  1.2500e+00,  1.2500e+00,  1.0859e+00,  1.1875e+00,\n",
      "         9.6094e-01,  1.0859e+00,  7.1484e-01,  9.2578e-01,  9.2969e-01,\n",
      "         7.1484e-01,  6.5625e-01,  1.2109e+00,  1.2188e+00,  1.1719e+00,\n",
      "         1.3203e+00,  1.2266e+00,  1.0469e+00,  1.0625e+00,  8.9062e-01,\n",
      "         9.7656e-01,  1.0938e+00,  1.1484e+00,  8.7500e-01,  1.0859e+00,\n",
      "         3.0938e+00,  9.6094e-01,  9.7266e-01,  1.0000e+00,  9.4922e-01,\n",
      "         9.7656e-01,  1.0078e+00,  9.4922e-01,  9.8438e-01,  1.0703e+00,\n",
      "         1.2500e+00,  2.0801e-01,  1.0312e+00,  1.0234e+00,  1.0469e+00,\n",
      "         1.0391e+00,  9.8828e-01,  9.9609e-01,  1.0547e+00,  9.7266e-01,\n",
      "         8.7109e-01,  1.2969e+00,  1.5234e+00,  1.6641e+00,  1.2578e+00,\n",
      "         1.1562e+00,  1.0469e+00,  1.1484e+00,  9.8438e-01,  1.0078e+00,\n",
      "         1.3906e+00,  1.3750e+00,  1.0078e+00,  1.3125e+00,  1.1250e+00,\n",
      "         1.2500e+00,  8.6328e-01,  1.0391e+00,  9.8438e-01,  1.8984e+00,\n",
      "         8.9844e-01,  1.0859e+00,  9.8047e-01,  9.4141e-01,  1.0625e+00,\n",
      "         9.4141e-01,  1.1250e+00,  9.9609e-01,  9.8047e-01,  1.2578e+00,\n",
      "         1.2422e+00,  9.7266e-01,  1.3516e+00,  1.1172e+00,  1.0703e+00,\n",
      "         9.3359e-01,  1.0078e+00,  1.0078e+00,  1.0938e+00,  1.1016e+00,\n",
      "         1.0781e+00,  1.2812e+00,  9.7266e-01,  9.2188e-01,  1.1797e+00,\n",
      "         1.0938e+00,  9.5312e-01,  1.2266e+00,  1.2969e+00,  9.5703e-01,\n",
      "         9.4531e-01,  1.3047e+00,  1.0469e+00,  9.1016e-01,  1.2812e+00,\n",
      "         9.0625e-01,  1.2812e+00,  1.2812e+00,  1.0703e+00,  1.1094e+00,\n",
      "         1.0078e+00,  1.2734e+00,  1.1562e+00,  1.0234e+00,  1.0156e+00,\n",
      "         1.0781e+00,  9.8047e-01,  9.6094e-01,  1.0547e+00,  1.2578e+00,\n",
      "         1.1719e+00,  1.1172e+00,  1.1172e+00,  1.0469e+00,  1.1172e+00,\n",
      "         1.0938e+00,  1.2656e+00,  1.1172e+00,  1.1875e+00,  1.9219e+00,\n",
      "         1.2422e+00,  9.3359e-01,  1.0391e+00,  1.0000e+00,  1.1172e+00,\n",
      "         1.0391e+00,  1.1094e+00,  1.2109e+00,  9.8047e-01,  1.2578e+00,\n",
      "         1.0078e+00,  1.2812e+00,  1.3203e+00,  7.9297e-01,  9.7656e-01,\n",
      "         9.9219e-01,  1.0078e+00,  1.2109e+00,  9.8828e-01,  1.1641e+00,\n",
      "         1.2188e+00,  1.2031e+00,  2.5000e-01,  9.4531e-01,  1.3281e+00,\n",
      "         9.2969e-01,  1.2344e+00,  9.9609e-01,  1.3438e+00,  1.3438e+00,\n",
      "         1.0625e+00,  1.1562e+00,  1.3047e+00,  1.0547e+00,  9.8828e-01,\n",
      "         1.3438e+00,  9.9219e-01,  1.0703e+00,  7.9297e-01,  1.2656e+00,\n",
      "         1.0391e+00,  2.2188e+00,  1.0781e+00,  1.2656e+00,  1.2109e+00,\n",
      "         1.1406e+00,  1.1719e+00,  9.6875e-01,  1.0938e+00,  9.2578e-01,\n",
      "         9.1797e-01,  7.2266e-01,  8.8672e-01,  1.0234e+00,  9.6484e-01,\n",
      "         1.0234e+00,  1.2734e+00,  9.5703e-01,  9.4531e-01,  9.4141e-01,\n",
      "         1.0000e+00,  9.4922e-01,  1.1797e+00,  1.0938e+00,  9.4141e-01,\n",
      "         1.0625e+00,  9.7266e-01,  9.9609e-01,  3.5352e-01,  1.4375e+00,\n",
      "         1.5391e+00,  1.1094e+00,  9.1797e-01,  4.4922e-01,  1.1094e+00,\n",
      "         9.8828e-01,  1.1328e+00,  9.2969e-01,  1.1172e+00,  1.0469e+00,\n",
      "         1.2109e+00,  9.9219e-01,  1.3594e+00,  9.1797e-01,  1.3828e+00,\n",
      "         9.9219e-01,  1.0859e+00,  9.5703e-01,  1.2266e+00,  1.2344e+00,\n",
      "         1.1406e+00,  1.0234e+00,  1.1953e+00,  1.2266e+00,  9.6484e-01,\n",
      "         1.0781e+00,  9.5312e-01,  1.1328e+00,  1.1875e+00,  1.0547e+00,\n",
      "         1.0234e+00,  9.9219e-01,  1.0547e+00,  9.5312e-01,  1.2266e+00,\n",
      "         1.3125e+00,  1.1641e+00,  1.0391e+00,  1.2188e+00,  9.1406e-01,\n",
      "         9.2578e-01,  9.3359e-01,  9.6875e-01,  1.0625e+00,  1.0938e+00,\n",
      "         1.0391e+00,  8.9453e-01,  9.9609e-01,  8.9062e-01,  9.2578e-01,\n",
      "         9.2969e-01,  9.2578e-01,  8.6328e-01,  9.1797e-01,  1.0938e+00,\n",
      "         1.0156e+00,  1.0469e+00,  1.0078e+00,  1.3047e+00,  1.0078e+00,\n",
      "         1.1562e+00,  2.0156e+00,  1.0703e+00,  1.0391e+00,  1.4375e+00,\n",
      "         9.4922e-01,  9.4141e-01,  1.2266e+00,  9.1406e-01,  1.1094e+00,\n",
      "         1.0703e+00,  1.1641e+00,  1.3359e+00,  9.4141e-01,  1.1094e+00,\n",
      "         1.2656e+00,  1.1328e+00,  1.0000e+00,  9.8438e-01,  9.4922e-01,\n",
      "         1.0156e+00,  1.1953e+00,  1.1953e+00,  1.7578e+00,  1.0703e+00,\n",
      "         1.1484e+00,  1.1328e+00,  1.1172e+00,  8.8672e-01,  1.1797e+00,\n",
      "         1.1562e+00,  9.2578e-01,  2.2344e+00,  9.2578e-01,  1.1562e+00,\n",
      "         9.4922e-01,  1.0469e+00,  1.0469e+00,  1.3594e+00,  1.1719e+00,\n",
      "         9.4531e-01,  1.1406e+00,  8.8672e-01,  1.0234e+00,  1.1094e+00,\n",
      "         2.0625e+00,  1.1406e+00,  1.4219e+00,  1.0078e+00,  1.1562e+00,\n",
      "         1.2266e+00,  1.0156e+00,  1.0859e+00,  1.0625e+00,  1.1250e+00,\n",
      "         1.1094e+00,  1.0156e+00,  1.0859e+00,  1.3203e+00,  1.1016e+00,\n",
      "         1.1641e+00,  9.6484e-01,  1.1484e+00,  1.2422e+00,  1.0078e+00,\n",
      "         9.6484e-01,  1.1172e+00,  1.0938e+00,  9.7656e-01,  5.2734e-01,\n",
      "         1.3906e+00,  1.2500e+00,  8.8672e-01,  1.3672e+00,  9.7266e-01,\n",
      "         1.0469e+00,  1.1016e+00,  9.5703e-01,  1.1094e+00,  1.1797e+00,\n",
      "         1.2812e+00,  8.7891e-01,  1.3281e+00,  1.4062e+00,  1.0625e+00,\n",
      "         1.1797e+00, -4.1199e-04,  1.0391e+00,  1.1719e+00,  1.0391e+00,\n",
      "         1.1562e+00,  3.0469e-01,  8.9844e-01,  1.0703e+00,  1.0547e+00,\n",
      "         1.0859e+00,  1.0156e+00,  1.1875e+00,  9.4531e-01,  1.3438e+00,\n",
      "         1.1016e+00,  1.0234e+00,  1.0469e+00,  9.9219e-01,  9.2188e-01,\n",
      "         8.9844e-01,  9.1406e-01,  9.5312e-01,  9.8438e-01,  9.7266e-01,\n",
      "         1.0234e+00,  1.0781e+00,  9.6094e-01,  1.0312e+00,  1.0703e+00,\n",
      "         1.0156e+00,  1.0781e+00,  1.0859e+00,  1.3125e+00,  1.0234e+00,\n",
      "         1.2578e+00,  1.0078e+00,  1.1641e+00,  6.9141e-01,  1.1406e+00,\n",
      "         9.1016e-01,  9.1406e-01,  1.0781e+00,  1.2891e+00,  1.0234e+00,\n",
      "         1.0156e+00,  1.0078e+00,  1.1953e+00,  9.6094e-01,  1.0000e+00,\n",
      "         1.3203e+00,  1.0234e+00,  1.0547e+00,  1.0625e+00,  1.0547e+00,\n",
      "         1.2188e+00,  1.0000e+00,  9.9219e-01,  1.2656e+00,  1.0859e+00,\n",
      "         1.0234e+00,  1.0703e+00,  1.3203e+00,  1.0234e+00,  1.1719e+00,\n",
      "         1.2031e+00,  1.1016e+00,  1.1719e+00,  1.4219e+00,  9.1016e-01,\n",
      "         8.9062e-01,  1.0938e+00,  1.0234e+00,  1.2188e+00,  9.4531e-01,\n",
      "         1.1719e+00,  1.1406e+00,  1.0078e+00,  1.1641e+00,  1.1484e+00,\n",
      "         1.0625e+00,  1.1250e+00,  8.9453e-01,  9.7656e-01,  1.0156e+00,\n",
      "         9.3359e-01,  1.0859e+00,  1.0547e+00,  1.0938e+00,  1.0469e+00,\n",
      "         1.2344e+00,  9.9609e-01,  1.1250e+00,  1.0156e+00,  1.2500e+00,\n",
      "         9.7266e-01,  2.1191e-01,  9.8047e-01,  1.2812e+00,  1.0156e+00,\n",
      "         1.2500e+00,  1.0234e+00,  9.4531e-01,  1.0391e+00,  1.0391e+00,\n",
      "         1.0938e+00,  1.5391e+00,  1.0469e+00,  9.9219e-01,  1.0391e+00,\n",
      "         9.9609e-01,  9.5703e-01,  1.0156e+00,  9.8828e-01,  1.0234e+00,\n",
      "         1.0625e+00,  1.0312e+00,  1.1172e+00,  8.5547e-01,  1.0312e+00,\n",
      "         1.5000e+00,  1.0312e+00,  1.1875e+00,  9.8438e-01,  1.1016e+00,\n",
      "         9.7266e-01,  1.1328e+00,  1.3828e+00,  9.7656e-01,  1.0703e+00,\n",
      "         1.0156e+00,  1.2266e+00,  9.8047e-01,  1.0469e+00,  1.1406e+00,\n",
      "         1.1953e+00,  8.8672e-01,  1.0703e+00,  1.0312e+00,  1.1016e+00,\n",
      "         9.8438e-01,  9.8438e-01,  1.4219e+00,  1.3359e+00,  9.5312e-01,\n",
      "         2.5469e+00,  1.0625e+00,  1.1016e+00,  9.8047e-01,  4.5508e-01,\n",
      "         1.1172e+00,  1.2031e+00,  1.2031e+00,  1.0234e+00,  1.2031e+00,\n",
      "         9.5312e-01,  1.0859e+00,  1.2422e+00,  1.0078e+00,  1.1562e+00,\n",
      "         1.0078e+00,  1.1094e+00,  9.1016e-01,  9.8047e-01,  1.2109e+00,\n",
      "         1.1484e+00,  1.0391e+00,  9.5703e-01,  8.0469e-01,  9.8828e-01,\n",
      "         1.0000e+00,  1.0781e+00,  1.0781e+00,  1.0078e+00,  1.0938e+00,\n",
      "         1.1094e+00,  1.2031e+00,  1.1875e+00,  1.1484e+00,  1.0156e+00,\n",
      "         1.2031e+00,  1.0312e+00,  9.5312e-01,  1.2266e+00,  1.3125e+00,\n",
      "         1.0859e+00,  1.0547e+00,  1.3984e+00,  1.0703e+00,  1.1328e+00,\n",
      "         9.8047e-01,  1.1016e+00,  1.0391e+00,  1.0703e+00,  1.1484e+00,\n",
      "         1.0078e+00,  7.6172e-01,  1.1016e+00,  9.8047e-01,  1.0625e+00,\n",
      "         1.1094e+00,  1.0391e+00,  1.1875e+00], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.encoder.block.6.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0510,  0.0168, -0.0154,  ...,  0.0036, -0.0483,  0.0554],\n",
      "        [ 0.0352, -0.0034,  0.0361,  ...,  0.0369,  0.0688, -0.0286],\n",
      "        [-0.0359,  0.0518,  0.0217,  ..., -0.0074, -0.0386, -0.0182],\n",
      "        ...,\n",
      "        [-0.0227, -0.0154, -0.0505,  ..., -0.0178,  0.0718, -0.0339],\n",
      "        [ 0.0137,  0.0175, -0.0317,  ...,  0.0161,  0.0518,  0.0199],\n",
      "        [ 0.0015, -0.0258,  0.0118,  ...,  0.0554, -0.0439, -0.0294]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.6.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.1523, -0.0972,  0.1235,  ..., -0.1719, -0.0109,  0.1455],\n",
      "        [ 0.2715, -0.1875,  0.2109,  ..., -0.2412, -0.3633,  0.1572],\n",
      "        [ 0.4492,  0.2432,  0.0679,  ..., -0.1924,  0.1953,  0.1758],\n",
      "        ...,\n",
      "        [-0.4258, -0.0732,  0.0122,  ..., -0.3652,  0.2500, -0.1084],\n",
      "        [ 0.1895,  0.3496, -0.4043,  ...,  0.0669,  0.1348, -0.1855],\n",
      "        [-0.1211,  0.0437, -0.2109,  ...,  0.5117, -0.2891, -0.0349]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.6.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.4727, -0.8516, -0.1768,  ..., -0.2598,  0.1797,  0.7188],\n",
      "        [ 0.5625, -0.1504,  0.4102,  ..., -0.9609,  0.2832,  0.1582],\n",
      "        [ 0.8984,  0.3613,  0.6211,  ..., -0.2363,  0.7695,  0.1465],\n",
      "        ...,\n",
      "        [-0.6289, -0.5273,  0.2432,  ...,  0.2285, -0.1533,  0.7305],\n",
      "        [-0.4023, -0.6484, -0.1118,  ..., -0.8672,  0.0160, -0.1260],\n",
      "        [-0.6875,  0.9570, -0.7617,  ...,  0.0193,  0.0806, -0.4121]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.6.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1875, -0.6914, -1.1016,  ...,  0.5312, -0.4316, -0.8906],\n",
      "        [ 0.7695, -0.0938, -0.4590,  ...,  1.2188, -0.6602,  0.7812],\n",
      "        [ 0.1963, -0.1021, -0.6523,  ..., -0.8672,  0.2930,  0.8828],\n",
      "        ...,\n",
      "        [ 0.0530,  1.3984,  0.5898,  ..., -0.1436, -0.4023,  0.9180],\n",
      "        [-0.4316, -0.2930, -0.6328,  ..., -0.2119,  0.2852, -0.5703],\n",
      "        [-1.1484, -0.4473, -0.0122,  ..., -0.5898, -0.6406, -0.2344]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.6.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 1.9043e-01,  1.8945e-01,  1.8750e-01,  2.1094e-01,  1.6504e-01,\n",
      "         1.6895e-01,  4.6875e-02,  2.2266e-01, -9.0820e-02,  1.9043e-01,\n",
      "         2.1875e-01,  1.9141e-01,  1.2598e-01,  1.7578e-01,  1.4453e-01,\n",
      "         1.8945e-01,  1.7578e-01,  1.8848e-01,  2.0703e-01,  1.6699e-01,\n",
      "         1.4062e-01,  1.8066e-01,  3.7909e-05,  1.6602e-01,  1.6504e-01,\n",
      "         1.8652e-01,  1.5918e-01,  1.7285e-01,  1.7969e-01,  1.5039e-01,\n",
      "         1.6504e-01,  1.7188e-01,  1.9336e-01,  1.5137e-01,  1.9434e-01,\n",
      "         1.7773e-01,  1.5234e-01,  2.4609e-01,  1.4746e-01,  2.1582e-01,\n",
      "         1.7773e-01,  1.7188e-01,  1.8848e-01,  1.8945e-01, -6.7383e-02,\n",
      "         1.6895e-01,  1.3574e-01,  1.6992e-01,  1.9434e-01,  5.2246e-02,\n",
      "         2.0508e-01,  1.8945e-01,  1.3965e-01,  1.6113e-01,  2.9175e-02,\n",
      "         1.5430e-01,  6.4941e-02, -1.7871e-01,  1.9434e-01,  1.6895e-01,\n",
      "         2.0996e-01,  1.5527e-01,  1.9043e-01,  1.6699e-01,  1.6211e-01,\n",
      "         5.7861e-02,  1.6406e-01,  1.6504e-01,  2.1484e-01,  1.6895e-01,\n",
      "         1.8066e-01,  2.0508e-01,  1.6309e-01,  1.8848e-01,  1.6113e-01,\n",
      "         1.5918e-01,  1.5527e-01,  1.9824e-01,  1.5039e-01,  2.0898e-01,\n",
      "         1.6113e-01,  2.0117e-01,  1.8262e-01,  2.1094e-01,  1.7188e-01,\n",
      "         1.8945e-01,  2.1777e-01,  1.9531e-01,  1.9336e-01,  1.6602e-01,\n",
      "         1.8945e-01,  1.4551e-01,  1.5234e-01,  1.6309e-01,  1.5039e-01,\n",
      "         1.5527e-01,  1.8555e-01,  1.7188e-01,  1.6797e-01,  1.4355e-01,\n",
      "         2.0117e-01,  1.9141e-01,  1.3965e-01,  1.6406e-01,  1.8066e-01,\n",
      "         1.6406e-01,  1.7871e-01,  1.7676e-01,  1.5430e-01,  9.2773e-02,\n",
      "         1.8164e-01,  1.8359e-01,  1.7090e-01,  1.3672e-01,  1.7676e-01,\n",
      "         1.5723e-01,  1.8945e-01,  1.5332e-01,  1.9434e-01,  1.7285e-01,\n",
      "         1.4551e-01,  1.7285e-01,  1.9141e-01,  1.6699e-01,  1.6895e-01,\n",
      "         1.8945e-01,  2.0312e-01,  1.8652e-01,  1.7285e-01,  1.9238e-01,\n",
      "         1.0303e-01,  2.0898e-01,  1.6309e-01,  1.3184e-01,  1.8359e-01,\n",
      "        -1.6699e-01,  2.1289e-01,  1.6504e-01,  1.8164e-01,  1.9531e-01,\n",
      "         1.6406e-01,  2.2168e-01,  2.0898e-01,  1.7773e-01,  1.8457e-01,\n",
      "         2.1387e-01,  1.6992e-01,  2.0215e-01,  1.5918e-01,  1.8945e-01,\n",
      "         1.8066e-01,  1.9336e-01,  1.8750e-01,  2.1484e-01,  1.6211e-01,\n",
      "         1.6016e-01,  1.8262e-01,  1.4941e-01,  1.8164e-01,  1.6113e-01,\n",
      "         1.5723e-01,  1.6406e-01,  1.2988e-01,  6.6406e-02,  1.5625e-01,\n",
      "         1.4648e-01,  1.7578e-01,  1.9434e-01,  1.9727e-01,  2.1777e-01,\n",
      "         2.2852e-01,  1.9727e-01,  1.8848e-01,  1.4551e-01,  1.8164e-01,\n",
      "         1.5430e-01,  2.0215e-01,  8.3984e-02,  1.6113e-01,  3.0029e-02,\n",
      "         1.6992e-01,  1.5918e-01,  1.8262e-01,  1.6211e-01,  2.0410e-01,\n",
      "         2.1387e-01,  1.5332e-01,  1.9629e-01,  1.8164e-01,  1.7578e-01,\n",
      "         1.6016e-01,  1.6699e-01,  1.2305e-01,  1.8164e-01,  1.6406e-01,\n",
      "         5.8838e-02,  1.7676e-01,  2.0410e-01,  1.6895e-01, -1.2500e-01,\n",
      "         1.5039e-01,  1.9824e-01,  1.7871e-01,  1.6992e-01,  1.6895e-01,\n",
      "         2.1191e-01,  1.6602e-01,  1.7480e-01,  1.5918e-01,  1.8164e-01,\n",
      "         1.8262e-01,  1.5918e-01,  1.9531e-01,  1.9336e-01,  1.7090e-01,\n",
      "         1.9727e-01,  1.8945e-01,  1.7578e-01,  1.8945e-01,  1.5039e-01,\n",
      "         1.1035e-01,  1.8457e-01,  1.4746e-01,  1.8555e-01,  1.4453e-01,\n",
      "         1.7578e-01,  2.1973e-01,  2.0703e-01,  1.8652e-01,  2.1191e-01,\n",
      "         1.4746e-01,  1.7969e-01,  1.1230e-01,  1.4453e-01,  1.5625e-01,\n",
      "         1.0840e-01,  1.0889e-01,  2.0215e-01,  1.9727e-01,  1.6602e-01,\n",
      "         2.0898e-01,  2.0312e-01,  1.7578e-01,  1.8164e-01,  1.3965e-01,\n",
      "         1.6309e-01,  1.8652e-01,  1.9727e-01,  1.4160e-01,  1.8555e-01,\n",
      "         1.1572e-01,  1.4941e-01,  1.5625e-01,  1.4551e-01,  1.4941e-01,\n",
      "         1.5723e-01,  1.6895e-01,  1.3477e-01,  8.9355e-02,  1.6113e-01,\n",
      "         2.0801e-01,  1.0596e-01,  1.6406e-01,  1.8262e-01,  1.6211e-01,\n",
      "         1.6504e-01,  1.7090e-01,  1.7090e-01,  1.7578e-01,  1.6992e-01,\n",
      "         6.7383e-02,  2.3828e-01,  1.6406e-01,  3.8818e-02,  2.2461e-01,\n",
      "         1.9043e-01,  1.8262e-01,  1.6211e-01,  1.8066e-01,  1.7285e-01,\n",
      "         2.2754e-01,  2.4023e-01,  1.7285e-01,  1.7285e-01,  1.9043e-01,\n",
      "         1.5332e-01,  1.4844e-01,  1.7773e-01,  1.4941e-01,  1.4258e-01,\n",
      "         1.4746e-01,  1.7871e-01,  1.5820e-01,  1.7090e-01,  1.7578e-01,\n",
      "         1.4453e-01,  1.9727e-01,  1.8359e-01,  1.7285e-01,  1.9336e-01,\n",
      "         2.2168e-01,  1.6211e-01,  2.3633e-01,  1.9336e-01,  1.8848e-01,\n",
      "         1.5137e-01,  1.9043e-01,  1.6895e-01,  1.9336e-01,  1.7480e-01,\n",
      "         1.9824e-01,  2.1680e-01,  1.6406e-01,  1.1768e-01,  2.0703e-01,\n",
      "         1.8457e-01,  1.2256e-01,  2.3438e-01,  1.4258e-01,  1.7676e-01,\n",
      "         1.5332e-01,  1.9336e-01,  1.6895e-01,  1.5430e-01,  1.9824e-01,\n",
      "         1.5234e-01,  1.9336e-01,  2.3242e-01,  1.9043e-01,  1.8652e-01,\n",
      "         1.7676e-01,  1.4355e-01,  1.7676e-01,  1.6211e-01,  1.7188e-01,\n",
      "         1.7480e-01,  1.5137e-01,  1.6895e-01,  1.8262e-01,  2.2461e-01,\n",
      "         2.1289e-01,  2.0117e-01,  1.7773e-01,  1.6309e-01,  1.8652e-01,\n",
      "         1.8262e-01,  1.6602e-01,  1.7773e-01,  1.8164e-01,  1.1914e-01,\n",
      "         1.9727e-01,  1.6113e-01,  1.7480e-01,  1.8262e-01,  1.7773e-01,\n",
      "         1.9141e-01,  1.8457e-01,  2.1484e-01,  1.6992e-01,  1.8164e-01,\n",
      "         1.6309e-01,  2.0215e-01,  1.9824e-01,  1.1475e-01,  1.5723e-01,\n",
      "         1.6797e-01,  8.9844e-02,  1.5918e-01,  1.5527e-01,  1.8262e-01,\n",
      "         1.6016e-01,  2.0410e-01,  4.1260e-02,  1.4648e-01,  2.0703e-01,\n",
      "         1.5234e-01,  1.5137e-01,  1.7090e-01,  2.2949e-01,  1.3379e-01,\n",
      "         1.8066e-01,  1.8652e-01,  2.1777e-01,  1.7676e-01,  1.7383e-01,\n",
      "         7.3730e-02,  1.6992e-01,  1.5625e-01,  1.1865e-01,  2.0605e-01,\n",
      "         1.7480e-01,  9.9609e-02,  1.8359e-01,  2.1875e-01,  2.1582e-01,\n",
      "         1.6992e-01,  1.9824e-01,  1.7285e-01,  1.8750e-01,  1.6895e-01,\n",
      "         1.6602e-01,  3.2616e-04,  1.4551e-01,  1.8262e-01,  1.6797e-01,\n",
      "         1.6113e-01,  1.9336e-01,  1.6406e-01,  1.6797e-01,  1.5039e-01,\n",
      "         1.6211e-01,  1.5820e-01,  1.6406e-01,  1.6406e-01,  1.6699e-01,\n",
      "         1.6992e-01, -1.4062e-01,  1.5234e-01,  6.3477e-02,  1.7383e-01,\n",
      "         1.8945e-01,  1.8750e-01,  1.4746e-01,  5.7373e-02,  1.9141e-01,\n",
      "         1.7188e-01,  1.8750e-01,  1.5430e-01,  1.9727e-01,  1.9043e-01,\n",
      "         2.0312e-01,  1.7578e-01,  1.3477e-01,  1.4844e-01,  1.8457e-01,\n",
      "         1.5625e-01,  1.8066e-01,  1.6309e-01,  2.0117e-01,  2.3145e-01,\n",
      "         1.9043e-01,  1.3477e-01,  1.9922e-01,  2.1191e-01,  1.5527e-01,\n",
      "         1.9824e-01,  1.5332e-01,  2.0117e-01,  2.0801e-01,  1.9434e-01,\n",
      "         1.6016e-01,  1.6602e-01,  1.8066e-01,  1.7578e-01,  1.9727e-01,\n",
      "         2.1680e-01,  1.9922e-01,  1.8066e-01,  1.8848e-01,  1.5234e-01,\n",
      "         1.4648e-01,  1.5137e-01,  1.6992e-01,  1.9336e-01,  2.0020e-01,\n",
      "         1.8750e-01,  1.2402e-01,  1.6309e-01,  1.2598e-01,  1.6504e-01,\n",
      "         1.4453e-01,  1.5430e-01, -1.4941e-01,  1.5137e-01,  1.8652e-01,\n",
      "         1.7480e-01,  1.8066e-01,  1.6699e-01,  7.8125e-02,  1.8945e-01,\n",
      "         2.0801e-01,  1.1328e-01,  1.7773e-01,  1.6504e-01,  2.5586e-01,\n",
      "         1.6113e-01,  1.5723e-01,  1.8652e-01,  1.4746e-01,  1.8750e-01,\n",
      "         1.9141e-01,  1.7480e-01,  7.7148e-02,  1.7090e-01,  1.6309e-01,\n",
      "         1.9629e-01,  1.8652e-01,  1.8066e-01,  1.6992e-01,  1.4648e-01,\n",
      "         1.5723e-01,  1.9531e-01,  1.7383e-01,  5.3467e-02,  1.7480e-01,\n",
      "         2.0508e-01,  1.9727e-01,  1.9922e-01,  1.3574e-01,  1.9336e-01,\n",
      "         2.0703e-01,  1.5137e-01,  8.8867e-02,  1.7090e-01,  2.0215e-01,\n",
      "         1.6797e-01,  1.5430e-01,  1.4062e-01,  1.6699e-01,  2.0605e-01,\n",
      "         1.5234e-01,  1.5234e-01,  1.4062e-01,  1.5918e-01,  1.7969e-01,\n",
      "         1.3770e-01,  2.1289e-01,  2.0996e-01,  1.6113e-01,  2.0801e-01,\n",
      "         1.9922e-01,  1.8262e-01,  1.8848e-01,  1.8262e-01,  1.7188e-01,\n",
      "         1.8848e-01,  1.8750e-01,  1.7871e-01,  2.0410e-01,  1.8164e-01,\n",
      "         1.9922e-01,  1.7090e-01,  1.9434e-01,  2.2363e-01,  1.7090e-01,\n",
      "         1.7285e-01,  2.0117e-01,  1.8652e-01,  1.6992e-01,  6.9336e-02,\n",
      "         2.0508e-01,  2.1484e-01,  1.3086e-01,  2.4121e-01,  1.6602e-01,\n",
      "         1.8555e-01,  1.7578e-01,  1.5918e-01,  1.8652e-01,  1.5918e-01,\n",
      "         1.8848e-01,  1.3184e-01,  2.0898e-01,  2.0801e-01,  2.0312e-01,\n",
      "         2.0410e-01,  3.1250e-02,  1.7969e-01,  1.9727e-01,  1.6895e-01,\n",
      "         2.0801e-01,  8.1543e-02,  1.4941e-01,  1.7773e-01,  1.8359e-01,\n",
      "         1.8750e-01,  1.8164e-01,  2.1973e-01,  1.5723e-01,  1.8262e-01,\n",
      "         1.7871e-01,  1.6309e-01,  1.7578e-01,  1.6699e-01,  1.4551e-01,\n",
      "         1.4746e-01,  1.5234e-01,  1.5723e-01,  1.5918e-01,  1.5625e-01,\n",
      "         1.8555e-01,  2.1680e-01,  1.6602e-01,  1.8848e-01,  1.7773e-01,\n",
      "         1.9336e-01,  1.6016e-01,  1.7676e-01,  1.9043e-01,  1.6797e-01,\n",
      "         2.2559e-01,  1.7969e-01,  1.9531e-01,  9.9609e-02,  2.0801e-01,\n",
      "         1.5527e-01,  1.4941e-01,  1.8555e-01,  1.7871e-01,  1.7969e-01,\n",
      "         1.9629e-01,  1.5332e-01,  1.9238e-01,  1.6699e-01,  1.7383e-01,\n",
      "         2.2461e-01,  1.5918e-01,  2.0410e-01,  1.7773e-01,  1.6895e-01,\n",
      "         2.0410e-01,  1.4160e-01,  1.7188e-01,  2.2559e-01,  2.0215e-01,\n",
      "         1.6406e-01,  1.8359e-01,  1.6016e-01,  1.8164e-01,  1.3086e-01,\n",
      "         2.0117e-01,  1.6895e-01,  1.9629e-01,  2.4316e-01,  1.5723e-01,\n",
      "         1.3770e-01,  1.9043e-01,  1.8652e-01,  2.5195e-01,  1.6602e-01,\n",
      "         1.6699e-01,  1.9141e-01,  1.5137e-01,  1.6309e-01,  2.1875e-01,\n",
      "         1.9238e-01,  1.9336e-01,  1.5918e-01,  1.5625e-01,  1.4844e-01,\n",
      "         1.5234e-01,  1.7578e-01,  1.8555e-01,  1.6113e-01,  1.6895e-01,\n",
      "         1.9336e-01,  1.7480e-01,  1.6406e-01,  1.8066e-01,  1.8262e-01,\n",
      "         1.5234e-01, -8.3542e-04,  1.7090e-01,  2.3828e-01,  1.6992e-01,\n",
      "         1.8652e-01,  1.6113e-01,  1.5332e-01,  2.0801e-01,  1.7480e-01,\n",
      "         1.8848e-01,  1.4160e-01,  1.6895e-01,  1.5234e-01,  1.9336e-01,\n",
      "         1.6699e-01,  1.6895e-01,  1.6211e-01,  1.5625e-01,  1.8262e-01,\n",
      "         1.8457e-01,  1.7090e-01,  1.9141e-01,  1.3477e-01,  1.7871e-01,\n",
      "         1.0840e-01,  1.5723e-01,  1.9824e-01,  1.6113e-01,  2.0215e-01,\n",
      "         1.5234e-01,  1.9629e-01,  1.8652e-01,  1.4844e-01,  1.8066e-01,\n",
      "         1.6699e-01,  2.2559e-01,  1.4844e-01,  1.9238e-01,  1.9043e-01,\n",
      "         1.8652e-01,  1.2793e-01,  1.8164e-01,  1.6797e-01,  1.8457e-01,\n",
      "         1.5137e-01,  1.6895e-01,  1.7188e-01,  2.2461e-01,  1.6309e-01,\n",
      "         6.2500e-02,  1.8652e-01,  1.9336e-01,  1.7188e-01,  6.5430e-02,\n",
      "         1.9434e-01,  1.7480e-01,  1.9824e-01,  1.8848e-01,  2.1484e-01,\n",
      "         1.7188e-01,  1.2305e-01,  2.0703e-01,  1.5430e-01,  1.9531e-01,\n",
      "         1.8066e-01,  1.9141e-01,  1.3379e-01,  1.7676e-01,  1.7676e-01,\n",
      "         1.8848e-01,  1.7871e-01,  1.4453e-01,  1.1572e-01,  1.6016e-01,\n",
      "         1.8457e-01,  1.8750e-01,  1.8555e-01,  1.7676e-01,  2.0020e-01,\n",
      "         1.9336e-01,  1.9922e-01,  2.1777e-01,  2.1094e-01,  1.6992e-01,\n",
      "         1.9043e-01,  1.8262e-01,  1.5527e-01,  2.0020e-01,  2.0215e-01,\n",
      "         1.6895e-01,  1.7969e-01,  2.0996e-01,  1.7383e-01,  2.0703e-01,\n",
      "         1.7676e-01,  1.8359e-01,  1.7676e-01,  1.9531e-01,  2.2168e-01,\n",
      "         1.7383e-01,  1.3184e-01,  1.9238e-01,  1.4941e-01,  2.0020e-01,\n",
      "         1.9043e-01,  1.7773e-01,  2.1680e-01], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.encoder.block.6.layer.1.DenseReluDense.wi.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.3281, -0.0374,  0.7812,  ...,  0.0659,  0.2109, -0.4707],\n",
      "        [ 0.2969,  0.3379,  0.0623,  ..., -0.5352, -0.4336,  0.0167],\n",
      "        [-1.2109,  0.1953, -0.0299,  ..., -1.2578,  1.5234,  0.3730],\n",
      "        ...,\n",
      "        [-0.0315,  0.0747,  1.1250,  ..., -1.1953, -0.1826, -0.1226],\n",
      "        [-0.5469, -0.1650,  0.9609,  ..., -0.3828, -0.3340, -0.3203],\n",
      "        [ 0.1240, -0.2930, -0.7070,  ..., -0.6406, -0.9961,  0.7930]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.6.layer.1.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0243,  0.6836, -0.1406,  ..., -0.0371, -0.1226,  0.1494],\n",
      "        [ 0.4219,  0.6133,  0.9648,  ...,  0.7812, -0.3965, -0.2656],\n",
      "        [ 0.4355, -0.3965, -0.6172,  ..., -0.4902, -0.1650,  0.0874],\n",
      "        ...,\n",
      "        [-0.2969, -0.2109, -0.1543,  ..., -0.0776,  0.0757, -0.1387],\n",
      "        [ 0.1348,  0.2402,  0.5352,  ...,  0.5273, -0.3574, -0.1216],\n",
      "        [-0.0520, -0.8945,  0.0649,  ...,  0.5391,  0.1855, -0.3027]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.6.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 1.2109e+00,  1.0859e+00,  1.1172e+00,  1.1406e+00,  1.0234e+00,\n",
      "         1.1719e+00,  3.8086e-01,  1.2578e+00,  2.1875e+00,  1.1328e+00,\n",
      "         1.1797e+00,  1.2188e+00,  1.0000e+00,  1.2266e+00,  9.8438e-01,\n",
      "         1.1406e+00,  1.1875e+00,  1.1484e+00,  1.0312e+00,  1.0547e+00,\n",
      "         1.1016e+00,  1.1641e+00,  2.0156e+00,  1.0547e+00,  1.0469e+00,\n",
      "         1.2188e+00,  1.0078e+00,  1.0391e+00,  1.1797e+00,  9.8438e-01,\n",
      "         1.1328e+00,  1.0391e+00,  1.1953e+00,  1.0391e+00,  1.1562e+00,\n",
      "         1.1094e+00,  1.3750e+00,  1.2656e+00,  1.0312e+00,  1.6406e+00,\n",
      "         1.1250e+00,  1.3359e+00,  1.1875e+00,  1.1719e+00,  2.9219e+00,\n",
      "         1.1484e+00,  1.0859e+00,  1.1016e+00,  1.1094e+00,  7.3047e-01,\n",
      "         1.2891e+00,  1.0938e+00,  1.6562e+00,  1.0078e+00,  1.1328e+00,\n",
      "         1.1016e+00,  1.6562e+00,  1.0781e+00,  1.0938e+00,  1.1641e+00,\n",
      "         1.1641e+00,  1.0234e+00,  1.1094e+00,  1.0000e+00,  1.0625e+00,\n",
      "         1.4531e+00,  1.1172e+00,  1.1094e+00,  1.2734e+00,  1.0234e+00,\n",
      "         1.1016e+00,  1.5078e+00,  1.0391e+00,  1.1484e+00,  1.0469e+00,\n",
      "         1.0781e+00,  1.2891e+00,  1.0859e+00,  1.0156e+00,  1.1797e+00,\n",
      "         1.0469e+00,  1.1172e+00,  1.0625e+00,  1.2578e+00,  1.0781e+00,\n",
      "         1.1406e+00,  1.3516e+00,  1.1172e+00,  1.1641e+00,  1.0078e+00,\n",
      "         1.1562e+00,  1.0859e+00,  1.0234e+00,  1.1797e+00,  1.0703e+00,\n",
      "         1.0078e+00,  1.1016e+00,  1.0156e+00,  1.0156e+00,  1.0312e+00,\n",
      "         1.2109e+00,  1.0625e+00,  1.0938e+00,  1.0625e+00,  1.0938e+00,\n",
      "         1.5234e+00,  1.1328e+00,  1.1641e+00,  1.0781e+00,  1.8750e+00,\n",
      "         1.1719e+00,  1.0703e+00,  1.0938e+00,  1.1016e+00,  1.0938e+00,\n",
      "         1.0391e+00,  1.0859e+00,  9.6484e-01,  1.4297e+00,  1.0859e+00,\n",
      "         9.3750e-01,  1.0625e+00,  1.3906e+00,  1.0703e+00,  1.0781e+00,\n",
      "         1.1406e+00,  1.1016e+00,  1.1328e+00,  1.0703e+00,  1.1562e+00,\n",
      "         1.3828e+00,  1.1641e+00,  1.0469e+00,  9.9609e-01,  1.0078e+00,\n",
      "         1.1719e+00,  1.3438e+00,  1.0156e+00,  1.1875e+00,  1.1406e+00,\n",
      "         1.0547e+00,  1.3125e+00,  1.2578e+00,  1.2188e+00,  1.0156e+00,\n",
      "         1.2891e+00,  1.1094e+00,  1.1562e+00,  1.0312e+00,  1.1797e+00,\n",
      "         1.0859e+00,  1.1562e+00,  1.0938e+00,  1.2969e+00,  1.0781e+00,\n",
      "         1.0234e+00,  1.1484e+00,  1.0391e+00,  1.1328e+00,  1.1172e+00,\n",
      "         1.0312e+00,  1.1172e+00,  9.8828e-01,  3.0625e+00,  1.0703e+00,\n",
      "         9.8828e-01,  1.0938e+00,  1.1797e+00,  1.0391e+00,  1.1484e+00,\n",
      "         1.2500e+00,  1.0859e+00,  1.0859e+00,  1.3438e+00,  1.0781e+00,\n",
      "         1.0859e+00,  1.1484e+00,  5.8594e-01,  1.1875e+00,  5.4688e-01,\n",
      "         1.1094e+00,  1.0234e+00,  1.1094e+00,  9.9219e-01,  1.1562e+00,\n",
      "         1.3359e+00,  1.0000e+00,  1.2031e+00,  1.1484e+00,  1.1641e+00,\n",
      "         1.0703e+00,  1.0703e+00,  9.6484e-01,  1.0703e+00,  1.0312e+00,\n",
      "         4.1602e-01,  1.1094e+00,  1.3438e+00,  1.1406e+00,  1.0234e+00,\n",
      "         1.0078e+00,  1.1250e+00,  1.0859e+00,  1.1797e+00,  1.0469e+00,\n",
      "         1.2188e+00,  1.0703e+00,  1.0938e+00,  1.1016e+00,  1.0938e+00,\n",
      "         1.1797e+00,  1.0391e+00,  1.1484e+00,  1.2266e+00,  1.0703e+00,\n",
      "         1.2188e+00,  1.0469e+00,  1.0781e+00,  1.1953e+00,  2.3594e+00,\n",
      "         1.0391e+00,  1.1094e+00,  9.7656e-01,  1.0547e+00,  1.1797e+00,\n",
      "         1.0625e+00,  1.2656e+00,  1.2656e+00,  1.1250e+00,  1.2188e+00,\n",
      "         1.0547e+00,  1.1484e+00,  7.5000e-01,  9.8438e-01,  9.7266e-01,\n",
      "         8.0078e-01,  7.5000e-01,  1.2109e+00,  1.2266e+00,  1.0547e+00,\n",
      "         1.3516e+00,  1.2031e+00,  1.1094e+00,  1.1328e+00,  9.5703e-01,\n",
      "         9.9219e-01,  1.1562e+00,  1.1484e+00,  9.9609e-01,  1.1172e+00,\n",
      "         3.0781e+00,  9.8828e-01,  1.0938e+00,  1.0703e+00,  1.0156e+00,\n",
      "         1.0469e+00,  1.0781e+00,  9.8047e-01,  8.1250e-01,  1.0859e+00,\n",
      "         1.1797e+00,  4.4531e-01,  1.0156e+00,  1.1016e+00,  1.0938e+00,\n",
      "         1.0469e+00,  1.0703e+00,  1.0391e+00,  1.0625e+00,  1.0781e+00,\n",
      "         9.0625e-01,  1.2578e+00,  1.4297e+00,  1.5156e+00,  1.2188e+00,\n",
      "         1.2500e+00,  1.0938e+00,  1.1328e+00,  1.1016e+00,  1.0312e+00,\n",
      "         1.3125e+00,  1.3594e+00,  1.1172e+00,  1.3750e+00,  1.1250e+00,\n",
      "         1.2031e+00,  9.6875e-01,  1.0938e+00,  9.8047e-01,  1.7656e+00,\n",
      "         1.0156e+00,  1.1172e+00,  1.0156e+00,  1.0547e+00,  1.0469e+00,\n",
      "         1.0938e+00,  1.1562e+00,  1.1094e+00,  1.1094e+00,  1.2266e+00,\n",
      "         1.2031e+00,  1.0547e+00,  1.2734e+00,  1.1094e+00,  1.1094e+00,\n",
      "         1.0391e+00,  1.0469e+00,  1.0703e+00,  1.1016e+00,  1.1172e+00,\n",
      "         1.1250e+00,  1.2188e+00,  1.0859e+00,  9.2969e-01,  1.2109e+00,\n",
      "         1.1172e+00,  9.9609e-01,  1.2656e+00,  1.3203e+00,  1.0547e+00,\n",
      "         1.0000e+00,  1.2422e+00,  1.0938e+00,  1.0312e+00,  1.2031e+00,\n",
      "         1.0156e+00,  1.2188e+00,  1.2656e+00,  1.0859e+00,  1.1016e+00,\n",
      "         1.1016e+00,  1.2734e+00,  1.1016e+00,  1.0781e+00,  1.0781e+00,\n",
      "         1.0625e+00,  1.0547e+00,  1.0391e+00,  1.0781e+00,  1.2891e+00,\n",
      "         1.2109e+00,  1.1641e+00,  1.2109e+00,  1.1406e+00,  1.1641e+00,\n",
      "         1.1484e+00,  1.1875e+00,  1.1172e+00,  1.1953e+00,  1.8906e+00,\n",
      "         1.2031e+00,  1.0000e+00,  1.1328e+00,  1.0625e+00,  1.1094e+00,\n",
      "         1.1094e+00,  1.0781e+00,  1.1875e+00,  1.0703e+00,  1.2109e+00,\n",
      "         1.0312e+00,  1.2344e+00,  1.2500e+00,  8.9453e-01,  1.0234e+00,\n",
      "         1.0391e+00,  8.9844e-01,  1.2422e+00,  1.0078e+00,  1.2578e+00,\n",
      "         1.2812e+00,  1.1641e+00,  2.7734e-01,  1.0000e+00,  1.2969e+00,\n",
      "         1.0000e+00,  1.1719e+00,  1.0078e+00,  1.2422e+00,  1.2266e+00,\n",
      "         1.1328e+00,  1.1172e+00,  1.2188e+00,  1.1094e+00,  1.0859e+00,\n",
      "         1.2109e+00,  1.1172e+00,  9.5703e-01,  8.7891e-01,  1.2656e+00,\n",
      "         1.1172e+00,  2.3594e+00,  1.0938e+00,  1.2266e+00,  1.2109e+00,\n",
      "         1.1797e+00,  1.2031e+00,  1.0703e+00,  1.1484e+00,  1.0391e+00,\n",
      "         1.0078e+00,  6.7969e-01,  9.9609e-01,  1.0938e+00,  1.0469e+00,\n",
      "         1.1094e+00,  1.3438e+00,  1.0625e+00,  1.0078e+00,  1.0234e+00,\n",
      "         1.1016e+00,  1.0312e+00,  1.1641e+00,  1.1328e+00,  1.0312e+00,\n",
      "         1.0547e+00,  1.0078e+00,  1.0234e+00,  3.5547e-01,  1.4297e+00,\n",
      "         1.3906e+00,  1.1641e+00,  1.0234e+00,  4.4336e-01,  1.1953e+00,\n",
      "         1.0859e+00,  1.1250e+00,  1.0547e+00,  1.1719e+00,  1.1016e+00,\n",
      "         1.1797e+00,  1.1016e+00,  1.2578e+00,  9.8438e-01,  1.3203e+00,\n",
      "         1.0469e+00,  1.1797e+00,  1.0469e+00,  1.2422e+00,  1.2266e+00,\n",
      "         1.2266e+00,  1.0312e+00,  1.2188e+00,  1.1719e+00,  1.0156e+00,\n",
      "         1.0938e+00,  1.0156e+00,  1.1094e+00,  1.1641e+00,  1.1094e+00,\n",
      "         1.1016e+00,  1.0078e+00,  1.0859e+00,  1.0312e+00,  1.2812e+00,\n",
      "         1.2422e+00,  1.1875e+00,  1.0938e+00,  1.2344e+00,  1.0312e+00,\n",
      "         9.9219e-01,  1.0391e+00,  1.0391e+00,  1.1328e+00,  1.1562e+00,\n",
      "         1.1250e+00,  9.6094e-01,  1.0547e+00,  9.4531e-01,  1.1094e+00,\n",
      "         1.0000e+00,  9.9219e-01,  9.5312e-01,  9.8438e-01,  1.1719e+00,\n",
      "         1.0547e+00,  1.0781e+00,  1.0391e+00,  1.2734e+00,  1.1250e+00,\n",
      "         1.2266e+00,  1.9766e+00,  1.1875e+00,  1.0859e+00,  1.3125e+00,\n",
      "         1.0938e+00,  1.0859e+00,  1.2969e+00,  9.4531e-01,  1.1484e+00,\n",
      "         1.1250e+00,  1.1953e+00,  1.3203e+00,  1.0625e+00,  1.1016e+00,\n",
      "         1.2578e+00,  1.1094e+00,  1.1172e+00,  1.0312e+00,  1.0000e+00,\n",
      "         1.0703e+00,  1.1797e+00,  1.2109e+00,  1.8516e+00,  1.1719e+00,\n",
      "         1.1875e+00,  1.1328e+00,  1.1562e+00,  9.8828e-01,  1.2031e+00,\n",
      "         1.1719e+00,  1.0469e+00,  1.9844e+00,  1.0156e+00,  1.1875e+00,\n",
      "         1.0469e+00,  1.1172e+00,  1.0938e+00,  1.2266e+00,  1.1953e+00,\n",
      "         1.0547e+00,  1.2188e+00,  9.6094e-01,  1.0938e+00,  1.1250e+00,\n",
      "         2.0781e+00,  1.1953e+00,  1.4219e+00,  1.0703e+00,  1.1094e+00,\n",
      "         1.1953e+00,  1.1094e+00,  1.0469e+00,  1.1719e+00,  1.1719e+00,\n",
      "         1.1016e+00,  1.0625e+00,  1.0703e+00,  1.3047e+00,  1.1562e+00,\n",
      "         1.1719e+00,  1.0391e+00,  1.1484e+00,  1.2188e+00,  1.0625e+00,\n",
      "         1.0703e+00,  1.1250e+00,  1.1094e+00,  1.0391e+00,  4.2969e-01,\n",
      "         1.2734e+00,  1.2422e+00, -9.4531e-01,  1.3281e+00,  1.0938e+00,\n",
      "         1.1328e+00,  1.1406e+00,  1.0156e+00,  1.1328e+00,  1.1328e+00,\n",
      "         1.2188e+00,  9.3750e-01,  1.3516e+00,  1.3281e+00,  1.0859e+00,\n",
      "         1.1562e+00, -1.9455e-03,  1.1172e+00,  1.1875e+00,  1.0547e+00,\n",
      "         1.1484e+00,  3.4766e-01,  9.7266e-01,  1.0547e+00,  1.0625e+00,\n",
      "         1.1328e+00,  1.0625e+00,  1.1562e+00,  9.6875e-01,  1.1641e+00,\n",
      "         1.1094e+00,  1.0234e+00,  1.0547e+00,  1.0391e+00,  9.9219e-01,\n",
      "         1.0078e+00,  1.0625e+00,  9.9219e-01,  1.0547e+00,  1.0469e+00,\n",
      "         1.0938e+00,  1.1250e+00,  1.0547e+00,  1.0703e+00,  1.0938e+00,\n",
      "         1.0625e+00,  1.0547e+00,  1.1328e+00,  1.2578e+00,  1.0781e+00,\n",
      "         1.2656e+00,  1.0781e+00,  1.1172e+00,  6.5234e-01,  1.1406e+00,\n",
      "         1.0234e+00,  9.8828e-01,  1.0859e+00,  1.2422e+00,  1.1172e+00,\n",
      "         1.1797e+00,  1.0391e+00,  1.2656e+00,  1.0156e+00,  1.0625e+00,\n",
      "         1.3203e+00,  1.0625e+00,  1.0078e+00,  1.0781e+00,  1.1094e+00,\n",
      "         1.2344e+00,  1.0234e+00,  1.0859e+00,  1.2500e+00,  1.1250e+00,\n",
      "         1.0469e+00,  1.1328e+00,  1.2578e+00,  1.0391e+00,  1.0703e+00,\n",
      "         1.1641e+00,  1.1016e+00,  1.1094e+00,  1.3828e+00,  1.0078e+00,\n",
      "         8.0469e-01,  1.1016e+00,  1.0859e+00,  1.2422e+00,  1.0312e+00,\n",
      "         1.1406e+00,  1.1328e+00,  1.0469e+00,  1.2109e+00,  1.2188e+00,\n",
      "         1.1172e+00,  1.1094e+00,  1.0312e+00,  1.0234e+00,  1.0156e+00,\n",
      "         1.0234e+00,  1.0938e+00,  1.0703e+00,  1.1094e+00,  1.0625e+00,\n",
      "         1.2188e+00,  9.8438e-01,  1.1641e+00,  1.0703e+00,  1.1953e+00,\n",
      "         9.4141e-01,  3.1836e-01,  1.0156e+00,  1.3047e+00,  1.1016e+00,\n",
      "         1.2500e+00,  1.1172e+00,  9.9609e-01,  1.0156e+00,  1.1016e+00,\n",
      "         1.1797e+00,  1.5625e+00,  1.0859e+00,  1.1172e+00,  1.0938e+00,\n",
      "         1.0859e+00,  1.0000e+00,  1.0703e+00,  1.0625e+00,  1.1016e+00,\n",
      "         1.1016e+00,  1.1172e+00,  1.1406e+00,  1.0469e+00,  1.0703e+00,\n",
      "         1.5547e+00,  1.0859e+00,  1.2031e+00,  1.0312e+00,  1.1562e+00,\n",
      "         1.0391e+00,  1.1875e+00,  1.2734e+00,  1.0547e+00,  1.1406e+00,\n",
      "         1.0859e+00,  1.2422e+00,  1.0469e+00,  1.1172e+00,  1.1797e+00,\n",
      "         1.2734e+00,  9.5312e-01,  1.1016e+00,  1.0781e+00,  1.1641e+00,\n",
      "         1.0625e+00,  1.0234e+00,  1.3359e+00,  1.2969e+00,  1.0078e+00,\n",
      "         2.2812e+00,  1.1406e+00,  1.0625e+00,  1.0625e+00,  4.8047e-01,\n",
      "         1.1797e+00,  1.1875e+00,  1.2344e+00,  1.0859e+00,  1.1953e+00,\n",
      "         1.0625e+00,  1.1172e+00,  1.2109e+00,  1.0859e+00,  1.2031e+00,\n",
      "         1.0156e+00,  1.0781e+00,  9.9219e-01,  1.0625e+00,  1.2109e+00,\n",
      "         1.1641e+00,  1.1094e+00,  1.0391e+00,  8.5156e-01,  1.0547e+00,\n",
      "         1.0781e+00,  1.1016e+00,  1.1406e+00,  1.1172e+00,  1.1797e+00,\n",
      "         1.1484e+00,  1.2344e+00,  1.1797e+00,  1.1328e+00,  1.1328e+00,\n",
      "         1.2109e+00,  1.0156e+00,  1.0078e+00,  1.2422e+00,  1.3281e+00,\n",
      "         1.1328e+00,  1.1016e+00,  1.3359e+00,  1.1406e+00,  1.0859e+00,\n",
      "         1.0938e+00,  1.1094e+00,  1.0625e+00,  1.0625e+00,  1.1719e+00,\n",
      "         1.0391e+00,  7.9688e-01,  1.1250e+00,  1.0625e+00,  1.1250e+00,\n",
      "         1.1016e+00,  1.1406e+00,  1.2812e+00], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.encoder.block.7.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0231, -0.0361, -0.0669,  ..., -0.0161, -0.0579, -0.0167],\n",
      "        [ 0.0176,  0.0037,  0.0282,  ...,  0.0210,  0.0130, -0.0151],\n",
      "        [-0.0576,  0.0098,  0.0354,  ..., -0.0400,  0.0166,  0.0236],\n",
      "        ...,\n",
      "        [-0.0820,  0.0147,  0.0266,  ..., -0.0096,  0.0334,  0.0168],\n",
      "        [-0.0432,  0.0053,  0.0033,  ..., -0.0408,  0.0239, -0.0265],\n",
      "        [ 0.0130, -0.0011, -0.0095,  ...,  0.0259, -0.0065,  0.0364]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.7.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0161, -0.0732, -0.2539,  ..., -0.0186,  0.3105,  0.0143],\n",
      "        [ 0.1157, -0.0996, -0.2598,  ...,  0.0141,  0.1689,  0.0117],\n",
      "        [-0.1533,  0.0320,  0.0265,  ..., -0.1777, -0.1426,  0.1445],\n",
      "        ...,\n",
      "        [-0.7969,  0.0325,  0.2168,  ..., -0.2500,  0.2578, -0.1631],\n",
      "        [-0.3867, -0.0718,  0.1787,  ...,  0.0361,  0.1006,  0.1025],\n",
      "        [ 0.0874, -0.1846, -0.1943,  ...,  0.3047,  0.0156, -0.0225]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.7.layer.0.SelfAttention.v.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.2734,  0.0530,  1.0312,  ..., -0.3809, -0.1689, -0.5234],\n",
      "        [ 0.0476, -0.0679, -0.3672,  ...,  0.7578, -0.9141, -0.2002],\n",
      "        [-0.3438,  0.1982,  0.7578,  ..., -0.7344,  1.2578, -0.4941],\n",
      "        ...,\n",
      "        [-0.1670,  0.4160,  0.2812,  ...,  0.1182, -0.3477,  0.1123],\n",
      "        [ 0.7148, -0.0151, -0.9023,  ...,  1.1719, -0.0427,  0.0253],\n",
      "        [-0.9492,  0.3457,  0.1748,  ..., -0.1221, -0.0830, -0.0232]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.7.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-1.3984, -0.0542,  0.4102,  ...,  0.7852,  0.1484, -0.3145],\n",
      "        [-0.7148,  0.0060, -0.1777,  ..., -0.0537,  0.8164,  1.2266],\n",
      "        [-0.7578,  0.0557,  0.1060,  ...,  0.0131,  1.2266, -1.0156],\n",
      "        ...,\n",
      "        [-0.3301, -1.5781,  1.1797,  ..., -0.9570,  1.2969,  0.9336],\n",
      "        [ 0.2090,  0.4453, -0.7695,  ..., -0.3223,  0.1094,  0.3711],\n",
      "        [ 0.2031, -0.1445,  0.2734,  ...,  0.9219,  0.3555,  0.1025]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.7.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 1.7871e-01,  1.8750e-01,  1.7773e-01,  2.0020e-01,  1.6797e-01,\n",
      "         1.7090e-01,  3.9795e-02,  2.0312e-01,  9.9121e-02,  1.8164e-01,\n",
      "         2.0117e-01,  1.8262e-01,  1.2891e-01,  1.6992e-01,  1.5625e-01,\n",
      "         1.8945e-01,  1.7188e-01,  1.8652e-01,  1.8945e-01,  1.7578e-01,\n",
      "         1.4551e-01,  1.7676e-01,  4.1748e-02,  1.6699e-01,  1.7676e-01,\n",
      "         1.8262e-01,  1.6113e-01,  1.6309e-01,  1.7578e-01,  1.5723e-01,\n",
      "         1.6016e-01,  1.6797e-01,  1.9141e-01,  1.5723e-01,  1.9141e-01,\n",
      "         1.8457e-01,  1.5039e-01,  2.1289e-01,  1.6016e-01,  1.9336e-01,\n",
      "         1.7480e-01,  1.7090e-01,  1.7676e-01,  1.8555e-01, -7.4219e-02,\n",
      "         1.7578e-01,  1.3477e-01,  1.6992e-01,  1.8945e-01,  5.3711e-02,\n",
      "         1.9336e-01,  1.8945e-01,  1.4160e-01,  1.6309e-01, -6.2180e-04,\n",
      "         1.6113e-01,  7.6660e-02,  1.7773e-01,  1.9531e-01,  1.7773e-01,\n",
      "         1.8945e-01,  1.5332e-01,  1.8262e-01,  1.6113e-01,  1.7090e-01,\n",
      "         5.6152e-02,  1.6016e-01,  1.6016e-01,  1.9531e-01,  1.6797e-01,\n",
      "         1.7090e-01,  1.9336e-01,  1.6895e-01,  1.7969e-01,  1.6309e-01,\n",
      "         1.6211e-01,  1.5234e-01,  1.8262e-01,  1.5918e-01,  1.9238e-01,\n",
      "         1.6016e-01,  1.8945e-01,  1.7383e-01,  2.0215e-01,  1.7578e-01,\n",
      "         1.8457e-01,  2.0703e-01,  1.9434e-01,  1.8848e-01,  1.6504e-01,\n",
      "         1.8652e-01,  1.5039e-01,  1.6797e-01,  1.6211e-01,  1.5527e-01,\n",
      "         1.5820e-01,  1.8262e-01,  1.7188e-01,  1.6797e-01,  1.4551e-01,\n",
      "         1.9141e-01,  1.8750e-01,  1.3281e-01,  1.6113e-01,  1.7090e-01,\n",
      "         1.5430e-01,  1.7578e-01,  1.6406e-01,  1.5723e-01,  1.0059e-01,\n",
      "         1.7773e-01,  1.8164e-01,  1.6797e-01,  1.3477e-01,  1.7090e-01,\n",
      "         1.6406e-01,  1.7773e-01,  1.5039e-01,  1.6797e-01,  1.6504e-01,\n",
      "         1.5039e-01,  1.7383e-01,  1.8262e-01,  1.7285e-01,  1.4844e-01,\n",
      "         1.7773e-01,  1.8945e-01,  1.7383e-01,  1.7188e-01,  1.9141e-01,\n",
      "         1.1230e-01,  1.9434e-01,  1.6406e-01,  1.3867e-01,  1.7188e-01,\n",
      "         1.5039e-01,  2.0410e-01,  1.6406e-01,  1.8262e-01,  1.9238e-01,\n",
      "         1.6211e-01,  2.0703e-01,  1.8848e-01,  1.6992e-01,  1.7383e-01,\n",
      "         2.0312e-01,  1.6406e-01,  1.9141e-01,  1.6406e-01,  1.8652e-01,\n",
      "         1.6895e-01,  1.8945e-01,  1.7773e-01,  2.0020e-01,  1.5723e-01,\n",
      "         1.7480e-01,  1.7773e-01,  1.5527e-01,  1.6699e-01,  1.6992e-01,\n",
      "         1.5234e-01,  1.6309e-01,  1.3086e-01,  7.4219e-02,  1.5723e-01,\n",
      "         1.4844e-01,  1.8066e-01,  1.8262e-01,  1.8750e-01,  2.0508e-01,\n",
      "         2.0020e-01,  1.8750e-01,  1.7871e-01,  1.3281e-01,  1.8262e-01,\n",
      "         1.5723e-01,  1.9141e-01,  7.9102e-02,  1.6309e-01,  3.1250e-02,\n",
      "         1.7773e-01,  1.6406e-01,  1.8262e-01,  1.6309e-01,  1.9336e-01,\n",
      "         2.0215e-01,  1.5527e-01,  1.8262e-01,  1.7188e-01,  1.7773e-01,\n",
      "         1.6113e-01,  1.7676e-01,  1.2500e-01,  1.8262e-01,  1.5820e-01,\n",
      "         5.7617e-02,  1.7969e-01,  1.9043e-01,  1.7773e-01,  1.3281e-01,\n",
      "         1.5625e-01,  1.8848e-01,  1.7383e-01,  1.7676e-01,  1.6992e-01,\n",
      "         1.9727e-01,  1.6504e-01,  1.6895e-01,  1.6504e-01,  1.7871e-01,\n",
      "         1.7480e-01,  1.6602e-01,  1.8848e-01,  1.9434e-01,  1.7090e-01,\n",
      "         2.0215e-01,  1.7969e-01,  1.7773e-01,  1.7871e-01,  1.5039e-01,\n",
      "         1.0938e-01,  1.8359e-01,  1.4746e-01,  1.8262e-01,  1.4746e-01,\n",
      "         1.8555e-01,  2.0996e-01,  1.9629e-01,  1.8066e-01,  2.0312e-01,\n",
      "         1.4551e-01,  1.7676e-01,  1.2305e-01,  1.5527e-01,  1.5918e-01,\n",
      "         1.1328e-01,  1.1572e-01,  1.8750e-01,  1.8750e-01,  1.6016e-01,\n",
      "         1.7773e-01,  2.0312e-01,  1.7285e-01,  1.7773e-01,  1.4258e-01,\n",
      "         1.6504e-01,  1.7969e-01,  1.9141e-01,  1.5137e-01,  1.8262e-01,\n",
      "         1.0938e-01,  1.5527e-01,  1.5039e-01,  1.4746e-01,  1.4941e-01,\n",
      "         1.7285e-01,  1.6211e-01,  1.3574e-01,  8.6426e-02,  1.7383e-01,\n",
      "         1.9336e-01,  1.2891e-01,  1.6699e-01,  1.7676e-01,  1.5625e-01,\n",
      "         1.6797e-01,  1.7188e-01,  1.7285e-01,  1.6992e-01,  1.7188e-01,\n",
      "         7.3730e-02,  2.2070e-01,  1.5918e-01,  3.1738e-02,  2.0117e-01,\n",
      "         1.8945e-01,  1.8848e-01,  1.6016e-01,  1.8457e-01,  1.6309e-01,\n",
      "         2.1191e-01,  2.0898e-01,  1.7285e-01,  1.8652e-01,  1.8457e-01,\n",
      "         1.4746e-01,  1.5527e-01,  1.7188e-01,  1.4551e-01,  1.3672e-01,\n",
      "         1.5430e-01,  1.7480e-01,  1.6016e-01,  1.7090e-01,  1.7285e-01,\n",
      "         1.5918e-01,  1.8457e-01,  1.7871e-01,  1.6797e-01,  1.8945e-01,\n",
      "         2.0605e-01,  1.6992e-01,  2.1484e-01,  1.7480e-01,  1.7480e-01,\n",
      "         1.4844e-01,  1.7871e-01,  1.7383e-01,  1.7969e-01,  1.6895e-01,\n",
      "         1.9043e-01,  1.9922e-01,  1.5723e-01,  1.2012e-01,  1.9336e-01,\n",
      "         1.7969e-01,  1.2598e-01,  2.0605e-01,  1.4453e-01,  1.7871e-01,\n",
      "         1.5723e-01,  1.8262e-01,  1.6992e-01,  1.6113e-01,  1.8359e-01,\n",
      "         1.6016e-01,  1.8164e-01,  2.1191e-01,  1.8652e-01,  1.9043e-01,\n",
      "         1.6797e-01,  1.4062e-01,  1.7285e-01,  1.6309e-01,  1.7188e-01,\n",
      "         1.7480e-01,  1.6016e-01,  1.5918e-01,  1.6797e-01,  1.9824e-01,\n",
      "         2.0410e-01,  1.9629e-01,  1.8262e-01,  1.6602e-01,  1.8555e-01,\n",
      "         1.8359e-01,  1.6699e-01,  1.6895e-01,  1.6309e-01,  1.2598e-01,\n",
      "         1.9336e-01,  1.6992e-01,  1.7383e-01,  1.7773e-01,  1.7480e-01,\n",
      "         1.9531e-01,  1.8359e-01,  1.9336e-01,  1.7090e-01,  1.6602e-01,\n",
      "         1.7188e-01,  1.8848e-01,  1.8848e-01,  1.2402e-01,  1.6016e-01,\n",
      "         1.7188e-01,  8.3984e-02,  1.6211e-01,  1.6406e-01,  1.7578e-01,\n",
      "         1.6992e-01,  2.0312e-01,  4.7119e-02,  1.5039e-01,  2.0801e-01,\n",
      "         1.5820e-01,  1.5039e-01,  1.7188e-01,  2.0703e-01,  1.3574e-01,\n",
      "         1.7773e-01,  1.8262e-01,  2.0117e-01,  1.7773e-01,  1.7383e-01,\n",
      "         7.1777e-02,  1.7578e-01,  1.3867e-01,  1.3379e-01,  1.8945e-01,\n",
      "         1.6895e-01,  8.8867e-02,  1.8555e-01,  2.0215e-01,  2.0117e-01,\n",
      "         1.6797e-01,  1.9336e-01,  1.7090e-01,  1.7773e-01,  1.6504e-01,\n",
      "         1.5137e-01, -1.5354e-04,  1.5039e-01,  1.9336e-01,  1.6797e-01,\n",
      "         1.6309e-01,  1.8652e-01,  1.6016e-01,  1.6895e-01,  1.5527e-01,\n",
      "         1.7188e-01,  1.5723e-01,  1.6016e-01,  1.7188e-01,  1.6211e-01,\n",
      "         1.7285e-01,  1.4844e-01,  1.5039e-01,  6.6895e-02,  1.7773e-01,\n",
      "         1.7480e-01,  1.8750e-01,  1.5234e-01,  6.0059e-02,  1.8555e-01,\n",
      "         1.7969e-01,  1.7676e-01,  1.6016e-01,  1.8555e-01,  1.8652e-01,\n",
      "         1.8848e-01,  1.7285e-01,  1.3086e-01,  1.5527e-01,  1.6992e-01,\n",
      "         1.5625e-01,  1.7383e-01,  1.6016e-01,  1.7773e-01,  2.0508e-01,\n",
      "         1.7871e-01,  1.4355e-01,  2.0020e-01,  2.0117e-01,  1.5625e-01,\n",
      "         1.8359e-01,  1.5527e-01,  2.0410e-01,  1.8848e-01,  1.9043e-01,\n",
      "         1.6797e-01,  1.6309e-01,  1.8066e-01,  1.7090e-01,  1.9238e-01,\n",
      "         1.9824e-01,  1.7773e-01,  1.7676e-01,  1.7871e-01,  1.5332e-01,\n",
      "         1.5625e-01,  1.6602e-01,  1.6602e-01,  1.9043e-01,  1.9922e-01,\n",
      "         1.8750e-01,  1.2256e-01,  1.6211e-01,  1.3672e-01,  1.5918e-01,\n",
      "         1.4648e-01,  1.6309e-01,  1.4258e-01,  1.4648e-01,  1.8555e-01,\n",
      "         1.7188e-01,  1.6699e-01,  1.6797e-01,  7.4707e-02,  1.7773e-01,\n",
      "         1.8848e-01,  1.1475e-01,  1.8555e-01,  1.6113e-01,  2.2559e-01,\n",
      "         1.6797e-01,  1.6699e-01,  1.7676e-01,  1.5234e-01,  1.8457e-01,\n",
      "         1.8652e-01,  1.7969e-01,  8.2520e-02,  1.7773e-01,  1.6895e-01,\n",
      "         1.8555e-01,  1.7480e-01,  1.7090e-01,  1.6602e-01,  1.5430e-01,\n",
      "         1.5625e-01,  1.9043e-01,  1.6895e-01,  5.3223e-02,  1.7090e-01,\n",
      "         1.9727e-01,  1.9238e-01,  1.8945e-01,  1.4844e-01,  1.9629e-01,\n",
      "         2.0020e-01,  1.5918e-01,  8.5449e-02,  1.7676e-01,  1.9922e-01,\n",
      "         1.6504e-01,  1.6113e-01,  1.5234e-01,  1.5137e-01,  1.9434e-01,\n",
      "         1.5625e-01,  1.5430e-01,  1.4746e-01,  1.6211e-01,  1.8262e-01,\n",
      "         1.3867e-01,  2.0215e-01,  1.8066e-01,  1.6504e-01,  1.9531e-01,\n",
      "         1.9043e-01,  1.7676e-01,  1.8164e-01,  1.8359e-01,  1.7188e-01,\n",
      "         1.7480e-01,  1.8750e-01,  1.7773e-01,  1.9238e-01,  1.7383e-01,\n",
      "         1.8945e-01,  1.6113e-01,  1.8359e-01,  2.0215e-01,  1.7090e-01,\n",
      "         1.7578e-01,  1.8945e-01,  1.8359e-01,  1.6895e-01,  6.7871e-02,\n",
      "         1.8164e-01,  1.9727e-01,  1.4746e-01,  2.3242e-01,  1.6504e-01,\n",
      "         1.9238e-01,  1.7188e-01,  1.6797e-01,  1.7773e-01,  1.5234e-01,\n",
      "         1.7285e-01,  1.3379e-01,  1.9824e-01,  1.9727e-01,  1.9141e-01,\n",
      "         1.8848e-01,  3.2715e-02,  1.7285e-01,  1.8359e-01,  1.7188e-01,\n",
      "         1.9824e-01,  8.0078e-02,  1.4551e-01,  1.8359e-01,  1.7871e-01,\n",
      "         1.8164e-01,  1.7676e-01,  2.1094e-01,  1.6211e-01,  1.6699e-01,\n",
      "         1.8652e-01,  1.6406e-01,  1.7188e-01,  1.6504e-01,  1.4648e-01,\n",
      "         1.4648e-01,  1.5625e-01,  1.6211e-01,  1.5625e-01,  1.6211e-01,\n",
      "         1.8359e-01,  1.8848e-01,  1.7188e-01,  1.9336e-01,  1.8066e-01,\n",
      "         1.8359e-01,  1.5918e-01,  1.7578e-01,  1.7090e-01,  1.7090e-01,\n",
      "         2.0605e-01,  1.7090e-01,  1.8848e-01,  9.9609e-02,  1.9531e-01,\n",
      "         1.6504e-01,  1.5430e-01,  1.7578e-01,  1.8066e-01,  1.7090e-01,\n",
      "         1.8945e-01,  1.5820e-01,  1.8457e-01,  1.6113e-01,  1.7578e-01,\n",
      "         2.0410e-01,  1.5430e-01,  1.9238e-01,  1.8457e-01,  1.7188e-01,\n",
      "         1.9434e-01,  1.5234e-01,  1.6699e-01,  1.9824e-01,  1.8262e-01,\n",
      "         1.6992e-01,  1.7383e-01,  1.5723e-01,  1.6895e-01,  1.2500e-01,\n",
      "         1.9043e-01,  1.6992e-01,  1.7676e-01,  2.1582e-01,  1.6406e-01,\n",
      "         1.3184e-01,  1.7383e-01,  1.6895e-01,  2.3047e-01,  1.6992e-01,\n",
      "         1.5723e-01,  1.7969e-01,  1.5625e-01,  1.6992e-01,  2.0508e-01,\n",
      "         1.8652e-01,  1.8164e-01,  1.5820e-01,  1.5820e-01,  1.6016e-01,\n",
      "         1.5527e-01,  1.7188e-01,  1.7969e-01,  1.6895e-01,  1.6113e-01,\n",
      "         1.8164e-01,  1.7773e-01,  1.6797e-01,  1.8262e-01,  1.7188e-01,\n",
      "         1.3574e-01,  2.6855e-02,  1.6797e-01,  2.0410e-01,  1.7090e-01,\n",
      "         1.8457e-01,  1.6113e-01,  1.5918e-01,  1.9336e-01,  1.7090e-01,\n",
      "         1.7773e-01,  1.3867e-01,  1.6504e-01,  1.5625e-01,  1.7969e-01,\n",
      "         1.6895e-01,  1.7090e-01,  1.6797e-01,  1.6309e-01,  1.8359e-01,\n",
      "         1.8164e-01,  1.6602e-01,  1.9141e-01,  1.4648e-01,  1.7285e-01,\n",
      "         1.1230e-01,  1.5527e-01,  1.9531e-01,  1.7285e-01,  1.9531e-01,\n",
      "         1.6211e-01,  1.8457e-01,  1.7188e-01,  1.5234e-01,  1.7480e-01,\n",
      "         1.6992e-01,  2.0508e-01,  1.5430e-01,  1.8066e-01,  1.8457e-01,\n",
      "         1.7676e-01,  1.2793e-01,  1.7676e-01,  1.6699e-01,  1.8750e-01,\n",
      "         1.5625e-01,  1.7383e-01,  1.6504e-01,  2.0117e-01,  1.6211e-01,\n",
      "         4.5898e-02,  1.8652e-01,  1.7578e-01,  1.6992e-01,  6.9824e-02,\n",
      "         1.8750e-01,  1.6504e-01,  1.9434e-01,  1.8066e-01,  1.9629e-01,\n",
      "         1.7188e-01,  1.2402e-01,  1.9336e-01,  1.6309e-01,  1.9141e-01,\n",
      "         1.7871e-01,  1.8164e-01,  1.4355e-01,  1.7480e-01,  1.7969e-01,\n",
      "         1.7871e-01,  1.7188e-01,  1.6016e-01,  1.2793e-01,  1.6895e-01,\n",
      "         1.8652e-01,  1.7871e-01,  1.7969e-01,  1.8457e-01,  1.8359e-01,\n",
      "         1.8652e-01,  1.7578e-01,  2.1094e-01,  1.9531e-01,  1.8262e-01,\n",
      "         1.9141e-01,  1.7578e-01,  1.5820e-01,  1.9238e-01,  2.0215e-01,\n",
      "         1.7480e-01,  1.7188e-01,  2.0312e-01,  1.6992e-01,  1.8359e-01,\n",
      "         1.7676e-01,  1.7773e-01,  1.7773e-01,  1.9336e-01,  2.1191e-01,\n",
      "         1.7285e-01,  1.4941e-01,  1.9043e-01,  1.5723e-01,  1.9043e-01,\n",
      "         1.8555e-01,  1.7578e-01,  2.0410e-01], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.encoder.block.7.layer.1.DenseReluDense.wi.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.5820,  0.9023, -0.2275,  ..., -0.1201,  0.3555,  0.2031],\n",
      "        [ 0.0820, -0.0811,  0.1650,  ..., -0.5234, -0.3770, -0.0598],\n",
      "        [-0.3750, -0.5000, -0.2119,  ..., -0.8555, -0.3887,  0.3125],\n",
      "        ...,\n",
      "        [ 0.2988, -0.2217, -0.0115,  ..., -0.5898, -0.2314,  0.0557],\n",
      "        [ 0.4023,  0.5195,  0.4102,  ..., -0.1582, -0.3359,  0.1904],\n",
      "        [ 0.3691,  1.0938, -0.7969,  ...,  0.3574,  0.7031, -0.2090]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.7.layer.1.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0388,  0.0522,  0.5469,  ...,  0.1484, -0.3418,  0.5195],\n",
      "        [ 0.3555, -0.3457, -0.4453,  ..., -0.0786, -0.1582,  0.0767],\n",
      "        [-0.0361,  0.1338,  0.5000,  ...,  0.3789,  0.0771,  0.2695],\n",
      "        ...,\n",
      "        [ 0.1270,  0.1377, -0.0547,  ...,  0.2559,  0.2031,  0.1436],\n",
      "        [ 0.0009,  0.2500,  0.3477,  ..., -0.1738,  0.2695,  0.4707],\n",
      "        [-0.3125,  0.3027,  0.2197,  ..., -0.5430,  0.1914,  0.3047]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.7.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([1.2031, 1.0938, 1.0938, 1.1562, 1.1406, 1.2031, 0.3047, 1.2188, 2.0625,\n",
      "        1.1875, 1.1719, 1.1875, 1.1172, 1.2422, 1.0781, 1.1406, 1.1797, 1.1250,\n",
      "        1.0391, 1.1016, 1.1406, 1.1875, 2.3750, 1.0625, 1.1328, 1.2344, 1.0703,\n",
      "        1.0547, 1.2266, 1.0781, 1.1875, 1.0703, 1.1875, 1.0938, 1.1641, 1.1406,\n",
      "        1.3672, 1.2266, 1.1016, 1.8750, 1.1094, 1.3203, 1.0625, 1.2109, 2.8906,\n",
      "        1.1406, 1.0938, 1.1797, 1.1016, 0.5664, 1.1875, 1.1328, 1.5469, 1.0625,\n",
      "        1.1719, 1.0859, 1.9297, 1.1172, 1.1328, 1.1797, 1.1641, 1.1094, 1.1172,\n",
      "        1.0938, 1.1562, 1.3281, 1.1172, 1.1484, 1.2891, 1.1016, 1.1016, 1.5078,\n",
      "        1.0703, 1.1641, 1.0781, 1.1172, 1.2969, 1.1172, 1.1016, 1.1484, 1.0469,\n",
      "        1.1328, 1.1406, 1.2422, 1.1250, 1.2266, 1.2734, 1.1562, 1.2109, 1.1328,\n",
      "        1.2188, 1.0938, 1.0469, 1.1953, 1.1641, 1.0547, 1.0938, 1.0938, 1.1250,\n",
      "        1.0781, 1.2109, 1.1172, 1.0625, 1.1094, 1.1875, 1.5156, 1.1641, 1.1953,\n",
      "        1.1406, 1.9922, 1.1797, 1.1484, 1.1016, 1.1250, 1.1094, 1.0938, 1.0938,\n",
      "        1.0234, 1.3203, 1.1328, 1.0469, 1.0859, 1.3281, 1.0703, 1.0000, 1.1172,\n",
      "        1.0859, 1.1094, 1.1016, 1.2031, 1.3047, 1.1484, 1.1172, 1.0781, 1.1094,\n",
      "        1.1094, 1.3203, 1.0469, 1.1875, 1.1172, 1.0469, 1.2891, 1.2031, 1.2578,\n",
      "        1.0625, 1.3203, 1.0938, 1.1172, 1.1562, 1.1484, 1.1406, 1.1562, 1.1094,\n",
      "        1.2734, 1.1562, 1.0703, 1.1094, 1.1641, 1.1484, 1.1016, 1.1250, 1.1406,\n",
      "        1.0312, 3.3750, 1.0859, 1.0391, 1.1562, 1.1719, 1.0391, 1.1016, 1.2109,\n",
      "        1.1406, 1.1641, 1.2969, 1.1328, 1.1172, 1.1406, 0.4805, 1.1406, 0.5000,\n",
      "        1.1094, 1.1094, 1.1328, 1.0547, 1.1641, 1.2969, 1.0625, 1.1562, 1.1953,\n",
      "        1.1484, 1.0938, 1.1250, 1.1484, 1.1250, 1.0938, 0.4512, 1.1484, 1.2969,\n",
      "        1.1641, 1.0703, 1.0703, 1.1328, 1.1172, 1.2109, 1.1328, 1.1641, 1.1328,\n",
      "        1.1406, 1.1250, 1.1484, 1.2422, 1.1094, 1.1328, 1.2188, 1.1172, 1.2109,\n",
      "        1.1016, 1.1172, 1.1719, 2.0469, 1.0703, 1.1562, 1.0938, 1.0781, 1.2266,\n",
      "        1.1328, 1.2031, 1.1328, 1.2266, 1.1562, 1.1094, 1.1328, 0.8789, 1.0938,\n",
      "        1.0938, 0.9141, 1.0234, 1.2031, 1.1641, 0.9727, 1.2734, 1.1875, 1.1406,\n",
      "        1.1328, 1.0234, 1.0938, 1.1953, 1.1094, 1.0625, 1.1797, 3.0156, 1.0859,\n",
      "        1.1406, 1.1719, 1.0938, 1.1094, 1.1328, 0.9922, 0.6523, 1.1406, 1.1328,\n",
      "        0.4844, 1.0938, 1.1641, 1.1250, 1.0938, 1.1562, 1.0938, 1.1094, 1.1484,\n",
      "        0.9844, 1.2578, 1.4141, 1.3516, 1.2109, 1.2812, 1.1406, 1.0938, 1.1250,\n",
      "        1.1094, 1.2656, 1.3125, 1.1953, 1.2969, 1.1719, 1.2266, 1.0938, 1.1328,\n",
      "        1.0781, 1.6797, 1.0703, 1.1484, 1.0938, 1.0859, 1.1484, 1.1875, 1.1172,\n",
      "        1.1719, 1.1484, 1.2344, 1.2344, 1.1016, 1.2109, 1.0781, 1.0859, 1.1172,\n",
      "        1.1094, 1.1562, 1.1250, 1.1641, 1.1250, 1.2188, 1.0938, 0.9609, 1.1797,\n",
      "        1.1484, 1.1094, 1.1875, 1.3125, 1.0938, 1.0703, 1.2031, 1.1250, 1.1406,\n",
      "        1.2031, 1.0547, 1.3359, 1.2266, 1.0859, 1.1250, 1.1797, 1.2500, 1.1875,\n",
      "        1.0469, 1.1328, 1.1328, 1.0938, 1.0547, 1.1016, 1.2500, 1.2188, 1.1641,\n",
      "        1.2266, 1.1641, 1.1250, 1.2109, 1.1328, 1.1953, 1.1641, 1.8906, 1.1562,\n",
      "        1.0703, 1.2266, 1.1172, 1.1484, 1.1016, 1.1016, 1.1797, 1.1250, 1.2266,\n",
      "        1.1172, 1.2109, 1.2031, 1.0234, 1.0859, 1.1094, 0.8398, 1.3203, 1.0703,\n",
      "        1.2188, 1.3203, 1.1562, 0.3613, 1.0391, 1.2812, 1.1484, 1.1250, 1.1016,\n",
      "        1.1250, 1.2422, 1.1641, 1.1562, 1.2188, 1.1406, 1.1094, 1.1094, 1.1484,\n",
      "        0.8945, 1.0078, 1.2422, 1.1250, 2.6094, 1.1406, 1.1641, 1.1875, 1.2422,\n",
      "        1.1953, 1.1172, 1.1484, 1.1094, 1.1016, 0.6328, 1.1094, 1.1719, 1.0625,\n",
      "        1.1797, 1.3672, 1.0938, 1.0312, 1.0625, 1.1016, 1.0859, 1.1953, 1.1953,\n",
      "        1.0547, 1.1094, 1.1328, 1.1016, 0.4297, 1.4453, 1.3281, 1.1719, 1.0938,\n",
      "        0.4707, 1.1641, 1.1250, 1.1094, 1.0703, 1.1875, 1.1016, 1.1875, 1.1484,\n",
      "        1.1172, 1.0312, 1.3281, 1.1094, 1.1797, 1.0938, 1.1641, 1.1875, 1.2266,\n",
      "        1.0078, 1.3047, 1.1406, 1.0781, 1.1484, 1.0859, 1.1641, 1.1328, 1.1172,\n",
      "        1.1016, 1.0781, 1.1328, 1.0859, 1.3750, 1.2344, 1.1328, 1.1562, 1.1953,\n",
      "        1.0859, 1.0703, 1.1016, 1.0703, 1.1250, 1.1094, 1.1328, 1.0234, 1.1641,\n",
      "        1.0312, 1.1562, 1.1016, 1.0703, 0.9492, 1.1016, 1.1641, 1.1016, 1.1250,\n",
      "        1.1328, 1.2578, 1.1016, 1.1641, 2.0000, 1.1875, 1.1328, 1.2188, 1.1641,\n",
      "        1.1641, 1.3047, 1.0625, 1.1641, 1.1406, 1.2578, 1.5156, 1.1641, 1.1641,\n",
      "        1.1875, 1.1641, 1.1172, 1.0859, 1.0469, 1.0781, 1.1953, 1.2812, 2.0156,\n",
      "        1.2266, 1.1797, 1.1484, 1.0938, 1.0391, 1.2109, 1.1484, 1.1094, 1.7891,\n",
      "        1.0859, 1.2188, 1.1328, 1.0781, 1.1406, 1.1406, 1.1875, 1.1016, 1.2031,\n",
      "        1.0625, 1.1562, 1.1406, 1.9141, 1.1016, 1.3594, 1.1250, 1.1484, 1.1328,\n",
      "        1.0859, 1.1250, 1.1406, 1.1719, 1.1172, 1.1172, 1.1172, 1.2344, 1.1562,\n",
      "        1.1875, 1.1016, 1.1406, 1.1953, 1.1328, 1.1484, 1.1484, 1.1641, 1.1641,\n",
      "        0.4004, 1.2109, 1.2266, 1.0859, 1.3359, 1.0938, 1.1328, 1.1562, 1.1250,\n",
      "        1.1562, 1.1016, 1.2188, 0.9961, 1.2656, 1.3281, 1.1406, 1.1172, 0.3516,\n",
      "        1.1016, 1.1797, 1.1562, 1.1641, 0.4395, 1.0859, 1.1562, 1.0938, 1.1719,\n",
      "        1.1484, 1.1250, 1.0625, 1.1250, 1.1406, 1.1172, 1.1484, 1.1328, 1.0547,\n",
      "        1.0547, 1.1406, 1.1016, 1.1250, 1.0391, 1.1484, 1.1406, 1.1094, 1.1328,\n",
      "        1.0859, 1.1250, 1.1172, 1.1406, 1.1562, 1.1016, 1.2812, 1.1172, 1.1953,\n",
      "        0.8086, 1.1172, 1.0391, 1.0469, 1.0938, 1.2422, 1.1328, 1.1719, 1.1641,\n",
      "        1.1797, 1.1016, 1.1484, 1.2734, 1.1406, 1.0547, 1.1172, 1.1172, 1.2812,\n",
      "        1.1094, 1.1172, 1.1641, 1.1562, 1.1094, 1.1875, 1.2109, 1.1016, 1.0625,\n",
      "        1.1953, 1.1484, 1.2266, 1.3125, 1.0547, 0.6992, 1.1406, 1.1016, 1.2891,\n",
      "        1.0625, 1.1250, 1.1953, 1.0781, 1.2031, 1.2344, 1.1406, 1.0781, 1.0859,\n",
      "        1.1406, 1.1172, 1.0547, 1.1719, 1.0703, 1.1484, 1.0781, 1.2109, 1.0859,\n",
      "        1.0469, 1.0312, 1.1953, 0.8555, 0.4375, 1.0547, 1.3047, 1.1250, 1.2031,\n",
      "        1.1016, 1.1172, 1.1172, 1.1953, 1.1797, 1.4766, 1.1016, 1.1328, 1.0859,\n",
      "        1.1250, 1.0625, 1.1562, 1.1641, 1.1094, 1.1094, 1.1484, 1.1797, 1.0781,\n",
      "        1.1094, 1.5703, 1.1328, 1.2344, 1.1250, 1.1016, 1.1094, 1.1484, 1.1719,\n",
      "        1.1016, 1.1406, 1.1406, 1.2422, 1.0781, 1.1094, 1.1562, 1.2812, 1.0781,\n",
      "        1.1328, 1.1172, 1.1719, 1.1094, 1.1250, 1.3359, 1.2188, 1.1406, 2.1250,\n",
      "        1.1719, 1.1406, 1.0859, 0.5156, 1.1562, 1.2266, 1.1875, 1.1328, 1.1562,\n",
      "        1.1172, 1.1875, 1.1953, 1.1016, 1.2188, 1.0703, 1.1016, 1.1094, 1.1875,\n",
      "        1.2344, 1.1328, 1.1328, 1.0547, 0.9336, 1.0859, 1.0859, 1.1875, 1.1641,\n",
      "        1.1094, 1.1797, 1.1328, 1.2656, 1.1172, 1.1328, 1.1562, 1.2422, 1.0156,\n",
      "        1.1250, 1.1719, 1.3516, 1.2188, 1.1641, 1.3047, 1.1562, 1.1797, 1.1172,\n",
      "        1.0703, 1.0938, 1.1016, 1.1562, 1.0312, 0.9531, 1.1719, 1.1016, 1.1094,\n",
      "        1.1250, 1.1406, 1.1797], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.8.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0542,  0.0104, -0.0042,  ...,  0.0028, -0.0016,  0.0598],\n",
      "        [-0.0067, -0.0349, -0.0225,  ..., -0.0088,  0.0238,  0.0105],\n",
      "        [-0.0016, -0.0366, -0.0364,  ...,  0.0130, -0.0199,  0.0459],\n",
      "        ...,\n",
      "        [ 0.0060, -0.0297, -0.0012,  ...,  0.0393, -0.0151,  0.0028],\n",
      "        [ 0.0053,  0.0684, -0.0118,  ...,  0.0076, -0.0479, -0.0048],\n",
      "        [-0.0532, -0.0388, -0.0194,  ...,  0.0029, -0.0315, -0.0195]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.8.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[-4.9805e-01,  1.0303e-01,  2.7466e-02,  ..., -1.2256e-01,\n",
      "         -1.6797e-01,  1.2988e-01],\n",
      "        [-1.7383e-01,  2.2559e-01, -1.2158e-01,  ..., -1.0059e-01,\n",
      "          9.2285e-02, -2.2949e-01],\n",
      "        [-1.4282e-02,  3.4375e-01, -1.5137e-01,  ..., -9.3262e-02,\n",
      "         -1.7480e-01, -6.1768e-02],\n",
      "        ...,\n",
      "        [-1.3199e-03,  2.6953e-01,  3.0273e-01,  ...,  3.7891e-01,\n",
      "         -3.2227e-01,  4.3164e-01],\n",
      "        [ 1.5234e-01, -1.7676e-01, -5.3125e-01,  ...,  3.3691e-02,\n",
      "         -4.2725e-04,  1.2988e-01],\n",
      "        [-1.8555e-01, -3.0664e-01, -1.5527e-01,  ...,  4.7461e-01,\n",
      "          2.2461e-01, -5.3906e-01]], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.8.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.6797,  0.7891, -0.9570,  ..., -0.4824,  1.3672,  0.6641],\n",
      "        [-0.4902, -0.6172,  0.0051,  ...,  0.0796,  0.4824,  0.2832],\n",
      "        [-0.5859,  0.2451, -0.2061,  ..., -0.0192,  0.0172, -0.0566],\n",
      "        ...,\n",
      "        [-0.3262, -0.5859, -0.2754,  ..., -1.6172,  0.5781, -0.6953],\n",
      "        [ 0.9727,  0.6133,  0.1709,  ...,  0.5977, -1.0625, -1.5859],\n",
      "        [-0.4707, -0.4180, -0.5469,  ...,  1.2109, -1.2500,  0.3828]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.8.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-3.6523e-01,  2.9297e-01,  4.0234e-01,  ..., -1.1520e-03,\n",
      "         -3.7305e-01,  5.0000e-01],\n",
      "        [-1.1797e+00, -4.7461e-01, -1.0000e+00,  ..., -4.6484e-01,\n",
      "          1.0859e+00, -4.5703e-01],\n",
      "        [ 1.1406e+00, -5.9766e-01,  8.9453e-01,  ..., -4.2578e-01,\n",
      "          1.1797e+00,  8.2422e-01],\n",
      "        ...,\n",
      "        [ 1.1816e-01, -2.8906e-01, -2.1387e-01,  ..., -1.4531e+00,\n",
      "          8.9844e-01, -1.8516e+00],\n",
      "        [-1.4766e+00, -8.3984e-01, -7.7637e-02,  ..., -1.7188e-01,\n",
      "         -1.4922e+00,  3.1445e-01],\n",
      "        [-6.5625e-01,  6.5430e-02,  1.8945e-01,  ..., -1.8945e-01,\n",
      "         -2.1875e-01, -4.8828e-01]], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.8.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 1.7969e-01,  1.6992e-01,  1.6992e-01,  1.8652e-01,  1.7285e-01,\n",
      "         1.6895e-01,  3.7842e-02,  1.9824e-01,  1.0645e-01,  1.7188e-01,\n",
      "         1.8066e-01,  1.7871e-01,  1.3867e-01,  1.5527e-01,  1.5723e-01,\n",
      "         1.8750e-01,  1.6992e-01,  1.7773e-01,  1.6895e-01,  1.6602e-01,\n",
      "         1.4746e-01,  1.7188e-01,  5.0293e-02,  1.6504e-01,  1.6797e-01,\n",
      "         1.8066e-01,  1.6504e-01,  1.6113e-01,  1.7676e-01,  1.6406e-01,\n",
      "         1.6406e-01,  1.6895e-01,  1.8555e-01,  1.5625e-01,  1.8652e-01,\n",
      "         1.8262e-01,  1.4844e-01,  1.9531e-01,  1.5723e-01,  1.7676e-01,\n",
      "         1.6895e-01,  1.6211e-01,  1.6699e-01,  1.7871e-01,  9.7168e-02,\n",
      "         1.6504e-01,  1.3086e-01,  1.6504e-01,  1.8457e-01,  4.6631e-02,\n",
      "         1.8750e-01,  1.8164e-01,  1.4160e-01,  1.6504e-01,  4.7119e-02,\n",
      "         1.6211e-01,  9.8145e-02,  1.7480e-01,  1.8555e-01,  1.6113e-01,\n",
      "         1.8750e-01,  1.5527e-01,  1.8164e-01,  1.7285e-01,  1.6895e-01,\n",
      "         5.0537e-02,  1.6602e-01,  1.6016e-01,  1.8555e-01,  1.6797e-01,\n",
      "         1.7773e-01,  1.6602e-01,  1.6406e-01,  1.7383e-01,  1.6504e-01,\n",
      "         1.6113e-01,  1.5234e-01,  1.7578e-01,  1.5527e-01,  2.0117e-01,\n",
      "         1.6113e-01,  1.7773e-01,  1.7871e-01,  1.8652e-01,  1.6992e-01,\n",
      "         1.7871e-01,  1.8945e-01,  1.7871e-01,  1.8359e-01,  1.6406e-01,\n",
      "         1.7773e-01,  1.5332e-01,  1.6113e-01,  1.6699e-01,  1.6211e-01,\n",
      "         1.6309e-01,  1.8848e-01,  1.7383e-01,  1.7383e-01,  1.5137e-01,\n",
      "         1.8457e-01,  1.8652e-01,  1.1865e-01,  1.5527e-01,  1.7578e-01,\n",
      "         1.4941e-01,  1.6992e-01,  1.6797e-01,  1.6016e-01,  1.0254e-01,\n",
      "         1.7285e-01,  1.7773e-01,  1.6992e-01,  1.4551e-01,  1.7090e-01,\n",
      "         1.6699e-01,  1.7676e-01,  1.6211e-01,  1.5820e-01,  1.7578e-01,\n",
      "         1.4551e-01,  1.7578e-01,  1.7090e-01,  1.6406e-01,  1.4453e-01,\n",
      "         1.6797e-01,  1.9238e-01,  1.7773e-01,  1.6211e-01,  1.7285e-01,\n",
      "         1.1035e-01,  1.8652e-01,  1.6992e-01,  1.4453e-01,  1.6699e-01,\n",
      "         1.4648e-01,  1.8750e-01,  1.6602e-01,  1.7969e-01,  1.8457e-01,\n",
      "         1.6504e-01,  1.9434e-01,  1.6602e-01,  1.6992e-01,  1.7383e-01,\n",
      "         1.9238e-01,  1.6113e-01,  1.8555e-01,  1.6895e-01,  1.7676e-01,\n",
      "         1.7773e-01,  1.7969e-01,  1.8359e-01,  1.9043e-01,  1.5527e-01,\n",
      "         1.7383e-01,  1.7188e-01,  1.5820e-01,  1.6504e-01,  1.6797e-01,\n",
      "         1.5820e-01,  1.6895e-01,  1.4160e-01,  8.1055e-02,  1.5918e-01,\n",
      "         1.4941e-01,  1.6602e-01,  1.7578e-01,  1.7578e-01,  1.7773e-01,\n",
      "         1.8555e-01,  1.8848e-01,  1.8652e-01,  1.2891e-01,  1.8750e-01,\n",
      "         1.5820e-01,  1.8652e-01,  6.8848e-02,  1.5430e-01,  2.9297e-02,\n",
      "         1.7480e-01,  1.6602e-01,  1.8066e-01,  1.6797e-01,  1.9043e-01,\n",
      "         2.0215e-01,  1.5723e-01,  1.7383e-01,  1.8262e-01,  1.7285e-01,\n",
      "         1.6309e-01,  1.7188e-01,  1.4355e-01,  1.7773e-01,  1.6113e-01,\n",
      "         5.4688e-02,  1.7578e-01,  1.7969e-01,  1.7578e-01,  1.3965e-01,\n",
      "         1.4844e-01,  1.8652e-01,  1.7285e-01,  1.7285e-01,  1.7090e-01,\n",
      "         1.8164e-01,  1.6211e-01,  1.6602e-01,  1.6016e-01,  1.7969e-01,\n",
      "         1.7188e-01,  1.5918e-01,  1.7871e-01,  1.8164e-01,  1.7285e-01,\n",
      "         1.8945e-01,  1.6699e-01,  1.6797e-01,  1.7676e-01,  1.4551e-01,\n",
      "         1.1377e-01,  1.8652e-01,  1.6113e-01,  1.8262e-01,  1.5039e-01,\n",
      "         1.7383e-01,  1.9434e-01,  1.8359e-01,  1.7285e-01,  1.9922e-01,\n",
      "         1.5723e-01,  1.6895e-01,  1.2988e-01,  1.4844e-01,  1.5723e-01,\n",
      "         1.3281e-01,  1.2500e-01,  1.8164e-01,  1.8164e-01,  1.3574e-01,\n",
      "         1.7773e-01,  1.9238e-01,  1.7188e-01,  1.8262e-01,  1.4648e-01,\n",
      "         1.6992e-01,  1.7969e-01,  1.8359e-01,  1.4941e-01,  1.8164e-01,\n",
      "         1.2207e-01,  1.5137e-01,  1.5723e-01,  1.5625e-01,  1.6504e-01,\n",
      "         1.6016e-01,  1.6992e-01,  1.4160e-01,  8.0566e-02,  1.6699e-01,\n",
      "         1.8750e-01,  1.3379e-01,  1.5918e-01,  1.8457e-01,  1.6406e-01,\n",
      "         1.6309e-01,  1.6406e-01,  1.6406e-01,  1.7090e-01,  1.6895e-01,\n",
      "         8.1055e-02,  1.9336e-01,  1.5723e-01,  3.0884e-02,  1.8262e-01,\n",
      "         1.8359e-01,  1.8164e-01,  1.3965e-01,  1.7285e-01,  1.7383e-01,\n",
      "         1.9824e-01,  1.9629e-01,  1.7188e-01,  1.8262e-01,  1.9141e-01,\n",
      "         1.4355e-01,  1.5625e-01,  1.7676e-01,  1.4746e-01,  1.4160e-01,\n",
      "         1.5723e-01,  1.7188e-01,  1.6016e-01,  1.6992e-01,  1.7285e-01,\n",
      "         1.5430e-01,  1.7773e-01,  1.7188e-01,  1.6699e-01,  1.8750e-01,\n",
      "         1.9824e-01,  1.7090e-01,  1.9531e-01,  1.7480e-01,  1.7383e-01,\n",
      "         1.5820e-01,  1.7773e-01,  1.8066e-01,  1.7773e-01,  1.7480e-01,\n",
      "         1.8066e-01,  1.8555e-01,  1.6992e-01,  1.3281e-01,  1.8652e-01,\n",
      "         1.7480e-01,  1.3184e-01,  1.9336e-01,  1.5430e-01,  1.6406e-01,\n",
      "         1.5527e-01,  1.7383e-01,  1.6602e-01,  1.5723e-01,  1.8262e-01,\n",
      "         1.6211e-01,  1.7188e-01,  1.9824e-01,  1.7871e-01,  1.7969e-01,\n",
      "         1.7578e-01,  1.3672e-01,  1.7188e-01,  1.6309e-01,  1.7188e-01,\n",
      "         1.7480e-01,  1.5820e-01,  1.6406e-01,  1.6211e-01,  1.8164e-01,\n",
      "         1.9336e-01,  1.8652e-01,  1.8457e-01,  1.6211e-01,  1.8457e-01,\n",
      "         1.7188e-01,  1.6113e-01,  1.7871e-01,  1.5625e-01,  1.3281e-01,\n",
      "         1.8652e-01,  1.7871e-01,  1.8457e-01,  1.8848e-01,  1.6895e-01,\n",
      "         1.7188e-01,  1.7676e-01,  1.7383e-01,  1.6797e-01,  1.4941e-01,\n",
      "         1.7090e-01,  1.8066e-01,  1.7090e-01,  1.3086e-01,  1.6406e-01,\n",
      "         1.6602e-01,  8.3496e-02,  1.7285e-01,  1.5625e-01,  1.7285e-01,\n",
      "         1.6406e-01,  1.9141e-01,  5.7129e-02,  1.4258e-01,  1.8750e-01,\n",
      "         1.6406e-01,  1.4551e-01,  1.7285e-01,  1.8164e-01,  1.3574e-01,\n",
      "         1.7871e-01,  1.7773e-01,  1.8457e-01,  1.7969e-01,  1.6504e-01,\n",
      "         7.2754e-02,  1.6992e-01,  1.2500e-01,  1.4551e-01,  1.9043e-01,\n",
      "         1.7188e-01,  7.3242e-02,  1.7383e-01,  1.8945e-01,  1.8750e-01,\n",
      "         1.6699e-01,  1.7871e-01,  1.7090e-01,  1.7383e-01,  1.6504e-01,\n",
      "         1.6406e-01, -7.4863e-05,  1.5430e-01,  1.8945e-01,  1.6211e-01,\n",
      "         1.6895e-01,  1.8555e-01,  1.6113e-01,  1.6211e-01,  1.6113e-01,\n",
      "         1.6992e-01,  1.6309e-01,  1.6211e-01,  1.7285e-01,  1.6797e-01,\n",
      "         1.7285e-01,  1.4941e-01,  1.5820e-01,  5.9326e-02,  1.7676e-01,\n",
      "         1.7578e-01,  1.9238e-01,  1.5723e-01,  6.3477e-02,  1.7480e-01,\n",
      "         1.7188e-01,  1.7480e-01,  1.6699e-01,  1.8359e-01,  1.7969e-01,\n",
      "         1.8555e-01,  1.8066e-01,  1.2012e-01,  1.5723e-01,  1.5137e-01,\n",
      "         1.5723e-01,  1.7773e-01,  1.6504e-01,  1.7480e-01,  1.9238e-01,\n",
      "         1.9043e-01,  1.3574e-01,  1.8457e-01,  1.8457e-01,  1.5723e-01,\n",
      "         1.8066e-01,  1.6309e-01,  1.8848e-01,  1.7480e-01,  1.8066e-01,\n",
      "         1.6504e-01,  1.6699e-01,  1.7969e-01,  1.7676e-01,  1.9531e-01,\n",
      "         1.8848e-01,  1.7773e-01,  1.6797e-01,  1.7090e-01,  1.5430e-01,\n",
      "         1.5723e-01,  1.6406e-01,  1.7578e-01,  1.8164e-01,  1.8359e-01,\n",
      "         1.8359e-01,  1.3184e-01,  1.5723e-01,  1.3770e-01,  1.5918e-01,\n",
      "         1.5332e-01,  1.6309e-01,  1.4648e-01,  1.5918e-01,  1.7090e-01,\n",
      "         1.7871e-01,  1.8066e-01,  1.6504e-01,  6.2256e-02,  1.8066e-01,\n",
      "         1.8945e-01,  1.1768e-01,  1.7285e-01,  1.7285e-01,  1.8945e-01,\n",
      "         1.7188e-01,  1.7578e-01,  1.6211e-01,  1.6504e-01,  1.7969e-01,\n",
      "         1.7773e-01,  1.7871e-01,  8.0566e-02,  1.7578e-01,  1.6504e-01,\n",
      "         1.7676e-01,  1.7383e-01,  1.7188e-01,  1.6797e-01,  1.6113e-01,\n",
      "         1.5723e-01,  1.7578e-01,  1.6406e-01,  4.4434e-02,  1.6895e-01,\n",
      "         1.8359e-01,  1.7969e-01,  1.7480e-01,  1.5137e-01,  1.9141e-01,\n",
      "         1.9629e-01,  1.6211e-01,  9.1797e-02,  1.7383e-01,  1.8164e-01,\n",
      "         1.6992e-01,  1.5723e-01,  1.4746e-01,  1.4160e-01,  1.8359e-01,\n",
      "         1.5625e-01,  1.5332e-01,  1.4648e-01,  1.6602e-01,  1.8359e-01,\n",
      "         1.5137e-01,  1.8848e-01,  1.6113e-01,  1.6309e-01,  1.9043e-01,\n",
      "         1.7383e-01,  1.7871e-01,  1.7773e-01,  1.7285e-01,  1.6699e-01,\n",
      "         1.8164e-01,  1.7383e-01,  1.6406e-01,  1.7773e-01,  1.7090e-01,\n",
      "         1.7676e-01,  1.6699e-01,  1.7871e-01,  1.8555e-01,  1.6992e-01,\n",
      "         1.7285e-01,  1.8652e-01,  1.8457e-01,  1.6211e-01,  6.5918e-02,\n",
      "         1.6602e-01,  1.8848e-01,  1.4453e-01,  2.1387e-01,  1.6895e-01,\n",
      "         1.8262e-01,  1.6406e-01,  1.6113e-01,  1.8066e-01,  1.3965e-01,\n",
      "         1.7871e-01,  1.4355e-01,  1.8652e-01,  1.8945e-01,  1.8164e-01,\n",
      "         1.9141e-01,  3.2715e-02,  1.7383e-01,  1.7676e-01,  1.7480e-01,\n",
      "         1.8945e-01,  9.0820e-02,  1.5918e-01,  1.7383e-01,  1.7871e-01,\n",
      "         1.7871e-01,  1.7578e-01,  1.9824e-01,  1.6211e-01,  1.5430e-01,\n",
      "         1.7871e-01,  1.6699e-01,  1.7578e-01,  1.6602e-01,  1.5234e-01,\n",
      "         1.5625e-01,  1.6895e-01,  1.6016e-01,  1.6211e-01,  1.6309e-01,\n",
      "         1.7969e-01,  1.8164e-01,  1.7188e-01,  1.8848e-01,  1.7480e-01,\n",
      "         1.8164e-01,  1.6113e-01,  1.7090e-01,  1.5234e-01,  1.6309e-01,\n",
      "         1.9727e-01,  1.7285e-01,  1.8066e-01,  1.1279e-01,  1.9043e-01,\n",
      "         1.6211e-01,  1.6016e-01,  1.7285e-01,  1.7188e-01,  1.6992e-01,\n",
      "         1.7578e-01,  1.5527e-01,  1.7480e-01,  1.6406e-01,  1.7188e-01,\n",
      "         1.9531e-01,  1.6504e-01,  1.9141e-01,  1.8262e-01,  1.7090e-01,\n",
      "         1.9141e-01,  1.5430e-01,  1.6992e-01,  1.8457e-01,  1.8066e-01,\n",
      "         1.6406e-01,  1.7188e-01,  1.5137e-01,  1.6504e-01,  1.2158e-01,\n",
      "         1.8457e-01,  1.7578e-01,  1.7969e-01,  2.0215e-01,  1.6797e-01,\n",
      "         1.2158e-01,  1.6992e-01,  1.7188e-01,  2.1191e-01,  1.7676e-01,\n",
      "         1.5527e-01,  1.9043e-01,  1.5039e-01,  1.7285e-01,  1.9531e-01,\n",
      "         1.7676e-01,  1.7090e-01,  1.6699e-01,  1.5430e-01,  1.6211e-01,\n",
      "         1.6504e-01,  1.7578e-01,  1.7773e-01,  1.6602e-01,  1.5332e-01,\n",
      "         1.6309e-01,  1.7480e-01,  1.6113e-01,  1.6699e-01,  1.7188e-01,\n",
      "         1.1914e-01,  2.0752e-02,  1.6797e-01,  1.9238e-01,  1.7188e-01,\n",
      "         1.7090e-01,  1.6504e-01,  1.6895e-01,  1.8457e-01,  1.7090e-01,\n",
      "         1.8457e-01,  1.3770e-01,  1.6113e-01,  1.5918e-01,  1.8262e-01,\n",
      "         1.7285e-01,  1.6309e-01,  1.6602e-01,  1.6016e-01,  1.7871e-01,\n",
      "         1.8066e-01,  1.6504e-01,  1.8457e-01,  1.5039e-01,  1.6797e-01,\n",
      "         1.1182e-01,  1.5625e-01,  1.9238e-01,  1.7188e-01,  1.8555e-01,\n",
      "         1.6113e-01,  1.7578e-01,  1.5723e-01,  1.5332e-01,  1.7285e-01,\n",
      "         1.7090e-01,  2.0020e-01,  1.5234e-01,  1.7676e-01,  1.7773e-01,\n",
      "         1.7480e-01,  1.3867e-01,  1.6992e-01,  1.6504e-01,  1.7090e-01,\n",
      "         1.5430e-01,  1.7188e-01,  1.6602e-01,  1.9336e-01,  1.6309e-01,\n",
      "         5.1514e-02,  1.7188e-01,  1.7578e-01,  1.6797e-01,  6.4453e-02,\n",
      "         1.7090e-01,  1.6504e-01,  1.8750e-01,  1.8164e-01,  1.7285e-01,\n",
      "         1.6406e-01,  1.3086e-01,  1.8457e-01,  1.6113e-01,  1.7871e-01,\n",
      "         1.7480e-01,  1.7773e-01,  1.4746e-01,  1.8262e-01,  1.6895e-01,\n",
      "         1.7578e-01,  1.7285e-01,  1.5332e-01,  1.3379e-01,  1.7188e-01,\n",
      "         1.7773e-01,  1.8457e-01,  1.7383e-01,  1.8262e-01,  1.8164e-01,\n",
      "         1.8359e-01,  1.7578e-01,  1.8848e-01,  1.9043e-01,  1.7383e-01,\n",
      "         1.7676e-01,  1.7578e-01,  1.6602e-01,  1.8457e-01,  1.9531e-01,\n",
      "         1.6797e-01,  1.6797e-01,  1.9141e-01,  1.8164e-01,  1.7871e-01,\n",
      "         1.7480e-01,  1.7676e-01,  1.7480e-01,  1.8750e-01,  1.9434e-01,\n",
      "         1.6211e-01,  1.5430e-01,  1.7969e-01,  1.5234e-01,  1.8164e-01,\n",
      "         1.8359e-01,  1.6797e-01,  1.9531e-01], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.encoder.block.8.layer.1.DenseReluDense.wi.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.7383,  0.9414, -0.0884,  ..., -0.6523, -0.5078,  0.0378],\n",
      "        [-0.3984, -0.5391,  0.3145,  ...,  0.2383,  0.1260,  0.4141],\n",
      "        [-0.0557, -0.2236, -0.7773,  ..., -0.4629,  0.0967,  0.3691],\n",
      "        ...,\n",
      "        [ 0.0557, -0.1104,  0.1748,  ..., -0.4727,  0.1445, -0.1235],\n",
      "        [ 0.2002, -0.4609,  0.5117,  ...,  0.1289,  0.8242,  0.1855],\n",
      "        [-0.1084, -0.7188,  0.3613,  ..., -0.3887,  0.5547, -0.6328]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.8.layer.1.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0933, -0.1846, -0.1235,  ...,  0.1875, -0.1250,  0.1099],\n",
      "        [ 0.0251, -0.1680, -0.0583,  ...,  0.2773, -0.1133, -0.1904],\n",
      "        [ 0.0679,  0.4004,  0.0264,  ..., -0.0581, -0.1309, -0.1943],\n",
      "        ...,\n",
      "        [ 0.0549,  0.1475,  0.2715,  ..., -0.2715,  0.0084, -0.1680],\n",
      "        [-0.0425, -0.0058, -0.1738,  ..., -0.1748,  0.0154, -0.2793],\n",
      "        [ 0.0442,  0.2930, -0.1133,  ...,  0.0571, -0.2695,  0.1094]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.8.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([1.1562, 1.1250, 1.1016, 1.1328, 1.2344, 1.2656, 0.3027, 1.1875, 1.8516,\n",
      "        1.2344, 1.1172, 1.1406, 1.1797, 1.2422, 1.1797, 1.1328, 1.1641, 1.1641,\n",
      "        0.8945, 1.1797, 1.2109, 1.2266, 2.7812, 1.1406, 1.2031, 1.2109, 1.1797,\n",
      "        1.1172, 1.2344, 1.1484, 1.1719, 1.1719, 1.1484, 1.1484, 1.1172, 1.1797,\n",
      "        1.2969, 1.1719, 1.1562, 1.8906, 1.1172, 1.2266, 0.9688, 1.2109, 2.8281,\n",
      "        1.2344, 1.0938, 1.2188, 1.1562, 0.5156, 1.1484, 1.1250, 1.4297, 1.1641,\n",
      "        1.1797, 1.1719, 2.2656, 1.1719, 1.2109, 1.2031, 1.1953, 1.2031, 1.2266,\n",
      "        1.1953, 1.2500, 1.2188, 1.1875, 1.2109, 1.2109, 1.1719, 1.2344, 1.1875,\n",
      "        1.1719, 1.2344, 1.1562, 1.2422, 1.3047, 1.1641, 1.1797, 1.2422, 1.1406,\n",
      "        1.1172, 1.1406, 1.1250, 1.1719, 1.2188, 1.1875, 1.1719, 1.1953, 1.1562,\n",
      "        1.2188, 1.1875, 1.1172, 1.2031, 1.2656, 1.2109, 1.1875, 1.1328, 1.1406,\n",
      "        1.1641, 1.1641, 1.1875, 0.9141, 1.1797, 1.1641, 1.4844, 1.2188, 1.2109,\n",
      "        1.1562, 1.9531, 1.1016, 1.1875, 1.1484, 1.1875, 1.1328, 1.1719, 1.1328,\n",
      "        1.1406, 1.1875, 1.1562, 1.1797, 1.1484, 1.2656, 1.1953, 0.9062, 1.2344,\n",
      "        1.1953, 1.1641, 1.1484, 1.2109, 1.2656, 1.1094, 1.1875, 1.1719, 1.1094,\n",
      "        1.0312, 1.2812, 1.1641, 1.2344, 1.0938, 1.0938, 1.1875, 1.0625, 1.2734,\n",
      "        1.0234, 1.2656, 1.1875, 1.0625, 1.1562, 1.1953, 1.1953, 1.1328, 1.1094,\n",
      "        1.1797, 1.2266, 1.2109, 1.1719, 1.2578, 1.1562, 1.1641, 1.1797, 1.2344,\n",
      "        1.0312, 3.5312, 1.2109, 1.0625, 1.1562, 1.1562, 1.0391, 0.9375, 1.2344,\n",
      "        1.1250, 1.2109, 1.1719, 1.1875, 1.0703, 1.1953, 0.4297, 1.1172, 0.4395,\n",
      "        1.2266, 1.1953, 1.2109, 1.1797, 1.1172, 1.1953, 1.1484, 1.0781, 1.1875,\n",
      "        1.2109, 1.1562, 1.1641, 1.2188, 1.1328, 1.1797, 0.4219, 1.2188, 1.2109,\n",
      "        1.2109, 1.2109, 1.1875, 1.1562, 1.1406, 1.2656, 1.1250, 1.1406, 1.1797,\n",
      "        1.1094, 1.1875, 1.1562, 1.2422, 1.1328, 1.1641, 1.2656, 1.1562, 1.1562,\n",
      "        1.1484, 1.1797, 1.1719, 1.6094, 1.0703, 1.1953, 1.1250, 1.0938, 1.2500,\n",
      "        1.1172, 1.1641, 0.8711, 1.2109, 1.1328, 1.1016, 1.1719, 1.1016, 1.2109,\n",
      "        1.0938, 0.9922, 1.2109, 1.1719, 1.1328, 0.8906, 1.2344, 1.1953, 1.2188,\n",
      "        1.1484, 1.1406, 1.1719, 1.1875, 1.1719, 1.1797, 1.1875, 2.6719, 1.1953,\n",
      "        1.1719, 1.2812, 1.1797, 1.1797, 1.1641, 1.0312, 0.5977, 1.1406, 1.0547,\n",
      "        0.6172, 1.1797, 1.1562, 1.2031, 1.1719, 1.2031, 1.1719, 1.1719, 1.1953,\n",
      "        1.0859, 1.1797, 1.3984, 1.1953, 1.1797, 1.2422, 1.1719, 0.9297, 1.1953,\n",
      "        1.0859, 1.1719, 1.2344, 1.2734, 1.2422, 1.1797, 1.0703, 1.1406, 1.2109,\n",
      "        1.0391, 1.7109, 1.1719, 1.1719, 1.1953, 1.1875, 1.2266, 1.3125, 1.1016,\n",
      "        1.2188, 1.1953, 1.2266, 1.2188, 1.1641, 1.0547, 1.1016, 1.1172, 1.1328,\n",
      "        1.1250, 1.1562, 1.1484, 1.1641, 1.1484, 1.1875, 1.1953, 0.9727, 1.2266,\n",
      "        1.1406, 1.1406, 1.1641, 1.2734, 1.1484, 1.1250, 1.1406, 1.1016, 1.2031,\n",
      "        1.1562, 1.1797, 1.2422, 1.1953, 1.1484, 1.1406, 1.1953, 1.2266, 1.1328,\n",
      "        1.1016, 1.1953, 1.1797, 1.1328, 1.1328, 1.1406, 1.1797, 1.1719, 1.1797,\n",
      "        1.2188, 1.2891, 1.1719, 1.2109, 1.0391, 1.1562, 1.0391, 1.8828, 1.1719,\n",
      "        1.1953, 1.2891, 1.1641, 1.1406, 1.1250, 1.1250, 1.1484, 1.2422, 1.1719,\n",
      "        1.1953, 1.1719, 1.1094, 1.1250, 1.2031, 1.2109, 0.7344, 1.3125, 1.2031,\n",
      "        1.2500, 1.3359, 1.1719, 0.4297, 1.1250, 1.2578, 1.2188, 1.0469, 1.1328,\n",
      "        0.9492, 1.2188, 1.1953, 1.1406, 1.1953, 1.1328, 1.1797, 1.0781, 1.2656,\n",
      "        0.7852, 1.0859, 1.2422, 1.2344, 2.5000, 1.1328, 1.1562, 1.2109, 1.2812,\n",
      "        1.2344, 1.2031, 1.1484, 1.2266, 1.1328, 0.6992, 1.2266, 1.2031, 1.1562,\n",
      "        1.1875, 1.3438, 1.1328, 1.1328, 1.1875, 1.1797, 1.1797, 1.2812, 1.1797,\n",
      "        1.1797, 1.1250, 1.1875, 1.1875, 0.5039, 1.3984, 1.1719, 1.1953, 1.2109,\n",
      "        0.4648, 1.2188, 1.1797, 1.1094, 1.2031, 1.2344, 1.0859, 1.2109, 1.2344,\n",
      "        0.9531, 1.1797, 1.2500, 1.1797, 1.2266, 1.1328, 1.0859, 1.1484, 1.2578,\n",
      "        1.0547, 1.2734, 0.9844, 1.1328, 1.1719, 1.1562, 1.1953, 1.1172, 1.1172,\n",
      "        1.1875, 1.1641, 1.1641, 1.1406, 1.3438, 1.1719, 1.1328, 1.1328, 1.1875,\n",
      "        1.1484, 1.1328, 1.1250, 1.1641, 1.1562, 1.1406, 1.1641, 1.0781, 1.1328,\n",
      "        1.1328, 1.2188, 1.1562, 1.1797, 1.0469, 1.1875, 1.1953, 1.2031, 1.2266,\n",
      "        1.1719, 1.0938, 1.0938, 1.1797, 1.8984, 1.2656, 1.1953, 1.1641, 1.2031,\n",
      "        1.1875, 1.2969, 1.1719, 1.1250, 1.2109, 1.2578, 1.6172, 1.1875, 1.1484,\n",
      "        1.1875, 1.1953, 1.1562, 1.1797, 1.1719, 1.1797, 1.1719, 1.2188, 1.9688,\n",
      "        1.2734, 1.1875, 1.2109, 1.1094, 1.1953, 1.1797, 1.1406, 1.1953, 1.7812,\n",
      "        1.1328, 1.2031, 1.2266, 1.1328, 1.1875, 1.0234, 1.2266, 1.1797, 1.2422,\n",
      "        1.1406, 1.2109, 1.1953, 1.8438, 1.1250, 1.1406, 1.2031, 1.1719, 1.0703,\n",
      "        1.1328, 1.1719, 1.1484, 1.2266, 1.1094, 1.1484, 1.1797, 1.2031, 1.1719,\n",
      "        1.1250, 1.1250, 1.1250, 1.1172, 1.2031, 1.2109, 1.1484, 1.1641, 1.1562,\n",
      "        0.3906, 1.1406, 1.1562, 1.1641, 1.2969, 1.1797, 1.1641, 1.1406, 1.1875,\n",
      "        1.1484, 1.0234, 1.2344, 1.1484, 1.2031, 1.2422, 1.1953, 1.1797, 0.4434,\n",
      "        1.1406, 1.1875, 1.2109, 1.1172, 0.5234, 1.1641, 1.1875, 1.1562, 1.1094,\n",
      "        1.1719, 1.1094, 1.1016, 0.9961, 1.1875, 1.2344, 1.1953, 1.1406, 1.1094,\n",
      "        1.1562, 1.1875, 1.1562, 1.2109, 1.1484, 1.2031, 1.1719, 1.1406, 1.1875,\n",
      "        1.1172, 1.1797, 1.1250, 1.2031, 1.1094, 1.1484, 1.2188, 1.1719, 1.1328,\n",
      "        0.8359, 1.1797, 1.1719, 1.1094, 1.1250, 1.2266, 1.2266, 1.1641, 1.1719,\n",
      "        1.1953, 1.1484, 1.1484, 1.1953, 1.2031, 1.0234, 1.1797, 1.1562, 1.2344,\n",
      "        1.2031, 1.2109, 1.1094, 1.1094, 1.1172, 1.1641, 1.1328, 1.0625, 1.0156,\n",
      "        1.1719, 1.2266, 1.2188, 1.1406, 1.1953, 0.6211, 1.1797, 1.1328, 1.2109,\n",
      "        1.1172, 1.1094, 1.1953, 1.1562, 1.2578, 1.2188, 1.0703, 1.0391, 1.1797,\n",
      "        1.1562, 1.1875, 1.1562, 1.1797, 1.1719, 1.1953, 1.0469, 1.0547, 1.1641,\n",
      "        1.1172, 0.9258, 1.1953, 0.7812, 0.4824, 1.1406, 1.2344, 1.1484, 1.1953,\n",
      "        1.1406, 1.1641, 1.1328, 1.2031, 1.1953, 1.4688, 1.1875, 1.1875, 1.0703,\n",
      "        1.1953, 1.1250, 1.1641, 1.1641, 1.1953, 1.1250, 1.1328, 1.1641, 1.1719,\n",
      "        1.1406, 1.5156, 1.1641, 1.2109, 1.2344, 1.1094, 1.2266, 1.1484, 1.0781,\n",
      "        1.2266, 1.2031, 1.1172, 1.1875, 1.1875, 1.1953, 1.1484, 1.2891, 1.1641,\n",
      "        1.1016, 1.1719, 1.1953, 1.1797, 1.2031, 1.2734, 1.1250, 1.1641, 1.8906,\n",
      "        1.2109, 1.1641, 1.1719, 0.5469, 1.1875, 1.1875, 1.1953, 1.1406, 1.0078,\n",
      "        1.1328, 1.2422, 1.1953, 1.2188, 1.1953, 1.0859, 1.0703, 1.1719, 1.1953,\n",
      "        1.1719, 1.1406, 1.2344, 1.1641, 1.0078, 1.1484, 1.2031, 1.2266, 1.1641,\n",
      "        1.1719, 1.1406, 1.1875, 1.2188, 1.0781, 1.1172, 1.1328, 1.2578, 1.0547,\n",
      "        1.2109, 1.1719, 1.2734, 1.2422, 1.2031, 1.2031, 1.1875, 1.1328, 1.1328,\n",
      "        1.1641, 1.1406, 1.0781, 1.1016, 1.0391, 0.9883, 1.1250, 1.1641, 1.1172,\n",
      "        1.0547, 1.2109, 1.1797], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.9.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0265, -0.0525,  0.0289,  ..., -0.0245, -0.0061,  0.0015],\n",
      "        [-0.0369,  0.0018,  0.0062,  ..., -0.0229, -0.0315, -0.0112],\n",
      "        [ 0.0537,  0.0364,  0.0713,  ...,  0.0581,  0.0515,  0.0040],\n",
      "        ...,\n",
      "        [ 0.0199,  0.0146, -0.0045,  ..., -0.0150,  0.0289, -0.0079],\n",
      "        [ 0.0439,  0.0001, -0.0041,  ..., -0.0081,  0.0242,  0.0095],\n",
      "        [-0.0075, -0.0200,  0.0347,  ..., -0.0088,  0.0092, -0.0123]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.9.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.3457, -0.4922,  0.4082,  ..., -0.1367, -0.1621, -0.4277],\n",
      "        [-0.1396,  0.1045, -0.1875,  ..., -0.0107,  0.0869, -0.0142],\n",
      "        [-0.0204, -0.3066,  0.4844,  ..., -0.2334,  0.1216, -0.0693],\n",
      "        ...,\n",
      "        [ 0.2432, -0.3379, -0.0879,  ...,  0.2227,  0.0801, -0.1465],\n",
      "        [ 0.1504, -0.0173, -0.1396,  ..., -0.2227, -0.0664, -0.2432],\n",
      "        [ 0.0962, -0.3477,  0.0781,  ..., -0.0183, -0.0496,  0.1484]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.9.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.8828, -0.5430,  0.1641,  ..., -0.2734, -0.3281, -0.8750],\n",
      "        [-0.1299, -0.0547, -0.5742,  ..., -0.8945, -0.1523, -0.3223],\n",
      "        [-1.3125,  0.3242,  0.0090,  ..., -0.0262,  0.3086, -0.5664],\n",
      "        ...,\n",
      "        [-0.5938, -0.3867,  1.2812,  ...,  0.0757, -0.2520, -0.3301],\n",
      "        [ 0.1631, -0.1250,  0.5586,  ...,  0.4238, -0.1582, -0.5234],\n",
      "        [-0.0091, -0.2910,  0.1396,  ...,  2.5469, -1.6406,  0.2451]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.9.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-1.1406,  0.8047,  0.3887,  ...,  2.2500, -0.1406, -0.1396],\n",
      "        [ 1.1953, -0.0371,  0.4160,  ..., -1.6562,  0.6562,  1.3516],\n",
      "        [-0.2852, -0.4375, -0.3008,  ..., -1.8359,  0.0928,  0.9766],\n",
      "        ...,\n",
      "        [ 0.7188,  1.6875, -0.1553,  ..., -0.9688, -0.6562, -0.4805],\n",
      "        [ 0.5547, -0.2363, -0.3711,  ...,  0.7344,  0.5234,  1.0938],\n",
      "        [ 1.4844, -0.2891,  1.0078,  ...,  0.9531, -1.5312,  0.5703]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.9.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 0.1924,  0.1787,  0.1807,  0.2002,  0.1904,  0.1855,  0.0405,  0.1982,\n",
      "         0.1270,  0.2021,  0.2041,  0.1943,  0.1475,  0.1660,  0.1836,  0.1992,\n",
      "         0.1787,  0.1836,  0.1709,  0.1914,  0.1729,  0.1934,  0.0620,  0.1836,\n",
      "         0.1973,  0.1885,  0.1846,  0.1748,  0.1807,  0.1875,  0.1797,  0.1885,\n",
      "         0.1904,  0.1885,  0.1934,  0.2002,  0.1660,  0.1836,  0.1846,  0.1738,\n",
      "         0.1768,  0.1797,  0.1738,  0.1895,  0.1069,  0.1875,  0.1592,  0.1875,\n",
      "         0.1992,  0.0603,  0.1973,  0.1953,  0.1592,  0.1885,  0.0003,  0.1729,\n",
      "         0.1426,  0.1865,  0.2178,  0.1953,  0.1924,  0.1787,  0.1943,  0.1914,\n",
      "         0.1914,  0.0557,  0.1904,  0.1826,  0.1953,  0.1973,  0.1875,  0.1650,\n",
      "         0.1885,  0.1914,  0.1846,  0.1875,  0.1572,  0.1924,  0.1738,  0.2139,\n",
      "         0.1768,  0.1934,  0.2031,  0.2061,  0.1855,  0.1963,  0.1865,  0.2002,\n",
      "         0.2031,  0.1855,  0.1973,  0.1738,  0.1816,  0.1699,  0.1826,  0.1826,\n",
      "         0.2012,  0.1973,  0.1953,  0.1719,  0.1875,  0.2002,  0.1357,  0.1816,\n",
      "         0.1904,  0.1582,  0.1934,  0.1855,  0.1855,  0.1191,  0.1875,  0.2080,\n",
      "         0.1885,  0.1738,  0.1914,  0.1865,  0.1904,  0.1758,  0.1650,  0.1924,\n",
      "         0.1895,  0.2012,  0.1885,  0.1895,  0.1416,  0.1914,  0.2070,  0.1865,\n",
      "         0.1807,  0.1934,  0.1396,  0.2021,  0.1924,  0.1650,  0.1836,  0.1533,\n",
      "         0.2041,  0.1855,  0.1943,  0.1836,  0.1797,  0.2012,  0.1729,  0.1855,\n",
      "         0.1758,  0.2002,  0.1963,  0.1924,  0.1826,  0.1885,  0.1953,  0.1924,\n",
      "         0.1826,  0.1885,  0.1924,  0.2100,  0.1816,  0.1904,  0.1729,  0.1816,\n",
      "         0.1875,  0.1846,  0.1494, -0.0928,  0.1807,  0.1660,  0.1934,  0.1758,\n",
      "         0.1875,  0.1719,  0.1982,  0.1992,  0.2031,  0.1416,  0.1973,  0.1738,\n",
      "         0.1943,  0.0811,  0.1719,  0.0347,  0.1924,  0.1807,  0.1875,  0.1963,\n",
      "         0.1943,  0.2041,  0.1787,  0.1758,  0.1904,  0.1924,  0.1865,  0.2031,\n",
      "         0.1709,  0.2012,  0.1816,  0.0635,  0.1895,  0.1982,  0.1953,  0.1592,\n",
      "         0.1826,  0.1982,  0.1836,  0.1904,  0.1934,  0.1895,  0.1914,  0.1895,\n",
      "         0.1797,  0.1982,  0.1875,  0.1904,  0.1846,  0.1807,  0.1934,  0.1982,\n",
      "         0.1885,  0.1895,  0.1836,  0.1533,  0.1289,  0.1924,  0.1807,  0.1982,\n",
      "         0.1572,  0.1963,  0.2002,  0.2031,  0.1924,  0.2090,  0.1689,  0.1836,\n",
      "         0.1562,  0.1914,  0.1855,  0.1475,  0.1602,  0.1875,  0.1855,  0.1484,\n",
      "         0.1875,  0.2070,  0.1924,  0.2051,  0.1787,  0.1846,  0.1973,  0.2002,\n",
      "         0.1738,  0.2002,  0.1514,  0.1699,  0.1738,  0.1826,  0.1826,  0.1865,\n",
      "         0.1816,  0.1641,  0.1025,  0.1943,  0.1885,  0.1660,  0.1816,  0.1982,\n",
      "         0.1885,  0.1885,  0.1904,  0.1855,  0.1797,  0.1875,  0.1001,  0.2041,\n",
      "         0.1572,  0.0371,  0.1924,  0.1953,  0.1992,  0.1475,  0.1982,  0.1895,\n",
      "         0.2002,  0.1836,  0.1914,  0.1963,  0.2129,  0.1436,  0.1885,  0.1865,\n",
      "         0.1670,  0.1523,  0.1865,  0.1992,  0.1807,  0.1943,  0.1934,  0.1895,\n",
      "         0.1816,  0.1982,  0.1914,  0.1904,  0.1992,  0.1924,  0.1982,  0.1904,\n",
      "         0.1943,  0.1758,  0.1953,  0.1992,  0.2051,  0.1768,  0.1914,  0.1973,\n",
      "         0.1865,  0.1494,  0.1934,  0.1826,  0.1592,  0.1846,  0.1846,  0.1875,\n",
      "         0.1846,  0.1846,  0.1797,  0.1934,  0.1855,  0.1855,  0.1836,  0.2002,\n",
      "         0.1982,  0.1934,  0.1934,  0.1475,  0.1787,  0.1768,  0.1914,  0.1895,\n",
      "         0.1777,  0.1865,  0.1836,  0.1963,  0.1992,  0.2168,  0.2012,  0.1826,\n",
      "         0.1973,  0.1885,  0.1729,  0.1943,  0.1621,  0.1445,  0.1953,  0.1855,\n",
      "         0.1982,  0.1904,  0.1738,  0.1982,  0.1885,  0.1797,  0.1973,  0.1572,\n",
      "         0.1826,  0.1768,  0.1816,  0.1582,  0.1943,  0.1777,  0.0952,  0.1787,\n",
      "         0.1807,  0.1816,  0.1729,  0.2051,  0.0806,  0.1699,  0.1943,  0.1855,\n",
      "         0.1504,  0.1875,  0.1875,  0.1504,  0.1914,  0.1934,  0.1963,  0.1816,\n",
      "         0.1846,  0.0967,  0.1836,  0.1250,  0.1729,  0.1904,  0.1855,  0.0786,\n",
      "         0.1816,  0.1992,  0.2021,  0.1787,  0.1963,  0.1914,  0.1885,  0.1924,\n",
      "         0.1836,  0.0535,  0.1885,  0.2129,  0.1807,  0.1875,  0.1914,  0.1816,\n",
      "         0.1895,  0.1758,  0.1885,  0.1855,  0.1816,  0.1875,  0.1846,  0.1768,\n",
      "         0.1758,  0.1777,  0.0791,  0.1953,  0.1680,  0.1943,  0.1787,  0.0737,\n",
      "         0.1875,  0.1992,  0.1875,  0.1807,  0.1963,  0.1875,  0.1963,  0.1982,\n",
      "         0.1318,  0.1768,  0.1670,  0.1865,  0.1914,  0.1836,  0.1787,  0.1934,\n",
      "         0.1963,  0.1602,  0.1943,  0.1914,  0.1846,  0.1924,  0.1807,  0.2080,\n",
      "         0.1855,  0.2002,  0.1777,  0.1875,  0.1895,  0.1982,  0.2080,  0.1914,\n",
      "         0.1924,  0.1904,  0.1758,  0.1777,  0.1836,  0.1953,  0.1914,  0.1963,\n",
      "         0.1973,  0.1865,  0.1514,  0.1816,  0.1621,  0.1904,  0.1758,  0.1797,\n",
      "         0.1641,  0.1914,  0.1973,  0.2021,  0.2021,  0.1865,  0.0742,  0.1885,\n",
      "         0.1924,  0.1309,  0.1885,  0.1826,  0.2021,  0.1875,  0.1924,  0.1738,\n",
      "         0.1846,  0.2051,  0.2051,  0.1816,  0.1133,  0.2100,  0.1865,  0.1885,\n",
      "         0.1826,  0.1924,  0.1914,  0.1738,  0.1807,  0.1904,  0.1875,  0.0479,\n",
      "         0.1875,  0.1953,  0.2080,  0.1924,  0.1895,  0.2061,  0.2012,  0.1934,\n",
      "         0.1279,  0.2021,  0.1904,  0.1963,  0.1758,  0.1650,  0.1533,  0.2021,\n",
      "         0.1855,  0.1758,  0.1719,  0.1787,  0.1982,  0.1611,  0.2021,  0.1611,\n",
      "         0.1836,  0.2021,  0.1846,  0.1943,  0.1934,  0.2031,  0.1699,  0.1875,\n",
      "         0.1865,  0.1895,  0.1797,  0.1934,  0.1904,  0.1797,  0.1875,  0.1934,\n",
      "         0.1865,  0.1963,  0.1973,  0.1904,  0.1885,  0.0742,  0.1729,  0.1855,\n",
      "         0.1797,  0.2266,  0.1836,  0.2002,  0.1816,  0.1846,  0.1855,  0.1445,\n",
      "         0.1826,  0.1699,  0.1914,  0.1885,  0.1982,  0.2002,  0.0396,  0.1865,\n",
      "         0.1885,  0.1924,  0.1953,  0.1123,  0.1846,  0.1846,  0.1904,  0.1904,\n",
      "         0.2041,  0.1924,  0.1855,  0.1611,  0.1943,  0.1855,  0.2012,  0.1797,\n",
      "         0.1865,  0.1826,  0.1836,  0.1885,  0.1836,  0.1895,  0.1895,  0.1885,\n",
      "         0.1963,  0.2012,  0.1885,  0.1953,  0.1816,  0.1875,  0.1729,  0.1797,\n",
      "         0.1953,  0.2002,  0.1914,  0.1289,  0.2100,  0.1836,  0.1797,  0.1758,\n",
      "         0.1729,  0.1963,  0.1992,  0.1895,  0.1885,  0.1855,  0.1797,  0.2021,\n",
      "         0.1768,  0.2051,  0.1982,  0.1855,  0.2002,  0.1719,  0.1836,  0.1855,\n",
      "         0.1943,  0.1768,  0.1855,  0.1611,  0.1904,  0.1309,  0.2012,  0.1855,\n",
      "         0.1855,  0.2012,  0.1914,  0.1338,  0.1875,  0.1748,  0.2246,  0.1895,\n",
      "         0.1729,  0.2070,  0.1758,  0.1797,  0.2002,  0.1963,  0.1758,  0.1992,\n",
      "         0.1787,  0.1875,  0.1719,  0.1875,  0.1904,  0.1826,  0.1709,  0.1768,\n",
      "         0.1973,  0.1729,  0.1826,  0.1807,  0.1328,  0.0359,  0.1768,  0.1973,\n",
      "         0.1797,  0.1758,  0.1875,  0.2031,  0.2100,  0.2002,  0.1963,  0.1562,\n",
      "         0.1807,  0.1807,  0.2012,  0.1895,  0.1826,  0.1846,  0.1729,  0.2021,\n",
      "         0.1982,  0.1914,  0.1943,  0.1699,  0.1895,  0.1299,  0.1631,  0.2012,\n",
      "         0.1973,  0.2002,  0.1924,  0.1992,  0.1562,  0.1875,  0.2002,  0.1836,\n",
      "         0.2031,  0.1729,  0.1963,  0.1855,  0.1816,  0.1611,  0.1914,  0.1865,\n",
      "         0.1885,  0.1865,  0.1943,  0.1787,  0.2002,  0.1885,  0.0723,  0.2061,\n",
      "         0.1953,  0.1885,  0.0835,  0.1855,  0.1729,  0.1904,  0.1924,  0.1709,\n",
      "         0.1934,  0.1396,  0.1826,  0.1846,  0.1924,  0.1826,  0.1914,  0.1660,\n",
      "         0.1973,  0.1885,  0.1875,  0.1895,  0.1846,  0.1582,  0.1914,  0.1924,\n",
      "         0.1943,  0.1982,  0.1943,  0.2002,  0.1904,  0.1836,  0.1982,  0.2021,\n",
      "         0.1992,  0.1934,  0.1924,  0.1914,  0.2080,  0.2051,  0.1846,  0.1904,\n",
      "         0.1973,  0.1943,  0.1885,  0.1885,  0.1973,  0.1855,  0.2109,  0.2178,\n",
      "         0.1777,  0.1846,  0.1914,  0.1758,  0.2041,  0.1836,  0.1826,  0.2012],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.9.layer.1.DenseReluDense.wi.weight\n",
      "Parameter containing:\n",
      "tensor([[-3.0664e-01, -2.1875e-01,  3.3398e-01,  ...,  6.7871e-02,\n",
      "         -4.7852e-01, -3.7109e-01],\n",
      "        [ 1.1250e+00, -1.8652e-01, -8.6719e-01,  ...,  8.2812e-01,\n",
      "         -9.0820e-02,  1.8047e+00],\n",
      "        [ 8.5156e-01, -1.8945e-01, -2.0020e-02,  ...,  4.4922e-01,\n",
      "          2.8711e-01, -4.7363e-02],\n",
      "        ...,\n",
      "        [ 1.4688e+00, -1.2305e-01,  3.7109e-01,  ...,  5.5859e-01,\n",
      "         -3.4570e-01,  7.3438e-01],\n",
      "        [ 1.7166e-03, -8.2422e-01, -3.5547e-01,  ..., -6.5625e-01,\n",
      "         -2.3242e-01, -1.1572e-01],\n",
      "        [-4.4336e-01, -3.1055e-01, -1.4355e-01,  ..., -6.4844e-01,\n",
      "          3.7598e-02, -3.2422e-01]], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.9.layer.1.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.1973,  0.3633, -0.4062,  ...,  0.4902, -0.5547,  0.0938],\n",
      "        [-0.3848, -0.3203, -0.1309,  ...,  0.1396, -0.4258, -0.2969],\n",
      "        [ 0.1064,  0.1309, -0.3301,  ...,  0.0571, -0.3926,  0.1660],\n",
      "        ...,\n",
      "        [ 0.3730,  0.2852, -0.7188,  ..., -0.0369, -0.0850,  0.2295],\n",
      "        [ 0.0549, -0.1992, -0.0312,  ..., -0.1206,  0.3145,  0.1543],\n",
      "        [-0.1108,  0.0889,  0.2246,  ..., -0.1021, -0.2734,  0.0947]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.9.layer.1.layer_norm.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([1.3047, 1.1875, 1.1641, 1.2500, 1.3906, 1.3281, 0.3242, 1.3281, 1.5234,\n",
      "        1.4062, 1.1719, 1.1562, 1.2578, 1.2812, 1.3438, 1.3281, 1.2578, 1.2734,\n",
      "        0.8672, 1.3438, 1.3750, 1.3516, 3.5781, 1.2656, 1.4062, 1.2891, 1.3594,\n",
      "        1.2109, 1.3594, 1.3203, 1.3516, 1.2891, 1.3516, 1.3203, 1.2188, 1.3516,\n",
      "        1.3828, 1.2656, 1.3359, 1.7578, 1.2109, 1.3203, 0.9062, 1.3359, 3.1875,\n",
      "        1.3984, 1.1250, 1.3203, 1.3125, 0.5117, 1.2422, 1.3125, 1.4922, 1.3281,\n",
      "        1.3047, 1.2500, 2.9062, 1.3125, 1.4219, 1.4297, 1.3125, 1.3984, 1.3594,\n",
      "        1.3125, 1.3906, 1.2266, 1.3516, 1.3281, 1.3281, 1.3047, 1.3438, 1.0000,\n",
      "        1.3203, 1.3672, 1.3438, 1.3984, 1.3203, 1.2734, 1.3203, 1.3672, 1.2812,\n",
      "        1.2656, 1.3281, 1.2656, 1.3203, 1.3281, 1.2266, 1.3281, 1.3203, 1.3281,\n",
      "        1.3906, 1.3438, 1.2969, 1.3516, 1.4844, 1.3594, 1.3594, 1.3203, 1.3516,\n",
      "        1.3828, 1.2578, 1.2734, 0.7852, 1.3047, 1.2969, 1.5859, 1.3828, 1.4141,\n",
      "        1.3438, 2.2344, 1.3047, 1.3906, 1.2578, 1.3438, 1.3125, 1.3906, 1.2891,\n",
      "        1.3125, 1.1328, 1.3516, 1.4375, 1.2734, 1.3516, 1.3438, 0.8945, 1.3047,\n",
      "        1.3984, 1.2500, 1.2891, 1.3359, 1.3047, 1.2656, 1.3828, 1.4062, 1.2969,\n",
      "        1.0391, 1.4766, 1.3516, 1.2969, 1.1719, 1.2344, 1.2266, 1.1172, 1.3984,\n",
      "        1.2188, 1.3594, 1.3516, 1.2734, 1.3281, 1.3125, 1.3359, 1.2422, 1.2344,\n",
      "        1.3125, 1.3516, 1.4062, 1.2891, 1.4219, 1.2344, 1.2578, 1.4062, 1.3359,\n",
      "        1.2109, 4.2812, 1.4062, 1.1875, 1.3203, 1.2188, 1.1641, 0.9922, 1.2109,\n",
      "        1.3125, 1.3672, 1.1797, 1.3672, 1.2812, 1.3281, 0.4316, 1.1484, 0.4746,\n",
      "        1.4141, 1.3828, 1.3047, 1.3906, 1.2969, 1.3203, 1.3047, 1.1562, 1.3125,\n",
      "        1.2969, 1.3359, 1.4375, 1.4688, 1.2891, 1.3359, 0.5039, 1.2969, 1.3203,\n",
      "        1.3047, 1.3672, 1.3906, 1.2812, 1.3438, 1.3203, 1.2812, 1.2500, 1.3438,\n",
      "        1.2344, 1.3750, 1.3438, 1.3281, 1.2500, 1.2266, 1.3984, 1.3438, 1.3047,\n",
      "        1.2422, 1.2969, 1.2344, 1.4844, 1.1484, 1.3125, 1.3047, 1.2188, 1.3125,\n",
      "        1.3359, 1.2812, 0.9531, 1.3281, 1.3047, 1.2734, 1.3047, 1.2812, 1.4375,\n",
      "        1.2891, 1.2188, 1.3281, 1.3438, 1.1328, 0.8594, 1.2891, 1.3359, 1.3906,\n",
      "        1.3359, 1.2734, 1.2734, 1.3125, 1.3438, 1.4062, 1.3203, 2.7500, 1.3203,\n",
      "        1.3984, 1.4453, 1.3750, 1.3828, 1.3984, 1.1953, 0.6211, 1.2891, 1.1641,\n",
      "        0.9102, 1.3281, 1.4062, 1.3828, 1.3594, 1.3281, 1.2656, 1.3594, 1.3125,\n",
      "        1.3594, 1.2969, 1.3906, 1.1172, 1.2109, 1.3438, 1.3281, 0.9219, 1.3438,\n",
      "        1.2578, 1.2969, 1.2969, 1.3516, 1.3359, 1.3125, 1.0625, 1.4141, 1.4141,\n",
      "        1.1641, 1.7734, 1.4609, 1.3281, 1.3438, 1.3672, 1.4062, 1.5000, 1.1875,\n",
      "        1.3906, 1.3594, 1.2969, 1.3203, 1.3047, 1.0938, 1.2109, 1.2422, 1.3672,\n",
      "        1.2734, 1.2969, 1.2656, 1.2969, 1.3750, 1.3203, 1.3516, 1.2031, 1.3594,\n",
      "        1.2891, 1.2812, 1.2969, 1.4219, 1.2969, 1.3672, 1.2266, 1.2734, 1.4688,\n",
      "        1.2422, 1.3906, 1.0781, 1.2500, 1.2031, 1.3828, 1.3750, 1.1172, 1.2578,\n",
      "        1.2578, 1.3516, 1.3281, 1.3281, 1.2812, 1.2891, 1.3281, 1.2891, 1.3359,\n",
      "        1.3984, 1.4609, 1.2891, 1.3203, 1.0547, 1.2578, 1.0547, 2.0781, 1.1953,\n",
      "        1.3281, 1.3984, 1.3203, 1.1641, 1.3438, 1.2656, 1.2344, 1.4062, 1.1406,\n",
      "        1.2969, 1.1875, 1.1641, 1.3281, 1.3906, 1.2969, 0.7070, 1.3828, 1.3672,\n",
      "        1.3594, 1.4922, 1.2734, 0.5117, 1.2812, 1.2812, 1.3906, 1.1016, 1.3125,\n",
      "        0.9766, 1.2344, 1.3281, 1.2344, 1.2500, 1.3281, 1.3281, 1.0781, 1.3359,\n",
      "        0.7383, 1.3516, 1.3125, 1.3516, 2.9062, 1.2812, 1.2422, 1.2578, 1.3672,\n",
      "        1.2969, 1.3750, 1.2656, 1.3438, 1.3047, 0.9492, 1.3594, 1.3359, 1.3359,\n",
      "        1.3750, 1.3672, 1.3750, 1.3672, 1.3516, 1.3047, 1.3594, 1.3750, 1.3281,\n",
      "        1.3672, 1.2578, 1.3984, 1.3281, 0.6016, 1.4766, 1.1562, 1.3750, 1.3906,\n",
      "        0.5039, 1.3516, 1.3984, 1.2422, 1.4141, 1.3516, 1.2031, 1.3828, 1.4453,\n",
      "        0.8047, 1.3359, 1.2422, 1.3516, 1.3281, 1.3438, 1.0156, 1.2344, 1.3203,\n",
      "        1.1328, 1.3281, 1.0547, 1.2969, 1.2500, 1.3516, 1.3125, 1.1406, 1.2812,\n",
      "        1.2578, 1.3359, 1.3281, 1.3359, 1.4297, 1.2422, 1.2891, 1.2812, 1.2656,\n",
      "        1.3828, 1.3750, 1.3359, 1.3516, 1.2578, 1.2656, 1.3516, 1.2500, 1.3203,\n",
      "        1.3125, 1.3672, 1.3750, 1.3594, 1.2266, 1.3438, 1.3281, 1.3906, 1.3906,\n",
      "        1.3594, 0.9258, 1.2188, 1.2188, 1.9609, 1.4141, 1.4141, 1.2578, 1.3672,\n",
      "        1.3594, 1.4062, 1.3125, 1.3281, 1.3047, 1.3906, 1.7734, 1.4141, 1.3281,\n",
      "        1.2812, 1.2812, 1.2578, 1.2812, 1.3984, 1.3438, 1.2891, 1.3438, 2.1094,\n",
      "        1.4531, 1.2578, 1.3281, 1.1328, 1.4141, 1.2891, 1.2578, 1.3594, 2.0625,\n",
      "        1.4062, 1.2891, 1.3203, 1.2812, 1.2969, 0.9453, 1.3672, 1.3906, 1.3594,\n",
      "        1.3984, 1.2891, 1.2891, 2.0156, 1.2109, 1.0391, 1.3203, 1.2891, 1.1562,\n",
      "        1.3203, 1.3047, 1.2656, 1.2734, 1.2188, 1.2188, 1.3047, 1.2188, 1.2500,\n",
      "        1.2656, 1.2500, 1.2266, 1.2422, 1.3984, 1.3828, 1.2812, 1.3359, 1.3750,\n",
      "        0.4395, 1.1719, 1.2578, 1.4219, 1.4219, 1.3438, 1.3125, 1.2344, 1.3203,\n",
      "        1.2500, 0.9961, 1.3750, 1.3594, 1.2578, 1.2656, 1.3203, 1.2969, 0.5273,\n",
      "        1.2734, 1.2812, 1.3984, 1.2734, 0.5781, 1.3594, 1.3359, 1.3203, 1.2422,\n",
      "        1.3203, 1.2344, 1.3125, 0.9922, 1.3594, 1.4922, 1.3750, 1.3594, 1.3672,\n",
      "        1.3672, 1.3594, 1.3281, 1.3672, 1.3672, 1.3594, 1.2188, 1.3203, 1.3672,\n",
      "        1.2578, 1.3047, 1.2578, 1.3750, 1.1719, 1.2969, 1.3359, 1.2969, 1.3047,\n",
      "        0.9688, 1.2891, 1.3516, 1.3281, 1.2188, 1.2656, 1.3594, 1.2734, 1.3906,\n",
      "        1.3047, 1.3672, 1.3125, 1.2734, 1.3984, 1.1953, 1.3516, 1.3047, 1.3125,\n",
      "        1.3516, 1.3594, 1.1719, 1.2891, 1.3047, 1.2734, 1.1797, 1.2969, 1.0625,\n",
      "        1.2891, 1.3516, 1.3281, 1.2891, 1.3516, 0.5977, 1.3594, 1.2891, 1.2656,\n",
      "        1.2578, 1.1719, 1.3359, 1.3047, 1.4375, 1.3203, 1.1797, 1.1406, 1.3438,\n",
      "        1.3359, 1.3594, 1.2891, 1.3594, 1.2969, 1.3203, 1.1484, 0.9727, 1.2812,\n",
      "        1.1562, 0.9531, 1.2500, 0.7695, 0.5625, 1.2812, 1.2969, 1.2891, 1.2734,\n",
      "        1.3281, 1.3672, 1.2266, 1.3750, 1.3672, 1.5156, 1.3438, 1.3438, 1.2812,\n",
      "        1.3984, 1.3203, 1.2969, 1.3516, 1.3594, 1.3203, 1.3359, 1.2812, 1.2656,\n",
      "        1.2656, 1.6250, 1.2891, 1.3203, 1.4062, 1.2422, 1.4141, 1.2891, 0.9805,\n",
      "        1.3281, 1.3750, 1.3203, 1.2812, 1.3594, 1.3125, 1.2188, 1.3516, 1.3750,\n",
      "        1.3516, 1.3516, 1.3516, 1.4219, 1.2969, 1.3125, 1.2031, 1.2969, 1.9141,\n",
      "        1.3750, 1.2891, 1.3750, 0.5391, 1.2578, 1.3281, 1.2500, 1.2656, 0.9141,\n",
      "        1.3438, 1.3047, 1.2812, 1.3906, 1.2734, 1.2031, 1.2188, 1.3438, 1.4375,\n",
      "        1.2344, 1.1797, 1.4062, 1.3281, 1.2188, 1.3594, 1.3750, 1.3828, 1.3203,\n",
      "        1.2969, 1.3594, 1.3047, 1.2812, 1.1953, 1.2188, 1.2969, 1.2422, 1.1875,\n",
      "        1.4219, 1.2734, 1.2578, 1.3203, 1.3594, 1.3203, 1.2500, 1.2578, 1.3516,\n",
      "        1.2344, 1.3438, 1.2734, 1.2891, 1.2422, 1.2344, 1.3203, 1.4219, 1.1562,\n",
      "        1.2266, 1.3516, 1.3281], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.10.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0325, -0.0113,  0.0215,  ...,  0.1099,  0.0513,  0.0142],\n",
      "        [-0.0216, -0.0015, -0.0298,  ...,  0.0454,  0.0356,  0.0139],\n",
      "        [ 0.0417,  0.0049, -0.0149,  ...,  0.0405,  0.0728,  0.0144],\n",
      "        ...,\n",
      "        [-0.0152, -0.0483,  0.0510,  ...,  0.0223, -0.0103,  0.0121],\n",
      "        [-0.0219,  0.0728,  0.0002,  ...,  0.0400, -0.0211, -0.0153],\n",
      "        [ 0.0080,  0.0069,  0.0645,  ...,  0.0051, -0.0243, -0.0251]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.10.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0488,  0.0378,  0.1152,  ..., -0.1162, -0.3535, -0.0288],\n",
      "        [ 0.1113, -0.3184, -0.0327,  ...,  0.2314, -0.4492,  0.1475],\n",
      "        [ 0.0947,  0.4414,  0.4629,  ..., -0.2832, -0.2314, -0.1079],\n",
      "        ...,\n",
      "        [ 0.0559, -0.3633,  0.2969,  ..., -0.0096, -0.1807,  0.1152],\n",
      "        [-0.1729, -0.0918, -0.2949,  ...,  0.2090, -0.0067, -0.2500],\n",
      "        [ 0.1562, -0.4082,  0.2305,  ..., -0.0781,  0.1465, -0.0518]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.10.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.5898, -0.1660, -0.2197,  ..., -0.2754,  0.3906,  0.2773],\n",
      "        [-0.4902,  0.2520, -0.4844,  ..., -0.4922,  1.5000,  0.4629],\n",
      "        [-0.3418,  0.2002,  0.0383,  ...,  0.2910, -0.4238, -0.1279],\n",
      "        ...,\n",
      "        [ 0.7344,  0.9844,  0.3203,  ...,  0.8242, -0.4023, -0.1777],\n",
      "        [-0.4629, -0.4258,  0.5430,  ..., -0.2354,  0.3926,  1.1250],\n",
      "        [-0.6523,  0.1875,  0.2139,  ...,  0.2793, -1.5000,  0.3965]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.10.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.3184,  0.3750,  0.6250,  ..., -0.2061,  1.9766,  0.2656],\n",
      "        [ 0.5312,  1.1406,  0.6953,  ..., -0.8359,  0.2871, -0.0569],\n",
      "        [ 0.9141,  0.5938, -0.9102,  ..., -1.0078, -0.5898, -1.7109],\n",
      "        ...,\n",
      "        [ 0.0069,  1.7266, -0.0859,  ...,  0.2021, -0.2354, -0.7148],\n",
      "        [ 0.6172, -0.5469,  0.6211,  ...,  0.3203,  0.6875,  1.4453],\n",
      "        [-0.4160, -1.5938,  0.1826,  ..., -0.1885, -0.8320, -1.3125]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.10.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([0.2080, 0.1895, 0.1846, 0.2080, 0.2002, 0.1885, 0.0391, 0.2031, 0.0981,\n",
      "        0.1973, 0.1914, 0.2031, 0.1621, 0.1709, 0.1729, 0.2051, 0.1895, 0.2070,\n",
      "        0.1543, 0.2002, 0.1787, 0.1982, 0.0537, 0.1963, 0.2012, 0.2041, 0.1914,\n",
      "        0.1816, 0.1934, 0.1934, 0.1855, 0.2070, 0.2061, 0.1875, 0.1846, 0.1992,\n",
      "        0.1582, 0.2002, 0.1807, 0.1533, 0.1904, 0.1699, 0.1650, 0.2061, 0.1069,\n",
      "        0.1924, 0.1514, 0.1924, 0.2148, 0.0566, 0.1885, 0.1963, 0.1592, 0.1895,\n",
      "        0.0020, 0.1787, 0.1377, 0.2051, 0.2363, 0.1924, 0.2100, 0.1807, 0.2002,\n",
      "        0.1982, 0.1982, 0.0413, 0.1992, 0.1885, 0.2012, 0.2031, 0.1934, 0.1416,\n",
      "        0.1885, 0.2002, 0.1973, 0.1924, 0.1475, 0.2129, 0.1768, 0.2168, 0.1953,\n",
      "        0.1953, 0.2070, 0.2188, 0.1914, 0.2090, 0.1934, 0.2061, 0.2100, 0.2002,\n",
      "        0.2119, 0.1943, 0.1904, 0.1777, 0.1943, 0.1973, 0.2070, 0.2031, 0.2080,\n",
      "        0.1631, 0.1885, 0.2002, 0.1299, 0.1855, 0.1904, 0.1514, 0.2012, 0.1973,\n",
      "        0.1953, 0.1050, 0.1924, 0.2061, 0.1982, 0.1680, 0.2119, 0.2012, 0.1943,\n",
      "        0.1875, 0.1465, 0.1934, 0.1855, 0.2090, 0.1875, 0.1943, 0.1387, 0.1973,\n",
      "        0.2236, 0.1953, 0.1855, 0.1973, 0.1177, 0.2109, 0.1953, 0.1787, 0.2021,\n",
      "        0.1621, 0.2041, 0.1982, 0.1943, 0.2021, 0.1914, 0.2021, 0.1611, 0.1914,\n",
      "        0.1924, 0.2002, 0.1973, 0.1924, 0.1914, 0.2109, 0.2051, 0.1934, 0.2021,\n",
      "        0.1953, 0.1904, 0.2080, 0.1855, 0.1963, 0.1670, 0.1904, 0.1914, 0.1904,\n",
      "        0.1602, 0.0859, 0.1895, 0.1729, 0.2012, 0.1758, 0.1904, 0.1367, 0.1943,\n",
      "        0.2188, 0.2158, 0.1465, 0.2021, 0.1826, 0.2109, 0.0718, 0.1768, 0.0292,\n",
      "        0.2012, 0.2012, 0.1953, 0.2090, 0.1992, 0.2021, 0.1895, 0.1885, 0.1973,\n",
      "        0.1973, 0.1943, 0.2012, 0.1787, 0.2178, 0.1924, 0.0640, 0.2061, 0.1943,\n",
      "        0.1895, 0.1768, 0.1895, 0.1963, 0.1982, 0.1992, 0.2021, 0.1895, 0.1904,\n",
      "        0.1973, 0.1934, 0.2168, 0.1953, 0.1816, 0.2002, 0.1846, 0.2070, 0.1992,\n",
      "        0.2021, 0.1982, 0.1885, 0.1108, 0.1348, 0.1982, 0.1943, 0.2061, 0.1650,\n",
      "        0.2119, 0.2021, 0.1562, 0.1982, 0.2129, 0.1816, 0.1943, 0.1572, 0.1797,\n",
      "        0.1914, 0.1602, 0.1660, 0.1943, 0.1904, 0.1250, 0.1797, 0.2158, 0.2061,\n",
      "        0.1953, 0.1885, 0.1807, 0.2002, 0.2139, 0.1943, 0.1875, 0.1270, 0.1875,\n",
      "        0.1797, 0.1914, 0.1934, 0.1914, 0.1924, 0.1787, 0.0884, 0.1982, 0.1904,\n",
      "        0.1709, 0.1885, 0.2041, 0.1992, 0.1934, 0.2012, 0.1807, 0.1855, 0.1963,\n",
      "        0.0942, 0.2148, 0.1533, 0.0286, 0.1934, 0.1973, 0.2021, 0.1309, 0.2148,\n",
      "        0.1943, 0.2051, 0.2041, 0.2051, 0.1943, 0.2158, 0.1357, 0.2012, 0.2012,\n",
      "        0.1777, 0.1270, 0.1885, 0.1953, 0.1855, 0.2080, 0.2002, 0.1934, 0.1865,\n",
      "        0.2061, 0.1914, 0.1973, 0.2148, 0.2012, 0.2002, 0.1826, 0.2041, 0.1885,\n",
      "        0.2168, 0.2090, 0.2139, 0.1865, 0.2158, 0.2148, 0.2031, 0.1543, 0.2090,\n",
      "        0.1982, 0.1709, 0.1992, 0.1934, 0.1973, 0.1768, 0.1777, 0.1855, 0.1885,\n",
      "        0.1924, 0.1992, 0.1738, 0.1973, 0.2021, 0.2070, 0.2090, 0.1299, 0.1826,\n",
      "        0.1885, 0.2061, 0.1885, 0.1768, 0.1904, 0.2031, 0.1846, 0.2178, 0.2227,\n",
      "        0.2002, 0.1836, 0.2031, 0.1973, 0.1729, 0.2070, 0.1494, 0.1279, 0.2021,\n",
      "        0.1973, 0.2139, 0.2031, 0.1807, 0.2002, 0.2002, 0.1982, 0.2041, 0.1523,\n",
      "        0.1855, 0.1797, 0.1709, 0.1719, 0.1992, 0.1865, 0.0889, 0.1885, 0.1924,\n",
      "        0.1934, 0.1826, 0.2051, 0.0737, 0.1836, 0.1992, 0.1885, 0.1465, 0.1992,\n",
      "        0.1553, 0.1602, 0.2021, 0.1973, 0.1885, 0.1953, 0.2012, 0.0889, 0.2002,\n",
      "        0.1167, 0.1963, 0.1943, 0.1924, 0.0559, 0.1865, 0.2080, 0.2041, 0.1914,\n",
      "        0.1992, 0.2021, 0.1885, 0.1934, 0.1982, 0.0508, 0.1904, 0.2217, 0.1934,\n",
      "        0.1973, 0.1934, 0.2002, 0.1982, 0.1826, 0.1924, 0.1895, 0.1826, 0.1904,\n",
      "        0.2070, 0.1914, 0.1738, 0.1787, 0.0728, 0.1934, 0.1562, 0.2021, 0.2012,\n",
      "        0.0752, 0.1904, 0.2129, 0.1895, 0.1963, 0.1963, 0.2061, 0.1992, 0.2100,\n",
      "        0.1143, 0.1943, 0.1611, 0.1777, 0.1992, 0.1973, 0.1738, 0.1992, 0.2041,\n",
      "        0.1611, 0.1914, 0.1523, 0.2061, 0.2031, 0.1836, 0.2178, 0.1846, 0.2119,\n",
      "        0.1797, 0.2031, 0.2158, 0.2031, 0.2148, 0.2080, 0.1973, 0.1982, 0.1875,\n",
      "        0.1865, 0.1895, 0.1875, 0.2031, 0.2021, 0.2002, 0.1943, 0.1611, 0.1865,\n",
      "        0.1660, 0.1992, 0.1855, 0.1904, 0.1758, 0.1973, 0.2041, 0.2188, 0.2207,\n",
      "        0.1904, 0.0601, 0.1982, 0.1943, 0.1055, 0.2080, 0.2041, 0.2051, 0.2012,\n",
      "        0.1885, 0.1611, 0.1885, 0.1982, 0.2139, 0.1934, 0.0928, 0.2129, 0.1787,\n",
      "        0.2021, 0.1953, 0.1982, 0.1963, 0.1846, 0.1748, 0.2041, 0.1807, 0.0371,\n",
      "        0.2080, 0.2041, 0.2188, 0.1914, 0.1875, 0.2012, 0.2109, 0.2031, 0.1152,\n",
      "        0.2178, 0.2109, 0.2012, 0.2002, 0.1699, 0.1553, 0.2051, 0.1855, 0.1758,\n",
      "        0.1865, 0.1836, 0.2012, 0.1279, 0.1934, 0.1426, 0.1934, 0.2061, 0.1797,\n",
      "        0.2061, 0.1992, 0.1982, 0.1807, 0.1934, 0.1953, 0.1934, 0.1836, 0.1943,\n",
      "        0.1934, 0.1934, 0.1875, 0.1836, 0.1934, 0.2061, 0.2002, 0.1934, 0.1895,\n",
      "        0.0732, 0.1670, 0.2002, 0.1787, 0.2197, 0.1934, 0.2051, 0.1904, 0.1953,\n",
      "        0.2021, 0.1426, 0.1982, 0.1787, 0.1807, 0.1855, 0.2100, 0.2100, 0.0288,\n",
      "        0.1816, 0.2012, 0.2012, 0.1934, 0.1113, 0.1855, 0.1943, 0.2041, 0.1953,\n",
      "        0.2031, 0.2100, 0.1875, 0.1475, 0.1963, 0.2041, 0.2012, 0.1924, 0.1797,\n",
      "        0.1904, 0.1777, 0.1895, 0.1992, 0.1934, 0.2148, 0.2002, 0.2051, 0.2119,\n",
      "        0.1943, 0.2148, 0.1787, 0.1895, 0.1475, 0.1934, 0.2129, 0.2090, 0.2051,\n",
      "        0.1235, 0.2090, 0.1875, 0.1787, 0.1865, 0.1787, 0.1973, 0.2041, 0.2061,\n",
      "        0.1953, 0.2070, 0.1934, 0.2080, 0.1797, 0.2021, 0.2080, 0.1924, 0.2217,\n",
      "        0.1826, 0.1953, 0.2051, 0.2070, 0.1895, 0.1963, 0.1660, 0.1924, 0.1260,\n",
      "        0.2080, 0.1963, 0.1865, 0.2002, 0.1934, 0.1187, 0.1992, 0.1963, 0.1973,\n",
      "        0.2090, 0.1689, 0.2080, 0.1826, 0.1895, 0.2090, 0.2041, 0.1885, 0.1963,\n",
      "        0.1895, 0.1797, 0.1846, 0.1982, 0.1973, 0.1768, 0.1777, 0.1699, 0.2012,\n",
      "        0.1895, 0.1748, 0.1895, 0.1152, 0.0317, 0.1816, 0.2080, 0.1846, 0.1836,\n",
      "        0.1953, 0.1963, 0.2217, 0.1963, 0.2070, 0.1602, 0.1875, 0.1748, 0.2207,\n",
      "        0.2002, 0.1943, 0.1895, 0.1748, 0.2031, 0.1973, 0.1934, 0.1914, 0.1719,\n",
      "        0.2051, 0.1221, 0.1807, 0.2041, 0.2080, 0.2061, 0.2021, 0.1953, 0.1426,\n",
      "        0.1748, 0.2012, 0.1934, 0.2061, 0.1865, 0.2109, 0.2002, 0.1826, 0.1758,\n",
      "        0.1904, 0.1895, 0.1953, 0.2080, 0.2012, 0.1768, 0.2002, 0.1924, 0.0378,\n",
      "        0.1943, 0.2012, 0.1982, 0.0835, 0.1943, 0.1777, 0.2051, 0.2100, 0.1562,\n",
      "        0.2012, 0.1553, 0.1865, 0.1875, 0.2090, 0.2002, 0.2021, 0.1660, 0.2188,\n",
      "        0.1816, 0.1885, 0.1914, 0.1904, 0.1797, 0.1973, 0.2090, 0.1963, 0.1875,\n",
      "        0.2119, 0.2021, 0.2021, 0.1846, 0.2031, 0.2119, 0.1982, 0.1953, 0.1982,\n",
      "        0.1846, 0.2041, 0.1631, 0.1885, 0.1973, 0.1982, 0.1953, 0.1914, 0.1865,\n",
      "        0.1992, 0.2139, 0.2109, 0.2109, 0.1807, 0.2070, 0.2041, 0.1787, 0.2051,\n",
      "        0.1826, 0.1963, 0.2178], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.10.layer.1.DenseReluDense.wi.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.4180,  0.0933, -0.0869,  ..., -0.5234, -0.6094,  0.1533],\n",
      "        [-0.3242, -1.9688,  0.2314,  ...,  0.1177, -1.1484, -1.1953],\n",
      "        [-0.1426,  0.8359, -0.2930,  ...,  0.9531,  0.2500, -0.1367],\n",
      "        ...,\n",
      "        [ 0.1260,  0.6719, -1.6562,  ..., -0.1128,  1.0625,  1.3438],\n",
      "        [-1.6094,  0.2275, -0.4004,  ..., -0.3145,  0.1069,  0.5508],\n",
      "        [ 0.5664, -0.2969, -0.1543,  ..., -1.5312, -0.3945, -0.2324]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.10.layer.1.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.4336,  0.0073,  0.0291,  ...,  0.1309, -0.4414, -0.0068],\n",
      "        [ 0.1748, -0.5781,  0.2461,  ...,  1.3359,  0.2461,  0.4551],\n",
      "        [ 0.1816,  0.9570,  0.5508,  ..., -0.5195, -0.0952,  0.4941],\n",
      "        ...,\n",
      "        [ 0.2158,  0.3086,  0.7148,  ..., -0.1992,  0.3359, -0.3320],\n",
      "        [ 0.2949, -0.6289,  0.2559,  ..., -0.0337, -0.0422, -0.7578],\n",
      "        [ 0.1050,  0.7266, -0.6680,  ..., -0.2432, -0.0099,  0.2891]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.10.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([1.2500, 1.2422, 1.1797, 1.2344, 1.4922, 1.3516, 0.3633, 1.3125, 0.9023,\n",
      "        1.5078, 1.1250, 1.1797, 1.3281, 1.3438, 1.4453, 1.4531, 1.2812, 1.2812,\n",
      "        0.8008, 1.4141, 1.4141, 1.3984, 4.2500, 1.3828, 1.4688, 1.3516, 1.4766,\n",
      "        1.2344, 1.3672, 1.4531, 1.3750, 1.4141, 1.3672, 1.4766, 1.1875, 1.4297,\n",
      "        1.3281, 1.1797, 1.5000, 1.4219, 1.2266, 1.2578, 0.8828, 1.3672, 2.9688,\n",
      "        1.4766, 1.1094, 1.3906, 1.3281, 0.5625, 1.2031, 1.3750, 1.3672, 1.4531,\n",
      "        1.3281, 1.3828, 2.9688, 1.3672, 1.5000, 1.4922, 1.3906, 1.4922, 1.4297,\n",
      "        1.4609, 1.5156, 1.0859, 1.4453, 1.4375, 1.2266, 1.3438, 1.4062, 0.7656,\n",
      "        1.4531, 1.4688, 1.4453, 1.4531, 1.3359, 1.3047, 1.4453, 1.3594, 1.4688,\n",
      "        1.2891, 1.4141, 1.2500, 1.3281, 1.3516, 1.2656, 1.3828, 1.3359, 1.4922,\n",
      "        1.3828, 1.5078, 1.4375, 1.3438, 1.5938, 1.4844, 1.4141, 1.4141, 1.4531,\n",
      "        1.4844, 1.2891, 1.3047, 0.7148, 1.4375, 1.3750, 1.5938, 1.3906, 1.5391,\n",
      "        1.4609, 2.1094, 1.2891, 1.4219, 1.3438, 1.4375, 1.3984, 1.5156, 1.3906,\n",
      "        1.3750, 0.9414, 1.4219, 1.4922, 1.3984, 1.3281, 1.4375, 0.8594, 1.3672,\n",
      "        1.4219, 1.3516, 1.3281, 1.3828, 1.1875, 1.3359, 1.4531, 1.4922, 1.3359,\n",
      "        0.9336, 1.4219, 1.4219, 1.3516, 1.2109, 1.3594, 1.1719, 1.0547, 1.4453,\n",
      "        1.2812, 1.3594, 1.4062, 1.2422, 1.4219, 1.3438, 1.4375, 1.2891, 1.3047,\n",
      "        1.3594, 1.5234, 1.4609, 1.3125, 1.4922, 1.1953, 1.3281, 1.5547, 1.3828,\n",
      "        1.2578, 4.3750, 1.4609, 1.3281, 1.3984, 1.2656, 1.2109, 0.8359, 1.1953,\n",
      "        1.3594, 1.4453, 1.0625, 1.4375, 1.2969, 1.3594, 0.4473, 1.1641, 0.4922,\n",
      "        1.4219, 1.5312, 1.3516, 1.4922, 1.2656, 1.3203, 1.3594, 1.0781, 1.3281,\n",
      "        1.3438, 1.3672, 1.4297, 1.6953, 1.4141, 1.4844, 0.5391, 1.3906, 1.2969,\n",
      "        1.3750, 1.5156, 1.5156, 1.3438, 1.4375, 1.4297, 1.4219, 1.2109, 1.3984,\n",
      "        1.2969, 1.4375, 1.4609, 1.3672, 1.3906, 1.2969, 1.4062, 1.4297, 1.3047,\n",
      "        1.3359, 1.4141, 1.2891, 0.9258, 1.1797, 1.3281, 1.4688, 1.2812, 1.3047,\n",
      "        1.4453, 1.2344, 0.8672, 1.3906, 1.3438, 1.3516, 1.3359, 1.5312, 1.5781,\n",
      "        1.4609, 1.4375, 1.8672, 1.3672, 1.2031, 0.8438, 1.2266, 1.3750, 1.4766,\n",
      "        1.3828, 1.4297, 1.4062, 1.4062, 1.3594, 1.5234, 1.2891, 2.1562, 1.5078,\n",
      "        1.4453, 1.5391, 1.4688, 1.4531, 1.4766, 1.2578, 0.6094, 1.3594, 1.1641,\n",
      "        1.0156, 1.4531, 1.3984, 1.4297, 1.3906, 1.3906, 1.4062, 1.4688, 1.4141,\n",
      "        1.5312, 1.2109, 1.2109, 0.8555, 1.2031, 1.3672, 1.3984, 0.8359, 1.4219,\n",
      "        1.3047, 1.3203, 1.2344, 1.4688, 1.3906, 1.4141, 0.9648, 1.5234, 1.5547,\n",
      "        1.1016, 1.6016, 1.5000, 1.3750, 1.4844, 1.4766, 1.5781, 1.6641, 1.2266,\n",
      "        1.5078, 1.4375, 1.3594, 1.3125, 1.3828, 1.1094, 1.2578, 1.4219, 1.5078,\n",
      "        1.2812, 1.4531, 1.3125, 1.2891, 1.3906, 1.2812, 1.4375, 1.2500, 1.3672,\n",
      "        1.3516, 1.3594, 1.2422, 1.4453, 1.4141, 1.4688, 1.1484, 1.3125, 1.5469,\n",
      "        1.2188, 1.4688, 1.0078, 1.2266, 1.3281, 1.4062, 1.5156, 0.9414, 1.2656,\n",
      "        1.3281, 1.4844, 1.4375, 1.5469, 1.4375, 1.3516, 1.3516, 1.2812, 1.3203,\n",
      "        1.4531, 1.5078, 1.3359, 1.4062, 0.9805, 1.3906, 1.0234, 1.9062, 1.2344,\n",
      "        1.3750, 1.4922, 1.4141, 1.2656, 1.3359, 1.2812, 1.3047, 1.5156, 0.9688,\n",
      "        1.3984, 1.1875, 1.0156, 1.5000, 1.5391, 1.4297, 0.6172, 1.4688, 1.4375,\n",
      "        1.3906, 1.4922, 1.2344, 0.6406, 1.3750, 1.2891, 1.5391, 1.1797, 1.3750,\n",
      "        0.8164, 1.2266, 1.4297, 1.2891, 1.2031, 1.3438, 1.4219, 0.9336, 1.4453,\n",
      "        0.6953, 1.5625, 1.3438, 1.4297, 2.8594, 1.2969, 1.2422, 1.2812, 1.4453,\n",
      "        1.3047, 1.4844, 1.3359, 1.4531, 1.3984, 1.0312, 1.5469, 1.4297, 1.4219,\n",
      "        1.4375, 1.3672, 1.4531, 1.4453, 1.3516, 1.4297, 1.4922, 1.4062, 1.4219,\n",
      "        1.4375, 1.3359, 1.4922, 1.4688, 0.6758, 1.4922, 1.0391, 1.3203, 1.5625,\n",
      "        0.5234, 1.3984, 1.5312, 1.3125, 1.4844, 1.3906, 1.2422, 1.2969, 1.5000,\n",
      "        0.6680, 1.4766, 1.1328, 1.4297, 1.4219, 1.4062, 0.9414, 1.2969, 1.3984,\n",
      "        1.1719, 1.2891, 0.8086, 1.4453, 1.2969, 1.4609, 1.3203, 1.1797, 1.3750,\n",
      "        1.3203, 1.5000, 1.3438, 1.4375, 1.3906, 1.2734, 1.3125, 1.3516, 1.3281,\n",
      "        1.4531, 1.4609, 1.4141, 1.4688, 1.3516, 1.3203, 1.3203, 1.3516, 1.3672,\n",
      "        1.4062, 1.4609, 1.4609, 1.5859, 1.2656, 1.4766, 1.3828, 1.4922, 1.5312,\n",
      "        1.4609, 0.8086, 1.2891, 1.2344, 1.8594, 1.3984, 1.4766, 1.2109, 1.4531,\n",
      "        1.4922, 1.3203, 1.4531, 1.3359, 1.4141, 1.4062, 1.7344, 1.5312, 1.3359,\n",
      "        1.3281, 1.3594, 1.4062, 1.4297, 1.4531, 1.3984, 1.2734, 1.3438, 2.1094,\n",
      "        1.4922, 1.3125, 1.3906, 1.1953, 1.5781, 1.3516, 1.3281, 1.4844, 2.0156,\n",
      "        1.4453, 1.2812, 1.4766, 1.3203, 1.3906, 0.8320, 1.4062, 1.5781, 1.3672,\n",
      "        1.4766, 1.4531, 1.3984, 1.8125, 1.2656, 0.8906, 1.4297, 1.2344, 1.1328,\n",
      "        1.3594, 1.3906, 1.2891, 1.3828, 1.2266, 1.3281, 1.3750, 1.1250, 1.3203,\n",
      "        1.2891, 1.3281, 1.2266, 1.1719, 1.4766, 1.4688, 1.2812, 1.3359, 1.4922,\n",
      "        0.4434, 1.1016, 1.2891, 1.5234, 1.3828, 1.4297, 1.4219, 1.2656, 1.4688,\n",
      "        1.3359, 1.0000, 1.4062, 1.5312, 1.2344, 1.2812, 1.4062, 1.4141, 0.5781,\n",
      "        1.3750, 1.3516, 1.4688, 1.2734, 0.8164, 1.4453, 1.4219, 1.3203, 1.3203,\n",
      "        1.4453, 1.1719, 1.3984, 0.9297, 1.3828, 1.5703, 1.4766, 1.4531, 1.5156,\n",
      "        1.5078, 1.5469, 1.4922, 1.4766, 1.3984, 1.3906, 1.3125, 1.4375, 1.4375,\n",
      "        1.2891, 1.4688, 1.2578, 1.4375, 1.1094, 1.3438, 1.3438, 1.4062, 1.3281,\n",
      "        1.1172, 1.3438, 1.4688, 1.3672, 1.2422, 1.2031, 1.4609, 1.3281, 1.4766,\n",
      "        1.2969, 1.4531, 1.3984, 1.2500, 1.3984, 1.1250, 1.3438, 1.2969, 1.3125,\n",
      "        1.4531, 1.4219, 1.1562, 1.3047, 1.3750, 1.3594, 1.1172, 1.3125, 0.9844,\n",
      "        1.3750, 1.5469, 1.3203, 1.1641, 1.5234, 0.5781, 1.4141, 1.2891, 1.0312,\n",
      "        1.4062, 1.2891, 1.3984, 1.4375, 1.4844, 1.3281, 1.1797, 1.1328, 1.4766,\n",
      "        1.4062, 1.4766, 1.3750, 1.4844, 1.3516, 1.3672, 1.1094, 0.8711, 1.3672,\n",
      "        1.1484, 0.8828, 1.2500, 0.6953, 0.7070, 1.3203, 1.3047, 1.3828, 1.3047,\n",
      "        1.4766, 1.4688, 1.3672, 1.4766, 1.4922, 1.5078, 1.3828, 1.4219, 1.3359,\n",
      "        1.5000, 1.3750, 1.4219, 1.4062, 1.4375, 1.3438, 1.3906, 1.4375, 1.4297,\n",
      "        1.2969, 1.5469, 1.3672, 1.3828, 1.4766, 1.3516, 1.5234, 1.2891, 0.8672,\n",
      "        1.4219, 1.4141, 1.3906, 1.2266, 1.4688, 1.3828, 1.3047, 1.3906, 1.4922,\n",
      "        1.2969, 1.3672, 1.4062, 1.5625, 1.4062, 1.2812, 1.1797, 1.5000, 1.8516,\n",
      "        1.3750, 1.4062, 1.4922, 0.6602, 1.3594, 1.3750, 1.2266, 1.3438, 0.7969,\n",
      "        1.4531, 1.3359, 1.2656, 1.4766, 1.3125, 1.2344, 1.2891, 1.4062, 1.4609,\n",
      "        1.1875, 1.2422, 1.4922, 1.4219, 1.3203, 1.3750, 1.4141, 1.3906, 1.3359,\n",
      "        1.3750, 1.3672, 1.3750, 1.2422, 1.1172, 1.2188, 1.3906, 1.3125, 1.2109,\n",
      "        1.5547, 1.3281, 0.9414, 1.3594, 1.4375, 1.2578, 1.4219, 1.2891, 1.4531,\n",
      "        1.3516, 1.3828, 1.2734, 1.2891, 1.2188, 1.3516, 1.3281, 1.4609, 1.2734,\n",
      "        1.2031, 1.4375, 1.2734], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.11.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0381, -0.0045,  0.0262,  ..., -0.0129, -0.0371,  0.0265],\n",
      "        [-0.0140, -0.0737, -0.0101,  ..., -0.0452, -0.0349, -0.0176],\n",
      "        [ 0.0204, -0.0019,  0.0049,  ..., -0.0177,  0.0518, -0.0306],\n",
      "        ...,\n",
      "        [ 0.0369, -0.0186, -0.0104,  ..., -0.0201, -0.0505, -0.0189],\n",
      "        [-0.0017, -0.0245, -0.0019,  ...,  0.0349, -0.0140, -0.0117],\n",
      "        [ 0.0659,  0.0066,  0.0120,  ...,  0.0096,  0.0085,  0.0344]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.11.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.1475, -0.1309,  0.0240,  ..., -0.1143,  0.0801, -0.2871],\n",
      "        [-0.1504, -0.1011, -0.0996,  ..., -0.1045, -0.1250, -0.0376],\n",
      "        [ 0.0962,  0.1406,  0.2256,  ...,  0.1660, -0.1299, -0.4102],\n",
      "        ...,\n",
      "        [ 0.2793, -0.2871, -0.1758,  ...,  0.3027, -0.2031, -0.1758],\n",
      "        [ 0.2109, -0.1738, -0.1562,  ...,  0.1445, -0.3164,  0.2383],\n",
      "        [ 0.0349,  0.2207,  0.2158,  ...,  0.1572,  0.1621, -0.2168]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.11.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1953,  0.7461,  0.9062,  ...,  0.9336,  0.6094, -0.8086],\n",
      "        [ 0.7031, -1.1562,  0.5078,  ...,  0.9336, -1.3828, -0.5195],\n",
      "        [ 1.4219, -0.0220, -1.8516,  ..., -0.3320, -0.7109,  0.2598],\n",
      "        ...,\n",
      "        [-0.1885, -0.5977, -0.6836,  ...,  1.2891, -0.7148, -0.2373],\n",
      "        [-0.2061,  1.3359,  1.4453,  ...,  0.4414, -2.0781,  0.7891],\n",
      "        [ 0.3574, -0.3281,  1.0312,  ...,  0.5039, -0.2637,  1.7656]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.11.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-1.8047,  1.6641, -1.2891,  ...,  1.7500, -1.4531, -0.0972],\n",
      "        [ 1.5781,  4.1250,  0.6562,  ...,  1.4688, -0.5742,  0.5938],\n",
      "        [-2.3750, -1.4062,  1.1172,  ...,  1.7109, -2.3594, -0.9727],\n",
      "        ...,\n",
      "        [-2.4062, -0.4863,  1.2422,  ...,  1.1094,  0.0796,  0.6758],\n",
      "        [-0.5078,  1.7031,  0.2002,  ...,  0.9727,  1.2891,  0.9297],\n",
      "        [ 1.0547,  0.9219,  1.0781,  ..., -0.3770,  0.7422, -2.6719]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.11.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 0.1826,  0.1748,  0.1582,  0.1787,  0.1914,  0.1719,  0.0393,  0.1836,\n",
      "         0.0688,  0.1875,  0.1738,  0.1963,  0.1465,  0.1504,  0.1797,  0.1816,\n",
      "         0.1660,  0.1816,  0.1270,  0.1777,  0.1641,  0.1816,  0.0635,  0.1777,\n",
      "         0.1914,  0.1699,  0.1885,  0.1689,  0.1797,  0.1836,  0.1738,  0.1826,\n",
      "         0.1797,  0.1719,  0.1553,  0.1875,  0.1309,  0.1562,  0.1895,  0.1123,\n",
      "         0.1562,  0.1455,  0.1504,  0.2041, -0.0703,  0.1758,  0.1367,  0.1777,\n",
      "         0.1826,  0.0532,  0.1611,  0.1729,  0.1416,  0.1973, -0.0016,  0.1709,\n",
      "         0.1201,  0.1816,  0.2090,  0.1895,  0.1953,  0.1738,  0.1807,  0.1895,\n",
      "         0.1846,  0.0339,  0.1826,  0.1768,  0.1689,  0.1826,  0.1787,  0.1211,\n",
      "         0.1748,  0.1836,  0.1846,  0.1895,  0.1348,  0.1904,  0.1807,  0.2168,\n",
      "         0.1846,  0.1748,  0.1963,  0.1787,  0.1729,  0.1885,  0.1660,  0.1904,\n",
      "         0.1934,  0.1924,  0.1865,  0.1709,  0.1758,  0.1670,  0.1826,  0.1904,\n",
      "         0.1855,  0.1914,  0.1992,  0.1602,  0.1641,  0.1885,  0.1196,  0.1855,\n",
      "         0.1787,  0.1299,  0.1865,  0.1826,  0.1865,  0.1001,  0.1709,  0.1943,\n",
      "         0.1709,  0.1621,  0.1875,  0.1943,  0.1729,  0.1914,  0.1250,  0.1748,\n",
      "         0.1787,  0.2002,  0.1631,  0.1787,  0.1206,  0.1836,  0.2021,  0.1709,\n",
      "         0.1738,  0.1768,  0.0947,  0.1807,  0.1885,  0.1699,  0.1826,  0.1367,\n",
      "         0.1748,  0.1768,  0.1777,  0.1689,  0.1650,  0.1660,  0.1377,  0.1738,\n",
      "         0.1650,  0.1680,  0.1768,  0.1680,  0.1738,  0.1807,  0.1875,  0.1836,\n",
      "         0.1826,  0.1729,  0.1797,  0.1963,  0.1611,  0.1953,  0.1484,  0.1797,\n",
      "         0.1807,  0.1729,  0.1562, -0.0708,  0.1826,  0.1641,  0.1787,  0.1631,\n",
      "         0.1670,  0.0869,  0.1670,  0.2090,  0.1992,  0.1226,  0.1953,  0.1719,\n",
      "         0.1797,  0.0630,  0.1523, -0.0259,  0.1855,  0.1904,  0.1738,  0.2021,\n",
      "         0.1787,  0.1807,  0.1846,  0.1592,  0.1729,  0.1680,  0.1836,  0.1914,\n",
      "         0.1797,  0.1992,  0.1914,  0.0591,  0.1855,  0.1768,  0.1895,  0.1797,\n",
      "         0.1885,  0.1709,  0.1670,  0.2061,  0.1973,  0.1680,  0.1758,  0.1787,\n",
      "         0.1787,  0.1914,  0.1758,  0.1758,  0.1758,  0.1641,  0.1787,  0.1758,\n",
      "         0.1738,  0.1748,  0.1670,  0.0825,  0.1143,  0.1748,  0.1904,  0.1777,\n",
      "         0.1533,  0.1855,  0.1719,  0.1006,  0.1816,  0.1855,  0.1709,  0.1719,\n",
      "         0.1553,  0.1885,  0.1816,  0.1855,  0.1738,  0.1680,  0.1523,  0.1084,\n",
      "         0.1514,  0.1836,  0.1924,  0.1963,  0.1836,  0.1855,  0.1797,  0.1826,\n",
      "         0.1855,  0.1729,  0.1147,  0.1729,  0.1738,  0.1826,  0.1816,  0.1826,\n",
      "         0.1904,  0.1738,  0.0718,  0.1768,  0.1660,  0.1465,  0.1797,  0.1865,\n",
      "         0.1826,  0.1934,  0.1846,  0.1719,  0.1826,  0.1885,  0.0918,  0.1836,\n",
      "         0.1309,  0.0214,  0.1689,  0.1836,  0.1875,  0.1245,  0.1992,  0.1670,\n",
      "         0.1650,  0.1650,  0.1953,  0.1641,  0.2012,  0.1289,  0.1846,  0.1836,\n",
      "         0.1611,  0.1069,  0.1846,  0.1709,  0.1836,  0.2012,  0.1895,  0.1885,\n",
      "         0.1680,  0.1953,  0.1787,  0.1719,  0.1904,  0.1826,  0.1719,  0.1650,\n",
      "         0.1904,  0.1758,  0.1826,  0.1865,  0.1875,  0.1719,  0.1855,  0.1904,\n",
      "         0.1924,  0.1553,  0.1875,  0.1748,  0.1572,  0.1670,  0.1758,  0.1865,\n",
      "         0.1826,  0.1572,  0.1689,  0.1875,  0.1621,  0.1953,  0.1611,  0.1748,\n",
      "         0.1797,  0.1836,  0.2061,  0.1138,  0.1660,  0.1680,  0.1924,  0.1836,\n",
      "         0.1836,  0.1885,  0.1787,  0.1680,  0.1914,  0.1953,  0.1797,  0.1797,\n",
      "         0.1797,  0.1738,  0.1523,  0.1846,  0.1426,  0.1089,  0.1787,  0.1836,\n",
      "         0.1836,  0.2041,  0.1602,  0.1797,  0.1719,  0.1660,  0.1943,  0.1299,\n",
      "         0.1719,  0.1504,  0.1465,  0.1787,  0.1826,  0.1758,  0.0752,  0.1699,\n",
      "         0.1826,  0.1738,  0.1650,  0.1797,  0.0688,  0.1836,  0.1836,  0.1836,\n",
      "         0.1348,  0.1875,  0.0947,  0.1582,  0.1934,  0.1797,  0.1572,  0.1758,\n",
      "         0.1895,  0.0806,  0.1934,  0.1006,  0.2139,  0.1562,  0.1768,  0.0430,\n",
      "         0.1768,  0.1738,  0.1777,  0.1807,  0.1787,  0.1855,  0.1709,  0.1924,\n",
      "         0.1914,  0.0425,  0.1885,  0.1982,  0.1816,  0.1855,  0.1836,  0.1807,\n",
      "         0.1865,  0.1836,  0.1807,  0.1836,  0.1719,  0.1738,  0.1914,  0.1709,\n",
      "         0.1777,  0.1807,  0.0747,  0.1592,  0.1348,  0.1748,  0.1963,  0.0625,\n",
      "         0.1816,  0.1963,  0.1758,  0.1885,  0.1816,  0.1777,  0.1680,  0.1934,\n",
      "         0.0928,  0.1768,  0.1426,  0.1758,  0.1797,  0.1768,  0.1572,  0.1777,\n",
      "         0.1895,  0.1533,  0.1768,  0.1133,  0.1787,  0.1719,  0.1797,  0.1865,\n",
      "         0.1592,  0.1865,  0.1729,  0.1904,  0.1875,  0.1875,  0.1904,  0.1748,\n",
      "         0.1738,  0.1924,  0.1670,  0.1777,  0.1846,  0.1807,  0.1895,  0.1807,\n",
      "         0.1729,  0.1758,  0.1611,  0.1699,  0.1680,  0.1826,  0.1875,  0.1963,\n",
      "         0.1641,  0.1787,  0.1963,  0.1943,  0.1934,  0.1875,  0.0488,  0.1807,\n",
      "         0.1641,  0.0908,  0.1797,  0.1904,  0.1787,  0.1797,  0.1865,  0.1309,\n",
      "         0.1797,  0.1777,  0.2002,  0.1689,  0.0806,  0.1943,  0.1738,  0.1719,\n",
      "         0.1787,  0.1807,  0.1758,  0.1777,  0.1748,  0.1699,  0.1641,  0.0310,\n",
      "         0.1904,  0.1738,  0.1934,  0.1602,  0.1807,  0.1768,  0.1846,  0.2080,\n",
      "         0.1025,  0.2061,  0.1807,  0.1904,  0.1758,  0.1572,  0.1377,  0.1797,\n",
      "         0.1875,  0.1592,  0.1807,  0.1826,  0.1758,  0.0952,  0.1807,  0.1216,\n",
      "         0.1846,  0.1738,  0.1641,  0.1855,  0.1738,  0.1768,  0.1650,  0.1621,\n",
      "         0.1855,  0.1836,  0.1514,  0.1807,  0.1709,  0.1768,  0.1699,  0.1602,\n",
      "         0.1777,  0.1885,  0.1689,  0.1885,  0.1777,  0.0669,  0.1416,  0.1670,\n",
      "         0.1826,  0.1914,  0.1836,  0.1914,  0.1680,  0.1777,  0.1719,  0.1270,\n",
      "         0.1650,  0.1650,  0.1621,  0.1602,  0.1943,  0.1846,  0.0242,  0.1807,\n",
      "         0.1768,  0.1865,  0.1689,  0.1211,  0.1855,  0.1787,  0.1836,  0.1719,\n",
      "         0.1904,  0.1816,  0.1758,  0.1348,  0.1875,  0.1943,  0.1895,  0.1768,\n",
      "         0.1816,  0.1904,  0.1729,  0.1846,  0.1934,  0.1807,  0.1953,  0.1729,\n",
      "         0.1924,  0.1895,  0.1729,  0.1875,  0.1709,  0.1660,  0.1328,  0.1748,\n",
      "         0.1777,  0.1875,  0.1787,  0.1084,  0.1973,  0.1875,  0.1748,  0.1689,\n",
      "         0.1484,  0.1943,  0.1826,  0.1836,  0.1650,  0.1875,  0.1777,  0.1719,\n",
      "         0.1709,  0.1768,  0.1924,  0.1729,  0.1836,  0.1680,  0.1777,  0.1680,\n",
      "         0.1729,  0.1807,  0.1758,  0.1426,  0.1748,  0.1143,  0.1797,  0.1768,\n",
      "         0.1748,  0.1680,  0.1914,  0.0928,  0.1797,  0.1729,  0.1006,  0.1855,\n",
      "         0.1523,  0.1836,  0.1719,  0.1660,  0.1846,  0.1768,  0.1650,  0.1797,\n",
      "         0.1777,  0.1758,  0.1748,  0.1914,  0.1855,  0.1846,  0.1582,  0.1475,\n",
      "         0.1875,  0.1641,  0.1641,  0.1660,  0.1011,  0.0275,  0.1729,  0.1816,\n",
      "         0.1768,  0.1572,  0.1748,  0.1885,  0.1982,  0.1924,  0.1846,  0.1484,\n",
      "         0.1719,  0.1660,  0.2002,  0.1934,  0.1836,  0.1758,  0.1699,  0.1963,\n",
      "         0.1934,  0.1855,  0.1787,  0.1875,  0.1826,  0.1045,  0.1729,  0.1797,\n",
      "         0.1953,  0.1807,  0.1846,  0.1699,  0.1338,  0.1768,  0.1875,  0.1709,\n",
      "         0.1807,  0.1709,  0.1895,  0.1699,  0.1572,  0.1797,  0.1738,  0.1768,\n",
      "         0.1689,  0.1895,  0.1865,  0.1572,  0.1699,  0.1855,  0.0273,  0.1787,\n",
      "         0.1846,  0.1973,  0.0776,  0.1719,  0.1641,  0.1650,  0.1807,  0.1240,\n",
      "         0.1943,  0.1357,  0.1631,  0.1748,  0.1787,  0.1807,  0.1787,  0.1641,\n",
      "         0.1924,  0.1611,  0.1738,  0.1777,  0.1787,  0.1865,  0.1865,  0.1963,\n",
      "         0.1865,  0.1748,  0.1992,  0.1885,  0.1807,  0.1592,  0.1709,  0.1768,\n",
      "         0.1914,  0.1670,  0.1680,  0.1768,  0.1719,  0.0923,  0.1787,  0.1816,\n",
      "         0.1641,  0.1826,  0.1641,  0.1738,  0.1797,  0.1836,  0.1943,  0.1836,\n",
      "         0.1543,  0.1973,  0.1777,  0.1670,  0.1709,  0.1689,  0.1729,  0.1807],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.11.layer.1.DenseReluDense.wi.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.5625,  0.5625, -0.1641,  ..., -0.3867, -0.3184, -0.2988],\n",
      "        [-1.7812, -0.5742,  0.6758,  ...,  0.0187, -0.0623, -0.3926],\n",
      "        [ 0.5078,  0.5391,  0.5977,  ...,  0.2090, -1.6562,  0.4219],\n",
      "        ...,\n",
      "        [ 0.1226,  0.6797, -0.3066,  ..., -0.0659, -0.2207,  0.0228],\n",
      "        [ 1.2422,  0.2598, -0.1982,  ..., -0.9609,  0.3223, -0.8828],\n",
      "        [ 0.3906, -0.5859,  0.1426,  ...,  0.7969, -0.3320, -0.7617]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.11.layer.1.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0260,  0.2168, -0.8984,  ...,  0.0579,  0.5391,  0.7344],\n",
      "        [-0.4395,  0.0457, -0.4668,  ..., -0.6211, -0.0496, -0.4336],\n",
      "        [ 0.4336,  0.3789,  0.4688,  ...,  0.0713,  0.1016,  0.4551],\n",
      "        ...,\n",
      "        [ 0.2949,  0.2119, -0.9414,  ...,  0.0427,  0.1177,  0.4648],\n",
      "        [ 0.2158, -0.1514, -0.9492,  ..., -0.0728, -0.1543,  0.6367],\n",
      "        [-0.2051,  0.4785,  0.2578,  ...,  0.2969, -0.0131, -0.3203]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.block.11.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([0.8984, 0.8750, 0.8125, 0.8164, 1.0938, 0.9453, 0.2930, 0.9375, 0.6172,\n",
      "        1.0234, 0.8398, 0.8984, 0.9336, 0.8828, 1.0859, 1.0078, 0.8984, 0.8750,\n",
      "        0.4980, 1.0312, 0.9961, 0.9805, 3.0000, 1.0000, 1.0547, 0.8906, 1.0859,\n",
      "        0.8984, 0.9531, 1.0234, 1.0078, 0.9922, 0.9766, 1.0469, 0.8633, 1.0547,\n",
      "        0.8906, 0.7852, 1.1406, 0.7148, 0.8867, 0.8281, 0.6602, 0.9727, 0.9766,\n",
      "        1.0781, 0.7500, 1.0312, 0.9180, 0.3809, 0.8359, 1.0000, 0.9414, 1.0312,\n",
      "        0.8398, 0.9531, 1.7500, 0.9922, 1.0625, 1.0312, 1.0078, 1.1016, 1.0391,\n",
      "        1.0000, 1.1094, 0.5000, 1.0469, 0.9922, 0.9062, 1.0078, 1.0000, 0.5352,\n",
      "        1.0703, 1.0391, 0.9961, 1.0391, 0.9023, 0.9609, 1.0156, 0.9922, 1.0625,\n",
      "        0.9453, 1.0625, 0.8945, 0.9766, 1.0000, 0.8555, 1.0156, 0.9766, 1.0547,\n",
      "        1.0078, 1.0859, 1.0234, 0.9570, 1.1250, 1.0625, 0.9961, 1.0469, 1.0703,\n",
      "        1.0234, 0.9023, 0.9492, 0.5664, 1.0234, 0.9531, 1.0078, 1.0391, 1.0703,\n",
      "        1.1016, 1.3438, 0.9453, 1.0156, 0.9492, 1.0312, 0.9883, 1.0703, 0.9453,\n",
      "        0.9805, 0.5898, 1.0469, 1.1406, 1.0078, 0.9102, 1.0234, 0.5820, 0.9961,\n",
      "        1.0000, 0.9062, 0.9805, 0.9531, 0.7188, 0.9219, 1.0469, 1.0938, 0.9766,\n",
      "        0.6406, 0.9805, 1.0391, 0.9297, 0.8281, 0.9414, 0.7656, 0.7227, 0.9531,\n",
      "        0.9258, 0.9180, 0.9922, 0.8945, 0.9805, 0.9609, 1.0469, 0.8984, 0.9375,\n",
      "        0.9102, 1.0781, 1.0703, 0.9023, 1.1016, 0.8438, 0.9297, 1.1406, 1.0078,\n",
      "        0.9180, 2.4219, 1.0547, 1.0078, 0.9688, 0.8594, 0.8984, 0.5781, 0.8516,\n",
      "        1.0156, 0.9961, 0.7227, 1.0469, 0.9336, 0.9297, 0.3184, 0.7891, 0.3477,\n",
      "        1.0078, 1.0938, 0.9453, 1.1172, 0.9102, 0.9336, 1.0156, 0.7852, 0.9883,\n",
      "        0.9414, 1.0391, 1.1016, 1.1797, 1.0000, 1.1016, 0.3984, 0.9961, 0.8945,\n",
      "        1.0000, 1.1250, 1.1641, 0.9336, 0.9766, 1.0547, 1.0312, 0.8711, 1.0000,\n",
      "        0.9766, 1.0391, 0.9922, 0.9648, 0.9648, 0.8750, 1.0078, 1.0078, 0.9102,\n",
      "        0.9727, 0.9570, 0.9180, 0.7227, 0.8203, 0.9531, 1.0625, 0.9023, 0.8789,\n",
      "        1.0156, 0.8281, 0.7891, 1.0234, 0.9961, 0.9492, 0.9727, 1.2344, 1.1641,\n",
      "        1.0859, 1.1406, 1.6641, 0.9297, 0.8398, 0.5898, 0.8438, 0.9727, 1.0703,\n",
      "        1.0156, 1.0469, 1.0312, 1.0156, 0.9922, 1.1094, 0.9258, 1.1953, 1.0391,\n",
      "        1.0469, 1.1484, 1.0703, 1.0312, 1.0469, 0.9102, 0.3887, 0.9688, 0.7812,\n",
      "        0.8867, 1.0234, 1.0156, 1.0156, 1.0000, 1.0156, 0.9492, 1.0469, 1.0156,\n",
      "        0.9492, 0.8359, 0.8438, 0.3945, 0.7852, 0.9258, 1.0156, 0.5312, 1.0547,\n",
      "        0.9727, 0.9258, 0.8867, 1.0547, 0.9375, 1.0469, 0.6797, 1.0859, 1.0625,\n",
      "        0.8242, 1.0781, 1.1016, 0.9141, 1.0625, 1.0938, 1.0547, 1.1797, 0.8516,\n",
      "        1.0625, 1.0469, 0.9141, 0.9609, 1.0078, 0.8672, 0.9062, 0.9766, 1.0234,\n",
      "        0.9609, 1.0078, 1.0000, 0.9062, 0.9805, 0.8867, 1.0234, 0.9141, 0.9766,\n",
      "        0.9258, 1.0391, 0.8438, 1.0938, 0.9961, 1.0781, 0.7188, 0.9180, 1.1406,\n",
      "        0.8438, 1.0781, 0.7266, 0.8320, 0.9258, 0.9727, 1.0781, 0.6016, 0.9023,\n",
      "        0.9727, 1.0469, 0.9766, 1.1406, 1.0156, 0.9570, 0.9297, 0.9688, 0.9961,\n",
      "        0.9727, 1.0703, 0.9453, 0.9766, 0.6992, 0.9492, 0.6914, 1.1094, 0.8320,\n",
      "        1.0234, 1.0469, 1.0000, 0.8398, 0.9766, 0.8633, 0.8711, 1.0859, 0.6133,\n",
      "        1.0234, 0.7891, 0.6719, 1.1484, 1.0859, 1.0000, 0.3887, 1.0156, 1.0625,\n",
      "        0.9570, 1.1172, 0.8750, 0.4492, 1.0156, 0.9297, 1.1094, 0.8711, 1.0234,\n",
      "        0.6836, 0.8516, 1.0156, 0.9180, 0.8516, 1.0078, 1.0469, 0.5938, 1.0547,\n",
      "        0.4277, 1.1562, 0.8867, 1.0156, 1.6328, 0.9102, 0.9180, 0.9141, 1.0703,\n",
      "        0.8750, 1.0859, 0.9297, 1.0391, 0.9844, 0.7031, 1.1094, 1.0703, 1.0312,\n",
      "        1.0234, 0.9609, 1.0391, 1.0781, 1.0234, 1.0234, 1.0234, 0.9805, 0.9961,\n",
      "        1.1094, 0.9688, 1.1250, 1.0859, 0.5234, 0.9648, 0.6836, 0.9258, 1.1016,\n",
      "        0.3711, 1.0312, 1.0547, 0.9453, 1.0625, 0.9648, 0.8945, 0.9180, 1.1016,\n",
      "        0.4727, 1.0938, 0.6953, 1.0625, 0.9961, 1.0391, 0.6602, 0.9414, 0.9805,\n",
      "        0.8164, 0.8438, 0.6172, 1.0312, 0.9180, 1.0625, 0.9531, 0.8125, 0.9727,\n",
      "        0.9570, 1.0625, 0.9375, 1.0312, 0.9062, 0.8984, 0.9062, 0.9805, 0.9180,\n",
      "        1.0859, 1.0859, 1.0391, 1.0703, 0.9531, 0.8984, 1.0156, 1.0000, 0.9805,\n",
      "        1.0547, 1.0781, 1.0859, 1.1016, 0.9023, 1.0703, 0.9922, 1.0781, 1.1172,\n",
      "        1.0469, 0.4375, 0.8984, 0.8711, 1.1797, 1.0234, 1.0625, 0.8477, 1.1016,\n",
      "        1.0703, 0.8828, 1.0938, 0.9531, 1.0078, 0.9844, 1.0781, 1.0781, 0.9375,\n",
      "        0.9336, 0.9531, 0.9883, 1.0391, 1.0859, 1.0312, 0.8516, 0.9023, 1.0234,\n",
      "        1.0312, 0.9531, 0.9531, 0.8242, 1.1328, 0.9492, 0.8516, 1.1172, 1.1641,\n",
      "        1.1172, 0.9375, 1.1016, 0.9141, 0.9844, 0.6016, 0.9648, 1.1094, 0.9531,\n",
      "        1.0547, 1.0391, 1.0078, 0.9570, 0.8750, 0.6016, 1.0469, 0.8711, 0.7578,\n",
      "        0.9883, 0.9531, 0.9062, 0.9219, 0.9141, 0.9531, 1.0078, 0.8438, 0.9531,\n",
      "        0.9062, 0.9727, 0.8555, 0.7578, 1.0703, 1.0625, 0.8867, 0.9609, 1.0859,\n",
      "        0.3184, 0.7773, 0.9141, 1.1562, 0.9648, 1.0391, 0.9922, 0.8984, 1.0625,\n",
      "        0.9180, 0.7031, 0.9297, 1.0625, 0.8398, 0.8203, 0.9922, 0.9727, 0.3809,\n",
      "        0.9570, 0.9727, 1.0234, 0.8906, 0.6289, 1.1484, 1.0391, 0.9570, 0.9492,\n",
      "        1.0625, 0.8086, 1.0156, 0.5859, 1.0000, 1.1562, 1.0391, 1.0547, 1.1016,\n",
      "        1.0703, 1.1250, 1.1094, 1.1406, 1.0547, 1.0469, 0.9062, 1.0391, 1.0000,\n",
      "        0.9062, 1.0078, 0.9023, 1.0469, 0.7148, 0.9648, 0.8984, 1.0234, 0.9062,\n",
      "        0.8398, 0.9609, 1.0938, 1.0156, 0.8438, 0.8594, 1.0000, 0.9180, 1.0703,\n",
      "        0.9609, 1.0312, 0.9766, 0.8945, 1.0391, 0.8164, 1.0000, 0.9453, 0.9023,\n",
      "        1.0469, 1.0703, 0.7461, 0.9102, 0.9883, 1.0000, 0.8008, 0.9336, 0.6250,\n",
      "        1.0547, 1.1250, 0.9336, 0.7930, 1.0938, 0.3965, 1.0000, 0.9609, 0.5938,\n",
      "        1.0547, 0.8359, 1.0078, 1.0391, 1.0469, 0.9570, 0.8594, 0.7891, 1.1172,\n",
      "        1.0469, 1.0312, 0.9961, 1.0312, 0.9453, 0.9375, 0.8047, 0.6094, 1.0156,\n",
      "        0.8359, 0.6484, 0.9180, 0.4805, 0.4961, 0.9453, 0.9180, 0.9727, 0.8984,\n",
      "        1.0469, 1.0625, 0.9570, 1.1094, 1.0312, 0.9805, 1.0234, 1.0078, 0.9570,\n",
      "        1.0469, 0.9961, 1.0469, 0.9883, 1.1094, 0.9609, 0.9648, 0.9883, 1.0625,\n",
      "        0.9297, 1.0312, 1.0000, 0.9453, 1.0859, 0.8984, 1.1016, 0.9180, 0.5703,\n",
      "        1.0312, 1.0391, 0.9688, 0.8164, 1.0312, 0.9961, 0.8906, 0.9375, 1.1406,\n",
      "        0.9492, 0.9766, 1.0234, 1.1016, 1.0156, 0.8828, 0.8047, 1.0469, 1.3047,\n",
      "        0.9648, 1.0000, 1.0781, 0.4609, 0.9648, 0.9492, 0.8750, 0.9727, 0.5312,\n",
      "        1.0781, 0.9375, 0.8867, 1.1250, 0.9141, 0.8555, 0.9219, 1.0234, 1.0156,\n",
      "        0.8281, 0.8750, 1.0859, 1.0391, 0.9609, 0.9648, 1.0391, 1.0156, 0.9492,\n",
      "        0.9922, 0.9844, 1.0234, 0.8633, 0.8086, 0.8438, 1.0469, 0.8984, 0.8477,\n",
      "        1.0547, 0.9180, 0.6562, 0.9492, 1.0391, 0.8594, 1.0156, 0.8945, 1.0391,\n",
      "        0.9180, 0.9844, 0.9219, 0.9062, 0.8672, 1.0156, 0.9453, 1.0781, 0.8906,\n",
      "        0.8633, 1.0547, 0.8906], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.encoder.final_layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([0.3516, 0.3203, 0.3086, 0.3281, 0.5000, 0.3789, 0.0603, 0.3613, 0.1797,\n",
      "        0.4355, 0.3320, 0.2949, 0.3359, 0.2988, 0.4863, 0.4199, 0.3594, 0.3672,\n",
      "        0.1865, 0.4551, 0.4160, 0.4043, 0.1680, 0.4512, 0.4746, 0.3770, 0.4785,\n",
      "        0.3535, 0.3730, 0.4766, 0.4121, 0.3789, 0.3926, 0.4785, 0.3242, 0.4199,\n",
      "        0.2520, 0.2930, 0.5000, 0.1592, 0.3574, 0.2891, 0.2305, 0.3945, 0.2080,\n",
      "        0.4160, 0.2539, 0.4141, 0.3691, 0.1045, 0.3027, 0.3848, 0.3105, 0.4648,\n",
      "        0.0859, 0.3926, 0.1553, 0.4004, 0.4746, 0.4512, 0.3906, 0.5078, 0.4160,\n",
      "        0.4590, 0.4707, 0.0430, 0.4297, 0.4258, 0.3242, 0.4531, 0.4238, 0.1973,\n",
      "        0.4727, 0.4102, 0.4473, 0.4609, 0.2793, 0.4004, 0.4746, 0.4102, 0.4688,\n",
      "        0.3789, 0.4492, 0.3379, 0.3984, 0.4082, 0.3262, 0.3789, 0.4043, 0.4902,\n",
      "        0.3926, 0.4668, 0.4414, 0.3730, 0.4805, 0.4902, 0.4473, 0.4609, 0.4824,\n",
      "        0.4355, 0.3086, 0.4160, 0.2080, 0.4844, 0.4180, 0.2266, 0.4297, 0.4492,\n",
      "        0.4688, 0.2070, 0.3789, 0.4414, 0.3945, 0.4180, 0.4121, 0.4746, 0.3926,\n",
      "        0.4375, 0.2275, 0.4492, 0.5156, 0.4473, 0.3008, 0.4375, 0.1885, 0.4004,\n",
      "        0.4199, 0.3711, 0.3828, 0.3242, 0.1582, 0.3613, 0.4805, 0.4590, 0.3906,\n",
      "        0.2451, 0.3730, 0.4355, 0.3633, 0.3418, 0.4023, 0.2656, 0.2539, 0.3145,\n",
      "        0.3848, 0.3281, 0.4531, 0.3242, 0.4395, 0.3828, 0.4336, 0.3809, 0.3789,\n",
      "        0.3203, 0.4805, 0.4863, 0.3438, 0.4883, 0.3164, 0.3906, 0.5000, 0.4102,\n",
      "        0.3574, 0.1777, 0.4512, 0.4023, 0.4199, 0.3320, 0.3477, 0.1680, 0.3184,\n",
      "        0.4297, 0.4492, 0.2422, 0.4316, 0.3340, 0.3848, 0.0913, 0.3223, 0.0388,\n",
      "        0.4609, 0.4668, 0.3984, 0.4980, 0.3750, 0.3535, 0.4629, 0.3027, 0.3809,\n",
      "        0.3770, 0.4238, 0.4863, 0.4551, 0.4258, 0.4590, 0.0718, 0.4199, 0.3105,\n",
      "        0.4082, 0.4590, 0.5000, 0.3691, 0.4297, 0.4316, 0.4414, 0.3340, 0.4121,\n",
      "        0.4062, 0.4512, 0.4141, 0.3750, 0.4023, 0.3496, 0.3496, 0.4238, 0.3613,\n",
      "        0.4121, 0.4355, 0.3594, 0.1904, 0.2422, 0.3867, 0.4551, 0.3828, 0.3535,\n",
      "        0.4199, 0.3164, 0.2080, 0.4062, 0.3789, 0.3633, 0.3828, 0.2578, 0.5234,\n",
      "        0.4805, 0.4102, 0.3086, 0.3496, 0.3242, 0.1758, 0.3145, 0.3652, 0.4570,\n",
      "        0.4238, 0.4395, 0.4277, 0.4238, 0.3691, 0.4961, 0.3789, 0.2266, 0.4531,\n",
      "        0.4434, 0.4824, 0.4941, 0.4297, 0.4512, 0.3574, 0.1216, 0.4180, 0.3105,\n",
      "        0.2109, 0.4688, 0.4473, 0.4355, 0.4199, 0.4238, 0.4141, 0.4258, 0.4277,\n",
      "        0.1846, 0.3203, 0.2480, 0.0388, 0.3008, 0.3750, 0.4531, 0.2178, 0.4336,\n",
      "        0.3945, 0.3301, 0.3164, 0.4512, 0.3477, 0.3945, 0.2676, 0.5117, 0.4629,\n",
      "        0.2852, 0.2197, 0.4824, 0.4043, 0.4648, 0.4766, 0.4609, 0.5117, 0.3457,\n",
      "        0.4629, 0.4668, 0.3613, 0.3672, 0.4609, 0.3105, 0.3535, 0.4082, 0.4902,\n",
      "        0.4043, 0.4473, 0.4062, 0.3652, 0.4082, 0.3496, 0.4609, 0.3164, 0.3613,\n",
      "        0.3750, 0.3066, 0.3105, 0.3730, 0.4453, 0.4766, 0.2715, 0.3789, 0.4961,\n",
      "        0.3164, 0.4453, 0.2471, 0.3223, 0.4023, 0.4199, 0.4766, 0.1758, 0.3320,\n",
      "        0.4160, 0.4531, 0.4414, 0.4785, 0.4688, 0.3789, 0.2910, 0.3867, 0.3750,\n",
      "        0.3926, 0.4375, 0.4004, 0.4004, 0.2715, 0.3945, 0.2451, 0.1953, 0.3145,\n",
      "        0.4473, 0.4004, 0.4648, 0.3359, 0.3770, 0.3730, 0.3457, 0.4922, 0.1992,\n",
      "        0.4336, 0.2812, 0.2168, 0.4785, 0.4922, 0.4414, 0.1113, 0.4102, 0.4609,\n",
      "        0.3945, 0.3789, 0.3672, 0.0869, 0.4180, 0.2988, 0.4902, 0.2148, 0.4395,\n",
      "        0.1924, 0.2852, 0.4238, 0.3809, 0.2539, 0.4316, 0.4434, 0.1846, 0.4492,\n",
      "        0.1641, 0.4922, 0.3379, 0.4414, 0.0496, 0.3516, 0.3281, 0.3398, 0.4062,\n",
      "        0.3398, 0.4844, 0.3789, 0.4648, 0.4512, 0.0640, 0.5195, 0.4512, 0.4453,\n",
      "        0.4258, 0.3691, 0.4492, 0.4590, 0.4473, 0.4492, 0.4766, 0.3984, 0.3926,\n",
      "        0.4824, 0.4023, 0.4590, 0.4727, 0.0791, 0.3223, 0.2031, 0.3887, 0.5195,\n",
      "        0.0879, 0.4082, 0.4570, 0.3613, 0.4863, 0.3848, 0.3457, 0.3535, 0.4629,\n",
      "        0.1660, 0.4648, 0.2207, 0.4512, 0.4219, 0.4414, 0.2451, 0.3516, 0.3887,\n",
      "        0.2891, 0.3438, 0.1992, 0.4570, 0.3848, 0.4707, 0.3867, 0.3086, 0.4062,\n",
      "        0.4102, 0.4785, 0.4219, 0.4707, 0.3711, 0.3496, 0.3516, 0.3945, 0.3398,\n",
      "        0.4688, 0.4902, 0.4492, 0.4531, 0.4062, 0.3906, 0.4004, 0.4141, 0.4023,\n",
      "        0.4668, 0.4668, 0.4766, 0.5000, 0.3652, 0.4902, 0.4082, 0.4609, 0.4727,\n",
      "        0.4395, 0.0588, 0.3516, 0.3496, 0.1855, 0.4258, 0.4531, 0.3184, 0.4395,\n",
      "        0.4492, 0.2402, 0.4980, 0.4023, 0.4277, 0.3613, 0.1865, 0.4844, 0.4062,\n",
      "        0.3320, 0.3730, 0.4336, 0.4492, 0.4785, 0.4551, 0.3516, 0.3145, 0.0554,\n",
      "        0.4336, 0.3730, 0.4062, 0.3203, 0.5156, 0.3809, 0.3770, 0.5000, 0.2012,\n",
      "        0.4863, 0.3652, 0.4883, 0.3867, 0.3770, 0.2246, 0.3477, 0.5195, 0.3887,\n",
      "        0.4883, 0.4238, 0.4102, 0.2041, 0.3457, 0.2041, 0.4395, 0.3594, 0.2871,\n",
      "        0.4258, 0.4004, 0.3828, 0.3691, 0.3320, 0.4023, 0.4238, 0.2969, 0.3965,\n",
      "        0.3691, 0.4492, 0.3418, 0.2773, 0.4258, 0.4766, 0.3691, 0.4023, 0.4453,\n",
      "        0.0830, 0.2520, 0.3477, 0.4883, 0.3184, 0.4707, 0.4316, 0.3711, 0.4531,\n",
      "        0.3750, 0.2334, 0.3574, 0.4395, 0.3105, 0.3105, 0.3906, 0.3965, 0.0381,\n",
      "        0.3945, 0.3750, 0.4414, 0.3691, 0.1865, 0.5117, 0.4355, 0.3926, 0.3730,\n",
      "        0.4512, 0.3164, 0.4531, 0.1943, 0.4102, 0.5039, 0.4707, 0.4258, 0.5039,\n",
      "        0.4980, 0.4668, 0.4551, 0.4941, 0.4629, 0.4395, 0.3242, 0.4707, 0.4199,\n",
      "        0.3789, 0.4277, 0.3574, 0.4102, 0.2324, 0.4199, 0.3457, 0.4531, 0.3887,\n",
      "        0.2197, 0.3789, 0.5000, 0.4551, 0.3320, 0.2715, 0.4180, 0.3750, 0.4609,\n",
      "        0.3555, 0.4629, 0.3965, 0.3242, 0.4121, 0.3086, 0.4316, 0.3926, 0.3711,\n",
      "        0.4473, 0.4473, 0.2910, 0.3828, 0.4258, 0.3887, 0.2676, 0.3965, 0.1943,\n",
      "        0.3594, 0.4141, 0.3711, 0.2910, 0.4648, 0.1445, 0.4043, 0.3809, 0.1680,\n",
      "        0.4434, 0.3379, 0.3945, 0.4492, 0.4160, 0.3535, 0.3359, 0.2949, 0.4922,\n",
      "        0.4316, 0.4844, 0.4023, 0.4434, 0.4102, 0.4023, 0.2715, 0.2383, 0.4414,\n",
      "        0.3086, 0.2500, 0.3418, 0.1533, 0.0537, 0.3926, 0.3301, 0.4141, 0.3457,\n",
      "        0.4180, 0.4746, 0.4004, 0.4648, 0.4199, 0.3438, 0.4160, 0.4160, 0.3809,\n",
      "        0.4766, 0.4199, 0.4414, 0.4258, 0.4336, 0.3984, 0.4355, 0.3828, 0.4688,\n",
      "        0.4062, 0.2217, 0.4023, 0.3691, 0.4609, 0.3906, 0.4766, 0.3711, 0.2100,\n",
      "        0.4629, 0.4434, 0.4121, 0.3047, 0.4688, 0.4082, 0.3711, 0.3574, 0.4746,\n",
      "        0.3906, 0.3789, 0.3984, 0.5000, 0.4531, 0.3125, 0.2969, 0.4629, 0.1816,\n",
      "        0.4062, 0.3984, 0.4609, 0.1094, 0.3711, 0.3477, 0.3398, 0.4121, 0.1865,\n",
      "        0.4648, 0.3203, 0.3281, 0.4590, 0.3809, 0.3594, 0.3770, 0.4297, 0.4648,\n",
      "        0.3281, 0.3516, 0.4473, 0.4395, 0.3906, 0.4258, 0.4375, 0.4023, 0.3848,\n",
      "        0.4160, 0.3887, 0.3887, 0.3047, 0.3320, 0.3555, 0.4453, 0.3438, 0.3438,\n",
      "        0.4844, 0.3535, 0.1934, 0.3828, 0.4219, 0.3027, 0.4062, 0.3379, 0.4336,\n",
      "        0.3984, 0.4375, 0.3438, 0.3633, 0.3613, 0.3574, 0.3789, 0.4863, 0.3438,\n",
      "        0.3379, 0.4375, 0.3438], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.0.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0493,  0.0103,  0.0942,  ..., -0.0398, -0.0723, -0.0258],\n",
      "        [-0.0908,  0.0181,  0.0211,  ...,  0.0003, -0.0170,  0.0240],\n",
      "        [-0.0047, -0.0386,  0.0008,  ...,  0.0654, -0.0078, -0.0256],\n",
      "        ...,\n",
      "        [ 0.0991,  0.0149,  0.0981,  ...,  0.0233, -0.0019,  0.0157],\n",
      "        [-0.0496,  0.0835,  0.0215,  ...,  0.0947, -0.0469, -0.0825],\n",
      "        [-0.0420,  0.1221,  0.0505,  ...,  0.0096, -0.0356, -0.0942]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.0.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1738,  0.2246, -0.1602,  ..., -0.1377, -0.4941,  0.3379],\n",
      "        [ 0.2578, -0.0986, -0.0427,  ..., -0.1836,  0.3086, -0.0320],\n",
      "        [-0.0645,  0.0879,  0.2930,  ...,  0.2598, -0.7500,  0.0630],\n",
      "        ...,\n",
      "        [ 0.4805,  0.2090, -0.0015,  ...,  0.6992, -0.2354, -0.1235],\n",
      "        [-0.1050, -0.2793,  0.1123,  ..., -0.6094, -0.3496, -0.2363],\n",
      "        [-0.4727, -0.1533,  0.1963,  ..., -0.1816, -0.3203, -0.7031]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.0.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 1.2756e-02, -1.7188e-01, -6.0303e-02,  ..., -3.1055e-01,\n",
      "         -2.3242e-01, -6.1768e-02],\n",
      "        [ 4.9023e-01,  3.6328e-01, -1.6895e-01,  ...,  7.5989e-03,\n",
      "         -1.6992e-01,  2.9883e-01],\n",
      "        [-2.8516e-01, -1.2512e-02, -3.9551e-02,  ..., -2.3438e-01,\n",
      "          1.2939e-02, -3.5547e-01],\n",
      "        ...,\n",
      "        [ 8.6426e-02, -3.6523e-01,  3.9648e-01,  ...,  3.6914e-01,\n",
      "          3.0664e-01,  6.8848e-02],\n",
      "        [ 7.0496e-03, -4.3164e-01,  1.2158e-01,  ..., -3.1738e-02,\n",
      "          2.0801e-01,  3.2422e-01],\n",
      "        [-4.1748e-02,  2.1777e-01, -2.0703e-01,  ...,  6.8843e-06,\n",
      "         -2.0605e-01,  3.9648e-01]], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.0.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-2.9663e-02, -9.1797e-02, -1.7334e-02,  ..., -9.2773e-02,\n",
      "         -1.6797e-01,  1.7773e-01],\n",
      "        [ 1.4771e-02, -6.6016e-01,  5.0781e-01,  ..., -9.4922e-01,\n",
      "         -4.0430e-01, -8.7891e-01],\n",
      "        [ 8.7891e-01,  1.3281e+00, -2.5781e-01,  ..., -2.0117e-01,\n",
      "         -8.1250e-01, -4.3945e-01],\n",
      "        ...,\n",
      "        [ 2.5586e-01,  1.1719e-01,  1.1719e-01,  ...,  7.0801e-02,\n",
      "         -4.9316e-02,  4.2383e-01],\n",
      "        [-9.6680e-02, -2.1777e-01, -3.1494e-02,  ..., -5.9326e-02,\n",
      "          7.4219e-02, -3.0664e-01],\n",
      "        [ 9.1934e-04,  2.0801e-01, -7.1716e-03,  ..., -5.3516e-01,\n",
      "         -7.1777e-02, -6.4697e-03]], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "Parameter containing:\n",
      "tensor([[ 2.4062e+00,  3.9375e+00,  2.0469e+00, -2.2000e+01,  1.6484e+00,\n",
      "          1.7031e+00,  2.5000e+00,  7.1484e-01,  4.1875e+00,  2.4844e+00,\n",
      "          3.4062e+00,  3.7344e+00],\n",
      "        [ 3.3125e+00,  3.4531e+00,  4.4062e+00,  6.7578e-01,  1.0859e+00,\n",
      "          2.5938e+00,  5.3750e+00,  2.3750e+00,  3.9688e+00,  2.7344e+00,\n",
      "          2.9375e+00,  3.6875e+00],\n",
      "        [ 2.7812e+00,  2.0312e+00,  3.0469e+00,  1.0781e+00,  9.2578e-01,\n",
      "          1.8672e+00,  2.0625e+00,  2.1719e+00,  1.6172e+00,  2.0312e+00,\n",
      "          2.0312e+00,  1.6797e+00],\n",
      "        [ 2.3281e+00,  1.1484e+00,  2.0156e+00,  1.2109e+00,  7.4219e-01,\n",
      "          1.4375e+00,  2.9102e-01,  1.9766e+00,  5.6250e-01,  1.4688e+00,\n",
      "          1.3984e+00,  3.6328e-01],\n",
      "        [ 1.9219e+00,  5.3906e-01,  1.1797e+00,  1.2812e+00,  6.0156e-01,\n",
      "          1.1562e+00, -7.5391e-01,  1.8125e+00, -3.5156e-02,  1.0156e+00,\n",
      "          9.5312e-01, -7.9688e-01],\n",
      "        [ 1.5859e+00,  2.2705e-02,  5.3516e-01,  1.3047e+00,  5.0781e-01,\n",
      "          8.5156e-01, -1.2969e+00,  1.6406e+00, -3.2031e-01,  6.2109e-01,\n",
      "          5.3125e-01, -1.6797e+00],\n",
      "        [ 1.3125e+00, -2.3633e-01, -1.0596e-01,  1.3359e+00,  3.8086e-01,\n",
      "          6.7188e-01, -1.6406e+00,  1.4844e+00, -6.3672e-01,  2.5391e-01,\n",
      "          2.2754e-01, -2.3906e+00],\n",
      "        [ 1.0703e+00, -5.8984e-01, -6.7969e-01,  1.3281e+00,  3.2227e-01,\n",
      "          5.2734e-01, -1.8281e+00,  1.2812e+00, -8.9453e-01, -1.3281e-01,\n",
      "          3.0518e-03, -3.0156e+00],\n",
      "        [ 8.0859e-01, -8.2031e-01, -1.0312e+00,  1.3281e+00,  2.4121e-01,\n",
      "          4.3164e-01, -2.0156e+00,  1.1797e+00, -1.0859e+00, -5.1172e-01,\n",
      "         -2.3047e-01, -3.3594e+00],\n",
      "        [ 5.3125e-01, -1.0312e+00, -1.4531e+00,  1.3047e+00,  1.4258e-01,\n",
      "          3.0273e-01, -2.1875e+00,  1.0938e+00, -1.1797e+00, -7.8906e-01,\n",
      "         -4.2969e-01, -3.7344e+00],\n",
      "        [ 3.0078e-01, -1.1250e+00, -1.8047e+00,  1.2969e+00,  1.0010e-01,\n",
      "          1.7773e-01, -2.2500e+00,  9.6875e-01, -1.3984e+00, -1.1328e+00,\n",
      "         -5.6641e-01, -4.0625e+00],\n",
      "        [-8.3984e-01, -1.3125e+00, -2.0938e+00,  1.3359e+00, -5.3955e-02,\n",
      "          7.6172e-02, -2.3125e+00,  8.6719e-01, -1.4297e+00, -3.2617e-01,\n",
      "         -6.7188e-01, -4.1562e+00],\n",
      "        [-1.9922e-01, -1.3984e+00, -2.2188e+00,  1.2812e+00, -1.6479e-02,\n",
      "          1.0864e-02, -2.4219e+00,  7.6172e-01, -1.5078e+00, -1.2422e+00,\n",
      "         -8.3203e-01, -4.2500e+00],\n",
      "        [-4.7852e-01, -1.4844e+00, -2.3906e+00,  1.3125e+00, -4.9805e-02,\n",
      "         -1.0352e-01, -2.3906e+00,  6.7578e-01, -1.5547e+00, -1.4375e+00,\n",
      "         -8.4766e-01, -4.4062e+00],\n",
      "        [-6.9141e-01, -1.5938e+00, -2.5938e+00,  1.2734e+00, -8.0566e-02,\n",
      "         -1.5625e-01, -2.4688e+00,  6.1328e-01, -1.6016e+00, -1.6797e+00,\n",
      "         -9.8828e-01, -4.5625e+00],\n",
      "        [-8.0859e-01, -1.7656e+00, -2.7656e+00,  1.2812e+00, -1.9043e-01,\n",
      "         -2.1680e-01, -2.5938e+00,  5.4297e-01, -1.6406e+00, -1.7500e+00,\n",
      "         -1.1328e+00, -4.6250e+00],\n",
      "        [-1.1484e+00, -1.7578e+00, -2.9375e+00,  1.1953e+00, -2.1484e-01,\n",
      "         -2.9883e-01, -2.5781e+00,  3.7109e-01, -1.7109e+00, -2.0781e+00,\n",
      "         -1.2266e+00, -4.7188e+00],\n",
      "        [-1.5547e+00, -1.9062e+00, -3.1562e+00,  1.1406e+00, -3.5352e-01,\n",
      "         -3.6914e-01, -2.5781e+00,  1.7969e-01, -1.7812e+00, -2.3594e+00,\n",
      "         -1.3281e+00, -4.8438e+00],\n",
      "        [-1.7891e+00, -1.9609e+00, -3.2812e+00,  1.0859e+00, -3.5547e-01,\n",
      "         -4.6680e-01, -2.6562e+00,  7.4707e-02, -1.8594e+00, -2.5156e+00,\n",
      "         -1.4375e+00, -4.9688e+00],\n",
      "        [-2.0000e+00, -2.0469e+00, -3.3906e+00,  1.0078e+00, -4.1992e-01,\n",
      "         -5.3516e-01, -2.6094e+00, -6.5613e-03, -1.8750e+00, -2.7188e+00,\n",
      "         -1.5391e+00, -5.0312e+00],\n",
      "        [-2.1719e+00, -2.1406e+00, -3.4219e+00,  9.5312e-01, -5.3516e-01,\n",
      "         -6.8750e-01, -2.6406e+00, -1.7285e-01, -1.8906e+00, -2.8750e+00,\n",
      "         -1.5859e+00, -5.0000e+00],\n",
      "        [-2.3594e+00, -2.1875e+00, -3.5312e+00,  8.5156e-01, -6.0156e-01,\n",
      "         -6.9922e-01, -2.6094e+00, -2.7734e-01, -1.9844e+00, -2.9844e+00,\n",
      "         -1.6172e+00, -5.1250e+00],\n",
      "        [-2.4844e+00, -2.1719e+00, -3.5625e+00,  7.2266e-01, -6.2109e-01,\n",
      "         -7.6172e-01, -2.5625e+00, -4.1406e-01, -1.9297e+00, -3.0781e+00,\n",
      "         -1.7500e+00, -5.0625e+00],\n",
      "        [-2.6875e+00, -2.2500e+00, -3.6094e+00,  5.5078e-01, -6.4844e-01,\n",
      "         -8.0859e-01, -2.5156e+00, -5.7422e-01, -1.8672e+00, -3.1406e+00,\n",
      "         -1.6484e+00, -4.9688e+00],\n",
      "        [-2.6875e+00, -2.1562e+00, -3.5625e+00,  3.6523e-01, -6.8750e-01,\n",
      "         -8.2422e-01, -2.5156e+00, -5.6250e-01, -1.8906e+00, -3.1250e+00,\n",
      "         -1.6562e+00, -4.9375e+00],\n",
      "        [-2.7188e+00, -2.1406e+00, -3.5312e+00,  1.7285e-01, -7.0703e-01,\n",
      "         -8.1250e-01, -2.4531e+00, -6.8750e-01, -1.8203e+00, -2.7969e+00,\n",
      "         -1.6328e+00, -4.8125e+00],\n",
      "        [-2.7188e+00, -2.0625e+00, -3.3438e+00, -7.5684e-02, -7.4219e-01,\n",
      "         -7.8125e-01, -2.4375e+00, -7.9297e-01, -1.7500e+00, -2.3750e+00,\n",
      "         -1.5469e+00, -4.6250e+00],\n",
      "        [-2.6250e+00, -1.9609e+00, -3.1875e+00, -3.6914e-01, -7.2266e-01,\n",
      "         -7.0703e-01, -2.3281e+00, -8.1641e-01, -1.6719e+00, -1.6719e+00,\n",
      "         -1.4219e+00, -4.3438e+00],\n",
      "        [-2.4844e+00, -1.8438e+00, -2.9844e+00, -6.9922e-01, -7.1875e-01,\n",
      "         -6.1328e-01, -2.2656e+00, -8.5156e-01, -1.5781e+00, -6.4453e-01,\n",
      "         -1.2812e+00, -4.1250e+00],\n",
      "        [-2.2344e+00, -1.6250e+00, -2.6719e+00, -1.0078e+00, -7.4219e-01,\n",
      "         -4.2578e-01, -2.1406e+00, -8.0859e-01, -1.5234e+00,  9.4141e-01,\n",
      "         -1.0469e+00, -3.7031e+00],\n",
      "        [-1.6953e+00, -1.3047e+00, -2.0156e+00, -1.4375e+00, -6.7969e-01,\n",
      "         -1.5137e-01, -1.8516e+00, -7.5000e-01, -1.1719e+00,  4.6875e+00,\n",
      "         -6.2891e-01, -2.9375e+00],\n",
      "        [ 7.6250e+00,  3.2750e+01,  2.8750e+01, -3.6250e+01,  3.2750e+01,\n",
      "         -2.7625e+01, -3.6750e+01, -3.0875e+01, -4.0750e+01, -2.6375e+01,\n",
      "         -2.5000e+01,  3.1375e+01]], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.0.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([0.0986, 0.1006, 0.0986, 0.0977, 0.0908, 0.0977, 0.0933, 0.1099, 0.0986,\n",
      "        0.1064, 0.1001, 0.1104, 0.1650, 0.1309, 0.0874, 0.0977, 0.1118, 0.1001,\n",
      "        0.1309, 0.0933, 0.0947, 0.0967, 0.1484, 0.0903, 0.0889, 0.1177, 0.0933,\n",
      "        0.1406, 0.1094, 0.0923, 0.1006, 0.0518, 0.1089, 0.0898, 0.1260, 0.1055,\n",
      "        0.1133, 0.0942, 0.0884, 0.1299, 0.1245, 0.1138, 0.1089, 0.1235, 0.1060,\n",
      "        0.0981, 0.1309, 0.0972, 0.1035, 0.1108, 0.1357, 0.1035, 0.1147, 0.0952,\n",
      "        0.1768, 0.1060, 0.1387, 0.1045, 0.1050, 0.0957, 0.1118, 0.0869, 0.1060,\n",
      "        0.0991, 0.0952, 0.0718, 0.0962, 0.1064, 0.2354, 0.0801, 0.0981, 0.1270,\n",
      "        0.0962, 0.0938, 0.0942, 0.0947, 0.1279, 0.0981, 0.0864, 0.1216, 0.0996,\n",
      "        0.1094, 0.1040, 0.1357, 0.0996, 0.1030, 0.1025, 0.1035, 0.1035, 0.0879,\n",
      "        0.0986, 0.0894, 0.0903, 0.1045, 0.0928, 0.0972, 0.1006, 0.0996, 0.0903,\n",
      "        0.0903, 0.1250, 0.0972, 0.1211, 0.0850, 0.0928, 0.1768, 0.1030, 0.1035,\n",
      "        0.0908, 0.1230, 0.1152, 0.1094, 0.0981, 0.0903, 0.0933, 0.0938, 0.0986,\n",
      "        0.0957, 0.2148, 0.0996, 0.0830, 0.0894, 0.1514, 0.0947, 0.1074, 0.0996,\n",
      "        0.1079, 0.0967, 0.1064, 0.1338, 0.0811, 0.1035, 0.0933, 0.0869, 0.0549,\n",
      "        0.1309, 0.1055, 0.1133, 0.1069, 0.1016, 0.1064, 0.1152, 0.1299, 0.1777,\n",
      "        0.1128, 0.1152, 0.0913, 0.1260, 0.0898, 0.0933, 0.0967, 0.0952, 0.1064,\n",
      "        0.1191, 0.0908, 0.0918, 0.1138, 0.0864, 0.0439, 0.0928, 0.0850, 0.1025,\n",
      "        0.1094, 0.1602, 0.0771, 0.0767, 0.1016, 0.1289, 0.0918, 0.1177, 0.2090,\n",
      "        0.0986, 0.1045, 0.1270, 0.0967, 0.1108, 0.1025, 0.1328, 0.1396, 0.0957,\n",
      "        0.0991, 0.0898, 0.0991, 0.0981, 0.1475, 0.1494, 0.0913, 0.1006, 0.0957,\n",
      "        0.1069, 0.0913, 0.0986, 0.0815, 0.0957, 0.0801, 0.0859, 0.0942, 0.1104,\n",
      "        0.1084, 0.0854, 0.0928, 0.0986, 0.1094, 0.0928, 0.0952, 0.1270, 0.1060,\n",
      "        0.1025, 0.1045, 0.1084, 0.1182, 0.0962, 0.1104, 0.1021, 0.0928, 0.1143,\n",
      "        0.0918, 0.0879, 0.1079, 0.0903, 0.1104, 0.0996, 0.0693, 0.1016, 0.1050,\n",
      "        0.1001, 0.1416, 0.1162, 0.0986, 0.1245, 0.0972, 0.1016, 0.1025, 0.0898,\n",
      "        0.0942, 0.0703, 0.1235, 0.1074, 0.0801, 0.1157, 0.1523, 0.1147, 0.0962,\n",
      "        0.1006, 0.0889, 0.0947, 0.1045, 0.1079, 0.0786, 0.1040, 0.1089, 0.0913,\n",
      "        0.0942, 0.0947, 0.0903, 0.0957, 0.0962, 0.0996, 0.1074, 0.0923, 0.1299,\n",
      "        0.1318, 0.0938, 0.1035, 0.0962, 0.0972, 0.0820, 0.0991, 0.0972, 0.0962,\n",
      "        0.0884, 0.1416, 0.1147, 0.0938, 0.2520, 0.1079, 0.0972, 0.1177, 0.1016,\n",
      "        0.1001, 0.1152, 0.1904, 0.1011, 0.1118, 0.1060, 0.1191, 0.0938, 0.0889,\n",
      "        0.0894, 0.1289, 0.0918, 0.1030, 0.0830, 0.1001, 0.1045, 0.0977, 0.1069,\n",
      "        0.1006, 0.0840, 0.1201, 0.1245, 0.0957, 0.0557, 0.1060, 0.1152, 0.0918,\n",
      "        0.1025, 0.0947, 0.1133, 0.1045, 0.0938, 0.1270, 0.0874, 0.1143, 0.1260,\n",
      "        0.0991, 0.1133, 0.1250, 0.0967, 0.0898, 0.0874, 0.1191, 0.0986, 0.0850,\n",
      "        0.1157, 0.0923, 0.1099, 0.1523, 0.1055, 0.0981, 0.1045, 0.1064, 0.1055,\n",
      "        0.0938, 0.1030, 0.0952, 0.0928, 0.0923, 0.0952, 0.2070, 0.1157, 0.1167,\n",
      "        0.1079, 0.0933, 0.1069, 0.0967, 0.1104, 0.1030, 0.1191, 0.1016, 0.1260,\n",
      "        0.0898, 0.0977, 0.0977, 0.1099, 0.1055, 0.1084, 0.4141, 0.0972, 0.1162,\n",
      "        0.0952, 0.1162, 0.0481, 0.0786, 0.1006, 0.1025, 0.1318, 0.1035, 0.0864,\n",
      "        0.0942, 0.1104, 0.1196, 0.1128, 0.1006, 0.1143, 0.0894, 0.1104, 0.0898,\n",
      "        0.1279, 0.1338, 0.0981, 0.1030, 0.1768, 0.1006, 0.0957, 0.1216, 0.0923,\n",
      "        0.1211, 0.0791, 0.1123, 0.1001, 0.1113, 0.0967, 0.2031, 0.1289, 0.1050,\n",
      "        0.1040, 0.0889, 0.1006, 0.0972, 0.0781, 0.1055, 0.0933, 0.1016, 0.0869,\n",
      "        0.1030, 0.1260, 0.0996, 0.0879, 0.0864, 0.0942, 0.0884, 0.1118, 0.1089,\n",
      "        0.0957, 0.1069, 0.1006, 0.0923, 0.0796, 0.1172, 0.1436, 0.1001, 0.0928,\n",
      "        0.1904, 0.0928, 0.0928, 0.1196, 0.0903, 0.1025, 0.0991, 0.1157, 0.0947,\n",
      "        0.1191, 0.0874, 0.1221, 0.1011, 0.1021, 0.0894, 0.1133, 0.1289, 0.1104,\n",
      "        0.1348, 0.0996, 0.1177, 0.0938, 0.1108, 0.0898, 0.1050, 0.4199, 0.0991,\n",
      "        0.1050, 0.1001, 0.1011, 0.0942, 0.1055, 0.1191, 0.1904, 0.1040, 0.1118,\n",
      "        0.0835, 0.0952, 0.0889, 0.1152, 0.1064, 0.1035, 0.1099, 0.1074, 0.1021,\n",
      "        0.0825, 0.0913, 0.0898, 0.0923, 0.0649, 0.0874, 0.0991, 0.1001, 0.1050,\n",
      "        0.1016, 0.1055, 0.1035, 0.0850, 0.1196, 0.1128, 0.0928, 0.1387, 0.0957,\n",
      "        0.0928, 0.0747, 0.0903, 0.1055, 0.0996, 0.1016, 0.1113, 0.0991, 0.1094,\n",
      "        0.0923, 0.0977, 0.1089, 0.0918, 0.0952, 0.0879, 0.1250, 0.1069, 0.1074,\n",
      "        0.1016, 0.1270, 0.1045, 0.1089, 0.0825, 0.1104, 0.1084, 0.0869, 0.0957,\n",
      "        0.0991, 0.1128, 0.0938, 0.0942, 0.0981, 0.1992, 0.1108, 0.0942, 0.0947,\n",
      "        0.0854, 0.1138, 0.1006, 0.1172, 0.1187, 0.1396, 0.0942, 0.1157, 0.1172,\n",
      "        0.1001, 0.1040, 0.0981, 0.0996, 0.0991, 0.1074, 0.0938, 0.1118, 0.1021,\n",
      "        0.1216, 0.0947, 0.1113, 0.1260, 0.1035, 0.0923, 0.1182, 0.0967, 0.0938,\n",
      "        0.1128, 0.1465, 0.2197, 0.0840, 0.2451, 0.0952, 0.1050, 0.1177, 0.0889,\n",
      "        0.1079, 0.1108, 0.1089, 0.0986, 0.1367, 0.1611, 0.1060, 0.1543, 0.1055,\n",
      "        0.1045, 0.1216, 0.0957, 0.1011, 0.1113, 0.0898, 0.1045, 0.1050, 0.1196,\n",
      "        0.0918, 0.1221, 0.0928, 0.1436, 0.0996, 0.0903, 0.0952, 0.0962, 0.0815,\n",
      "        0.0884, 0.0879, 0.1021, 0.0918, 0.0962, 0.0986, 0.1118, 0.0938, 0.0977,\n",
      "        0.0981, 0.1060, 0.1172, 0.0947, 0.2598, 0.1006, 0.1113, 0.0928, 0.1060,\n",
      "        0.0898, 0.1172, 0.0913, 0.0884, 0.0562, 0.1182, 0.1089, 0.1025, 0.0942,\n",
      "        0.1030, 0.0928, 0.0962, 0.1177, 0.0991, 0.0679, 0.0933, 0.1040, 0.1094,\n",
      "        0.0894, 0.0898, 0.1318, 0.1074, 0.0913, 0.0977, 0.1128, 0.0962, 0.1138,\n",
      "        0.1021, 0.0923, 0.1196, 0.1729, 0.0952, 0.1143, 0.0991, 0.1094, 0.1187,\n",
      "        0.0942, 0.0952, 0.1035, 0.0879, 0.1055, 0.1143, 0.0991, 0.0967, 0.0903,\n",
      "        0.0776, 0.1050, 0.0352, 0.0996, 0.0996, 0.1167, 0.1133, 0.1416, 0.1001,\n",
      "        0.0654, 0.0981, 0.1147, 0.1226, 0.1079, 0.0977, 0.2129, 0.0977, 0.1123,\n",
      "        0.0952, 0.0981, 0.0972, 0.0972, 0.0972, 0.1084, 0.0972, 0.0898, 0.0962,\n",
      "        0.0991, 0.0991, 0.0952, 0.1050, 0.1030, 0.0967, 0.0928, 0.1104, 0.0825,\n",
      "        0.0933, 0.1011, 0.0947, 0.1069, 0.0908, 0.1104, 0.0962, 0.1064, 0.1582,\n",
      "        0.0908, 0.0996, 0.1011, 0.1211, 0.0903, 0.1094, 0.1084, 0.1021, 0.0903,\n",
      "        0.1040, 0.0413, 0.1050, 0.0864, 0.0962, 0.1738, 0.1338, 0.0928, 0.1177,\n",
      "        0.0967, 0.0400, 0.0898, 0.1641, 0.0947, 0.1084, 0.1250, 0.0928, 0.1260,\n",
      "        0.0996, 0.1035, 0.1748, 0.0952, 0.1099, 0.0938, 0.1016, 0.0928, 0.0996,\n",
      "        0.1182, 0.1030, 0.0996, 0.0977, 0.1035, 0.0957, 0.0996, 0.1025, 0.0967,\n",
      "        0.0986, 0.1191, 0.0991, 0.1201, 0.1128, 0.1738, 0.0962, 0.1226, 0.1025,\n",
      "        0.0889, 0.1367, 0.1040, 0.0972, 0.0942, 0.1387, 0.1079, 0.1138, 0.0981,\n",
      "        0.1196, 0.0923, 0.1216, 0.1113, 0.1094, 0.0820, 0.0942, 0.0859, 0.0986,\n",
      "        0.1045, 0.0933, 0.1226], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.0.layer.1.EncDecAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0510, -0.0103,  0.0620,  ..., -0.0156, -0.0210, -0.0513],\n",
      "        [-0.0053,  0.0141, -0.0488,  ..., -0.0036, -0.0044, -0.0030],\n",
      "        [-0.0830,  0.0085, -0.0010,  ...,  0.0182,  0.0339,  0.0527],\n",
      "        ...,\n",
      "        [ 0.0129,  0.0022, -0.0737,  ..., -0.0334,  0.0160,  0.0022],\n",
      "        [ 0.0515,  0.0214, -0.0188,  ...,  0.0410,  0.0498,  0.0044],\n",
      "        [-0.0596,  0.0159, -0.0112,  ...,  0.0156, -0.0820,  0.0256]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.0.layer.1.EncDecAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.2988,  0.0957, -0.0396,  ...,  0.1465,  0.1514, -0.1816],\n",
      "        [ 0.0957, -0.0732,  0.3008,  ..., -0.4434,  0.3711, -0.7109],\n",
      "        [ 0.3008, -0.2451, -0.1631,  ..., -0.1904, -0.0569, -0.1738],\n",
      "        ...,\n",
      "        [-0.0894, -0.1885, -0.0354,  ...,  0.1592,  0.3574,  0.0371],\n",
      "        [ 0.0547, -0.0820,  0.2041,  ...,  0.1592, -0.0947, -0.0164],\n",
      "        [-0.2480,  0.3633,  0.3262,  ...,  0.2578,  0.0933, -0.1680]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.0.layer.1.EncDecAttention.v.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1079, -0.0205, -0.1611,  ..., -0.2637,  0.2891,  0.5078],\n",
      "        [ 0.5234, -0.0415,  0.2754,  ..., -0.2461,  0.0265,  0.2520],\n",
      "        [ 0.2178,  0.3945, -0.1143,  ..., -0.1035,  0.3125, -0.2178],\n",
      "        ...,\n",
      "        [ 0.0981, -0.0078,  0.2734,  ...,  0.3105, -0.0913,  0.0894],\n",
      "        [-0.1309,  0.0859,  0.2969,  ...,  0.1289,  0.1064, -0.0581],\n",
      "        [-0.0874,  0.3125, -0.1533,  ..., -0.2461, -0.0952, -0.2275]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.0.layer.1.EncDecAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1187, -0.2188, -0.2305,  ...,  0.0669,  0.2246,  0.3125],\n",
      "        [-0.9453,  1.1875,  0.1533,  ...,  1.7969, -0.5039, -0.7578],\n",
      "        [-1.3125,  0.2539, -0.1641,  ...,  1.7969,  0.6719,  1.8203],\n",
      "        ...,\n",
      "        [ 0.4902, -0.1592,  0.2695,  ...,  0.1069,  0.4551,  0.0830],\n",
      "        [-0.6602, -0.2109, -0.0574,  ..., -0.3379, -0.4355,  0.0923],\n",
      "        [ 0.0991,  0.0610,  0.2812,  ...,  0.5039,  0.3418,  0.2480]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.0.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 0.0703,  0.0469,  0.0403,  0.1387,  0.0618,  0.0703,  0.0767,  0.0811,\n",
      "         0.0771,  0.0654,  0.0830,  0.0732,  0.0776,  0.0898,  0.0659,  0.0674,\n",
      "         0.0747,  0.0723,  0.0894,  0.0708,  0.0630,  0.0654,  0.0933,  0.0684,\n",
      "         0.0630,  0.0688,  0.0635,  0.0767,  0.0713,  0.0664,  0.0654,  0.0403,\n",
      "         0.0732,  0.0654,  0.0801,  0.0708,  0.0752,  0.0645,  0.0688,  0.0791,\n",
      "         0.0967,  0.0811,  0.0781,  0.0742,  0.0503,  0.0713,  0.0903,  0.0693,\n",
      "         0.0723,  0.0859,  0.0967,  0.0693,  0.0791,  0.0693,  0.4902,  0.0723,\n",
      "         0.1099,  0.0732,  0.0645,  0.0703,  0.0713,  0.0591,  0.0732,  0.0640,\n",
      "         0.0659,  0.0398,  0.0679,  0.0723,  0.0596,  0.0654,  0.0688,  0.0767,\n",
      "         0.0588,  0.0713,  0.0601,  0.0664,  0.0840,  0.0684,  0.0613,  0.0679,\n",
      "         0.0684,  0.0747,  0.0693,  0.0874,  0.0703,  0.0623,  0.1699,  0.0776,\n",
      "         0.0698,  0.0679,  0.0698,  0.0608,  0.0698,  0.0811,  0.0654,  0.0635,\n",
      "         0.0669,  0.0703,  0.0684,  0.0630,  0.0820,  0.0640,  0.0398,  0.0615,\n",
      "         0.0737,  0.1060,  0.0693,  0.0723,  0.0645,  0.0752,  0.0767,  0.0781,\n",
      "         0.0742,  0.0684,  0.0645,  0.0630,  0.0713,  0.0732,  0.0168,  0.0659,\n",
      "         0.0635,  0.0640,  0.0811,  0.0598,  0.0806,  0.0684,  0.0679,  0.0708,\n",
      "         0.0737,  0.0747,  0.0850,  0.0732,  0.0571,  0.0649,  0.0562,  0.0986,\n",
      "         0.0747,  0.0693,  0.0747,  0.0747,  0.0718,  0.0776,  0.0864,  0.1035,\n",
      "         0.0781,  0.0815,  0.0693,  0.0752,  0.0645,  0.0747,  0.0630,  0.0762,\n",
      "         0.0708,  0.1074,  0.0679,  0.0620,  0.0781,  0.0615,  0.0762,  0.0732,\n",
      "         0.0649,  0.0723,  0.0718,  0.0806,  0.0569,  0.0464,  0.0669,  0.0728,\n",
      "         0.0364,  0.0845,  0.0620,  0.0737,  0.0713,  0.0742,  0.0640,  0.0718,\n",
      "         0.0659,  0.0913,  0.0544,  0.0684,  0.0669,  0.0654,  0.0654,  0.0596,\n",
      "         0.0874,  0.0879,  0.0693,  0.0830,  0.0664,  0.0732,  0.0654,  0.0659,\n",
      "         0.0576,  0.0693,  0.0640,  0.0688,  0.0679,  0.0771,  0.0718,  0.0674,\n",
      "         0.0605,  0.0747,  0.0713,  0.0684,  0.0659,  0.0898,  0.0781,  0.0718,\n",
      "         0.0698,  0.0659,  0.0757,  0.0693,  0.0747,  0.0771,  0.0728,  0.0708,\n",
      "         0.0693,  0.0684,  0.0762,  0.0192,  0.0659,  0.0693,  0.0586,  0.0679,\n",
      "         0.0688,  0.0718,  0.0493,  0.0845,  0.0723,  0.0815,  0.0718,  0.0791,\n",
      "         0.0762,  0.0613,  0.0583,  0.0698,  0.0752,  0.0835,  0.0859,  0.0742,\n",
      "         0.1621,  0.0767,  0.0640,  0.0693,  0.0669,  0.0640,  0.0684,  0.0728,\n",
      "         0.0613,  0.0835,  0.0850,  0.0654,  0.0659,  0.0640,  0.0664,  0.0679,\n",
      "         0.0664,  0.0889,  0.0771,  0.0713,  0.1377,  0.0806,  0.0664,  0.0737,\n",
      "         0.0669,  0.0708,  0.0664,  0.0684,  0.0732,  0.0684,  0.0771,  0.0231,\n",
      "         0.0771,  0.0708,  0.0576,  0.0698,  0.0649,  0.0835,  0.0669,  0.0781,\n",
      "         0.0601,  0.0879,  0.0623,  0.0752,  0.0776,  0.0835,  0.0623,  0.0664,\n",
      "         0.0620,  0.0898,  0.0659,  0.0684,  0.0659,  0.0640,  0.0698,  0.0576,\n",
      "         0.0515,  0.0659,  0.0664,  0.0781,  0.0708,  0.0640,  0.0454,  0.0776,\n",
      "         0.0732,  0.0669,  0.0732,  0.0698,  0.0898,  0.0811,  0.0688,  0.0708,\n",
      "         0.0659,  0.0913,  0.0703,  0.0728,  0.0718,  0.1660,  0.0723,  0.0654,\n",
      "         0.0640,  0.0767,  0.0762,  0.0613,  0.0791,  0.0659,  0.0801,  0.0957,\n",
      "         0.0796,  0.0732,  0.0649,  0.1045,  0.0771,  0.0659,  0.0669,  0.0688,\n",
      "         0.0645,  0.0649,  0.0674,  0.1240,  0.0708,  0.0762,  0.0767,  0.0723,\n",
      "         0.0776,  0.0737,  0.0732,  0.0825,  0.0933,  0.0801,  0.0913,  0.0591,\n",
      "         0.0640,  0.0640,  0.0894,  0.0732,  0.0796,  0.0200,  0.0620,  0.0781,\n",
      "         0.0698,  0.0869,  0.0718,  0.0664,  0.0654,  0.0679,  0.0918,  0.0718,\n",
      "         0.0698,  0.0762,  0.0762,  0.0791,  0.0762,  0.0752,  0.0864,  0.0623,\n",
      "         0.0806,  0.0688,  0.0850,  0.0854,  0.0703,  0.0752,  0.0933,  0.0723,\n",
      "         0.0688,  0.0869,  0.0664,  0.0781,  0.0571,  0.0752,  0.0649,  0.0767,\n",
      "         0.0732,  0.1318,  0.0874,  0.0718,  0.0728,  0.0601,  0.0757,  0.0669,\n",
      "         0.0703,  0.0703,  0.0630,  0.0635,  0.0664,  0.0723,  0.0732,  0.0574,\n",
      "         0.0674,  0.0693,  0.0684,  0.0645,  0.0796,  0.0742,  0.0635,  0.0742,\n",
      "         0.0664,  0.0654,  0.0481,  0.0767,  0.0291,  0.0703,  0.0645,  0.0452,\n",
      "         0.0752,  0.0669,  0.0884,  0.0640,  0.0776,  0.0771,  0.0747,  0.0669,\n",
      "         0.0859,  0.0620,  0.0864,  0.0703,  0.0708,  0.0654,  0.0718,  0.0835,\n",
      "         0.0688,  0.0884,  0.0786,  0.0708,  0.0635,  0.0786,  0.0649,  0.0728,\n",
      "         0.0166,  0.0698,  0.0747,  0.0618,  0.0679,  0.0598,  0.0776,  0.0874,\n",
      "         0.1030,  0.0510,  0.0850,  0.0645,  0.0579,  0.0659,  0.0659,  0.0723,\n",
      "         0.0786,  0.0659,  0.0737,  0.0684,  0.0610,  0.0654,  0.0645,  0.0659,\n",
      "         0.0444,  0.0605,  0.0713,  0.0640,  0.0649,  0.0679,  0.0713,  0.0688,\n",
      "         0.0796,  0.0889,  0.0664,  0.0640,  0.0199,  0.0698,  0.0688,  0.0742,\n",
      "         0.0598,  0.0752,  0.0698,  0.0762,  0.0708,  0.0625,  0.0747,  0.0157,\n",
      "         0.0732,  0.0684,  0.0669,  0.0635,  0.0654,  0.0598,  0.0703,  0.0825,\n",
      "         0.0654,  0.0825,  0.0781,  0.0762,  0.0659,  0.0757,  0.0845,  0.0630,\n",
      "         0.0757,  0.0571,  0.0781,  0.0605,  0.0747,  0.0654,  0.1016,  0.0708,\n",
      "         0.0659,  0.0728,  0.0654,  0.0757,  0.0713,  0.0742,  0.0830,  0.0564,\n",
      "         0.0728,  0.0776,  0.0771,  0.0732,  0.0747,  0.0630,  0.0747,  0.0723,\n",
      "         0.0796,  0.0684,  0.0781,  0.0801,  0.0752,  0.0630,  0.0811,  0.0830,\n",
      "         0.0659,  0.0688,  0.0840,  0.0684,  0.0693,  0.0723,  0.0913,  0.0884,\n",
      "         0.0640,  0.0825,  0.0659,  0.0669,  0.0845,  0.0669,  0.0684,  0.0752,\n",
      "         0.0791,  0.0728,  0.1875,  0.3320,  0.0654,  0.0894,  0.0679,  0.0791,\n",
      "         0.0776,  0.0625,  0.0767,  0.0762,  0.0654,  0.0713,  0.0728,  0.0864,\n",
      "         0.0645,  0.0801,  0.0620,  0.0145, -0.0669,  0.0601,  0.0669,  0.0688,\n",
      "         0.0564,  0.0613,  0.0649,  0.0698,  0.0640,  0.0664,  0.0718,  0.0806,\n",
      "         0.0659,  0.0713,  0.0728,  0.0684,  0.0522,  0.0669,  0.1494,  0.0723,\n",
      "         0.0952,  0.0645,  0.0732,  0.0708,  0.0820,  0.0649,  0.0598,  0.0383,\n",
      "         0.0796,  0.0718,  0.0703,  0.0708,  0.0815,  0.0669,  0.0674,  0.0830,\n",
      "         0.0688,  0.0574,  0.0679,  0.0747,  0.0728,  0.0679,  0.0645,  0.0874,\n",
      "         0.0698,  0.0664,  0.0742,  0.0801,  0.0732,  0.0762,  0.0776,  0.0693,\n",
      "         0.0723,  0.1348,  0.0654,  0.0781,  0.0786,  0.0708,  0.0796,  0.0698,\n",
      "         0.0820,  0.0728,  0.0703,  0.0708,  0.0806,  0.0742,  0.0708,  0.0623,\n",
      "         0.0654,  0.0654,  0.0618,  0.0703,  0.0752,  0.0781,  0.0723,  0.0540,\n",
      "         0.0713,  0.0737,  0.0732,  0.0815,  0.0801,  0.0771,  0.0771,  0.1250,\n",
      "         0.0723,  0.0806,  0.0630,  0.0618,  0.0679,  0.0640,  0.0679,  0.0723,\n",
      "         0.0723,  0.0640,  0.0737,  0.0586,  0.0713,  0.0664,  0.0732,  0.0679,\n",
      "         0.0674,  0.0688,  0.0806,  0.0693,  0.0713,  0.0747,  0.0723,  0.0762,\n",
      "         0.0659,  0.0698,  0.0620,  0.0742,  0.0854,  0.0684,  0.0679,  0.0742,\n",
      "         0.0713,  0.0649,  0.0737,  0.0708,  0.0693,  0.0659,  0.0728,  0.0728,\n",
      "         0.0693,  0.0605,  0.0669,  0.0884,  0.0337,  0.0654,  0.0835,  0.0635,\n",
      "         0.0723,  0.0635,  0.0791,  0.0684,  0.0708,  0.0850,  0.0825,  0.0879,\n",
      "         0.0698,  0.0684,  0.0898,  0.0601,  0.0708,  0.0542,  0.0806,  0.0713,\n",
      "         0.0635,  0.0859,  0.0752,  0.0698,  0.0664,  0.0688,  0.0732,  0.0618,\n",
      "         0.0684,  0.0688,  0.0732,  0.0776,  0.0693,  0.0618,  0.0776,  0.0898,\n",
      "         0.0654,  0.0923,  0.0669,  0.0635,  0.0913,  0.0874,  0.0674,  0.0698,\n",
      "         0.1025,  0.0698,  0.0771,  0.0698,  0.0806,  0.0623,  0.0879,  0.0737,\n",
      "         0.0903,  0.0640,  0.0728,  0.0625,  0.0767,  0.0747,  0.0679,  0.0874],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.0.layer.2.DenseReluDense.wi.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.3184,  0.1826,  0.0430,  ..., -0.6094, -0.4922, -0.3281],\n",
      "        [ 0.1631,  0.0498,  0.1235,  ...,  0.0147,  0.3340,  0.1270],\n",
      "        [-0.2422,  0.1172,  0.5664,  ...,  0.4727, -0.3711,  0.2158],\n",
      "        ...,\n",
      "        [ 0.0898, -0.2773,  0.1592,  ...,  0.0835,  0.8008,  0.3555],\n",
      "        [ 0.9961, -0.1318,  0.5430,  ...,  0.0182, -0.3359, -0.1748],\n",
      "        [ 0.3047, -0.1816,  0.2041,  ..., -0.2539, -0.1758, -0.0571]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.0.layer.2.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1514, -0.1465, -0.0250,  ...,  0.1738, -0.1582,  0.1143],\n",
      "        [-0.5430, -0.1299,  0.2949,  ..., -0.0209,  0.2793,  0.3965],\n",
      "        [ 0.0708, -0.1816,  0.0554,  ...,  0.1455, -0.2295,  0.1309],\n",
      "        ...,\n",
      "        [-0.1846, -0.0198, -0.0515,  ..., -0.1152,  0.0674,  0.1445],\n",
      "        [ 0.2793, -0.0161, -0.0457,  ...,  0.0972, -0.1562,  0.1260],\n",
      "        [-0.1328, -0.0527, -0.1035,  ..., -0.2715, -0.5547,  0.0776]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.0.layer.2.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([0.6406, 0.3359, 0.3164, 1.0938, 0.5000, 0.6328, 0.5234, 0.6406, 0.5586,\n",
      "        0.5938, 0.9023, 0.5820, 0.5938, 0.7344, 0.5000, 0.5742, 0.6680, 0.5039,\n",
      "        0.7539, 0.5547, 0.5625, 0.6094, 0.8789, 0.5742, 0.5469, 0.5469, 0.4902,\n",
      "        0.6211, 0.5430, 0.5430, 0.5586, 0.2578, 0.6289, 0.5078, 0.6523, 0.5117,\n",
      "        0.6250, 0.3535, 0.5234, 0.7148, 0.7930, 0.6914, 0.6133, 0.6172, 0.2656,\n",
      "        0.5820, 0.7148, 0.5547, 0.5742, 0.6641, 0.6953, 0.5977, 0.6094, 0.5352,\n",
      "        4.9062, 0.6133, 1.1719, 0.5586, 0.5586, 0.5938, 0.5820, 0.4688, 0.6016,\n",
      "        0.5312, 0.5156, 0.3125, 0.5508, 0.5859, 0.3359, 0.5430, 0.5352, 0.6523,\n",
      "        0.5117, 0.5469, 0.5273, 0.5195, 0.7383, 0.5898, 0.5039, 0.5586, 0.5469,\n",
      "        0.6680, 0.5781, 0.6680, 0.5234, 0.5586, 1.1094, 0.6172, 0.6055, 0.5117,\n",
      "        0.5547, 0.5352, 0.5742, 0.6836, 0.5117, 0.5703, 0.5195, 0.5859, 0.4844,\n",
      "        0.5430, 0.6758, 0.5664, 0.3535, 0.4863, 0.5547, 0.8867, 0.5859, 0.5508,\n",
      "        0.4668, 0.6289, 0.6094, 0.5859, 0.6211, 0.5391, 0.5195, 0.5234, 0.6133,\n",
      "        0.5898, 0.1050, 0.4824, 0.4805, 0.5117, 0.6445, 0.5312, 0.6172, 0.5977,\n",
      "        0.6094, 0.6328, 0.5938, 0.6602, 0.7578, 0.5820, 0.5234, 0.5156, 0.4121,\n",
      "        0.7617, 0.6445, 0.5195, 0.6719, 0.8594, 0.5391, 0.6289, 0.6992, 0.7461,\n",
      "        0.6055, 0.7188, 0.4902, 0.5977, 0.5234, 0.6641, 0.5430, 0.5781, 0.4941,\n",
      "        0.7891, 0.4824, 0.5234, 0.6445, 0.5039, 0.5273, 0.7656, 0.5117, 0.6211,\n",
      "        0.6016, 0.5898, 0.4707, 0.3594, 0.5703, 0.4883, 0.2969, 0.6602, 0.6484,\n",
      "        0.5664, 0.5625, 0.6484, 0.5625, 0.6211, 0.5625, 0.6641, 0.5195, 0.5000,\n",
      "        0.5234, 0.5352, 0.6289, 0.5117, 0.7578, 0.6328, 0.5586, 0.4785, 0.5742,\n",
      "        0.6094, 0.5508, 0.5664, 0.4395, 0.5898, 0.5547, 0.5430, 0.5586, 0.6445,\n",
      "        0.5859, 0.5469, 0.5312, 0.7305, 0.5977, 0.5039, 0.5273, 0.7344, 0.6523,\n",
      "        0.6211, 0.5234, 0.6172, 0.5156, 0.6289, 0.6406, 0.5938, 0.6211, 0.6445,\n",
      "        0.5781, 0.5156, 0.6562, 0.1367, 0.5664, 0.5547, 0.4531, 0.5625, 0.5859,\n",
      "        0.6016, 0.2393, 0.6289, 0.5742, 0.6523, 0.5312, 0.6445, 0.6211, 0.5117,\n",
      "        0.5234, 0.5430, 0.6914, 0.6797, 0.9180, 0.6445, 0.9570, 0.6133, 0.5273,\n",
      "        0.5273, 0.5508, 0.5742, 0.5156, 0.6289, 0.4883, 0.6250, 0.6797, 0.5273,\n",
      "        0.5547, 0.5156, 0.4844, 0.5547, 0.5039, 0.9023, 0.6445, 0.5820, 0.7930,\n",
      "        0.9141, 0.5312, 0.6172, 0.5430, 0.5742, 0.5195, 0.5586, 0.5586, 0.5938,\n",
      "        0.7617, 0.1318, 0.6602, 0.5664, 0.5664, 0.5898, 0.5391, 0.6836, 0.5625,\n",
      "        0.6719, 0.1128, 0.5352, 0.5586, 0.6445, 0.6172, 0.7031, 0.5156, 0.5273,\n",
      "        0.5586, 0.7383, 0.5039, 0.5508, 0.5078, 0.5703, 0.5703, 0.5312, 0.2461,\n",
      "        0.5312, 0.4902, 0.6250, 0.6094, 0.5195, 0.1582, 0.6250, 0.5938, 0.4648,\n",
      "        0.6406, 0.5664, 0.6992, 0.6406, 0.5312, 0.6016, 0.5352, 0.6641, 0.3223,\n",
      "        0.6289, 0.5859, 0.9492, 0.6016, 0.5195, 0.5117, 0.5898, 0.5977, 0.5039,\n",
      "        0.5625, 0.5664, 0.6953, 0.6875, 0.6367, 0.5977, 0.5508, 0.6367, 0.6211,\n",
      "        0.5391, 0.5625, 0.5781, 0.4902, 0.5195, 0.5547, 0.9688, 0.6484, 0.6172,\n",
      "        0.5664, 0.5664, 0.5820, 0.5391, 0.6094, 0.6367, 0.5469, 0.6367, 0.6953,\n",
      "        0.5234, 0.6523, 0.5391, 0.7070, 0.5547, 0.5977, 0.1475, 0.5469, 0.6797,\n",
      "        0.5586, 0.6680, 0.4785, 0.5195, 0.5078, 0.5469, 0.6953, 0.5703, 0.5078,\n",
      "        0.6055, 0.5938, 0.6367, 0.5938, 0.5625, 0.6836, 0.5039, 0.6484, 0.5078,\n",
      "        0.6641, 0.6641, 0.5703, 0.5977, 0.8516, 0.5898, 0.5352, 0.7266, 0.5586,\n",
      "        0.6719, 0.4941, 0.6328, 0.4922, 0.6445, 0.6289, 1.0625, 0.6875, 0.6016,\n",
      "        0.5781, 0.5312, 0.5430, 0.5625, 0.5430, 0.5898, 0.5273, 0.5195, 0.5586,\n",
      "        0.5469, 0.5742, 0.5547, 0.4961, 0.5273, 0.5078, 0.5000, 0.6094, 0.6055,\n",
      "        0.5391, 0.5781, 0.5117, 0.5508, 0.2051, 0.6602, 0.2129, 0.5430, 0.4922,\n",
      "        0.5078, 0.5703, 0.5195, 0.7461, 0.5195, 0.5938, 0.8633, 0.6602, 0.5195,\n",
      "        0.6797, 0.4961, 0.6523, 0.5547, 0.6133, 0.4648, 0.5977, 0.6250, 0.5508,\n",
      "        0.6055, 0.5000, 0.6484, 0.5469, 0.5312, 0.5273, 0.6133, 0.0737, 0.6016,\n",
      "        0.6406, 0.4961, 0.5664, 0.5039, 0.5742, 0.7383, 0.6133, 0.4316, 0.6641,\n",
      "        0.5156, 0.5156, 0.5430, 0.5547, 0.6016, 0.5859, 0.5820, 0.6797, 0.5742,\n",
      "        0.4668, 0.5391, 0.5195, 0.5352, 0.5391, 0.5000, 0.5586, 0.5352, 0.5820,\n",
      "        0.5312, 0.5664, 0.5859, 0.6992, 0.6445, 0.5586, 0.5586, 0.1040, 0.5742,\n",
      "        0.5430, 0.4473, 0.4473, 0.6289, 0.5430, 0.5977, 0.6211, 0.5234, 0.5938,\n",
      "        0.1279, 0.5781, 0.5859, 0.5469, 0.5547, 0.5078, 0.2695, 0.5898, 0.5938,\n",
      "        0.5547, 0.6562, 0.6094, 0.6641, 0.4883, 0.5938, 0.5859, 0.5625, 0.6211,\n",
      "        0.5586, 0.6055, 0.5469, 0.6328, 0.5586, 0.7891, 0.6250, 0.5547, 0.6250,\n",
      "        0.4824, 0.5508, 0.6211, 0.5938, 0.6641, 0.3828, 0.5586, 0.6133, 0.6094,\n",
      "        0.5781, 0.5938, 0.6133, 0.5859, 0.5781, 0.6328, 0.5703, 0.6250, 0.6094,\n",
      "        0.6094, 0.5391, 0.6836, 0.6992, 0.5938, 0.5391, 0.6445, 0.5742, 0.5312,\n",
      "        0.6680, 0.7109, 0.6602, 0.5195, 0.6445, 0.6523, 0.5625, 0.6562, 0.5391,\n",
      "        0.5508, 0.6758, 0.6406, 0.5547, 1.2422, 3.4844, 0.5859, 0.6016, 0.6133,\n",
      "        0.6445, 0.6992, 0.5547, 0.6523, 0.5859, 0.5234, 0.6562, 0.7266, 0.6484,\n",
      "        0.5117, 0.5898, 0.4727, 0.1055, 0.4766, 0.4863, 0.5508, 0.5820, 0.4980,\n",
      "        0.5195, 0.5273, 0.5703, 0.5469, 0.5234, 0.5820, 0.6133, 0.4961, 0.6016,\n",
      "        0.6055, 0.5625, 0.4238, 0.5938, 0.9961, 0.5469, 0.7656, 0.5469, 0.6484,\n",
      "        0.5703, 0.6328, 0.4883, 0.4922, 0.2158, 0.7070, 0.6094, 0.6406, 0.5273,\n",
      "        0.5820, 0.5430, 0.5625, 0.6562, 0.5977, 0.3066, 0.4980, 0.5820, 0.6055,\n",
      "        0.5586, 0.5234, 0.7070, 0.6914, 0.5391, 0.8398, 0.6445, 0.5742, 0.6484,\n",
      "        0.6016, 0.5469, 0.5273, 0.7500, 0.5508, 0.5820, 0.6719, 0.6133, 0.6484,\n",
      "        0.5352, 0.6133, 0.5938, 0.5430, 0.5195, 0.9180, 0.6328, 0.6055, 0.5156,\n",
      "        0.7383, 0.5234, 0.5742, 0.5625, 0.5859, 0.6680, 0.6133, 0.5156, 0.5977,\n",
      "        0.5859, 0.6484, 0.6719, 0.6133, 0.6406, 0.5977, 0.7773, 0.5625, 0.6367,\n",
      "        0.5273, 0.5039, 0.6211, 0.5195, 0.5820, 0.5664, 0.5586, 0.5820, 0.5898,\n",
      "        0.5273, 0.5547, 0.5234, 0.6289, 0.5625, 0.5781, 0.5586, 0.6367, 0.5117,\n",
      "        0.6523, 0.6133, 0.5859, 0.6445, 0.5078, 0.6133, 0.5586, 0.6328, 0.7539,\n",
      "        0.5117, 0.5703, 0.6133, 0.6016, 0.4844, 0.5430, 0.6055, 0.6133, 0.5586,\n",
      "        0.6211, 0.8203, 0.4941, 0.4922, 0.5547, 0.7695, 0.1768, 0.5508, 0.6680,\n",
      "        0.5547, 0.5977, 0.5430, 0.5156, 0.3906, 0.6250, 0.5273, 0.5898, 0.8047,\n",
      "        0.5820, 0.5664, 0.6328, 0.5156, 0.6445, 0.3379, 0.6562, 0.5859, 0.5430,\n",
      "        0.7539, 0.6055, 0.5469, 0.5000, 0.5703, 0.4766, 0.5664, 0.5586, 0.5664,\n",
      "        0.5703, 0.6133, 0.5703, 0.6484, 0.6055, 0.7109, 0.6094, 0.7734, 0.5781,\n",
      "        0.4707, 0.6172, 0.6875, 0.6445, 0.5742, 0.8438, 0.5820, 0.6719, 0.5391,\n",
      "        0.6641, 0.5508, 0.7266, 0.6406, 0.5820, 0.5156, 0.6055, 0.5664, 0.5781,\n",
      "        0.6289, 0.5859, 0.6875], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.1.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0247,  0.0505,  0.0449,  ...,  0.0347,  0.0669,  0.0728],\n",
      "        [ 0.0640,  0.0248, -0.0752,  ..., -0.0025, -0.0060,  0.0281],\n",
      "        [-0.0175,  0.0320, -0.0498,  ...,  0.0273,  0.0153,  0.0254],\n",
      "        ...,\n",
      "        [ 0.0635, -0.0396,  0.0510,  ...,  0.0139,  0.0708,  0.0122],\n",
      "        [ 0.0583, -0.0206,  0.0386,  ..., -0.0066,  0.0547,  0.0854],\n",
      "        [ 0.0330, -0.0229, -0.0554,  ...,  0.0327,  0.0403, -0.0036]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.1.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0422,  0.1436, -0.0479,  ..., -0.0295, -0.2949, -0.2793],\n",
      "        [ 0.3750, -0.3887,  0.0732,  ..., -0.1260, -0.2910,  0.0044],\n",
      "        [-0.2168, -0.0090,  0.1621,  ..., -0.0374, -0.1533, -0.2432],\n",
      "        ...,\n",
      "        [ 0.2080,  0.2715,  0.5000,  ..., -0.3945,  0.2354,  0.3223],\n",
      "        [-0.0815,  0.1064,  0.1094,  ..., -0.2061,  0.1021, -0.2324],\n",
      "        [ 0.2256, -0.0466,  0.0864,  ..., -0.3887, -0.0130, -0.3770]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.1.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.1797,  0.6719,  0.6133,  ...,  0.2227, -0.2354,  0.2949],\n",
      "        [-0.4414,  0.5859,  0.5469,  ..., -0.2412,  0.3086, -0.2422],\n",
      "        [-0.3594, -0.6367,  0.3789,  ...,  0.3730, -0.0469, -0.0249],\n",
      "        ...,\n",
      "        [ 0.1143, -0.0874, -0.0115,  ...,  0.0447, -0.2100,  0.1553],\n",
      "        [ 0.2432, -0.0530,  0.1045,  ...,  0.6836, -0.5547, -0.4512],\n",
      "        [-0.4004,  0.0840, -0.0315,  ...,  0.2812, -0.4258, -0.2217]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.1.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0820, -0.0781, -0.1650,  ..., -0.0640, -0.4727, -0.1328],\n",
      "        [-1.2656, -0.9844,  1.1094,  ...,  1.0625,  0.2275, -0.9727],\n",
      "        [-0.0610, -0.0981, -0.3340,  ..., -0.0047, -0.0339, -0.5508],\n",
      "        ...,\n",
      "        [-0.2129,  0.3613, -0.2168,  ..., -0.0791, -0.3301,  0.4648],\n",
      "        [ 0.1572, -0.0811,  0.1738,  ..., -0.1943, -0.0125,  0.1982],\n",
      "        [-0.0708, -0.3145,  0.0737,  ..., -0.1807, -0.2275,  0.0232]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.1.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([0.1562, 0.0593, 0.0688, 0.2051, 0.1426, 0.1504, 0.1494, 0.1719, 0.1396,\n",
      "        0.1572, 0.1230, 0.1680, 0.1035, 0.1475, 0.1455, 0.1523, 0.1426, 0.1104,\n",
      "        0.1484, 0.1660, 0.1572, 0.1475, 0.1797, 0.1494, 0.1377, 0.1138, 0.1445,\n",
      "        0.1270, 0.1533, 0.1436, 0.1396, 0.0781, 0.1562, 0.1436, 0.1631, 0.1240,\n",
      "        0.1641, 0.1396, 0.1270, 0.1748, 0.1719, 0.1582, 0.1562, 0.1611, 0.0752,\n",
      "        0.1436, 0.1719, 0.1455, 0.1523, 0.1670, 0.1484, 0.1504, 0.1445, 0.1436,\n",
      "        0.4395, 0.1436, 0.1865, 0.1631, 0.1768, 0.1338, 0.1445, 0.1338, 0.1514,\n",
      "        0.1426, 0.1445, 0.0796, 0.1514, 0.1367, 0.0908, 0.1406, 0.1445, 0.1621,\n",
      "        0.1348, 0.1455, 0.1406, 0.1436, 0.1484, 0.1426, 0.1328, 0.1455, 0.1523,\n",
      "        0.1494, 0.1611, 0.1416, 0.1367, 0.1504, 0.2432, 0.1494, 0.1465, 0.1396,\n",
      "        0.1387, 0.1445, 0.1436, 0.1533, 0.1484, 0.1592, 0.1406, 0.1543, 0.1377,\n",
      "        0.1494, 0.1582, 0.1406, 0.0996, 0.1416, 0.1396, 0.1934, 0.1562, 0.1465,\n",
      "        0.1328, 0.1641, 0.1562, 0.1699, 0.1494, 0.1523, 0.1387, 0.1270, 0.1592,\n",
      "        0.1396, 0.0425, 0.1406, 0.1367, 0.1436, 0.1191, 0.1377, 0.1602, 0.1318,\n",
      "        0.1543, 0.1719, 0.1592, 0.1099, 0.1270, 0.1367, 0.1377, 0.1396, 0.0767,\n",
      "        0.1641, 0.1484, 0.1221, 0.1592, 0.1084, 0.1235, 0.1621, 0.1592, 0.1836,\n",
      "        0.1445, 0.1602, 0.1309, 0.1245, 0.1309, 0.1660, 0.1572, 0.1426, 0.1455,\n",
      "        0.1455, 0.1475, 0.1582, 0.1719, 0.1445, 0.0645, 0.0962, 0.1230, 0.1387,\n",
      "        0.1367, 0.1377, 0.1187, 0.0728, 0.1582, 0.1426, 0.0583, 0.1592, 0.0938,\n",
      "        0.1494, 0.1553, 0.1611, 0.1436, 0.1543, 0.1494, 0.1758, 0.0962, 0.1230,\n",
      "        0.1543, 0.1582, 0.1602, 0.1514, 0.1426, 0.1436, 0.1572, 0.0398, 0.1426,\n",
      "        0.1235, 0.1533, 0.1475, 0.1230, 0.1553, 0.1465, 0.1338, 0.1621, 0.1553,\n",
      "        0.1572, 0.1260, 0.1455, 0.1807, 0.1572, 0.1250, 0.1406, 0.1592, 0.1631,\n",
      "        0.1582, 0.1475, 0.1611, 0.1387, 0.1484, 0.1484, 0.1445, 0.1572, 0.1562,\n",
      "        0.1426, 0.1426, 0.1689, 0.0378, 0.1177, 0.1602, 0.1113, 0.1523, 0.1406,\n",
      "        0.1680, 0.1475, 0.1709, 0.1494, 0.1621, 0.1406, 0.1621, 0.1377, 0.1484,\n",
      "        0.1338, 0.1338, 0.1562, 0.1631, 0.1089, 0.1631, 0.1543, 0.1621, 0.1299,\n",
      "        0.1387, 0.1465, 0.1426, 0.1436, 0.1543, 0.1289, 0.1592, 0.1670, 0.1348,\n",
      "        0.1455, 0.1426, 0.1377, 0.1289, 0.1504, 0.1089, 0.1660, 0.1475, 0.1816,\n",
      "        0.1240, 0.1523, 0.1592, 0.1396, 0.1445, 0.1309, 0.1416, 0.1436, 0.1602,\n",
      "        0.1660, 0.1191, 0.1660, 0.1426, 0.0986, 0.1484, 0.1377, 0.1797, 0.1387,\n",
      "        0.1406, 0.0376, 0.0952, 0.1523, 0.1494, 0.1611, 0.1621, 0.1475, 0.1523,\n",
      "        0.1162, 0.1348, 0.1338, 0.1377, 0.1328, 0.1523, 0.1416, 0.1377, 0.0835,\n",
      "        0.1484, 0.1426, 0.1748, 0.1719, 0.1445, 0.0459, 0.1494, 0.1641, 0.1387,\n",
      "        0.1572, 0.1562, 0.1572, 0.1484, 0.1562, 0.1523, 0.1445, 0.1787, 0.1348,\n",
      "        0.1514, 0.1523, 0.2363, 0.1602, 0.1494, 0.1543, 0.1670, 0.1475, 0.1328,\n",
      "        0.1357, 0.1494, 0.1514, 0.1719, 0.1670, 0.1475, 0.1592, 0.1396, 0.1621,\n",
      "        0.1328, 0.1338, 0.1436, 0.1328, 0.1484, 0.1455, 0.1816, 0.1465, 0.1592,\n",
      "        0.1641, 0.1475, 0.1650, 0.1377, 0.1699, 0.1484, 0.0737, 0.1504, 0.1523,\n",
      "        0.1299, 0.1328, 0.1318, 0.1602, 0.1602, 0.1602, 0.0483, 0.1660, 0.1680,\n",
      "        0.1504, 0.1182, 0.1094, 0.1299, 0.1396, 0.1436, 0.1729, 0.1387, 0.1318,\n",
      "        0.1416, 0.1504, 0.1680, 0.1455, 0.1328, 0.1689, 0.1377, 0.1543, 0.1533,\n",
      "        0.1602, 0.1309, 0.1426, 0.1562, 0.1455, 0.1514, 0.1357, 0.1689, 0.1475,\n",
      "        0.1670, 0.1089, 0.1611, 0.1289, 0.1582, 0.1396, 0.2363, 0.1641, 0.1543,\n",
      "        0.1680, 0.1455, 0.1592, 0.1504, 0.1338, 0.1582, 0.1523, 0.1504, 0.1455,\n",
      "        0.1445, 0.1328, 0.1445, 0.1260, 0.1494, 0.1426, 0.1406, 0.1621, 0.1660,\n",
      "        0.1396, 0.1465, 0.1299, 0.1406, 0.1367, 0.1523, 0.0344, 0.1367, 0.1602,\n",
      "        0.0854, 0.1650, 0.1387, 0.1602, 0.1445, 0.1426, 0.1235, 0.1543, 0.1396,\n",
      "        0.1787, 0.1206, 0.1514, 0.1377, 0.1641, 0.1299, 0.1455, 0.1738, 0.1514,\n",
      "        0.1572, 0.1494, 0.1572, 0.1484, 0.1416, 0.1387, 0.1543, 0.0437, 0.1445,\n",
      "        0.1689, 0.1387, 0.1631, 0.1445, 0.1455, 0.1787, 0.1846, 0.1099, 0.1602,\n",
      "        0.1377, 0.1514, 0.1592, 0.1562, 0.1621, 0.1582, 0.1494, 0.1592, 0.1533,\n",
      "        0.1279, 0.1309, 0.1455, 0.1387, 0.0771, 0.1396, 0.1436, 0.1367, 0.1465,\n",
      "        0.1455, 0.1514, 0.1465, 0.1523, 0.1680, 0.1543, 0.1338, 0.1045, 0.1494,\n",
      "        0.1309, 0.0302, 0.1328, 0.1602, 0.1533, 0.1387, 0.1621, 0.1445, 0.1650,\n",
      "        0.0776, 0.1426, 0.1455, 0.1514, 0.1396, 0.1533, 0.1357, 0.1504, 0.1494,\n",
      "        0.1416, 0.1455, 0.1572, 0.1650, 0.1309, 0.1553, 0.1445, 0.1426, 0.1631,\n",
      "        0.1455, 0.1328, 0.1533, 0.1621, 0.1211, 0.1611, 0.1631, 0.1426, 0.1553,\n",
      "        0.1562, 0.1455, 0.1426, 0.1465, 0.1641, 0.0928, 0.1514, 0.1562, 0.1182,\n",
      "        0.1377, 0.1641, 0.1553, 0.1455, 0.1494, 0.1572, 0.1572, 0.1611, 0.1475,\n",
      "        0.1533, 0.1445, 0.1621, 0.1562, 0.1465, 0.1504, 0.1611, 0.1455, 0.1416,\n",
      "        0.1543, 0.1328, 0.1270, 0.1387, 0.1123, 0.1670, 0.1572, 0.1465, 0.1504,\n",
      "        0.1216, 0.1562, 0.1484, 0.1426, 0.1592, 0.4102, 0.1631, 0.1455, 0.1533,\n",
      "        0.1562, 0.1475, 0.1475, 0.1445, 0.1787, 0.1504, 0.1562, 0.1196, 0.1455,\n",
      "        0.1396, 0.1621, 0.1357, 0.0369, 0.1191, 0.1445, 0.1367, 0.1514, 0.1338,\n",
      "        0.1514, 0.1387, 0.1582, 0.1348, 0.1436, 0.1523, 0.1631, 0.1309, 0.1426,\n",
      "        0.1562, 0.1504, 0.0913, 0.1455, 0.3164, 0.1436, 0.1367, 0.1445, 0.1650,\n",
      "        0.1377, 0.1631, 0.1377, 0.1387, 0.0579, 0.1729, 0.1572, 0.1611, 0.1455,\n",
      "        0.1475, 0.1455, 0.1504, 0.1797, 0.1514, 0.1270, 0.1309, 0.1572, 0.1553,\n",
      "        0.1406, 0.1396, 0.1318, 0.1602, 0.1406, 0.1406, 0.1592, 0.1367, 0.1611,\n",
      "        0.1328, 0.1455, 0.1377, 0.1641, 0.1279, 0.1602, 0.1260, 0.1455, 0.1436,\n",
      "        0.1357, 0.1240, 0.1357, 0.1465, 0.1416, 0.1553, 0.1582, 0.0986, 0.1396,\n",
      "        0.1396, 0.1572, 0.0879, 0.1562, 0.1367, 0.1602, 0.1523, 0.0894, 0.1494,\n",
      "        0.1357, 0.1572, 0.1562, 0.1631, 0.1719, 0.1543, 0.2090, 0.1416, 0.1650,\n",
      "        0.1406, 0.1465, 0.1299, 0.1406, 0.1299, 0.1465, 0.1533, 0.1260, 0.1533,\n",
      "        0.1504, 0.1416, 0.1445, 0.1553, 0.1445, 0.1543, 0.1436, 0.1445, 0.1328,\n",
      "        0.1523, 0.1641, 0.1367, 0.1309, 0.1406, 0.1543, 0.1582, 0.1631, 0.1494,\n",
      "        0.1328, 0.1562, 0.1387, 0.1885, 0.1289, 0.1484, 0.1514, 0.1465, 0.1494,\n",
      "        0.1426, 0.0752, 0.1406, 0.1416, 0.1562, 0.1533, 0.1040, 0.1436, 0.1533,\n",
      "        0.1367, 0.1138, 0.1318, 0.1543, 0.1514, 0.1475, 0.1689, 0.1436, 0.1504,\n",
      "        0.1475, 0.1426, 0.1377, 0.1387, 0.1602, 0.0962, 0.1406, 0.1406, 0.1553,\n",
      "        0.1553, 0.1709, 0.1475, 0.1504, 0.0669, 0.1475, 0.1387, 0.1611, 0.1426,\n",
      "        0.1484, 0.1338, 0.1426, 0.0869, 0.1543, 0.1621, 0.1680, 0.1357, 0.1138,\n",
      "        0.1309, 0.1670, 0.1436, 0.1387, 0.1357, 0.1768, 0.1396, 0.1494, 0.1582,\n",
      "        0.1621, 0.1445, 0.0972, 0.1426, 0.1338, 0.1299, 0.1494, 0.1387, 0.1396,\n",
      "        0.1582, 0.1562, 0.1504], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.1.layer.1.EncDecAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0027, -0.0403, -0.0156,  ...,  0.0874, -0.1279,  0.0439],\n",
      "        [ 0.0332,  0.0144, -0.0181,  ..., -0.0170,  0.0352, -0.0466],\n",
      "        [-0.0525, -0.0073, -0.0366,  ...,  0.0098,  0.0569,  0.0074],\n",
      "        ...,\n",
      "        [-0.0030, -0.0091, -0.0298,  ...,  0.0205,  0.0142,  0.0371],\n",
      "        [-0.0066,  0.0208,  0.0003,  ..., -0.0004,  0.0869,  0.0068],\n",
      "        [-0.0413,  0.0030,  0.0090,  ..., -0.0422,  0.0581, -0.0459]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.1.layer.1.EncDecAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.5352, -0.0688,  0.0129,  ...,  0.0444, -0.5234, -0.4863],\n",
      "        [ 0.3906, -0.2988, -0.1621,  ...,  0.0942,  0.2266,  0.0388],\n",
      "        [-0.3281, -0.1064,  0.1562,  ..., -0.3027, -0.1553, -0.1235],\n",
      "        ...,\n",
      "        [ 0.0723, -0.1602, -0.2295,  ..., -0.0287,  0.0840, -0.1797],\n",
      "        [ 0.1738,  0.1650,  0.2500,  ...,  0.1260, -0.1162, -0.3750],\n",
      "        [-0.0273, -0.0471, -0.1138,  ...,  0.2480, -0.0013,  0.2832]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.1.layer.1.EncDecAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0198, -0.1875, -0.0986,  ..., -0.3184, -0.0100, -0.0275],\n",
      "        [-0.6719, -0.1777,  0.4707,  ...,  0.0674,  0.0581,  0.2178],\n",
      "        [ 0.1572, -0.3867,  0.4453,  ..., -0.0156,  0.4082, -0.4199],\n",
      "        ...,\n",
      "        [-0.1328,  0.0879,  0.0996,  ...,  0.1670,  0.2129,  0.0583],\n",
      "        [-0.0251,  0.0544, -0.2295,  ..., -0.2100,  0.3984, -0.4492],\n",
      "        [ 0.1680,  0.2021, -0.2021,  ...,  0.3262,  0.1436,  0.2891]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.1.layer.1.EncDecAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.2559, -0.2461,  0.4160,  ...,  0.2354,  0.1240,  0.2891],\n",
      "        [-0.4375,  0.1914,  0.0889,  ...,  1.0938, -0.2969, -0.3184],\n",
      "        [ 0.1357,  1.0234, -0.3281,  ...,  0.8828,  0.2314, -0.7148],\n",
      "        ...,\n",
      "        [-0.1367, -0.2168, -0.4902,  ...,  0.3457, -0.4023,  0.2021],\n",
      "        [-0.1953,  0.3242,  0.1270,  ..., -0.3027, -0.6680,  0.0270],\n",
      "        [ 0.5156, -0.2119, -0.4961,  ..., -0.1836, -0.2139,  0.4336]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.1.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 0.0752,  0.0247,  0.0280,  0.1045,  0.0781,  0.0635,  0.0771,  0.0894,\n",
      "         0.0610,  0.0757,  0.0664,  0.0859,  0.0659,  0.0605,  0.0801,  0.0776,\n",
      "         0.0703,  0.0532,  0.0811,  0.0869,  0.0742,  0.0728,  0.0845,  0.0815,\n",
      "         0.0723,  0.0571,  0.0806,  0.0654,  0.0747,  0.0762,  0.0762,  0.0304,\n",
      "         0.0820,  0.0732,  0.0806,  0.0603,  0.0825,  0.0664,  0.0708,  0.0874,\n",
      "         0.0942,  0.0771,  0.0757,  0.0762,  0.0378,  0.0703,  0.0903,  0.0728,\n",
      "         0.0835,  0.0811,  0.0806,  0.0742,  0.0776,  0.0728,  0.1748,  0.0742,\n",
      "         0.0796,  0.0771,  0.0786,  0.0786,  0.0688,  0.0762,  0.0869,  0.0728,\n",
      "         0.0801,  0.0391,  0.0732,  0.0757,  0.0337,  0.0732,  0.0752,  0.0815,\n",
      "         0.0718,  0.0742,  0.0737,  0.0752,  0.0571,  0.0728,  0.0674,  0.0635,\n",
      "         0.0742,  0.0786,  0.0737,  0.0557,  0.0679,  0.0684,  0.1050,  0.0801,\n",
      "         0.0747,  0.0786,  0.0664,  0.0728,  0.0747,  0.0728,  0.0718,  0.0796,\n",
      "         0.0786,  0.0820,  0.0776,  0.0723,  0.0698,  0.0688,  0.0366,  0.0747,\n",
      "         0.0713,  0.0957,  0.0791,  0.0649,  0.0708,  0.0781,  0.0781,  0.0771,\n",
      "         0.0684,  0.0698,  0.0684,  0.0688,  0.0747,  0.0752,  0.0106,  0.0708,\n",
      "         0.0698,  0.0737,  0.0566,  0.0708,  0.0791,  0.0640,  0.0752,  0.0669,\n",
      "         0.0718,  0.0542,  0.0615,  0.0623,  0.0698,  0.0747,  0.0640,  0.0850,\n",
      "         0.0791,  0.0574,  0.0776,  0.0532,  0.0588,  0.0688,  0.0737,  0.0835,\n",
      "         0.0791,  0.0742,  0.0664,  0.0601,  0.0654,  0.0723,  0.0698,  0.0693,\n",
      "         0.0723,  0.0728,  0.0840,  0.0742,  0.0850,  0.0752,  0.1235,  0.0608,\n",
      "         0.0664,  0.0693,  0.0693,  0.0679, -0.0562,  0.0310,  0.0767,  0.0579,\n",
      "         0.0260,  0.0781,  0.0386,  0.0728,  0.0820,  0.0737,  0.0708,  0.0776,\n",
      "         0.0752,  0.0903,  0.0332,  0.0571,  0.0752,  0.0688,  0.0796,  0.0752,\n",
      "         0.0737,  0.0688,  0.0757,  0.0698,  0.0737,  0.0574,  0.0732,  0.0688,\n",
      "         0.0605,  0.0786,  0.0703,  0.0635,  0.0806,  0.0771,  0.0723,  0.0635,\n",
      "         0.0811,  0.0806,  0.0688,  0.0649,  0.0742,  0.0742,  0.0815,  0.0776,\n",
      "         0.0708,  0.0835,  0.0649,  0.0615,  0.0718,  0.0762, -0.0801,  0.0747,\n",
      "         0.0747,  0.0649,  0.0815,  0.0234,  0.0527,  0.0718,  0.0586,  0.0737,\n",
      "         0.0649,  0.0879,  0.0349,  0.0918,  0.0776,  0.0776,  0.0659,  0.0791,\n",
      "         0.0630,  0.0806,  0.0635,  0.0645,  0.0728,  0.0830,  0.0513,  0.0801,\n",
      "         0.0679,  0.0894,  0.0664,  0.0728,  0.0825,  0.0723,  0.0630,  0.0771,\n",
      "         0.0630,  0.0781,  0.0742,  0.0698,  0.0732,  0.0757,  0.0742,  0.0603,\n",
      "         0.0732,  0.0520,  0.0801,  0.0762,  0.0815,  0.0554,  0.0830,  0.0811,\n",
      "         0.0742,  0.0688,  0.0747,  0.0654,  0.0737,  0.0811,  0.0771,  0.0310,\n",
      "         0.0835,  0.0679,  0.0359,  0.0776,  0.0674,  0.0771,  0.0703,  0.0674,\n",
      "         0.0126,  0.0413,  0.0776,  0.0674,  0.0869,  0.0820,  0.0791,  0.0859,\n",
      "         0.0571,  0.0608,  0.0708,  0.0737,  0.0679,  0.0767,  0.0757,  0.0742,\n",
      "         0.0388,  0.0796,  0.0742,  0.0845,  0.0854,  0.0752,  0.0146,  0.0762,\n",
      "         0.0854,  0.0732,  0.0835,  0.0806,  0.0752,  0.0688,  0.0742,  0.0732,\n",
      "         0.0742,  0.0796,  0.0752,  0.0757,  0.0840,  0.1011,  0.0840,  0.0728,\n",
      "         0.0781,  0.0703,  0.0645,  0.0737,  0.0674,  0.0684,  0.0762,  0.0811,\n",
      "         0.0859,  0.0767,  0.0767,  0.0613,  0.0786,  0.0640,  0.0645,  0.0757,\n",
      "         0.0684,  0.0767,  0.0659,  0.0859,  0.0674,  0.0781,  0.0918,  0.0752,\n",
      "         0.0732,  0.0674,  0.0728,  0.0618,  0.0396,  0.0679,  0.0825,  0.0569,\n",
      "         0.0659,  0.0645,  0.0781,  0.0747,  0.0825,  0.0081,  0.0801,  0.0791,\n",
      "         0.0815,  0.0618,  0.0393,  0.0615,  0.0752,  0.0742,  0.0776,  0.0664,\n",
      "         0.0728,  0.0684,  0.0815,  0.0718,  0.0708,  0.0723,  0.0840,  0.0684,\n",
      "         0.0718,  0.0776,  0.0801,  0.0569,  0.0684,  0.0737,  0.0679,  0.0728,\n",
      "         0.0713,  0.0835,  0.0747,  0.0757,  0.0574,  0.0791,  0.0645,  0.0674,\n",
      "         0.0649,  0.1001,  0.0786,  0.0791,  0.0835,  0.0728,  0.0713,  0.0747,\n",
      "         0.0688,  0.0786,  0.0752,  0.0845,  0.0618,  0.0703,  0.0664,  0.0674,\n",
      "         0.0664,  0.0723,  0.0752,  0.0723,  0.0757,  0.0786,  0.0776,  0.0791,\n",
      "         0.0688,  0.0747,  0.0566,  0.0703,  0.0291,  0.0684,  0.0806,  0.0312,\n",
      "         0.0781,  0.0698,  0.0820,  0.0732,  0.0718,  0.0579,  0.0845,  0.0728,\n",
      "         0.0850,  0.0654,  0.0581,  0.0664,  0.0786,  0.0684,  0.0703,  0.0703,\n",
      "         0.0698,  0.0732,  0.0513,  0.0752,  0.0801,  0.0698,  0.0732,  0.0825,\n",
      "         0.0184,  0.0684,  0.0781,  0.0732,  0.0723,  0.0757,  0.0767,  0.0732,\n",
      "         0.1104,  0.0435,  0.0713,  0.0708,  0.0752,  0.0762,  0.0796,  0.0698,\n",
      "         0.0830,  0.0747,  0.0674,  0.0718,  0.0620,  0.0737,  0.0801,  0.0688,\n",
      "         0.0342,  0.0723,  0.0684,  0.0684,  0.0762,  0.0723,  0.0728,  0.0718,\n",
      "         0.0605,  0.0815,  0.0796,  0.0664,  0.0189,  0.0825,  0.0703,  0.0972,\n",
      "         0.0718,  0.0820,  0.0801,  0.0806,  0.0767,  0.0806,  0.0747,  0.0240,\n",
      "         0.0718,  0.0752,  0.0747,  0.0796,  0.0747,  0.0703,  0.0737,  0.0757,\n",
      "         0.0742,  0.0669,  0.0742,  0.0659,  0.0684,  0.0811,  0.0664,  0.0796,\n",
      "         0.0737,  0.0669,  0.0610,  0.0801,  0.0806,  0.0583,  0.0845,  0.0884,\n",
      "         0.0669,  0.0708,  0.0688,  0.0669,  0.0703,  0.0649,  0.0815,  0.0444,\n",
      "         0.0718,  0.0698,  0.0547,  0.0732,  0.0688,  0.0728,  0.0669,  0.0752,\n",
      "         0.0825,  0.0815,  0.0845,  0.0752,  0.0762,  0.0762,  0.0762,  0.0752,\n",
      "         0.0732,  0.0742,  0.0752,  0.0742,  0.0747,  0.0776,  0.0674,  0.0703,\n",
      "         0.0718,  0.0593,  0.0742,  0.0742,  0.0737,  0.0791,  0.0591,  0.0742,\n",
      "         0.0718,  0.0645,  0.0442,  0.1729,  0.0811,  0.0732,  0.0693,  0.0737,\n",
      "         0.0742,  0.0718,  0.0732,  0.0811,  0.0693,  0.0728,  0.0540,  0.0781,\n",
      "         0.0762,  0.0781,  0.0649,  0.0086,  0.0527,  0.0718,  0.0747,  0.0698,\n",
      "         0.0728,  0.0786,  0.0742,  0.0830,  0.0747,  0.0708,  0.0806,  0.0776,\n",
      "         0.0645,  0.0757,  0.0684,  0.0767,  0.0381,  0.0801,  0.1211,  0.0723,\n",
      "         0.0732,  0.0747,  0.0825,  0.0718,  0.0815,  0.0679,  0.0737,  0.0278,\n",
      "         0.0840,  0.0742,  0.0747,  0.0791,  0.0752,  0.0757,  0.0732,  0.0859,\n",
      "         0.0713,  0.0505,  0.0728,  0.0757,  0.0742,  0.0732,  0.0723,  0.0583,\n",
      "         0.0728,  0.0728,  0.0762,  0.0762,  0.0718,  0.0752,  0.0649,  0.0688,\n",
      "         0.0583,  0.0718,  0.0645,  0.0684,  0.0688,  0.0703,  0.0708,  0.0728,\n",
      "         0.0579,  0.0684,  0.0791,  0.0718,  0.0679,  0.0752,  0.0505,  0.0752,\n",
      "         0.0679,  0.0786,  0.0576,  0.0762,  0.0684,  0.0811,  0.0752, -0.0378,\n",
      "         0.0659,  0.0654,  0.0811,  0.0820,  0.0713,  0.0732,  0.0708,  0.1084,\n",
      "         0.0728,  0.0776,  0.0718,  0.0796,  0.0708,  0.0786,  0.0640,  0.0688,\n",
      "         0.0757,  0.0591,  0.0801,  0.0742,  0.0669,  0.0708,  0.0752,  0.0742,\n",
      "         0.0718,  0.0718,  0.0757,  0.0640,  0.0679,  0.0732,  0.0654,  0.0669,\n",
      "         0.0806,  0.0679,  0.0718,  0.0757,  0.0713,  0.0684,  0.0762,  0.0713,\n",
      "         0.0820,  0.0693,  0.0796,  0.0762,  0.0728,  0.0708,  0.0684,  0.0388,\n",
      "         0.0635,  0.0752,  0.0776,  0.0825,  0.0317,  0.0791,  0.0674,  0.0708,\n",
      "         0.0693,  0.0693,  0.0403,  0.0757,  0.0703,  0.0786,  0.0718,  0.0830,\n",
      "         0.0771,  0.0698,  0.0630,  0.0688,  0.0732,  0.0398,  0.0649,  0.0674,\n",
      "         0.0757,  0.0815,  0.0815,  0.0771,  0.0737,  0.0608,  0.0684,  0.0732,\n",
      "         0.0776,  0.0669,  0.0811,  0.0654,  0.0688,  0.0518,  0.0815,  0.0752,\n",
      "         0.0767,  0.0674,  0.0547,  0.0654,  0.0879,  0.0571,  0.0752,  0.0664,\n",
      "         0.0918,  0.0669,  0.0825,  0.0806,  0.0747,  0.0771,  0.0537,  0.0791,\n",
      "         0.0625,  0.0649,  0.0742,  0.0762,  0.0688,  0.0806,  0.0771,  0.0791],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.1.layer.2.DenseReluDense.wi.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.6602,  0.2393,  0.4941,  ..., -0.1992,  0.1328,  0.0073],\n",
      "        [-0.2305,  0.6992,  0.5352,  ..., -0.0825,  0.1245, -0.1357],\n",
      "        [ 0.0474,  0.0229,  0.0566,  ..., -0.3652,  0.3105, -0.6797],\n",
      "        ...,\n",
      "        [ 0.2256,  0.2676,  0.6289,  ...,  0.0167,  0.2275,  0.4375],\n",
      "        [-0.3027,  0.1992,  0.3477,  ..., -0.3887,  0.1475,  0.3496],\n",
      "        [ 0.5000,  0.8438,  0.3125,  ..., -0.1826,  0.2578, -0.2461]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.1.layer.2.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1270, -0.2598,  0.1553,  ...,  0.0129,  0.0825, -0.1709],\n",
      "        [-0.0630,  0.0579,  0.5469,  ...,  0.2402, -0.2188,  0.9062],\n",
      "        [-0.1387, -0.7109, -0.2617,  ..., -0.0223, -1.0000,  1.0391],\n",
      "        ...,\n",
      "        [-0.1982,  0.1108, -0.1260,  ..., -0.4766,  0.0140,  0.1855],\n",
      "        [ 0.0393,  0.2852, -0.2930,  ...,  0.1172, -0.2334,  0.1680],\n",
      "        [ 0.2773, -0.0281, -0.3281,  ..., -0.2266,  0.0630, -0.0928]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.1.layer.2.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([1.1484, 0.4160, 0.4297, 1.3594, 0.9766, 1.1328, 0.8359, 1.2500, 0.7578,\n",
      "        0.9609, 1.5859, 1.0625, 0.7812, 0.9883, 1.0234, 1.0391, 0.9805, 0.6875,\n",
      "        1.0469, 1.0234, 1.0078, 1.0547, 1.0938, 1.0000, 0.9844, 0.7344, 0.9609,\n",
      "        0.9219, 0.9805, 0.9570, 0.9844, 0.4648, 1.0469, 0.9414, 1.0859, 0.7773,\n",
      "        0.9609, 0.4863, 0.9336, 1.1875, 1.0312, 1.0312, 0.9297, 1.0547, 0.5000,\n",
      "        0.9492, 1.0703, 0.9883, 0.9727, 1.1562, 0.9688, 1.0000, 0.9883, 0.9805,\n",
      "        2.0469, 1.1094, 1.1719, 1.0312, 1.0625, 1.1016, 0.9609, 0.9961, 1.0312,\n",
      "        0.9609, 0.9570, 0.3359, 0.9766, 1.0156, 0.5625, 0.9766, 0.9141, 0.9883,\n",
      "        0.8828, 0.9219, 0.9961, 0.9453, 1.0000, 1.0391, 0.8945, 0.8711, 0.9102,\n",
      "        0.9727, 1.0391, 0.7812, 0.8320, 0.9180, 1.1484, 0.9492, 0.9414, 0.9961,\n",
      "        0.9453, 0.9297, 0.9180, 1.1172, 0.9336, 1.0391, 0.9727, 1.0000, 0.9414,\n",
      "        0.9258, 1.1406, 0.8594, 1.1172, 0.9688, 0.9180, 1.1328, 1.0391, 0.9414,\n",
      "        0.8281, 1.0234, 1.0234, 1.0078, 1.0859, 0.9648, 0.8906, 0.7930, 1.0938,\n",
      "        0.9727, 0.1865, 0.9023, 0.9297, 0.8867, 0.7227, 0.8672, 0.9570, 0.9180,\n",
      "        1.0078, 1.1250, 1.0078, 0.9844, 0.9922, 0.8945, 0.9961, 0.9688, 0.7227,\n",
      "        1.0547, 0.9414, 0.7500, 1.2344, 1.4297, 0.8242, 0.9258, 1.1094, 1.0859,\n",
      "        0.9688, 1.0156, 0.7969, 0.7969, 0.8281, 1.1562, 0.9102, 0.9297, 0.8398,\n",
      "        0.9961, 0.9414, 0.9414, 1.0781, 1.0000, 0.8047, 1.0625, 0.8828, 0.9375,\n",
      "        0.8828, 0.9297, 0.7227, 0.5273, 1.0156, 0.6797, 0.4375, 0.9883, 1.0547,\n",
      "        0.9531, 0.9648, 0.9531, 1.0234, 1.0156, 0.9336, 1.0000, 1.1797, 0.6953,\n",
      "        0.9805, 0.9844, 1.1016, 1.0547, 0.9766, 0.8867, 1.0156, 0.7227, 0.9648,\n",
      "        1.0312, 0.9883, 0.9961, 0.7695, 1.0938, 0.9648, 0.8125, 1.0469, 0.9961,\n",
      "        1.0547, 0.8516, 0.9688, 1.2578, 0.9805, 0.8281, 0.9688, 1.1250, 1.0234,\n",
      "        1.0391, 0.8555, 1.0703, 0.7383, 1.1484, 0.9766, 0.9219, 1.0859, 1.1094,\n",
      "        0.9648, 0.8594, 1.0391, 0.5039, 0.9492, 0.8633, 0.7461, 0.9609, 0.9023,\n",
      "        1.0156, 0.2021, 0.9492, 0.9883, 0.9922, 0.8164, 1.0938, 0.9297, 0.9492,\n",
      "        0.8906, 0.9062, 1.1484, 1.1016, 1.2266, 1.1562, 1.2969, 0.9805, 0.8711,\n",
      "        0.8164, 0.9570, 0.9922, 0.8672, 1.1562, 0.8398, 0.9688, 1.0547, 0.9102,\n",
      "        0.9727, 0.9609, 0.8828, 0.8203, 0.9570, 1.3359, 0.9844, 0.9883, 1.0234,\n",
      "        1.3672, 0.9883, 1.2031, 0.9570, 0.9062, 0.8633, 0.8008, 0.8984, 1.0547,\n",
      "        1.0156, 0.4629, 1.0781, 0.8906, 1.1953, 1.0469, 0.9688, 1.1484, 0.9219,\n",
      "        1.0000, 0.1836, 0.5703, 0.9961, 0.9336, 0.9805, 1.0312, 1.0078, 1.0469,\n",
      "        0.8320, 1.0391, 0.8828, 0.9180, 0.9414, 1.1016, 1.0000, 0.9531, 0.4121,\n",
      "        0.9531, 0.9688, 1.1641, 1.0703, 0.9922, 0.2871, 1.0625, 1.0234, 0.9062,\n",
      "        1.0625, 1.0938, 1.0859, 0.8516, 0.8398, 1.0234, 0.9961, 0.9531, 0.4355,\n",
      "        1.1016, 0.9219, 1.0469, 1.0078, 0.8672, 0.9531, 0.9570, 1.0547, 0.9141,\n",
      "        0.7539, 0.9961, 1.0938, 0.9180, 1.1250, 1.0000, 0.9961, 0.9062, 1.0156,\n",
      "        0.8516, 0.9023, 1.0469, 0.8086, 0.9023, 0.8203, 1.2266, 0.9648, 0.9883,\n",
      "        0.9180, 1.0078, 0.9648, 0.8594, 1.0469, 1.1641, 0.5469, 1.0391, 0.8828,\n",
      "        0.7852, 0.8203, 0.8633, 1.0938, 0.9727, 1.0547, 0.2832, 1.0781, 1.0938,\n",
      "        1.0156, 0.7891, 0.7305, 0.8516, 0.8867, 0.8555, 1.0859, 0.9023, 0.9336,\n",
      "        1.0391, 0.9805, 0.9961, 0.8516, 0.9336, 1.0547, 1.0234, 0.9805, 0.7812,\n",
      "        0.9688, 0.7695, 0.9414, 0.9336, 1.0078, 1.0078, 0.9102, 1.1016, 0.9180,\n",
      "        1.1328, 0.8125, 1.0156, 0.8203, 0.9766, 1.0078, 1.3438, 1.0000, 1.0391,\n",
      "        0.9805, 0.9297, 0.9727, 1.1172, 0.8945, 0.9805, 0.9922, 1.0234, 0.9570,\n",
      "        0.9180, 0.8359, 0.9297, 0.8203, 0.9219, 0.9141, 0.8516, 0.9531, 0.9805,\n",
      "        1.0078, 0.9336, 0.8945, 1.0781, 0.3047, 0.9453, 0.6133, 0.9023, 0.9727,\n",
      "        1.3125, 0.9414, 0.8828, 1.1094, 0.9609, 0.9258, 1.6953, 0.9531, 0.9180,\n",
      "        1.0859, 0.8750, 0.9336, 0.8594, 0.9688, 0.7969, 0.9570, 0.9297, 0.9648,\n",
      "        0.9570, 0.6211, 1.0469, 0.9141, 0.8242, 0.9531, 0.9922, 0.2598, 0.9219,\n",
      "        1.0234, 0.9219, 1.0391, 0.9375, 0.8477, 1.0625, 0.7305, 0.4453, 1.0938,\n",
      "        0.8672, 0.9609, 1.0312, 1.0469, 0.9805, 1.1250, 0.9883, 1.0312, 0.9961,\n",
      "        0.7695, 0.9180, 0.9727, 0.9180, 0.6602, 0.9844, 0.9258, 0.9062, 1.0469,\n",
      "        0.9375, 0.9492, 1.0781, 1.2734, 1.1250, 0.9883, 0.8867, 0.2754, 1.0156,\n",
      "        0.9531, 0.7227, 0.9023, 1.0156, 1.0156, 0.9023, 1.1016, 0.9414, 1.0547,\n",
      "        0.8242, 0.8867, 0.9688, 1.0156, 1.1172, 0.9023, 0.4805, 0.9414, 1.0156,\n",
      "        0.9883, 0.9023, 0.9531, 1.0234, 0.8477, 0.9531, 0.8477, 1.0312, 1.0391,\n",
      "        0.9648, 0.7148, 1.0391, 1.0000, 0.8125, 1.1641, 1.1094, 0.9961, 1.0391,\n",
      "        0.8906, 0.8555, 1.0625, 0.8672, 1.1875, 0.6133, 0.9375, 0.8711, 0.7578,\n",
      "        0.9883, 0.9609, 0.9336, 0.8008, 0.9688, 1.0547, 1.0078, 0.9883, 0.9492,\n",
      "        0.9414, 0.9492, 1.0391, 0.9492, 0.9688, 0.9766, 0.9492, 0.9961, 0.9453,\n",
      "        1.0000, 0.8164, 0.8086, 0.8867, 0.8086, 1.0000, 0.9531, 1.0000, 1.0547,\n",
      "        0.7578, 1.0234, 0.9570, 0.8242, 0.4902, 2.1406, 1.0312, 0.8203, 0.9531,\n",
      "        1.0234, 1.0703, 0.9727, 0.8477, 1.0625, 0.9727, 1.1875, 1.1797, 0.8438,\n",
      "        0.9883, 0.9844, 0.8320, 0.1709, 0.6836, 1.0000, 1.0625, 0.9570, 0.9922,\n",
      "        0.9648, 0.9453, 1.1094, 0.9609, 0.9844, 1.0234, 1.0234, 0.8828, 0.9141,\n",
      "        0.9688, 1.0781, 0.5312, 1.0469, 1.5234, 0.9375, 0.9023, 1.0078, 1.0234,\n",
      "        0.9648, 1.0078, 0.9492, 0.8984, 0.3184, 1.1094, 1.0703, 1.0703, 0.9727,\n",
      "        0.9688, 0.9492, 0.9297, 1.1797, 0.9609, 0.2930, 0.8086, 0.9922, 1.0156,\n",
      "        0.9023, 0.9102, 1.1484, 1.2266, 0.9570, 1.2734, 1.0078, 0.8438, 0.9883,\n",
      "        0.7969, 0.8906, 0.7188, 0.7344, 0.9102, 0.9062, 1.2812, 1.0469, 0.9883,\n",
      "        0.9492, 0.8750, 0.8125, 0.9531, 0.9336, 1.0078, 1.1172, 0.7031, 0.9844,\n",
      "        1.1562, 0.9727, 0.6367, 0.9961, 0.8984, 1.0234, 1.0078, 0.8906, 0.9414,\n",
      "        1.1094, 1.0859, 1.1172, 0.8984, 1.0625, 0.9219, 1.0469, 0.9258, 1.0000,\n",
      "        0.8789, 0.9219, 0.9883, 1.0312, 0.8594, 1.0078, 0.9727, 0.8555, 0.9766,\n",
      "        0.9609, 0.9375, 0.8789, 1.0625, 0.9688, 0.9609, 0.9805, 1.0625, 0.8477,\n",
      "        0.9492, 1.0156, 0.9258, 0.9102, 0.9648, 0.9570, 1.0078, 1.0859, 0.8750,\n",
      "        0.8398, 0.9375, 0.9453, 1.0234, 0.8359, 0.9961, 1.0000, 0.9727, 0.9141,\n",
      "        0.9492, 1.1484, 0.7031, 0.9922, 0.9922, 1.2109, 0.4102, 0.9961, 0.9141,\n",
      "        0.9102, 0.8594, 0.8750, 0.8438, 0.5820, 0.9883, 0.8398, 0.9062, 1.0859,\n",
      "        1.0078, 0.9570, 0.8047, 0.8711, 1.0312, 0.4922, 0.8477, 0.9180, 0.9883,\n",
      "        1.1953, 1.0000, 1.0859, 0.9766, 0.8086, 0.7344, 0.9492, 0.9453, 0.9258,\n",
      "        0.9609, 0.9062, 0.8672, 0.9648, 1.0000, 1.1172, 1.1562, 0.8203, 0.9258,\n",
      "        0.8555, 1.0391, 1.1562, 1.1641, 0.8398, 1.0781, 0.8750, 0.9961, 0.9375,\n",
      "        0.9297, 0.9141, 0.7266, 0.9844, 0.8281, 0.8750, 1.0625, 1.2188, 0.8594,\n",
      "        0.9609, 1.0703, 1.1016], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.2.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0366,  0.0327,  0.0118,  ...,  0.0481, -0.0272, -0.0193],\n",
      "        [ 0.0109, -0.0732,  0.0378,  ..., -0.0554,  0.0525, -0.0378],\n",
      "        [-0.0011, -0.0537,  0.0178,  ..., -0.0117,  0.0017,  0.0106],\n",
      "        ...,\n",
      "        [ 0.0154, -0.0850, -0.0170,  ...,  0.0275, -0.0024,  0.0216],\n",
      "        [ 0.0320,  0.0309, -0.0113,  ...,  0.0211,  0.0122, -0.0106],\n",
      "        [-0.0459,  0.0222, -0.0308,  ..., -0.0184,  0.0084, -0.0483]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.2.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.4199, -0.5664, -0.1025,  ..., -0.2695, -0.0184, -0.4121],\n",
      "        [-0.3242,  0.4141,  0.1533,  ...,  0.1445, -0.2021,  0.1533],\n",
      "        [ 0.4375, -0.2197,  0.3398,  ..., -0.1748,  0.3418, -0.2051],\n",
      "        ...,\n",
      "        [ 0.0596, -0.2158,  0.4043,  ...,  0.3398, -0.2520, -0.2539],\n",
      "        [-0.0952, -0.3809, -0.1138,  ...,  0.3906, -0.0801,  0.0100],\n",
      "        [-0.7695,  0.3691, -0.3770,  ...,  0.3574, -0.0303, -0.8125]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.2.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.3066,  0.8477, -0.1270,  ..., -0.7461,  0.3848, -1.0703],\n",
      "        [ 0.8320, -0.2402,  0.7148,  ..., -0.5508,  0.0154, -0.3711],\n",
      "        [ 0.2637, -0.6797,  0.7070,  ..., -0.6133,  0.3809, -0.6914],\n",
      "        ...,\n",
      "        [ 0.1621,  0.2988,  0.5977,  ...,  0.3555,  0.6367,  0.4082],\n",
      "        [ 0.7070,  0.0072, -0.0898,  ..., -0.2471,  1.0859, -0.6367],\n",
      "        [-0.0396,  0.1309,  0.0131,  ..., -0.6172, -0.6211,  0.6289]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.2.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 1.1084e-01, -1.7188e-01, -3.7695e-01,  ..., -3.8672e-01,\n",
      "          2.2363e-01,  1.4160e-01],\n",
      "        [-1.0078e+00, -1.9766e+00, -1.0547e+00,  ..., -5.2344e-01,\n",
      "          2.4023e-01, -1.5527e-01],\n",
      "        [ 3.3203e-01, -1.6895e-01, -2.7344e-01,  ..., -3.5352e-01,\n",
      "          6.0547e-01, -1.6211e-01],\n",
      "        ...,\n",
      "        [-1.8066e-01,  6.2891e-01,  9.4604e-03,  ..., -5.2734e-02,\n",
      "          2.8711e-01,  9.6875e-01],\n",
      "        [-9.2285e-02, -2.6489e-02,  4.9609e-01,  ...,  3.0469e-01,\n",
      "         -5.0000e-01,  5.8350e-02],\n",
      "        [ 1.4941e-01, -6.2500e-01, -3.1836e-01,  ..., -6.2891e-01,\n",
      "          7.3047e-01,  6.7902e-04]], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.2.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([0.1953, 0.0737, 0.0874, 0.2373, 0.1934, 0.1602, 0.1738, 0.2061, 0.1484,\n",
      "        0.1943, 0.1250, 0.2178, 0.0991, 0.1465, 0.1836, 0.1963, 0.1807, 0.1396,\n",
      "        0.1924, 0.2061, 0.2002, 0.1914, 0.2041, 0.1953, 0.1914, 0.1377, 0.1895,\n",
      "        0.1504, 0.1865, 0.1855, 0.1924, 0.0938, 0.2031, 0.1826, 0.1953, 0.1475,\n",
      "        0.2041, 0.1621, 0.1787, 0.2246, 0.2070, 0.1943, 0.1924, 0.1973, 0.1006,\n",
      "        0.1865, 0.2051, 0.1924, 0.1992, 0.2197, 0.1816, 0.1885, 0.1738, 0.1963,\n",
      "        0.3379, 0.1533, 0.1729, 0.2021, 0.2207, 0.1836, 0.1787, 0.1982, 0.1963,\n",
      "        0.1826, 0.2021, 0.1040, 0.1943, 0.1934, 0.0986, 0.1875, 0.2021, 0.2041,\n",
      "        0.1904, 0.1807, 0.1738, 0.1797, 0.1406, 0.1807, 0.1729, 0.1797, 0.1934,\n",
      "        0.1826, 0.2070, 0.1436, 0.1650, 0.1836, 0.2031, 0.2070, 0.1914, 0.1895,\n",
      "        0.1777, 0.1816, 0.1729, 0.1758, 0.1855, 0.2012, 0.1934, 0.1953, 0.2002,\n",
      "        0.1943, 0.1699, 0.1631, 0.0981, 0.1836, 0.1777, 0.2061, 0.1865, 0.1719,\n",
      "        0.1709, 0.1934, 0.2031, 0.2139, 0.1855, 0.1797, 0.1621, 0.1641, 0.1924,\n",
      "        0.1816, 0.0447, 0.1709, 0.1914, 0.1826, 0.1465, 0.1836, 0.2031, 0.1699,\n",
      "        0.2080, 0.1699, 0.1787, 0.1289, 0.1299, 0.1641, 0.1865, 0.1836, 0.0981,\n",
      "        0.1992, 0.1816, 0.1455, 0.1777, 0.1187, 0.1504, 0.1885, 0.1914, 0.2041,\n",
      "        0.1729, 0.1768, 0.1660, 0.1514, 0.1777, 0.1846, 0.1885, 0.1787, 0.1924,\n",
      "        0.1650, 0.1973, 0.2012, 0.2119, 0.2061, 0.0500, 0.1055, 0.1660, 0.1641,\n",
      "        0.1660, 0.1631, 0.1533, 0.0967, 0.2041, 0.1436, 0.0850, 0.2012, 0.0957,\n",
      "        0.1855, 0.2178, 0.1953, 0.1943, 0.2012, 0.1807, 0.2207, 0.0884, 0.1445,\n",
      "        0.1973, 0.1904, 0.2070, 0.2109, 0.1641, 0.1680, 0.1992, 0.0288, 0.1914,\n",
      "        0.1416, 0.1943, 0.1934, 0.1582, 0.2129, 0.1777, 0.1650, 0.2100, 0.2080,\n",
      "        0.2051, 0.1689, 0.2031, 0.1914, 0.1885, 0.1650, 0.1855, 0.1807, 0.1943,\n",
      "        0.1885, 0.1709, 0.2236, 0.1621, 0.1572, 0.1797, 0.1699, 0.1992, 0.2109,\n",
      "        0.1748, 0.1680, 0.2100, 0.0327, 0.1289, 0.1758, 0.1465, 0.1855, 0.1641,\n",
      "        0.2021, 0.1982, 0.2070, 0.1943, 0.2031, 0.1738, 0.2139, 0.1748, 0.2070,\n",
      "        0.1836, 0.1719, 0.1768, 0.1973, 0.1104, 0.2070, 0.1621, 0.2061, 0.1650,\n",
      "        0.1660, 0.2100, 0.1797, 0.1797, 0.1875, 0.1719, 0.1982, 0.1934, 0.1689,\n",
      "        0.1846, 0.1846, 0.1855, 0.1650, 0.2080, 0.1162, 0.2002, 0.1836, 0.2275,\n",
      "        0.1187, 0.2061, 0.2109, 0.1855, 0.1826, 0.1768, 0.1768, 0.1895, 0.1895,\n",
      "        0.1982, 0.1074, 0.2119, 0.1846, 0.0957, 0.1982, 0.1904, 0.2021, 0.1729,\n",
      "        0.1621, 0.0339, 0.1099, 0.1846, 0.1807, 0.2031, 0.1943, 0.1934, 0.2051,\n",
      "        0.1377, 0.1436, 0.1807, 0.1738, 0.1777, 0.1992, 0.1865, 0.1758, 0.0952,\n",
      "        0.1943, 0.1855, 0.2314, 0.2119, 0.1914, 0.0361, 0.1943, 0.2051, 0.1729,\n",
      "        0.2090, 0.2021, 0.1943, 0.1758, 0.1855, 0.1943, 0.1924, 0.1963, 0.1572,\n",
      "        0.1797, 0.1992, 0.2285, 0.2139, 0.1846, 0.2012, 0.1904, 0.1650, 0.1816,\n",
      "        0.1631, 0.1924, 0.1904, 0.2002, 0.2178, 0.1973, 0.1963, 0.1650, 0.2119,\n",
      "        0.1582, 0.1709, 0.1934, 0.1768, 0.1924, 0.1689, 0.1768, 0.1777, 0.1836,\n",
      "        0.1982, 0.1914, 0.1963, 0.1748, 0.2109, 0.1641, 0.0884, 0.1885, 0.1865,\n",
      "        0.1631, 0.1553, 0.1797, 0.1875, 0.1934, 0.2002, 0.0466, 0.2100, 0.2051,\n",
      "        0.1963, 0.1475, 0.1133, 0.1592, 0.1826, 0.1738, 0.1992, 0.1787, 0.1729,\n",
      "        0.1855, 0.2080, 0.2109, 0.1758, 0.1768, 0.2109, 0.1738, 0.1904, 0.2041,\n",
      "        0.1963, 0.1426, 0.1787, 0.1826, 0.1406, 0.1758, 0.1816, 0.2129, 0.1895,\n",
      "        0.2256, 0.1436, 0.2090, 0.1621, 0.1846, 0.1729, 0.2344, 0.1797, 0.1973,\n",
      "        0.1963, 0.1768, 0.1846, 0.1855, 0.1738, 0.1934, 0.1963, 0.2051, 0.1738,\n",
      "        0.1973, 0.1406, 0.1816, 0.1689, 0.1904, 0.1826, 0.1807, 0.2041, 0.1816,\n",
      "        0.1895, 0.1895, 0.1699, 0.1943, 0.1523, 0.1846, 0.0291, 0.1621, 0.1963,\n",
      "        0.0781, 0.1963, 0.1855, 0.1846, 0.1855, 0.1768, 0.1367, 0.1914, 0.1904,\n",
      "        0.2012, 0.1621, 0.1729, 0.1660, 0.1934, 0.1748, 0.1836, 0.1895, 0.1973,\n",
      "        0.1836, 0.1523, 0.2139, 0.2041, 0.1826, 0.1895, 0.1846, 0.0491, 0.1719,\n",
      "        0.2002, 0.1953, 0.2012, 0.1885, 0.1738, 0.1924, 0.1855, 0.1719, 0.1963,\n",
      "        0.1777, 0.2041, 0.1992, 0.2070, 0.1914, 0.2051, 0.1924, 0.1680, 0.1904,\n",
      "        0.1533, 0.1836, 0.1865, 0.1816, 0.0996, 0.1885, 0.1992, 0.1836, 0.1992,\n",
      "        0.1885, 0.1836, 0.2051, 0.1562, 0.2080, 0.2041, 0.1660, 0.0708, 0.1934,\n",
      "        0.1758, 0.0356, 0.1836, 0.1973, 0.2021, 0.1826, 0.2109, 0.2012, 0.2041,\n",
      "        0.0771, 0.1729, 0.1816, 0.2051, 0.1982, 0.1826, 0.1328, 0.1885, 0.1924,\n",
      "        0.1914, 0.1826, 0.1953, 0.1768, 0.1816, 0.1797, 0.1719, 0.1963, 0.1855,\n",
      "        0.1807, 0.1602, 0.2188, 0.1787, 0.1621, 0.1709, 0.2188, 0.1904, 0.1953,\n",
      "        0.1846, 0.1855, 0.1797, 0.1680, 0.2100, 0.1123, 0.1787, 0.1826, 0.1367,\n",
      "        0.1807, 0.1943, 0.1885, 0.1729, 0.1973, 0.2080, 0.1855, 0.2002, 0.1797,\n",
      "        0.2021, 0.1875, 0.1914, 0.1934, 0.1738, 0.1836, 0.1914, 0.1963, 0.1777,\n",
      "        0.1895, 0.1406, 0.1445, 0.1875, 0.1260, 0.2041, 0.1904, 0.1758, 0.1953,\n",
      "        0.1465, 0.1943, 0.1855, 0.1699, 0.1650, 0.3379, 0.2197, 0.1709, 0.1895,\n",
      "        0.2031, 0.1885, 0.1973, 0.1855, 0.2100, 0.1895, 0.1650, 0.1270, 0.1719,\n",
      "        0.1982, 0.1982, 0.1699, 0.0327, 0.1328, 0.1934, 0.1904, 0.1787, 0.1846,\n",
      "        0.1895, 0.1768, 0.2041, 0.2012, 0.1855, 0.2051, 0.2119, 0.1738, 0.1738,\n",
      "        0.1816, 0.1992, 0.1079, 0.1904, 0.2412, 0.1846, 0.1631, 0.1846, 0.1943,\n",
      "        0.1777, 0.1973, 0.1768, 0.1885, 0.0845, 0.2100, 0.1895, 0.2080, 0.1758,\n",
      "        0.1963, 0.2002, 0.1904, 0.2285, 0.1768, 0.1260, 0.1729, 0.1973, 0.1982,\n",
      "        0.1680, 0.1904, 0.1357, 0.1826, 0.1895, 0.1738, 0.2061, 0.1777, 0.1992,\n",
      "        0.1572, 0.1885, 0.1582, 0.1562, 0.1738, 0.1895, 0.1494, 0.1748, 0.1885,\n",
      "        0.1777, 0.1465, 0.1777, 0.1885, 0.1855, 0.1426, 0.1875, 0.1196, 0.1846,\n",
      "        0.1768, 0.2041, 0.1084, 0.1973, 0.1660, 0.1934, 0.2031, 0.1025, 0.1924,\n",
      "        0.1523, 0.2002, 0.2031, 0.1875, 0.1885, 0.1836, 0.2334, 0.1953, 0.2051,\n",
      "        0.1777, 0.2061, 0.1611, 0.2031, 0.1582, 0.1934, 0.1914, 0.1455, 0.1914,\n",
      "        0.1973, 0.1680, 0.1777, 0.1904, 0.1855, 0.1836, 0.1895, 0.1738, 0.1650,\n",
      "        0.1826, 0.2002, 0.1855, 0.1758, 0.2041, 0.1797, 0.2012, 0.1982, 0.1660,\n",
      "        0.1650, 0.1943, 0.1826, 0.2158, 0.1729, 0.1846, 0.1904, 0.1807, 0.1924,\n",
      "        0.1738, 0.0825, 0.1572, 0.1953, 0.2002, 0.1582, 0.1045, 0.1846, 0.1846,\n",
      "        0.1777, 0.1445, 0.1641, 0.1211, 0.1816, 0.1719, 0.1865, 0.1787, 0.1904,\n",
      "        0.1963, 0.1875, 0.1475, 0.1777, 0.1924, 0.0981, 0.1807, 0.1689, 0.1934,\n",
      "        0.1943, 0.2109, 0.1924, 0.1904, 0.0618, 0.1807, 0.1680, 0.2021, 0.1807,\n",
      "        0.1904, 0.1650, 0.1758, 0.0947, 0.1982, 0.1797, 0.2158, 0.1680, 0.1357,\n",
      "        0.1641, 0.2021, 0.1426, 0.1846, 0.1572, 0.2002, 0.1846, 0.1855, 0.2012,\n",
      "        0.1816, 0.1807, 0.1040, 0.1758, 0.1553, 0.1533, 0.1895, 0.1826, 0.1709,\n",
      "        0.1953, 0.2080, 0.1855], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.2.layer.1.EncDecAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0256,  0.0049, -0.0640,  ..., -0.0024,  0.0079, -0.0442],\n",
      "        [-0.0403,  0.0004, -0.0540,  ..., -0.0317, -0.0420, -0.0087],\n",
      "        [ 0.0081, -0.0312,  0.0125,  ...,  0.0479,  0.0287,  0.0160],\n",
      "        ...,\n",
      "        [ 0.0115, -0.0018, -0.0332,  ..., -0.0228, -0.0035,  0.0087],\n",
      "        [ 0.0297,  0.0454, -0.0134,  ...,  0.0126, -0.0228,  0.0684],\n",
      "        [-0.0117,  0.0262,  0.0579,  ...,  0.0850, -0.0410,  0.0684]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.2.layer.1.EncDecAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.2910, -0.3535,  0.1592,  ..., -0.5039,  0.0496, -0.1650],\n",
      "        [-0.1260, -0.1494, -0.0674,  ..., -0.5742, -0.1279,  0.1279],\n",
      "        [-0.0991,  0.0259,  0.3516,  ..., -0.2969,  0.2793,  0.0439],\n",
      "        ...,\n",
      "        [ 0.3301,  0.2080,  0.1157,  ...,  0.0075,  0.1147, -0.0491],\n",
      "        [ 0.2383,  0.0435, -0.3730,  ...,  0.2969, -0.6328, -0.2480],\n",
      "        [-0.2266, -0.3027,  0.1826,  ...,  0.1416, -0.1689, -0.0583]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.2.layer.1.EncDecAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.1162, -0.6250,  0.4453,  ..., -0.0221, -0.0942,  0.9805],\n",
      "        [-0.0068, -0.4180,  0.2236,  ...,  0.8008,  0.1543, -0.2217],\n",
      "        [-0.0048, -0.6133, -0.0206,  ..., -0.2812,  0.0986, -0.1250],\n",
      "        ...,\n",
      "        [ 0.3145,  0.0488,  0.0160,  ...,  0.2773, -0.0227,  0.0981],\n",
      "        [ 0.4492, -0.0134, -0.1836,  ...,  0.5508,  0.2227,  0.0796],\n",
      "        [ 0.1602,  0.2539, -0.1128,  ...,  0.4160,  0.0059,  0.2324]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.2.layer.1.EncDecAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1230,  0.6211, -0.4570,  ..., -0.2373,  0.1187,  0.1328],\n",
      "        [ 0.5938, -0.1084,  0.0170,  ...,  0.1250, -0.0762, -0.2949],\n",
      "        [-1.4453,  0.0043, -1.0703,  ...,  0.3164,  0.1885,  0.2969],\n",
      "        ...,\n",
      "        [-0.3652, -0.0132, -0.6055,  ..., -0.3516,  0.1455, -0.2500],\n",
      "        [ 0.0408, -0.4512, -0.4707,  ..., -0.3184,  0.0752,  0.1035],\n",
      "        [-0.0923,  0.2949,  0.6445,  ...,  0.2031,  0.0457,  0.0364]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.2.layer.1.layer_norm.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 7.3730e-02,  2.8442e-02,  3.5645e-02,  1.1328e-01,  8.1543e-02,\n",
      "         5.9326e-02,  8.0566e-02,  9.1309e-02,  6.1035e-02,  8.2031e-02,\n",
      "         6.1523e-02,  9.4238e-02,  6.8848e-02,  5.9814e-02,  7.6660e-02,\n",
      "         8.3496e-02,  7.2266e-02,  5.4932e-02,  7.5684e-02,  9.7656e-02,\n",
      "         8.2520e-02,  8.0078e-02,  8.5449e-02,  8.8867e-02,  8.3984e-02,\n",
      "         6.3477e-02,  8.1543e-02,  5.7861e-02,  7.9590e-02,  8.6914e-02,\n",
      "         8.2520e-02,  4.1748e-02,  8.8379e-02,  8.3496e-02,  8.9844e-02,\n",
      "         6.0303e-02,  9.5703e-02,  6.5918e-02,  7.7637e-02,  8.9844e-02,\n",
      "         9.7168e-02,  8.0078e-02,  7.3730e-02,  8.0566e-02,  4.1992e-02,\n",
      "         7.7637e-02,  1.0449e-01,  8.1055e-02,  9.0820e-02,  8.4961e-02,\n",
      "         8.0566e-02,  7.6172e-02,  7.2266e-02,  7.9590e-02,  1.4551e-01,\n",
      "         6.4453e-02,  6.9824e-02,  8.1055e-02,  8.0078e-02,  8.4473e-02,\n",
      "         7.1289e-02,  8.4961e-02,  8.0566e-02,  7.9102e-02,  8.8867e-02,\n",
      "         6.4453e-02,  8.0566e-02,  8.4473e-02,  3.3936e-02,  8.1543e-02,\n",
      "         8.8867e-02,  9.1797e-02,  7.7637e-02,  7.7148e-02,  7.5684e-02,\n",
      "         7.2754e-02,  5.4199e-02,  7.4707e-02,  7.8125e-02,  7.6660e-02,\n",
      "         8.4961e-02,  7.5195e-02,  7.9590e-02,  6.1768e-02,  6.8848e-02,\n",
      "         7.7148e-02,  8.9844e-02,  8.7402e-02,  7.9590e-02,  8.1543e-02,\n",
      "         7.0312e-02,  8.2520e-02,  7.7148e-02,  6.4941e-02,  7.5195e-02,\n",
      "         8.3008e-02,  8.1055e-02,  9.0332e-02,  8.0566e-02,  7.7148e-02,\n",
      "         7.2266e-02,  7.3242e-02,  3.7354e-02,  7.8613e-02,  7.7148e-02,\n",
      "         8.2031e-02,  8.4961e-02,  6.9336e-02,  8.3496e-02,  8.5938e-02,\n",
      "         8.8379e-02,  8.3008e-02,  7.4707e-02,  7.1777e-02,  7.2266e-02,\n",
      "         8.9355e-02,  7.4219e-02,  7.9102e-02,  3.4714e-04,  7.6660e-02,\n",
      "         8.8379e-02,  8.2031e-02,  6.2988e-02,  6.6406e-02,  8.0078e-02,\n",
      "         7.0801e-02,  8.0566e-02,  6.2988e-02,  7.5195e-02,  5.2979e-02,\n",
      "         5.4443e-02,  7.2266e-02,  7.6660e-02,  8.1055e-02,  5.3955e-02,\n",
      "         9.3262e-02,  7.8613e-02,  6.0303e-02,  7.5195e-02,  5.7373e-02,\n",
      "         6.1523e-02,  8.2031e-02,  7.2266e-02,  8.2520e-02,  7.3730e-02,\n",
      "         7.2754e-02,  7.3242e-02,  5.9326e-02,  6.8359e-02,  6.8848e-02,\n",
      "         7.9102e-02,  7.0312e-02,  8.3496e-02,  6.6406e-02,  9.0332e-02,\n",
      "         7.9102e-02,  9.4238e-02,  8.0566e-02,  1.3477e-01,  6.4453e-02,\n",
      "         7.0312e-02,  6.9824e-02,  7.1777e-02,  6.4453e-02,  6.0059e-02,\n",
      "         3.0762e-02,  8.1055e-02,  5.3955e-02,  2.9663e-02,  9.3750e-02,\n",
      "         3.3447e-02,  7.7148e-02,  9.5215e-02,  7.4219e-02,  7.5195e-02,\n",
      "         8.4961e-02,  8.2031e-02,  9.2773e-02, -2.9297e-02,  5.6885e-02,\n",
      "         9.0332e-02,  7.6660e-02,  8.3984e-02,  9.5215e-02,  6.7383e-02,\n",
      "         7.5195e-02,  7.4707e-02,  7.6660e-02,  8.0566e-02,  5.9814e-02,\n",
      "         8.2520e-02,  7.8613e-02,  7.0801e-02,  8.4473e-02,  7.4707e-02,\n",
      "         6.7871e-02,  7.9590e-02,  8.4961e-02,  8.2520e-02,  7.1777e-02,\n",
      "         8.4961e-02,  7.8125e-02,  8.1543e-02,  6.3477e-02,  7.3242e-02,\n",
      "         6.5918e-02,  8.8867e-02,  7.9590e-02,  7.7148e-02,  8.8379e-02,\n",
      "         6.8359e-02,  6.2012e-02,  6.6895e-02,  7.8125e-02,  8.0566e-02,\n",
      "         8.0078e-02,  7.2266e-02,  6.3965e-02,  8.6914e-02,  1.8555e-02,\n",
      "         5.5664e-02,  7.5684e-02,  5.9570e-02,  7.8613e-02,  6.5430e-02,\n",
      "         9.4238e-02,  6.7871e-02,  1.0156e-01,  7.3730e-02,  9.0820e-02,\n",
      "         6.7871e-02,  8.0566e-02,  6.7871e-02,  9.0820e-02,  7.1777e-02,\n",
      "         6.4941e-02,  7.4707e-02,  8.2520e-02,  5.5176e-02,  7.9590e-02,\n",
      "         5.9326e-02,  9.2285e-02,  7.4707e-02,  7.2754e-02,  8.8379e-02,\n",
      "         7.8613e-02,  7.3730e-02,  7.4707e-02,  7.0801e-02,  7.4707e-02,\n",
      "         7.7148e-02,  7.2266e-02,  8.0078e-02,  7.9590e-02,  7.5195e-02,\n",
      "         6.6406e-02,  8.7891e-02,  5.2490e-02,  8.2520e-02,  7.5195e-02,\n",
      "         9.2285e-02,  6.0303e-02,  8.5938e-02,  8.4961e-02,  7.0801e-02,\n",
      "         7.5684e-02,  8.3984e-02,  7.2754e-02,  8.0078e-02,  8.1543e-02,\n",
      "         8.1055e-02,  4.9072e-02,  8.6914e-02,  7.6172e-02, -4.1016e-02,\n",
      "         8.4961e-02,  8.3008e-02,  7.7637e-02,  7.7148e-02,  7.0312e-02,\n",
      "         5.6076e-04,  4.2969e-02,  8.1543e-02,  7.4707e-02,  8.9844e-02,\n",
      "         8.1543e-02,  8.5938e-02,  9.2773e-02,  6.0547e-02,  6.0303e-02,\n",
      "         7.4707e-02,  7.3730e-02,  7.0801e-02,  8.4961e-02,  7.8125e-02,\n",
      "         7.9590e-02,  4.2236e-02,  8.6914e-02,  7.8125e-02,  1.0010e-01,\n",
      "         8.6914e-02,  7.7148e-02,  7.1335e-04,  7.9102e-02,  9.1797e-02,\n",
      "         7.5684e-02,  7.6660e-02,  8.3008e-02,  7.9590e-02,  7.7148e-02,\n",
      "         7.7637e-02,  8.2520e-02,  8.2031e-02,  8.9844e-02,  9.2773e-02,\n",
      "         7.7637e-02,  9.8633e-02,  8.5938e-02,  8.5449e-02,  8.4961e-02,\n",
      "         8.3984e-02,  8.2031e-02,  6.5918e-02,  7.6172e-02,  6.7871e-02,\n",
      "         7.5195e-02,  7.2266e-02,  8.3496e-02,  9.1309e-02,  7.6172e-02,\n",
      "         8.9844e-02,  6.7383e-02,  8.6426e-02,  7.0312e-02,  7.3242e-02,\n",
      "         8.1543e-02,  7.9102e-02,  8.7402e-02,  6.8359e-02,  7.6660e-02,\n",
      "         7.2754e-02,  8.2520e-02,  8.9844e-02,  7.7637e-02,  8.0078e-02,\n",
      "         7.1777e-02,  8.0078e-02,  6.5918e-02,  3.8818e-02,  7.1289e-02,\n",
      "         8.0078e-02,  6.4941e-02,  6.7383e-02,  7.1777e-02,  7.9590e-02,\n",
      "         8.3984e-02,  9.4727e-02,  1.3550e-02,  9.0820e-02,  9.2285e-02,\n",
      "         8.1543e-02,  5.8838e-02,  4.5654e-02,  6.3965e-02,  8.2520e-02,\n",
      "         8.4961e-02,  8.3496e-02,  6.9336e-02,  7.5195e-02,  7.4219e-02,\n",
      "         8.3496e-02,  7.6172e-02,  7.5195e-02,  7.2754e-02,  9.2285e-02,\n",
      "         7.3730e-02,  8.1543e-02,  8.2031e-02,  8.8867e-02,  6.2256e-02,\n",
      "         7.0801e-02,  7.6172e-02,  6.9824e-02,  8.0566e-02,  7.4219e-02,\n",
      "         8.1055e-02,  7.8613e-02,  8.4473e-02,  6.8848e-02,  8.8379e-02,\n",
      "         6.3477e-02,  7.6172e-02,  6.7383e-02,  9.3750e-02,  7.6172e-02,\n",
      "         8.1055e-02,  9.0332e-02,  6.6406e-02,  7.6172e-02,  7.3730e-02,\n",
      "         7.6172e-02,  7.4707e-02,  8.3496e-02,  8.8379e-02,  6.6406e-02,\n",
      "         7.0312e-02,  6.4941e-02,  7.7637e-02,  6.9824e-02,  7.6172e-02,\n",
      "         7.9102e-02,  8.2520e-02,  8.0078e-02,  7.7637e-02,  8.3008e-02,\n",
      "         8.9844e-02,  7.3730e-02,  7.8125e-02,  7.2754e-02,  7.7637e-02,\n",
      "         2.8442e-02,  7.5195e-02,  9.2773e-02, -2.2339e-02,  9.2773e-02,\n",
      "         7.8613e-02,  8.1543e-02,  8.0566e-02,  7.6172e-02,  5.4932e-02,\n",
      "         9.5215e-02,  7.6660e-02,  9.0820e-02,  6.8359e-02,  6.8359e-02,\n",
      "         6.8848e-02,  8.6426e-02,  7.5195e-02,  7.8613e-02,  7.7148e-02,\n",
      "         7.9102e-02,  7.6660e-02,  5.5420e-02,  8.4473e-02,  8.1543e-02,\n",
      "         7.8613e-02,  8.5449e-02,  9.0332e-02,  1.9287e-02,  7.7637e-02,\n",
      "         7.5195e-02,  8.4961e-02,  7.7637e-02,  8.3008e-02,  7.5195e-02,\n",
      "         6.9824e-02,  1.2354e-01,  6.5430e-02,  8.0566e-02,  7.2754e-02,\n",
      "         8.5938e-02,  8.3008e-02,  8.6426e-02,  7.3242e-02,  8.3496e-02,\n",
      "         8.1055e-02,  6.9824e-02,  7.3730e-02,  7.0312e-02,  7.2754e-02,\n",
      "         8.2031e-02,  7.9102e-02,  3.3447e-02,  8.1543e-02,  7.7148e-02,\n",
      "         7.8613e-02,  8.6426e-02,  7.0801e-02,  7.7637e-02,  8.0078e-02,\n",
      "         6.5918e-02,  8.5449e-02,  8.7891e-02,  6.8359e-02,  2.2949e-02,\n",
      "         8.0078e-02,  7.1289e-02,  1.2793e-01,  7.8613e-02,  8.4473e-02,\n",
      "         9.1309e-02,  8.7402e-02,  8.1055e-02,  8.2520e-02,  8.3984e-02,\n",
      "        -3.7193e-05,  7.2754e-02,  7.8125e-02,  7.9590e-02,  8.0078e-02,\n",
      "         7.5195e-02,  7.9102e-02,  7.4707e-02,  8.1543e-02,  7.3730e-02,\n",
      "         7.0312e-02,  8.3008e-02,  6.6895e-02,  7.2266e-02,  9.2285e-02,\n",
      "         6.9824e-02,  9.3750e-02,  7.0312e-02,  7.8125e-02,  6.2988e-02,\n",
      "         9.0820e-02,  7.2266e-02,  6.4453e-02,  6.4453e-02,  8.5938e-02,\n",
      "         8.0078e-02,  7.9590e-02,  8.0566e-02,  7.6660e-02,  7.4707e-02,\n",
      "         6.8848e-02,  7.5684e-02,  4.9561e-02,  6.8848e-02,  7.2754e-02,\n",
      "         5.2002e-02,  7.4219e-02,  7.8125e-02,  7.7148e-02,  6.6406e-02,\n",
      "         8.1543e-02,  8.7891e-02,  9.2285e-02,  8.4961e-02,  7.4219e-02,\n",
      "         7.5195e-02,  7.6660e-02,  7.6660e-02,  7.8125e-02,  7.0312e-02,\n",
      "         8.1055e-02,  8.3008e-02,  8.1055e-02,  7.9102e-02,  7.4219e-02,\n",
      "         6.6895e-02,  7.2266e-02,  7.5684e-02,  6.1035e-02,  8.7402e-02,\n",
      "         8.3496e-02,  7.5195e-02,  7.8613e-02,  5.8105e-02,  8.0566e-02,\n",
      "         7.6660e-02,  6.1523e-02,  4.6631e-02,  1.6309e-01,  9.0332e-02,\n",
      "         7.6660e-02,  7.9590e-02,  7.7637e-02,  6.7383e-02,  8.2520e-02,\n",
      "         8.0078e-02,  8.9355e-02,  7.1777e-02,  7.5684e-02,  5.7617e-02,\n",
      "         7.4707e-02,  8.4473e-02,  8.7891e-02,  7.1289e-02,  1.9646e-04,\n",
      "         5.1025e-02,  7.9102e-02,  8.5449e-02,  7.3730e-02,  7.8125e-02,\n",
      "         8.3984e-02,  7.6172e-02,  8.3008e-02,  7.5195e-02,  7.7148e-02,\n",
      "         8.4473e-02,  8.9355e-02,  7.3730e-02,  7.5195e-02,  7.1289e-02,\n",
      "         8.4473e-02,  4.1016e-02,  8.0566e-02,  1.1230e-01,  7.3242e-02,\n",
      "         6.2988e-02,  8.4473e-02, -8.1055e-02,  7.1289e-02,  8.6914e-02,\n",
      "         7.6660e-02,  8.3496e-02,  3.5400e-02,  8.4961e-02,  8.1543e-02,\n",
      "         8.2031e-02,  7.9102e-02,  7.0801e-02,  8.6426e-02,  7.7148e-02,\n",
      "         8.6914e-02,  7.3730e-02,  6.7383e-02,  8.3496e-02,  7.9102e-02,\n",
      "         8.3496e-02,  7.5684e-02,  8.6426e-02,  5.8838e-02,  7.5684e-02,\n",
      "         7.4219e-02,  8.2031e-02,  7.6660e-02,  7.1289e-02,  7.8125e-02,\n",
      "         6.5430e-02,  7.4219e-02,  6.1768e-02,  6.4941e-02,  7.5195e-02,\n",
      "         7.1777e-02,  7.8613e-02,  7.2266e-02,  7.9102e-02,  7.5684e-02,\n",
      "         5.5908e-02,  7.8125e-02,  8.5938e-02,  7.3730e-02,  6.1279e-02,\n",
      "         7.4707e-02,  4.9316e-02,  8.5449e-02,  7.5195e-02,  9.3750e-02,\n",
      "         6.3477e-02,  8.3496e-02,  7.6660e-02,  8.5938e-02,  8.2520e-02,\n",
      "         4.0039e-02,  6.9824e-02,  7.2754e-02,  8.5938e-02,  8.5449e-02,\n",
      "         7.4707e-02,  6.9824e-02,  7.1777e-02,  1.1865e-01,  7.9102e-02,\n",
      "         8.5449e-02,  7.4219e-02,  8.3496e-02,  7.5684e-02,  8.3496e-02,\n",
      "         6.6895e-02,  8.0566e-02,  7.0312e-02,  5.7861e-02,  8.2520e-02,\n",
      "         7.8125e-02,  6.7871e-02,  7.3242e-02,  7.7637e-02,  8.3496e-02,\n",
      "         7.9102e-02,  7.8125e-02,  7.8125e-02,  7.0312e-02,  6.8359e-02,\n",
      "         7.4219e-02,  7.5195e-02,  7.0801e-02,  8.6426e-02,  7.0801e-02,\n",
      "         7.8125e-02,  7.7637e-02,  5.9570e-02,  6.5918e-02,  8.3008e-02,\n",
      "         7.1289e-02,  8.3496e-02,  7.3730e-02,  8.1055e-02,  8.2031e-02,\n",
      "         7.6660e-02,  8.2031e-02,  7.1777e-02,  4.0039e-02,  6.3477e-02,\n",
      "         8.1055e-02,  8.1543e-02,  7.2266e-02,  4.2725e-02,  7.1777e-02,\n",
      "         7.1289e-02,  7.4707e-02,  7.3242e-02,  7.0312e-02,  3.6865e-02,\n",
      "         8.7402e-02,  7.3242e-02,  8.1055e-02,  7.4219e-02,  7.8125e-02,\n",
      "         8.9355e-02,  7.1289e-02,  6.1279e-02,  7.3242e-02,  7.8125e-02,\n",
      "         4.4678e-02,  6.8848e-02,  6.8359e-02,  7.2754e-02,  9.4238e-02,\n",
      "         8.7402e-02,  7.8125e-02,  8.3984e-02,  6.1279e-02,  7.4707e-02,\n",
      "         7.5684e-02,  8.7402e-02,  7.2266e-02,  7.6172e-02,  6.4453e-02,\n",
      "         7.0801e-02,  6.4941e-02,  7.7148e-02,  6.9336e-02,  8.9844e-02,\n",
      "         6.6895e-02,  5.8350e-02,  7.6660e-02,  8.4961e-02,  5.2734e-02,\n",
      "         6.9336e-02,  7.0312e-02,  8.3496e-02,  7.1289e-02,  8.1543e-02,\n",
      "         9.2773e-02,  8.1055e-02,  7.5684e-02,  5.0537e-02,  8.1055e-02,\n",
      "         6.1523e-02,  6.4453e-02,  7.9102e-02,  7.7148e-02,  6.6895e-02,\n",
      "         8.5938e-02,  9.3750e-02,  7.4219e-02], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.decoder.block.2.layer.2.DenseReluDense.wi.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0752,  0.5312,  0.7969,  ..., -1.0547,  0.2461,  0.4746],\n",
      "        [ 1.5547, -0.4219,  0.3867,  ..., -0.2871, -0.3652,  0.2021],\n",
      "        [ 0.3184, -0.0479,  0.5430,  ..., -0.3145, -0.0157, -0.4199],\n",
      "        ...,\n",
      "        [-0.3320,  3.7188, -0.8555,  ...,  1.9609,  0.0820,  1.2734],\n",
      "        [ 0.2832, -0.2773,  0.0038,  ...,  0.5391,  0.4941,  0.6484],\n",
      "        [ 0.2051,  0.1924,  0.2812,  ..., -0.1797, -0.3008, -0.0674]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.2.layer.2.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[-1.4355e-01,  1.4844e-01, -2.4658e-02,  ..., -2.4536e-02,\n",
      "          2.6367e-02, -3.6719e-01],\n",
      "        [-5.4016e-03, -7.3853e-03,  2.5000e-01,  ...,  5.4688e-02,\n",
      "          2.9883e-01, -3.3691e-02],\n",
      "        [ 3.4668e-02, -6.4844e-01, -2.5391e-01,  ...,  1.1902e-02,\n",
      "         -2.5757e-02, -7.6904e-03],\n",
      "        ...,\n",
      "        [-3.3008e-01, -5.7373e-02, -2.1648e-04,  ..., -1.9287e-02,\n",
      "          1.7871e-01, -1.1475e-01],\n",
      "        [ 1.7822e-02, -1.6309e-01,  3.5645e-02,  ..., -1.4160e-02,\n",
      "         -1.2451e-01,  3.4570e-01],\n",
      "        [ 1.1963e-01,  4.2236e-02,  1.7285e-01,  ..., -6.8359e-03,\n",
      "          2.5635e-03,  3.6914e-01]], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.2.layer.2.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([1.6641, 0.7305, 0.7344, 1.7500, 1.4375, 1.7109, 1.1406, 1.7891, 1.0781,\n",
      "        1.3906, 2.4062, 1.4531, 1.1328, 1.6016, 1.4844, 1.5234, 1.3672, 1.0625,\n",
      "        1.4609, 1.4844, 1.4609, 1.5469, 1.4609, 1.4453, 1.4922, 1.0781, 1.4688,\n",
      "        1.3516, 1.4688, 1.4062, 1.4453, 0.7500, 1.5547, 1.4062, 1.6172, 1.1719,\n",
      "        1.4297, 0.7773, 1.3203, 1.6172, 1.3906, 1.4844, 1.2734, 1.5391, 0.7227,\n",
      "        1.3906, 1.4453, 1.5156, 1.5000, 1.6406, 1.2891, 1.3906, 1.5000, 1.5547,\n",
      "        1.9922, 1.6250, 1.3828, 1.4141, 1.5469, 1.5234, 1.3984, 1.4609, 1.4609,\n",
      "        1.3906, 1.5312, 0.4492, 1.4688, 1.4453, 0.9844, 1.4375, 1.2812, 1.3750,\n",
      "        1.3750, 1.3047, 1.4219, 1.4609, 1.5859, 1.5312, 1.3906, 1.2969, 1.3359,\n",
      "        1.5234, 1.5703, 1.0938, 1.1875, 1.3594, 1.6172, 1.4297, 1.4062, 1.4844,\n",
      "        1.3203, 1.3594, 1.3672, 1.6484, 1.5312, 1.4844, 1.4766, 1.5000, 1.4297,\n",
      "        1.3438, 1.5938, 1.2031, 1.7344, 1.4531, 1.3672, 1.3594, 1.5078, 1.3516,\n",
      "        1.2500, 1.4688, 1.5000, 1.4844, 1.5156, 1.3594, 1.3281, 1.2344, 1.4062,\n",
      "        1.5000, 0.2188, 1.4141, 1.3906, 1.3125, 1.0625, 1.2891, 1.3672, 1.2656,\n",
      "        1.5391, 1.5625, 1.4453, 1.5469, 1.4688, 1.2422, 1.4922, 1.4609, 0.9688,\n",
      "        1.5000, 1.3594, 1.1250, 1.8906, 2.0938, 1.2656, 1.3047, 1.7109, 1.5000,\n",
      "        1.3594, 1.4609, 1.1797, 1.1328, 1.2969, 1.6172, 1.2812, 1.3594, 1.2109,\n",
      "        1.2812, 1.4844, 1.4609, 1.5781, 1.5391, 1.1172, 1.5312, 1.2969, 1.3125,\n",
      "        1.3047, 1.2344, 1.0625, 0.8633, 1.5234, 1.0391, 0.7656, 1.4062, 1.6406,\n",
      "        1.3672, 1.4609, 1.3984, 1.4453, 1.3906, 1.4219, 1.3359, 1.6250, 1.0312,\n",
      "        1.4375, 1.4844, 1.5547, 1.6250, 1.4453, 1.2422, 1.5391, 1.0234, 1.3984,\n",
      "        1.5859, 1.3906, 1.4141, 1.1484, 1.5859, 1.3516, 1.2578, 1.4609, 1.3672,\n",
      "        1.5234, 1.2422, 1.4844, 1.7266, 1.4375, 1.1797, 1.4531, 1.4766, 1.4297,\n",
      "        1.5078, 1.3281, 1.5547, 1.1250, 1.9062, 1.3125, 1.3125, 1.5547, 1.5547,\n",
      "        1.4453, 1.2109, 1.5625, 0.6328, 1.4922, 1.2500, 1.1094, 1.4453, 1.2969,\n",
      "        1.5625, 0.1963, 1.3281, 1.4688, 1.3984, 1.1875, 1.5781, 1.3047, 1.4922,\n",
      "        1.3516, 1.3750, 1.4844, 1.5859, 1.7500, 1.6250, 2.2031, 1.3828, 1.2969,\n",
      "        1.1719, 1.3750, 1.3906, 1.2188, 1.6641, 1.3047, 1.3984, 1.4766, 1.4062,\n",
      "        1.4844, 1.3906, 1.3359, 1.1328, 1.5156, 1.8438, 1.5391, 1.4062, 1.4062,\n",
      "        1.8906, 1.4219, 1.6250, 1.4453, 1.4531, 1.3281, 1.2500, 1.4531, 1.5703,\n",
      "        1.4688, 0.8086, 1.5000, 1.2656, 1.8125, 1.4922, 1.4141, 1.6953, 1.4141,\n",
      "        1.4922, 0.2451, 0.9219, 1.4219, 1.3438, 1.4297, 1.4844, 1.5234, 1.5469,\n",
      "        1.2188, 1.6719, 1.2188, 1.3516, 1.3984, 1.5625, 1.3984, 1.4219, 0.6719,\n",
      "        1.4609, 1.4141, 1.7188, 1.5469, 1.3906, 0.4082, 1.5312, 1.4375, 1.2734,\n",
      "        1.5469, 1.4688, 1.5312, 1.2500, 1.1797, 1.5156, 1.4375, 1.2969, 0.7500,\n",
      "        1.5859, 1.2500, 1.4297, 1.5156, 1.2734, 1.3672, 1.3672, 1.5391, 1.3438,\n",
      "        1.1328, 1.3359, 1.4609, 1.3359, 1.5469, 1.5000, 1.4688, 1.2656, 1.3750,\n",
      "        1.2500, 1.3438, 1.5156, 1.2734, 1.3359, 1.2578, 1.9453, 1.4766, 1.3828,\n",
      "        1.3594, 1.3984, 1.3750, 1.2422, 1.5156, 1.8125, 0.8359, 1.4453, 1.2812,\n",
      "        1.2656, 1.2109, 1.2500, 1.5781, 1.3594, 1.5469, 0.4316, 1.6406, 1.5469,\n",
      "        1.5234, 1.0703, 1.3906, 1.2500, 1.3125, 1.3906, 1.5078, 1.3125, 1.3750,\n",
      "        1.4766, 1.4609, 1.3359, 1.1641, 1.3906, 1.4453, 1.5234, 1.3828, 1.1641,\n",
      "        1.3906, 1.1172, 1.3594, 1.3125, 1.4453, 1.4297, 1.4141, 1.4375, 1.4297,\n",
      "        1.6484, 1.2266, 1.4609, 1.2578, 1.2656, 1.4844, 1.7266, 1.4297, 1.5469,\n",
      "        1.4375, 1.3828, 1.3359, 1.6953, 1.2500, 1.3828, 1.5234, 1.5547, 1.3516,\n",
      "        1.3750, 1.2031, 1.3672, 1.2812, 1.3906, 1.3672, 1.3281, 1.4453, 1.4141,\n",
      "        1.4609, 1.3594, 1.2812, 1.5859, 0.4473, 1.3516, 0.7578, 1.3047, 1.4844,\n",
      "        1.5234, 1.4297, 1.3281, 1.5469, 1.5078, 1.3125, 2.5312, 1.3359, 1.3906,\n",
      "        1.4375, 1.3281, 1.2969, 1.1953, 1.4375, 1.1875, 1.3984, 1.3047, 1.3906,\n",
      "        1.3281, 0.9727, 1.4766, 1.3359, 1.2578, 1.4297, 1.4531, 0.5664, 1.3203,\n",
      "        1.5547, 1.4453, 1.4531, 1.4688, 1.3359, 1.4922, 1.0312, 0.6211, 1.4922,\n",
      "        1.3203, 1.4844, 1.5000, 1.5078, 1.3828, 1.5156, 1.4453, 1.5156, 1.4531,\n",
      "        1.1875, 1.4219, 1.4609, 1.3516, 0.9258, 1.4531, 1.3750, 1.3359, 1.4609,\n",
      "        1.3594, 1.4219, 1.5469, 2.0156, 1.5781, 1.5000, 1.2734, 0.4238, 1.5469,\n",
      "        1.3047, 0.9375, 1.3438, 1.4844, 1.5391, 1.3047, 1.6328, 1.4219, 1.5078,\n",
      "        1.3906, 1.3516, 1.3516, 1.5703, 1.5547, 1.3594, 0.7656, 1.3281, 1.4766,\n",
      "        1.4531, 1.2969, 1.3359, 1.4922, 1.3438, 1.3984, 1.2422, 1.5781, 1.5391,\n",
      "        1.3906, 0.9883, 1.5234, 1.4141, 1.2266, 1.7500, 1.6328, 1.5000, 1.4922,\n",
      "        1.3359, 1.2656, 1.4766, 1.2109, 1.6797, 0.8984, 1.3750, 1.2891, 1.0938,\n",
      "        1.3906, 1.3828, 1.3672, 1.1875, 1.4609, 1.5078, 1.4375, 1.3516, 1.3750,\n",
      "        1.3438, 1.4297, 1.4844, 1.4062, 1.4062, 1.4375, 1.3203, 1.3906, 1.3359,\n",
      "        1.3672, 1.1562, 1.1641, 1.2812, 1.1250, 1.4453, 1.3672, 1.3281, 1.5703,\n",
      "        1.1250, 1.5781, 1.3047, 1.2109, 0.6875, 2.0938, 1.5547, 1.1562, 1.4609,\n",
      "        1.4844, 1.6641, 1.3828, 1.3203, 1.4688, 1.4141, 1.7734, 1.7734, 1.1797,\n",
      "        1.4531, 1.3828, 1.2344, 0.2715, 0.9961, 1.4219, 1.6094, 1.3828, 1.4375,\n",
      "        1.4844, 1.4297, 1.6094, 1.4688, 1.3594, 1.4453, 1.4922, 1.3125, 1.3672,\n",
      "        1.4219, 1.5859, 0.8984, 1.4531, 1.9141, 1.4141, 1.1875, 1.4922, 1.4609,\n",
      "        1.3047, 1.4141, 1.3750, 1.3281, 0.5977, 1.5547, 1.4922, 1.5234, 1.5547,\n",
      "        1.4219, 1.4453, 1.2969, 1.6094, 1.4688, 0.4277, 1.2031, 1.4531, 1.4688,\n",
      "        1.3359, 1.3750, 1.7188, 1.7656, 1.4531, 1.7031, 1.4219, 1.2578, 1.3828,\n",
      "        1.0859, 1.3906, 1.0469, 1.0547, 1.3828, 1.3125, 1.8594, 1.4766, 1.4297,\n",
      "        1.3438, 1.3047, 1.2031, 1.4141, 1.3750, 1.2656, 1.5312, 0.9805, 1.4141,\n",
      "        1.5703, 1.4141, 0.8906, 1.4375, 1.3047, 1.4531, 1.4609, 1.5156, 1.2500,\n",
      "        1.6797, 1.5625, 1.5859, 1.2969, 1.5078, 1.2656, 1.4922, 1.3750, 1.4062,\n",
      "        1.3047, 1.5156, 1.5625, 1.5078, 1.2344, 1.4609, 1.3906, 1.2031, 1.4297,\n",
      "        1.4688, 1.3281, 1.3047, 1.4141, 1.4844, 1.3281, 1.5000, 1.5234, 1.2344,\n",
      "        1.3359, 1.5000, 1.3203, 1.3047, 1.4766, 1.3125, 1.5156, 1.5000, 1.2422,\n",
      "        1.2734, 1.3594, 1.2891, 1.4609, 1.2266, 1.4688, 1.4375, 1.3750, 1.3672,\n",
      "        1.3438, 1.4609, 1.1172, 1.4531, 1.4141, 1.8438, 0.8711, 1.3906, 1.3594,\n",
      "        1.3438, 1.2734, 1.2578, 1.3906, 0.8555, 1.4375, 1.1484, 1.3438, 1.4844,\n",
      "        1.4453, 1.3438, 1.2578, 1.2891, 1.4531, 0.8555, 1.2500, 1.2578, 1.4062,\n",
      "        1.4609, 1.3594, 1.6484, 1.4609, 1.0469, 1.1250, 1.4062, 1.4062, 1.2891,\n",
      "        1.3750, 1.3047, 1.2812, 1.4922, 1.4062, 1.4922, 1.6250, 1.1797, 1.2969,\n",
      "        1.2266, 1.5078, 1.8359, 1.6953, 1.1953, 1.4375, 1.2812, 1.4062, 1.4062,\n",
      "        1.4062, 1.2812, 1.0781, 1.4844, 1.1797, 1.3125, 1.4688, 1.6797, 1.2422,\n",
      "        1.4141, 1.6797, 1.6094], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.3.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0105,  0.0047,  0.0132,  ...,  0.0012, -0.0209,  0.0182],\n",
      "        [ 0.0146,  0.0036, -0.0247,  ...,  0.0145,  0.0110,  0.0026],\n",
      "        [-0.0461,  0.0311, -0.0188,  ..., -0.0243,  0.0079, -0.0347],\n",
      "        ...,\n",
      "        [-0.0476,  0.0559,  0.0198,  ...,  0.0051,  0.0119, -0.0359],\n",
      "        [-0.0203,  0.0182,  0.0071,  ..., -0.0205, -0.0053, -0.0041],\n",
      "        [ 0.0121, -0.0466, -0.0047,  ..., -0.0051, -0.0278, -0.0222]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.3.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0693,  0.0165,  0.2539,  ...,  0.1377,  0.0576,  0.0869],\n",
      "        [-0.1064, -0.5195, -0.1543,  ..., -0.0104, -0.1650,  0.0454],\n",
      "        [ 0.0430,  0.5508,  0.2305,  ...,  0.1001, -0.1934,  0.0728],\n",
      "        ...,\n",
      "        [ 0.1533, -0.1357,  0.2715,  ..., -0.1396,  0.1025,  0.1406],\n",
      "        [ 0.3242, -0.4570, -0.4883,  ...,  0.2383, -0.1182, -0.3242],\n",
      "        [-0.3477,  0.2539,  0.2422,  ...,  0.2793,  0.2197,  0.0120]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.3.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.6680,  0.4668, -0.8086,  ...,  1.4375, -0.6992,  0.3574],\n",
      "        [-1.2734,  0.4902,  0.1138,  ...,  0.0547, -0.3828, -0.2012],\n",
      "        [-1.0078,  0.3184,  0.1299,  ...,  0.8125,  0.1855,  0.4141],\n",
      "        ...,\n",
      "        [-0.6016,  0.8828,  0.1914,  ..., -0.2812,  0.3281,  0.4180],\n",
      "        [-0.2949, -0.2734, -0.3984,  ..., -0.0278,  0.2637,  0.0635],\n",
      "        [ 0.6016,  0.4668,  0.1445,  ..., -0.5117,  0.1836,  0.8008]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.3.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.2158, -0.3398,  0.7773,  ..., -0.2393,  0.0464,  0.5000],\n",
      "        [-1.0859,  0.6289,  1.0703,  ...,  0.5352, -0.7383,  0.1982],\n",
      "        [-0.9141, -0.1123, -1.4922,  ..., -0.2480,  0.4375, -0.2773],\n",
      "        ...,\n",
      "        [ 0.2969, -0.0481, -0.7266,  ..., -0.8984,  0.2119,  0.0287],\n",
      "        [-0.7383, -1.3984,  0.0933,  ...,  0.1455,  1.1875, -0.2402],\n",
      "        [ 0.0190,  0.3027,  0.0854,  ..., -0.0060,  0.3066,  0.4199]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.3.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 0.2021,  0.0952,  0.1162,  0.2490,  0.2109,  0.1641,  0.2002,  0.2441,\n",
      "         0.1719,  0.2061,  0.1318,  0.2256,  0.1167,  0.1514,  0.2285,  0.2246,\n",
      "         0.2021,  0.1572,  0.1982,  0.2246,  0.2061,  0.2236,  0.2188,  0.2217,\n",
      "         0.2129,  0.1562,  0.2100,  0.1543,  0.2168,  0.2178,  0.2227,  0.1309,\n",
      "         0.2324,  0.1943,  0.2275,  0.1709,  0.2227,  0.1689,  0.2080,  0.2324,\n",
      "         0.2227,  0.2109,  0.2109,  0.2100,  0.1377,  0.2100,  0.2295,  0.2070,\n",
      "         0.2314,  0.2373,  0.2002,  0.2021,  0.1992,  0.2256,  0.3086,  0.1641,\n",
      "         0.1904,  0.2246,  0.2363,  0.2051,  0.1895,  0.2246,  0.2197,  0.2080,\n",
      "         0.2197,  0.1260,  0.2197,  0.2070,  0.1328,  0.2061,  0.2158,  0.2217,\n",
      "         0.2100,  0.2041,  0.2031,  0.2051,  0.1484,  0.1934,  0.2041,  0.1943,\n",
      "         0.2100,  0.2051,  0.2246,  0.1592,  0.1895,  0.2119,  0.1807,  0.2188,\n",
      "         0.2168,  0.2148,  0.1924,  0.2080,  0.1904,  0.1855,  0.2246,  0.2188,\n",
      "         0.2139,  0.2207,  0.2295,  0.2139,  0.1816,  0.1826,  0.1045,  0.2207,\n",
      "         0.1885,  0.2070,  0.2100,  0.1963,  0.2012,  0.2168,  0.2236,  0.2295,\n",
      "         0.1973,  0.2061,  0.1982,  0.1865,  0.2061,  0.1992,  0.0535,  0.2070,\n",
      "         0.2246,  0.1875,  0.1592,  0.2119,  0.2207,  0.1924,  0.2246,  0.1768,\n",
      "         0.1924,  0.1338,  0.1396,  0.1826,  0.2061,  0.2207,  0.1113,  0.2148,\n",
      "         0.1953,  0.1738,  0.1924,  0.1245,  0.1621,  0.2061,  0.2051,  0.2207,\n",
      "         0.1934,  0.1836,  0.1895,  0.1670,  0.1904,  0.1953,  0.2256,  0.1943,\n",
      "         0.2070,  0.1895,  0.2197,  0.2207,  0.2188,  0.2334,  0.0520,  0.1177,\n",
      "         0.1963,  0.1836,  0.1846,  0.1816,  0.1738,  0.1147,  0.2305,  0.1494,\n",
      "         0.1006,  0.2227,  0.1035,  0.1963,  0.2217,  0.2266,  0.2061,  0.2197,\n",
      "         0.1943,  0.2051,  0.0967,  0.1699,  0.2061,  0.2041,  0.2393,  0.2461,\n",
      "         0.1885,  0.1855,  0.2217,  0.0332,  0.2129,  0.1455,  0.2168,  0.2178,\n",
      "         0.1904,  0.2217,  0.2021,  0.1836,  0.2178,  0.2178,  0.2236,  0.1865,\n",
      "         0.2334,  0.2002,  0.2148,  0.1875,  0.2109,  0.1846,  0.2139,  0.2129,\n",
      "         0.1963,  0.2598,  0.1777,  0.1641,  0.1924,  0.1973,  0.2344,  0.2295,\n",
      "         0.1934,  0.1865,  0.2324,  0.0364,  0.1455,  0.1914,  0.1758,  0.2061,\n",
      "         0.1816,  0.2109,  0.2158,  0.2285,  0.2148,  0.2236,  0.1846,  0.2266,\n",
      "         0.1914,  0.2275,  0.2061,  0.1904,  0.1826,  0.2070,  0.1235,  0.2266,\n",
      "         0.1777,  0.2227,  0.1758,  0.1855,  0.2344,  0.2021,  0.2041,  0.2041,\n",
      "         0.1855,  0.2129,  0.2061,  0.1982,  0.2236,  0.2139,  0.2012,  0.1865,\n",
      "         0.2158,  0.1235,  0.2178,  0.1904,  0.2637,  0.1279,  0.2314,  0.2383,\n",
      "         0.2139,  0.2178,  0.2061,  0.1963,  0.2148,  0.2178,  0.2119,  0.1025,\n",
      "         0.2148,  0.1963,  0.1001,  0.2227,  0.2109,  0.2295,  0.1973,  0.1709,\n",
      "         0.0327,  0.1270,  0.2061,  0.1865,  0.2217,  0.2119,  0.2139,  0.2422,\n",
      "         0.1611,  0.1455,  0.1953,  0.2012,  0.1973,  0.2393,  0.2012,  0.1992,\n",
      "         0.1143,  0.2178,  0.2100,  0.2314,  0.2217,  0.2061,  0.0378,  0.2070,\n",
      "         0.2129,  0.2031,  0.2197,  0.2246,  0.2031,  0.2002,  0.1924,  0.2217,\n",
      "         0.2021,  0.2178,  0.1621,  0.1934,  0.2188,  0.2334,  0.2246,  0.2070,\n",
      "         0.2129,  0.2168,  0.1729,  0.2051,  0.1855,  0.2051,  0.1982,  0.2080,\n",
      "         0.2393,  0.2051,  0.2256,  0.1748,  0.2334,  0.1836,  0.2031,  0.2129,\n",
      "         0.2061,  0.1992,  0.1846,  0.1807,  0.1953,  0.2148,  0.2070,  0.2070,\n",
      "         0.2139,  0.1855,  0.2266,  0.1680,  0.1011,  0.2051,  0.2021,  0.1807,\n",
      "         0.1729,  0.2012,  0.2031,  0.2100,  0.2246,  0.0508,  0.2236,  0.2314,\n",
      "         0.2227,  0.1553,  0.1250,  0.1816,  0.2012,  0.2061,  0.2246,  0.1943,\n",
      "         0.1973,  0.2070,  0.2363,  0.2305,  0.1953,  0.1953,  0.2285,  0.1992,\n",
      "         0.2041,  0.2129,  0.2129,  0.1543,  0.1924,  0.1934,  0.1553,  0.1943,\n",
      "         0.2139,  0.2285,  0.2129,  0.2344,  0.1699,  0.2227,  0.1963,  0.2002,\n",
      "         0.1895,  0.2295,  0.2217,  0.2178,  0.2090,  0.1865,  0.2158,  0.2148,\n",
      "         0.2080,  0.2100,  0.2148,  0.2178,  0.1797,  0.2002,  0.1514,  0.2051,\n",
      "         0.1807,  0.2256,  0.2002,  0.1992,  0.2158,  0.2021,  0.2217,  0.2080,\n",
      "         0.1963,  0.2051,  0.1699,  0.2119,  0.0304,  0.1885,  0.2188,  0.0684,\n",
      "         0.2109,  0.2070,  0.2002,  0.2061,  0.1982,  0.1514,  0.2070,  0.2051,\n",
      "         0.2236,  0.1758,  0.1836,  0.1865,  0.2158,  0.1885,  0.2100,  0.2041,\n",
      "         0.2090,  0.1992,  0.1650,  0.2295,  0.2100,  0.2109,  0.1943,  0.2119,\n",
      "         0.0454,  0.2012,  0.2070,  0.2061,  0.2070,  0.2275,  0.1924,  0.2031,\n",
      "         0.2158,  0.2070,  0.2051,  0.1973,  0.2148,  0.2129,  0.2129,  0.2129,\n",
      "         0.2217,  0.2285,  0.1729,  0.2041,  0.1855,  0.2109,  0.2188,  0.1963,\n",
      "         0.1147,  0.2119,  0.2080,  0.2031,  0.2168,  0.2080,  0.2148,  0.2256,\n",
      "         0.1504,  0.2324,  0.2227,  0.1846,  0.0757,  0.2188,  0.1992,  0.0400,\n",
      "         0.2061,  0.2217,  0.2236,  0.1963,  0.2305,  0.2188,  0.2275, -0.0713,\n",
      "         0.2002,  0.2051,  0.2207,  0.2061,  0.2002,  0.1562,  0.2158,  0.1973,\n",
      "         0.2188,  0.2090,  0.2090,  0.1865,  0.2021,  0.2139,  0.1953,  0.2227,\n",
      "         0.2168,  0.1875,  0.1719,  0.2256,  0.1914,  0.1875,  0.1729,  0.2373,\n",
      "         0.2139,  0.2109,  0.2031,  0.2051,  0.1885,  0.1777,  0.2246,  0.1377,\n",
      "         0.2031,  0.1768,  0.1553,  0.2109,  0.2031,  0.2129,  0.1895,  0.2119,\n",
      "         0.2422,  0.2051,  0.2139,  0.1953,  0.2197,  0.2109,  0.2070,  0.2051,\n",
      "         0.1973,  0.2090,  0.2031,  0.2168,  0.2021,  0.1992,  0.1611,  0.1562,\n",
      "         0.2070,  0.1543,  0.2158,  0.2012,  0.1855,  0.2344,  0.1650,  0.2256,\n",
      "         0.2002,  0.1875,  0.1729,  0.3066,  0.2344,  0.1846,  0.2178,  0.2256,\n",
      "         0.2197,  0.2100,  0.2100,  0.2285,  0.2070,  0.1865,  0.1299,  0.1885,\n",
      "         0.2275,  0.2188,  0.1875,  0.0349,  0.1533,  0.2080,  0.2178,  0.1973,\n",
      "         0.2100,  0.2139,  0.1992,  0.2109,  0.2188,  0.2021,  0.2275,  0.2266,\n",
      "         0.2002,  0.1895,  0.1953,  0.2197,  0.1338,  0.2148,  0.2363,  0.2090,\n",
      "         0.1689,  0.2100,  0.2109,  0.1787,  0.2139,  0.2021,  0.2070,  0.0996,\n",
      "         0.2412,  0.2256,  0.2109,  0.2021,  0.2061,  0.2178,  0.1982,  0.2402,\n",
      "         0.1963,  0.1533,  0.2012,  0.2158,  0.2139,  0.1904,  0.2090,  0.1494,\n",
      "         0.2002,  0.2090,  0.1943,  0.2070,  0.1973,  0.2178,  0.1699,  0.2119,\n",
      "         0.1748,  0.1816,  0.1934,  0.2051,  0.1611,  0.1982,  0.2148,  0.2012,\n",
      "         0.1543,  0.1895,  0.2119,  0.2041,  0.1543,  0.2012,  0.1270,  0.2041,\n",
      "         0.1953,  0.2266,  0.1270,  0.2334,  0.1914,  0.2236,  0.2178,  0.1074,\n",
      "         0.1943,  0.1553,  0.2363,  0.2090,  0.2148,  0.2021,  0.1865,  0.2393,\n",
      "         0.2012,  0.2178,  0.2021,  0.2227,  0.1895,  0.2217,  0.1836,  0.2109,\n",
      "         0.2100,  0.1660,  0.2285,  0.2090,  0.1904,  0.2012,  0.2178,  0.2217,\n",
      "         0.2129,  0.2207,  0.1797,  0.1865,  0.1973,  0.2041,  0.2002,  0.1914,\n",
      "         0.2236,  0.2012,  0.2227,  0.2197,  0.1816,  0.1943,  0.2070,  0.2090,\n",
      "         0.2295,  0.1963,  0.2178,  0.2090,  0.1953,  0.1973,  0.1943,  0.0996,\n",
      "         0.1758,  0.2188,  0.2080,  0.1572,  0.1235,  0.2148,  0.2031,  0.1934,\n",
      "         0.1787,  0.1973,  0.1108,  0.1904,  0.1953,  0.1836,  0.1992,  0.1963,\n",
      "         0.2070,  0.2012,  0.1543,  0.2021,  0.2139,  0.0957,  0.1914,  0.1943,\n",
      "         0.1943,  0.2070,  0.2158,  0.2100,  0.2207,  0.0703,  0.1963,  0.1982,\n",
      "         0.2148,  0.2002,  0.2139,  0.1826,  0.1963,  0.1021,  0.2227,  0.1904,\n",
      "         0.2402,  0.1885,  0.1465,  0.1885,  0.2178,  0.1494,  0.1963,  0.1875,\n",
      "         0.1904,  0.1943,  0.2197,  0.2158,  0.2070,  0.1904,  0.1113,  0.2090,\n",
      "         0.1777,  0.1670,  0.1963,  0.2002,  0.1953,  0.2080,  0.2334,  0.2061],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.3.layer.1.EncDecAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0325, -0.0442, -0.0432,  ...,  0.1226,  0.0366, -0.0435],\n",
      "        [ 0.0251,  0.0408,  0.0767,  ...,  0.0303, -0.0106, -0.0400],\n",
      "        [ 0.0222,  0.0752, -0.0148,  ..., -0.0161, -0.0236, -0.0045],\n",
      "        ...,\n",
      "        [-0.0186,  0.0908, -0.0417,  ..., -0.0155, -0.0767,  0.0439],\n",
      "        [ 0.0693,  0.0625,  0.0014,  ..., -0.0215, -0.0003,  0.0126],\n",
      "        [ 0.0481, -0.0564,  0.0194,  ..., -0.0811, -0.0266, -0.0874]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.3.layer.1.EncDecAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[-2.8516e-01,  2.6562e-01,  6.9922e-01,  ...,  7.7734e-01,\n",
      "         -3.4570e-01,  1.7773e-01],\n",
      "        [-1.0449e-01,  1.6113e-02, -2.5635e-02,  ..., -7.0801e-02,\n",
      "         -3.4766e-01,  5.2002e-02],\n",
      "        [ 6.8848e-02, -7.4609e-01, -3.0664e-01,  ..., -6.1719e-01,\n",
      "         -1.1133e-01,  2.1289e-01],\n",
      "        ...,\n",
      "        [ 6.1523e-02,  1.9824e-01, -7.1094e-01,  ..., -4.5117e-01,\n",
      "          3.0640e-02,  1.1816e-01],\n",
      "        [ 9.9609e-02, -4.4678e-02, -1.0469e+00,  ..., -3.5156e-02,\n",
      "         -4.6921e-04,  5.0000e-01],\n",
      "        [ 3.3203e-02, -5.0391e-01,  1.5430e-01,  ...,  7.0801e-02,\n",
      "          1.8652e-01, -6.2109e-01]], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.3.layer.1.EncDecAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0530,  0.2734, -0.3574,  ...,  0.5039,  0.2402,  0.1152],\n",
      "        [-0.4629,  0.3184,  0.6445,  ...,  0.7461, -0.1738,  0.0308],\n",
      "        [-0.5352, -0.8477, -0.2539,  ...,  0.0270,  0.3164,  0.5312],\n",
      "        ...,\n",
      "        [ 0.0064, -0.2852, -0.5898,  ..., -0.0718,  0.1318,  0.3438],\n",
      "        [ 0.1436,  0.5352,  0.4570,  ..., -0.1406,  0.2422,  0.6797],\n",
      "        [-0.6953, -0.0432,  0.3457,  ...,  0.2334,  0.0874, -0.4062]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.3.layer.1.EncDecAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.2129,  0.1099,  0.2188,  ...,  0.3262,  0.0559, -0.0693],\n",
      "        [-0.5156,  0.3418,  0.0986,  ...,  0.7031, -0.5625, -0.5625],\n",
      "        [-0.0654, -0.0654,  0.0962,  ..., -0.0028, -0.4199, -0.1660],\n",
      "        ...,\n",
      "        [ 0.1260, -0.3047,  0.3594,  ..., -0.2441, -0.0135,  0.0066],\n",
      "        [ 0.2773,  0.2432, -0.2402,  ...,  0.1729, -0.1455, -0.2852],\n",
      "        [ 0.1768, -0.2139, -0.3047,  ...,  0.1064, -0.4102, -0.0309]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.3.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 7.0312e-02,  4.1992e-02,  4.0283e-02,  1.0400e-01,  7.1289e-02,\n",
      "         6.1035e-02,  8.0566e-02,  8.4961e-02,  6.2500e-02,  8.6914e-02,\n",
      "         6.3965e-02,  8.9844e-02,  5.4443e-02,  5.5908e-02,  7.7148e-02,\n",
      "         7.9590e-02,  7.1777e-02,  6.4941e-02,  7.4219e-02,  9.3262e-02,\n",
      "         7.2266e-02,  7.7637e-02,  8.0078e-02,  8.7891e-02,  8.3008e-02,\n",
      "         6.2500e-02,  7.5195e-02,  5.2002e-02,  7.4707e-02,  8.1055e-02,\n",
      "         7.7148e-02,  5.5420e-02,  9.1309e-02,  7.9590e-02,  7.9102e-02,\n",
      "         6.0303e-02,  9.1797e-02,  6.9336e-02,  7.2754e-02,  8.7891e-02,\n",
      "         8.2520e-02,  7.0801e-02,  7.9102e-02,  7.8613e-02,  5.0537e-02,\n",
      "         7.3242e-02,  9.7656e-02,  8.3008e-02,  9.0820e-02,  8.0566e-02,\n",
      "         8.2520e-02,  7.0312e-02,  7.1289e-02,  7.6660e-02,  1.1377e-01,\n",
      "         6.5430e-02,  7.1289e-02,  7.6172e-02,  8.6426e-02,  8.3008e-02,\n",
      "         6.8359e-02,  7.9590e-02,  7.9102e-02,  7.7148e-02,  8.3984e-02,\n",
      "         8.9355e-02,  8.0566e-02,  7.7637e-02,  4.2236e-02,  7.3730e-02,\n",
      "         8.0078e-02,  8.7402e-02,  7.3730e-02,  7.5195e-02,  7.1289e-02,\n",
      "         7.5684e-02,  5.1025e-02,  7.4707e-02,  7.7148e-02,  7.4219e-02,\n",
      "         8.1543e-02,  7.3730e-02,  7.4219e-02,  6.6406e-02,  7.4219e-02,\n",
      "         7.3730e-02,  7.0801e-02,  8.4961e-02,  7.8125e-02,  7.9102e-02,\n",
      "         6.6406e-02,  7.6660e-02,  7.6660e-02,  6.5430e-02,  7.9590e-02,\n",
      "         8.6914e-02,  7.9102e-02,  8.4473e-02,  7.8125e-02,  6.7871e-02,\n",
      "         6.7383e-02,  7.3242e-02,  4.0283e-02,  7.2754e-02,  7.0801e-02,\n",
      "         7.5684e-02,  7.9102e-02,  6.6406e-02,  7.4707e-02,  7.8125e-02,\n",
      "         8.2031e-02,  8.8379e-02,  6.9336e-02,  6.8359e-02,  6.9336e-02,\n",
      "         8.3008e-02,  7.3242e-02,  7.1777e-02, -3.0756e-05,  7.0312e-02,\n",
      "         7.4219e-02,  7.7637e-02,  5.9326e-02,  7.3730e-02,  7.6172e-02,\n",
      "         6.6406e-02,  8.0566e-02,  6.6895e-02,  6.5918e-02,  5.2246e-02,\n",
      "         5.1758e-02,  6.4941e-02,  7.2266e-02,  7.7637e-02,  4.9805e-02,\n",
      "         8.4961e-02,  7.5684e-02,  6.0791e-02,  7.1777e-02,  5.6641e-02,\n",
      "         6.1523e-02,  7.7637e-02,  7.0312e-02,  8.2520e-02,  7.0801e-02,\n",
      "         7.1777e-02,  6.7871e-02,  6.5918e-02,  6.7383e-02,  7.1777e-02,\n",
      "         8.2520e-02,  7.4219e-02,  8.1055e-02,  6.7383e-02,  8.4961e-02,\n",
      "         7.9590e-02,  8.4961e-02,  8.0566e-02,  1.2109e-01,  5.8838e-02,\n",
      "         7.1289e-02,  6.8848e-02,  6.7383e-02,  5.9814e-02,  6.2256e-02,\n",
      "         3.8330e-02,  8.2031e-02,  5.5176e-02,  3.3691e-02,  8.3008e-02,\n",
      "         3.8818e-02,  7.5684e-02,  9.2285e-02,  7.9590e-02,  7.6660e-02,\n",
      "         8.4961e-02,  6.8359e-02,  8.3008e-02,  3.5645e-02,  6.4941e-02,\n",
      "         7.9102e-02,  7.8125e-02,  8.5449e-02,  8.1055e-02,  6.8359e-02,\n",
      "         7.4219e-02,  7.3730e-02,  7.8125e-02,  8.0078e-02,  5.4199e-02,\n",
      "         7.2266e-02,  7.4707e-02,  7.0312e-02,  8.1543e-02,  6.9824e-02,\n",
      "         6.6406e-02,  7.7148e-02,  8.4961e-02,  8.3008e-02,  6.7383e-02,\n",
      "         8.2031e-02,  7.7637e-02,  7.7637e-02,  6.7871e-02,  7.2266e-02,\n",
      "         6.2500e-02,  8.3496e-02,  8.4473e-02,  6.9336e-02,  9.7656e-02,\n",
      "         7.4219e-02,  5.9814e-02,  7.4219e-02,  7.5684e-02,  9.1309e-02,\n",
      "         7.9590e-02,  7.1777e-02,  6.4941e-02,  8.1055e-02, -5.8365e-04,\n",
      "         5.4199e-02,  7.5195e-02,  6.4941e-02,  8.5449e-02,  6.5430e-02,\n",
      "         8.2031e-02,  9.7168e-02,  9.8145e-02,  8.2031e-02,  8.1055e-02,\n",
      "         6.6895e-02,  8.3496e-02,  6.8848e-02,  8.7402e-02,  7.3730e-02,\n",
      "         6.5430e-02,  6.8848e-02,  7.9590e-02, -5.5420e-02,  7.9590e-02,\n",
      "         6.6406e-02,  8.5449e-02,  6.9336e-02,  7.4219e-02,  8.1543e-02,\n",
      "         7.5195e-02,  7.0801e-02,  7.1289e-02,  7.2266e-02,  7.2266e-02,\n",
      "         7.4707e-02,  6.9336e-02,  7.8613e-02,  7.4707e-02,  7.4219e-02,\n",
      "         6.5430e-02,  7.8613e-02,  5.5664e-02,  8.3008e-02,  7.5684e-02,\n",
      "         1.0010e-01,  6.1279e-02,  9.2773e-02,  7.7148e-02,  7.2266e-02,\n",
      "         7.4219e-02,  8.3008e-02,  7.4219e-02,  7.8613e-02,  8.2031e-02,\n",
      "         8.0566e-02,  6.9336e-02,  8.8867e-02,  7.3242e-02,  4.4922e-02,\n",
      "         8.1055e-02,  7.5195e-02,  7.3730e-02,  7.5684e-02,  6.1768e-02,\n",
      "         7.4005e-04,  5.1270e-02,  7.6660e-02,  7.0312e-02,  8.6426e-02,\n",
      "         7.9590e-02,  7.9102e-02,  8.5938e-02,  5.8105e-02,  5.9326e-02,\n",
      "         6.4453e-02,  7.2754e-02,  6.8359e-02,  7.9590e-02,  6.6406e-02,\n",
      "         7.7637e-02,  5.0537e-02,  8.3008e-02,  8.1055e-02,  9.2773e-02,\n",
      "         8.8379e-02,  7.8125e-02,  8.3618e-03,  7.4707e-02,  8.2031e-02,\n",
      "         7.5195e-02,  8.2520e-02,  8.0566e-02,  7.5684e-02,  7.8125e-02,\n",
      "         7.7148e-02,  8.1543e-02,  7.4219e-02,  8.2520e-02,  9.8633e-02,\n",
      "         7.3730e-02,  9.6680e-02,  8.8867e-02,  8.7891e-02,  8.3008e-02,\n",
      "         8.2031e-02,  7.9102e-02,  6.2988e-02,  7.6172e-02,  7.2266e-02,\n",
      "         6.8848e-02,  6.8848e-02,  7.9102e-02,  8.8867e-02,  7.5684e-02,\n",
      "         8.4961e-02,  6.4453e-02,  8.3008e-02,  6.8848e-02,  6.9824e-02,\n",
      "         7.8613e-02,  8.1055e-02,  9.1797e-02,  7.2266e-02,  6.9336e-02,\n",
      "         6.9336e-02,  8.3008e-02,  8.5938e-02,  6.9824e-02,  8.0566e-02,\n",
      "         7.0801e-02,  8.0078e-02,  6.2500e-02,  4.7119e-02,  7.0801e-02,\n",
      "         8.5449e-02,  6.4453e-02,  6.2500e-02,  7.3242e-02,  7.6660e-02,\n",
      "         7.8613e-02,  8.4961e-02,  2.8419e-04,  8.1055e-02,  9.3262e-02,\n",
      "         8.4473e-02,  5.9082e-02,  5.3955e-02,  6.4453e-02,  7.6172e-02,\n",
      "         8.3008e-02,  7.6660e-02,  6.6895e-02,  7.0801e-02,  7.2266e-02,\n",
      "         7.8125e-02,  7.9590e-02,  6.9336e-02,  7.0312e-02,  9.5703e-02,\n",
      "         7.4707e-02,  8.0078e-02,  8.5449e-02,  8.3984e-02,  5.8105e-02,\n",
      "         6.4453e-02,  6.9336e-02,  6.2256e-02,  7.6172e-02,  7.3730e-02,\n",
      "         7.0801e-02,  7.8125e-02,  8.0566e-02,  6.4453e-02,  8.4473e-02,\n",
      "         7.3730e-02,  6.8359e-02,  6.4941e-02,  8.3008e-02,  7.2754e-02,\n",
      "         7.7148e-02,  8.6426e-02,  6.3965e-02,  7.8125e-02,  7.3730e-02,\n",
      "         7.4707e-02,  7.8613e-02,  7.4219e-02,  8.9844e-02,  6.6406e-02,\n",
      "         7.0312e-02,  6.5918e-02,  7.2754e-02,  6.5430e-02,  7.1777e-02,\n",
      "         6.8848e-02,  7.8613e-02,  7.9590e-02,  7.9102e-02,  8.3496e-02,\n",
      "         8.4961e-02,  6.4941e-02,  7.4707e-02,  1.0645e-01,  7.4219e-02,\n",
      "         2.6001e-02,  7.5684e-02,  8.3008e-02,  2.3071e-02,  8.6914e-02,\n",
      "         7.2754e-02,  6.9336e-02,  7.4219e-02,  7.3242e-02,  5.8838e-02,\n",
      "         8.7891e-02,  7.1777e-02,  8.6914e-02,  6.1035e-02,  6.5430e-02,\n",
      "         6.3965e-02,  7.6660e-02,  7.2266e-02,  7.1777e-02,  6.9824e-02,\n",
      "         7.2754e-02,  7.5684e-02,  6.1279e-02,  8.5449e-02,  8.5938e-02,\n",
      "         7.4707e-02,  7.6660e-02,  8.1543e-02,  1.9043e-02,  7.3242e-02,\n",
      "         8.2031e-02,  7.6660e-02,  7.5684e-02,  7.6172e-02,  7.2754e-02,\n",
      "         7.2266e-02,  1.1670e-01,  1.1279e-01,  7.6172e-02,  6.9824e-02,\n",
      "         7.6660e-02,  8.0078e-02,  7.7637e-02,  7.3730e-02,  8.6426e-02,\n",
      "         7.9102e-02,  6.2012e-02,  7.1289e-02,  6.6406e-02,  7.6660e-02,\n",
      "         8.1543e-02,  7.7148e-02,  3.1982e-02,  7.5684e-02,  7.5195e-02,\n",
      "         7.1289e-02,  8.0078e-02,  7.2754e-02,  7.5195e-02,  7.5684e-02,\n",
      "         6.1523e-02,  7.7637e-02,  8.8379e-02,  6.9824e-02,  2.2217e-02,\n",
      "         7.9590e-02,  8.2031e-02,  1.4355e-01,  7.2754e-02,  7.9590e-02,\n",
      "         8.3008e-02,  8.6914e-02,  8.0078e-02,  8.0566e-02,  8.5449e-02,\n",
      "         4.1992e-02,  7.4707e-02,  7.6172e-02,  7.6660e-02,  7.5195e-02,\n",
      "         7.2754e-02,  8.4961e-02,  7.5684e-02,  7.4707e-02,  7.6660e-02,\n",
      "         6.9336e-02,  8.2031e-02,  6.3965e-02,  6.8359e-02,  8.2031e-02,\n",
      "         6.7871e-02,  8.7402e-02,  6.9824e-02,  7.5684e-02,  6.5918e-02,\n",
      "         8.5449e-02,  7.3242e-02,  6.5430e-02,  6.4941e-02,  8.7402e-02,\n",
      "         7.3730e-02,  8.1543e-02,  7.7148e-02,  7.1289e-02,  7.7148e-02,\n",
      "         7.1289e-02,  8.1543e-02,  5.4932e-02,  7.2266e-02,  6.6406e-02,\n",
      "         5.3711e-02,  7.8125e-02,  7.5684e-02,  8.2520e-02,  7.7148e-02,\n",
      "         6.9824e-02,  8.2031e-02,  7.3730e-02,  8.2031e-02,  8.2031e-02,\n",
      "         7.1289e-02,  7.1777e-02,  7.4707e-02,  7.4219e-02,  6.8848e-02,\n",
      "         7.8613e-02,  7.8613e-02,  7.4219e-02,  7.4707e-02,  7.1777e-02,\n",
      "         6.8359e-02,  6.9824e-02,  7.3242e-02,  5.6396e-02,  8.0078e-02,\n",
      "         7.9590e-02,  7.0312e-02,  8.3008e-02,  5.9326e-02,  8.2031e-02,\n",
      "         7.4707e-02,  6.9824e-02,  4.6631e-02,  1.2988e-01,  8.2031e-02,\n",
      "         7.4707e-02,  8.3496e-02,  7.5195e-02,  6.8848e-02,  7.4219e-02,\n",
      "         7.9102e-02,  8.2031e-02,  6.9336e-02,  7.0801e-02,  5.6641e-02,\n",
      "         7.3730e-02,  8.2031e-02,  8.3984e-02,  6.9824e-02,  1.0920e-04,\n",
      "         5.3467e-02,  7.8613e-02,  7.2754e-02,  6.9824e-02,  7.4707e-02,\n",
      "         8.0566e-02,  7.1777e-02,  7.7637e-02,  7.4219e-02,  7.1777e-02,\n",
      "         8.4961e-02,  8.3984e-02,  7.2754e-02,  6.9336e-02,  7.2754e-02,\n",
      "         7.8125e-02,  4.4678e-02,  7.5195e-02,  9.4238e-02,  7.3730e-02,\n",
      "         6.0303e-02,  7.4707e-02,  8.5449e-02,  6.9336e-02,  7.5684e-02,\n",
      "         6.9824e-02,  7.8125e-02,  4.2969e-02,  8.3496e-02,  7.8613e-02,\n",
      "         7.8125e-02,  7.7148e-02,  7.7148e-02,  8.3496e-02,  7.2266e-02,\n",
      "         9.3262e-02,  6.9824e-02,  9.3262e-02,  7.2266e-02,  8.3008e-02,\n",
      "         7.5195e-02,  7.3730e-02,  7.6172e-02,  5.4199e-02,  7.9102e-02,\n",
      "         7.5195e-02,  7.5195e-02,  7.4707e-02,  7.3242e-02,  7.3730e-02,\n",
      "         6.6406e-02,  7.5684e-02,  5.9326e-02,  6.2988e-02,  6.7871e-02,\n",
      "         7.1289e-02,  7.1777e-02,  7.5684e-02,  7.7148e-02,  7.5684e-02,\n",
      "         5.9326e-02,  7.2754e-02,  8.1055e-02,  7.0312e-02,  5.5420e-02,\n",
      "         7.5684e-02,  4.7852e-02,  7.7637e-02,  7.3242e-02,  8.9844e-02,\n",
      "         5.7617e-02,  7.9102e-02,  6.8848e-02,  8.3496e-02,  8.3496e-02,\n",
      "         4.0771e-02,  6.5918e-02,  7.1777e-02,  8.3984e-02,  8.0078e-02,\n",
      "         7.5195e-02,  6.5918e-02,  6.2256e-02,  1.0254e-01,  7.5684e-02,\n",
      "         7.5684e-02,  7.3730e-02,  8.3496e-02,  7.3730e-02,  7.7637e-02,\n",
      "         7.0312e-02,  7.1777e-02,  7.7637e-02,  6.0059e-02,  8.3496e-02,\n",
      "         8.2520e-02,  7.1777e-02,  7.6660e-02,  7.3730e-02,  8.1055e-02,\n",
      "         7.5684e-02,  8.1543e-02,  6.7871e-02,  6.9824e-02,  7.0312e-02,\n",
      "         7.3242e-02,  7.4219e-02,  7.2754e-02,  8.6426e-02,  6.8848e-02,\n",
      "         7.6660e-02,  7.6660e-02,  5.6396e-02,  6.9824e-02,  7.9102e-02,\n",
      "         7.0312e-02,  8.4961e-02,  7.3242e-02,  7.9590e-02,  7.7637e-02,\n",
      "         7.4707e-02,  7.6660e-02,  6.9824e-02, -3.9795e-02,  7.0312e-02,\n",
      "         7.5684e-02,  7.7148e-02,  6.0059e-02,  5.3711e-02,  7.4707e-02,\n",
      "         6.8359e-02,  7.2754e-02,  7.7148e-02,  7.3242e-02,  4.0527e-02,\n",
      "         9.4238e-02,  6.9336e-02,  7.4219e-02,  7.3242e-02,  7.0312e-02,\n",
      "         8.7891e-02,  7.1777e-02,  5.8105e-02,  7.9590e-02,  7.5684e-02,\n",
      "         4.8340e-02,  7.6172e-02,  6.7871e-02,  6.9336e-02,  8.7402e-02,\n",
      "         7.9590e-02,  7.8125e-02,  8.2520e-02,  5.5664e-02,  7.5195e-02,\n",
      "         7.6660e-02,  8.4473e-02,  7.3242e-02,  8.3984e-02,  6.2012e-02,\n",
      "         6.5918e-02,  5.8594e-02,  7.7637e-02,  6.6406e-02,  8.8867e-02,\n",
      "         6.6406e-02,  6.3477e-02,  6.6895e-02,  8.1055e-02,  6.3965e-02,\n",
      "         7.3242e-02,  6.8848e-02,  7.2266e-02,  6.3965e-02,  8.2031e-02,\n",
      "         8.3984e-02,  7.7148e-02,  7.5684e-02,  5.2002e-02,  7.4219e-02,\n",
      "         6.7383e-02,  6.3477e-02,  7.0312e-02,  7.3242e-02,  7.0801e-02,\n",
      "         7.5195e-02,  8.6426e-02,  8.2031e-02], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.decoder.block.3.layer.2.DenseReluDense.wi.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.8984,  0.2178,  0.0164,  ...,  0.1816,  0.8477,  0.3477],\n",
      "        [ 0.5273,  0.2070, -0.2988,  ..., -0.7773,  0.1504, -0.6992],\n",
      "        [-0.3262, -0.4805, -0.6914,  ..., -0.4941,  0.5938,  0.3320],\n",
      "        ...,\n",
      "        [-0.3262,  0.2012, -0.1484,  ..., -0.3789, -0.4844,  0.5781],\n",
      "        [-0.7422, -0.2041, -0.6523,  ..., -0.2754, -1.2031,  0.2324],\n",
      "        [ 0.1865,  0.5078, -0.4102,  ...,  0.4453,  0.6445, -0.3457]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.3.layer.2.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1230, -0.0708, -0.2412,  ...,  0.1748, -0.5117,  0.1191],\n",
      "        [-0.1245,  0.0938, -0.0425,  ...,  0.0918, -0.2012, -0.3984],\n",
      "        [-0.0933, -0.0527, -0.2930,  ..., -0.1602,  0.2578, -0.0205],\n",
      "        ...,\n",
      "        [ 0.3887, -0.4902,  0.4043,  ..., -0.0649, -0.0898,  0.1514],\n",
      "        [ 0.7188, -0.0859, -0.2559,  ..., -0.0762, -0.0908,  0.0918],\n",
      "        [ 0.0913, -0.0144, -0.1377,  ...,  0.3242, -0.1025,  0.0165]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.3.layer.2.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 2.0938,  1.1719,  1.2031,  2.1875,  2.0625,  2.3906,  1.5938,  2.3438,\n",
      "         1.5625,  1.8516,  3.4375,  2.0000,  1.5859,  2.5625,  2.0156,  2.0781,\n",
      "         1.9766,  1.4297,  2.0156,  2.0000,  2.0469,  2.0625,  1.8672,  2.0938,\n",
      "         2.0625,  1.5938,  2.0469,  2.0312,  2.0625,  1.8906,  2.0469,  1.1094,\n",
      "         2.1250,  1.9297,  2.1562,  1.5469,  2.0000,  1.1797,  1.8203,  2.1250,\n",
      "         1.8047,  2.0000,  1.9062,  2.1406,  1.1797,  1.9375,  1.9297,  2.0938,\n",
      "         1.9766,  2.2969,  1.7812,  1.8984,  2.0625,  2.0938,  2.4531,  2.1562,\n",
      "         1.8828,  1.9062,  2.0312,  2.1406,  1.8672,  2.1719,  2.0156,  1.9141,\n",
      "         2.1562,  0.8242,  1.8750,  1.9844,  1.4688,  2.0312,  1.8203,  1.9141,\n",
      "         1.8750,  1.8672,  2.0156,  1.9062,  2.2344,  2.0938,  1.8984,  1.8281,\n",
      "         1.8672,  1.9219,  2.1250,  1.5703,  1.7734,  1.9453,  2.3906,  2.1094,\n",
      "         1.9688,  1.9844,  1.7500,  1.8438,  1.8984,  2.2188,  2.1094,  2.0156,\n",
      "         2.0156,  1.9453,  2.0469,  1.9141,  2.4844,  1.6641,  2.4062,  2.1094,\n",
      "         1.8750,  1.7500,  2.0938,  1.8047,  1.7500,  2.0000,  1.9453,  1.9766,\n",
      "         1.9844,  1.9219,  1.8828,  1.7188,  2.0469,  2.2188, -0.3379,  1.9531,\n",
      "         1.9922,  1.7656,  1.5391,  1.8438,  1.9531,  1.8125,  2.0312,  2.0781,\n",
      "         1.9766,  2.3594,  2.0938,  1.7578,  2.0469,  1.9922,  1.3594,  1.9766,\n",
      "         1.9219,  1.6328,  2.5781,  2.6094,  1.7500,  1.8516,  2.3906,  2.0625,\n",
      "         1.8672,  2.0781,  1.7266,  1.5781,  1.8359,  2.2188,  1.7578,  1.7969,\n",
      "         1.6562,  1.6484,  2.0156,  2.0312,  2.1094,  2.0938,  1.7266,  2.2656,\n",
      "         1.9141,  1.7969,  1.7109,  1.7891,  1.5469,  1.2891,  2.0781,  1.4375,\n",
      "         1.2344,  1.8672,  2.2656,  1.7188,  1.8984,  1.8359,  2.0469,  1.9609,\n",
      "         1.8281,  1.7188,  2.1719,  1.4922,  2.0625,  2.0000,  2.2031,  2.1562,\n",
      "         2.0000,  1.7266,  2.0156,  1.4531,  1.9375,  2.2500,  1.9453,  2.0156,\n",
      "         1.7188,  2.2812,  1.8750,  1.7578,  1.9688,  1.9609,  1.9922,  1.8125,\n",
      "         2.0156,  2.2812,  2.0000,  1.6562,  1.9375,  2.0781,  1.9609,  2.0156,\n",
      "         1.7266,  2.1875,  1.6250,  2.7969,  1.8438,  1.8672,  2.0625,  2.1250,\n",
      "         2.0312,  1.7188,  2.1719,  0.8398,  2.1875,  1.8125,  1.6172,  1.8750,\n",
      "         1.7812,  2.0469,  0.7188,  1.8828,  2.0625,  1.9688,  1.6094,  2.0938,\n",
      "         1.7344,  2.1250,  1.8984,  1.7969,  2.1406,  2.0156,  2.5938,  2.2969,\n",
      "         3.4375,  1.9375,  1.6875,  1.7031,  1.8750,  1.9375,  1.7891,  2.2656,\n",
      "         1.8438,  1.8594,  1.8984,  2.0625,  2.1250,  1.9688,  1.8203,  1.5625,\n",
      "         1.9922,  2.5625,  2.0469,  1.8906,  1.9609,  2.5312,  2.0000,  2.1719,\n",
      "         1.9375,  1.9531,  1.8516,  1.7109,  1.9922,  2.1406,  2.0312,  1.3125,\n",
      "         1.9453,  1.9062,  2.6406,  2.1250,  2.0312,  2.2656,  1.9297,  2.0156,\n",
      "         0.4434,  1.4609,  1.9453,  1.8359,  1.9375,  1.9922,  2.1094,  2.0625,\n",
      "         1.7266,  2.5156,  1.8750,  1.8672,  1.8828,  2.1406,  1.8594,  2.0000,\n",
      "         1.1484,  1.9922,  1.9219,  2.3594,  2.0000,  1.8984,  0.5586,  2.0469,\n",
      "         2.0312,  1.9062,  2.0625,  2.0469,  2.1094,  1.7422,  1.6250,  1.9609,\n",
      "         1.9453,  1.7500,  1.1641,  2.2812,  1.7891,  2.0000,  1.9688,  1.7891,\n",
      "         1.8281,  1.9531,  2.2188,  1.8906,  1.6250,  1.7266,  1.9688,  1.8516,\n",
      "         2.0312,  1.9766,  2.0469,  1.6953,  1.8594,  1.7656,  1.8906,  2.1094,\n",
      "         1.8047,  1.9062,  1.7656,  2.9531,  1.9688,  1.9375,  1.8438,  1.9844,\n",
      "         2.0312,  1.7188,  2.0156,  2.6562,  1.1875,  2.1094,  1.7500,  1.6875,\n",
      "         1.6484,  1.7969,  2.0938,  1.9297,  2.0938,  0.4941,  2.2188,  2.2188,\n",
      "         2.0469,  1.4609,  2.1719,  1.7578,  1.8359,  1.7969,  2.0156,  1.8672,\n",
      "         1.9062,  2.1094,  1.9531,  1.8203,  1.6250,  1.9141,  1.9844,  2.1094,\n",
      "         1.9141,  1.6172,  1.9688,  1.5859,  1.8438,  1.7266,  2.2969,  1.9766,\n",
      "         2.0156,  2.0625,  1.9766,  2.1875,  1.8203,  1.9531,  1.8516,  1.7891,\n",
      "         1.8984,  2.2031,  1.9766,  2.0156,  1.9609,  1.8828,  1.7812,  2.4375,\n",
      "         1.8281,  1.9922,  2.0000,  2.0312,  1.9531,  1.8672,  1.6016,  1.8125,\n",
      "         1.7656,  1.8594,  1.9453,  1.8125,  1.9062,  1.8906,  2.0156,  1.8125,\n",
      "         1.8438,  2.0469,  0.7656,  1.8750,  0.9453,  1.9297,  2.0000,  2.0469,\n",
      "         2.0000,  1.8047,  2.0469,  2.0000,  1.8750,  3.5781,  1.7578,  1.8438,\n",
      "         2.0469,  1.7656,  1.7734,  1.7891,  1.9922,  1.6875,  1.9609,  1.7656,\n",
      "         1.8984,  1.8359,  1.3750,  2.1250,  1.8984,  1.7891,  1.8984,  2.0156,\n",
      "         0.9180,  1.8516,  2.0781,  2.0156,  1.8516,  2.0781,  1.9062,  2.0469,\n",
      "         1.4297,  0.8203,  1.9609,  1.8203,  2.0000,  2.0156,  2.0469,  1.8203,\n",
      "         2.0312,  2.0625,  2.0469,  1.9219,  1.8359,  2.0000,  2.0000,  1.8828,\n",
      "         1.0312,  2.1094,  1.8594,  1.8281,  1.9297,  1.8828,  2.0000,  2.0781,\n",
      "         2.8281,  2.1094,  2.0312,  1.7500,  0.5117,  2.0625,  1.8438,  1.2734,\n",
      "         1.8672,  2.0000,  2.0625,  1.7578,  2.0000,  1.9531,  2.1094,  2.1719,\n",
      "         1.9141,  1.9531,  2.1562,  2.1094,  1.8594,  1.2656,  1.8516,  2.0625,\n",
      "         1.9062,  1.8516,  1.9688,  1.9609,  1.8438,  1.9766,  1.7188,  2.1094,\n",
      "         2.0312,  1.9688,  1.4609,  2.1094,  1.9297,  1.7188,  2.6094,  2.1250,\n",
      "         2.0938,  2.0625,  1.9141,  1.7656,  2.0781,  1.7266,  2.2188,  1.3594,\n",
      "         1.9062,  1.7422,  1.5703,  1.9062,  1.8828,  1.9844,  1.6484,  1.9609,\n",
      "         2.1250,  1.8750,  1.9062,  1.8359,  1.8047,  1.9922,  1.8984,  1.9453,\n",
      "         1.8906,  1.9609,  1.8594,  2.0000,  1.8516,  1.8047,  1.6562,  1.6250,\n",
      "         1.8281,  1.5547,  1.9453,  1.9531,  1.8359,  2.0625,  1.5469,  2.0781,\n",
      "         1.7422,  1.7031,  0.8789,  2.5312,  2.0312,  1.6875,  1.9062,  1.9453,\n",
      "         2.2500,  1.9062,  1.7812,  1.9453,  2.0000,  2.6094,  2.5469,  1.7500,\n",
      "         1.9844,  1.8906,  1.7109,  0.4141,  1.4375,  2.0938,  2.2500,  1.9297,\n",
      "         1.9531,  2.0312,  2.0625,  2.1875,  2.0625,  1.8984,  2.0156,  1.8438,\n",
      "         1.8359,  1.6953,  1.9375,  2.1250,  1.3594,  1.9609,  2.4531,  1.9062,\n",
      "         1.5938,  2.0000,  1.8828,  1.8281,  1.8672,  1.9141,  1.8828,  1.0000,\n",
      "         2.0625,  2.0000,  2.0469,  2.1719,  1.9062,  1.9453,  1.9219,  2.1094,\n",
      "         1.9766,  0.7031,  1.7500,  1.8594,  1.9531,  1.7266,  1.9531,  2.5156,\n",
      "         2.4219,  2.0938,  2.0469,  1.9062,  1.8281,  1.8203,  1.5312,  2.0000,\n",
      "         1.5625,  1.5547,  1.8438,  1.7422,  2.6094,  2.0156,  1.8984,  1.8438,\n",
      "         1.8047,  1.6328,  1.9766,  1.8672,  1.5938,  2.1875,  1.3359,  1.8281,\n",
      "         1.9609,  1.9531,  1.3672,  2.0781,  1.8203,  2.0781,  1.9375,  2.2656,\n",
      "         1.7656,  2.3281,  2.0938,  2.1875,  1.7734,  1.9844,  1.7031,  1.8750,\n",
      "         1.8750,  1.8672,  1.8828,  2.1250,  2.2500,  2.1094,  1.6484,  1.9609,\n",
      "         2.0469,  1.6953,  1.8906,  1.9688,  1.8438,  1.9062,  1.8516,  2.0312,\n",
      "         1.7656,  2.0625,  1.9453,  1.7891,  1.8516,  1.9453,  1.8047,  1.8672,\n",
      "         2.0156,  1.8047,  2.1094,  1.9688,  1.6719,  1.8516,  1.8516,  1.7656,\n",
      "         2.0312,  1.7031,  2.0312,  1.9766,  1.8828,  1.8750,  1.8203,  1.8672,\n",
      "         1.6016,  2.0938,  1.9141,  2.5000,  1.4922,  1.9453,  1.8125,  1.8438,\n",
      "         1.7812,  1.7500,  2.0781,  1.3125,  1.9609,  1.6719,  1.7969,  2.1094,\n",
      "         1.8984,  1.8750,  1.7188,  1.9141,  1.9219,  1.3438,  1.7500,  1.7969,\n",
      "         1.8984,  2.0000,  1.8438,  2.2188,  1.8750,  1.4141,  1.6406,  1.9375,\n",
      "         1.9062,  1.7578,  1.9141,  1.7969,  1.7344,  2.1250,  1.9531,  2.0625,\n",
      "         2.0781,  1.6641,  1.8281,  1.7500,  2.0312,  2.6875,  2.2969,  1.6875,\n",
      "         1.9141,  1.7969,  1.9375,  1.8906,  1.9375,  1.7734,  1.5703,  1.8594,\n",
      "         1.6172,  1.7969,  1.9688,  2.2500,  1.7422,  1.9531,  2.2969,  2.2656],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.4.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0176,  0.0405,  0.0054,  ..., -0.0408,  0.0206, -0.0165],\n",
      "        [-0.0070,  0.0070,  0.0027,  ...,  0.0601, -0.0044,  0.0247],\n",
      "        [ 0.0508,  0.0293,  0.0508,  ..., -0.0352, -0.0095,  0.0605],\n",
      "        ...,\n",
      "        [-0.0349,  0.0231, -0.0147,  ..., -0.0300,  0.0267,  0.0630],\n",
      "        [-0.0194,  0.0503, -0.0041,  ..., -0.0728, -0.0251,  0.0439],\n",
      "        [ 0.0013, -0.0286, -0.0008,  ..., -0.0171,  0.0058,  0.0171]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.4.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.1138,  0.1309,  0.0243,  ...,  0.1196, -0.1543, -0.2139],\n",
      "        [ 0.4766, -0.2734, -0.1074,  ..., -0.0413,  0.1904,  0.1621],\n",
      "        [-0.5781,  0.1211, -0.3008,  ...,  0.1299, -0.0366, -0.6328],\n",
      "        ...,\n",
      "        [ 0.0588, -0.0620, -0.6094,  ...,  0.3301, -0.0835, -0.1523],\n",
      "        [ 0.2373, -0.6484, -0.1846,  ..., -0.3340,  0.0732,  0.1904],\n",
      "        [-0.5078,  0.6289, -0.1357,  ...,  0.0942, -0.0806, -0.2969]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.4.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.9141,  0.3613,  0.0045,  ...,  0.8789,  0.6289, -0.6914],\n",
      "        [-0.3770, -0.3906,  0.2344,  ...,  1.0156,  0.0039, -0.1475],\n",
      "        [-0.1182,  0.7188,  0.8047,  ..., -0.5742,  0.5938,  0.3848],\n",
      "        ...,\n",
      "        [-0.1602,  0.1191, -0.9023,  ..., -0.0057, -0.3711, -0.3262],\n",
      "        [ 0.7305, -0.9727,  0.5117,  ...,  0.0038,  0.0170, -0.8867],\n",
      "        [ 0.6914, -0.1118, -0.3750,  ...,  0.1074, -0.1338, -0.3125]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.4.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.2617,  0.0957, -0.9492,  ..., -0.4609,  0.2031,  0.5625],\n",
      "        [-0.4473, -0.1807,  0.6250,  ..., -0.0854, -0.5664, -0.0422],\n",
      "        [ 0.3418,  0.3164,  0.2988,  ..., -0.3516,  0.4199,  0.0376],\n",
      "        ...,\n",
      "        [-1.0391,  0.0579,  0.7734,  ..., -0.6914, -0.3262,  0.2598],\n",
      "        [ 0.0986,  0.9453,  0.9805,  ...,  0.0048,  0.2832,  0.1699],\n",
      "        [-0.2754,  0.0469,  0.0035,  ..., -0.3164,  0.4395, -0.2002]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.4.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 0.1982,  0.1060,  0.1147,  0.2520,  0.2100,  0.1504,  0.1943,  0.2490,\n",
      "         0.1689,  0.2168,  0.1436,  0.2295,  0.1030,  0.1475,  0.2178,  0.2295,\n",
      "         0.2041,  0.1660,  0.1963,  0.2344,  0.2139,  0.2285,  0.2090,  0.2305,\n",
      "         0.2168,  0.1582,  0.2041,  0.1377,  0.2031,  0.2168,  0.2305,  0.1436,\n",
      "         0.2295,  0.2070,  0.2217,  0.1602,  0.2139,  0.1797,  0.2100,  0.2324,\n",
      "         0.2197,  0.2090,  0.2129,  0.2061,  0.1328,  0.2109,  0.2373,  0.2109,\n",
      "         0.2275,  0.2441,  0.1953,  0.2041,  0.1934,  0.2266,  0.2754,  0.1611,\n",
      "         0.1865,  0.2295,  0.2275,  0.1953,  0.1895,  0.2344,  0.2246,  0.2090,\n",
      "         0.2188,  0.1406,  0.2148,  0.2100,  0.1172,  0.2119,  0.2217,  0.2129,\n",
      "         0.2041,  0.2109,  0.1943,  0.2021,  0.1396,  0.2051,  0.2080,  0.2012,\n",
      "         0.2236,  0.2178,  0.2373,  0.1592,  0.1982,  0.2158,  0.1602,  0.2441,\n",
      "         0.2197,  0.2002,  0.1875,  0.2109,  0.1982,  0.1768,  0.2305,  0.2295,\n",
      "         0.2168,  0.2246,  0.2285,  0.2051,  0.1670,  0.1797,  0.0991,  0.2109,\n",
      "         0.1924,  0.1855,  0.2090,  0.1943,  0.1973,  0.2188,  0.2256,  0.2422,\n",
      "         0.1992,  0.2061,  0.1992,  0.1885,  0.2090,  0.1963,  0.0562,  0.2109,\n",
      "         0.2168,  0.1963,  0.1650,  0.2109,  0.2217,  0.1953,  0.2236,  0.1797,\n",
      "         0.1885,  0.1270,  0.1260,  0.1836,  0.2061,  0.2334,  0.1182,  0.2070,\n",
      "         0.2061,  0.1680,  0.1758,  0.1309,  0.1621,  0.2070,  0.1914,  0.2236,\n",
      "         0.1934,  0.1816,  0.1943,  0.1670,  0.2002,  0.1973,  0.2236,  0.1973,\n",
      "         0.1914,  0.1719,  0.2168,  0.2266,  0.2246,  0.2324,  0.0645,  0.1187,\n",
      "         0.2012,  0.1699,  0.1836,  0.1836,  0.1797,  0.1245,  0.2295,  0.1445,\n",
      "         0.1123,  0.2158,  0.1001,  0.2012,  0.2285,  0.2168,  0.2197,  0.2363,\n",
      "         0.1963,  0.2109, -0.0933,  0.1602,  0.2324,  0.2051,  0.2344,  0.2393,\n",
      "         0.1777,  0.1934,  0.2139,  0.0388,  0.2100,  0.1357,  0.2148,  0.2197,\n",
      "         0.1885,  0.2344,  0.2012,  0.1992,  0.2129,  0.2256,  0.2422,  0.1904,\n",
      "         0.2520,  0.2031,  0.2227,  0.1904,  0.2100,  0.1768,  0.2178,  0.1992,\n",
      "         0.1982,  0.2539,  0.1709,  0.1641,  0.2012,  0.1973,  0.2373,  0.2207,\n",
      "         0.2031,  0.1904,  0.2305, -0.0339,  0.1367,  0.1904,  0.1777,  0.2109,\n",
      "         0.1787,  0.2227,  0.3125,  0.2266,  0.2178,  0.2227,  0.1914,  0.2324,\n",
      "         0.1943,  0.2324,  0.2090,  0.1914,  0.1846,  0.2041,  0.1206,  0.2188,\n",
      "         0.1738,  0.2305,  0.1992,  0.1973,  0.2354,  0.2148,  0.2012,  0.2139,\n",
      "         0.2031,  0.2217,  0.2041,  0.1992,  0.2256,  0.2061,  0.2031,  0.1885,\n",
      "         0.2246, -0.1211,  0.2188,  0.1924,  0.2500,  0.1318,  0.2373,  0.2354,\n",
      "         0.2109,  0.2051,  0.2109,  0.2070,  0.2061,  0.2119,  0.2090,  0.1123,\n",
      "         0.2119,  0.2012,  0.0977,  0.2256,  0.1992,  0.2178,  0.2012,  0.1729,\n",
      "         0.0320,  0.1289,  0.2109,  0.1875,  0.2344,  0.2139,  0.2227,  0.2314,\n",
      "         0.1562,  0.1406,  0.1963,  0.1943,  0.1992,  0.2305,  0.1943,  0.2197,\n",
      "         0.1196,  0.2139,  0.2314,  0.2402,  0.2246,  0.2080,  0.0310,  0.2100,\n",
      "         0.2217,  0.1992,  0.2051,  0.2285,  0.1963,  0.2080,  0.2080,  0.2217,\n",
      "         0.2070,  0.2100,  0.1699,  0.1934,  0.2109,  0.2354,  0.2334,  0.2109,\n",
      "         0.2207,  0.2061,  0.1709,  0.2158,  0.1855,  0.2012,  0.1855,  0.2070,\n",
      "         0.2305,  0.2197,  0.2227,  0.1777,  0.2207,  0.1797,  0.2031,  0.2148,\n",
      "         0.2168,  0.2158,  0.1934,  0.1797,  0.1973,  0.2031,  0.2148,  0.2217,\n",
      "         0.2236,  0.1826,  0.2246,  0.1582,  0.1128,  0.2012,  0.1953,  0.1875,\n",
      "         0.1709,  0.1973,  0.2148,  0.2256,  0.2383,  0.0576,  0.2344,  0.2354,\n",
      "         0.2334,  0.1562,  0.1328,  0.1846,  0.1982,  0.2100,  0.2217,  0.1963,\n",
      "         0.2061,  0.2080,  0.2324,  0.2139,  0.1895,  0.1982,  0.2217,  0.2021,\n",
      "         0.2100,  0.2129,  0.2275,  0.1602,  0.1914,  0.1904,  0.1367,  0.2021,\n",
      "         0.2178,  0.2344,  0.2129,  0.2354,  0.1807,  0.2275,  0.2051,  0.2012,\n",
      "         0.1934,  0.2344,  0.1992,  0.2246,  0.2217,  0.1914,  0.2021,  0.2168,\n",
      "         0.2080,  0.2246,  0.2070,  0.2256,  0.1855,  0.2070,  0.1416,  0.2070,\n",
      "         0.1797,  0.2168,  0.1846,  0.2021,  0.2197,  0.2051,  0.2344,  0.2129,\n",
      "         0.1904,  0.2188,  0.1904,  0.2139,  0.0325,  0.2002,  0.2178,  0.0688,\n",
      "         0.2178,  0.2148,  0.1836,  0.2197,  0.2012,  0.1426,  0.2109,  0.2021,\n",
      "         0.2363,  0.1807,  0.2002,  0.1895,  0.2168,  0.1963,  0.2109,  0.2012,\n",
      "         0.2139,  0.1924,  0.1709,  0.2324,  0.2363,  0.2080,  0.2100,  0.2061,\n",
      "         0.0371,  0.2031,  0.2109,  0.2139,  0.2002,  0.2266,  0.1934,  0.1982,\n",
      "         0.1982,  0.2539,  0.2051,  0.1992,  0.2168,  0.2178,  0.2080,  0.2012,\n",
      "         0.2148,  0.2227,  0.1631,  0.1992,  0.1953,  0.2178,  0.2197,  0.2070,\n",
      "         0.1123,  0.2129,  0.2285,  0.2090,  0.2266,  0.2188,  0.2109,  0.2256,\n",
      "         0.1611,  0.2217,  0.2314,  0.1885,  0.0703,  0.2227,  0.2070,  0.0417,\n",
      "         0.2070,  0.2129,  0.2295,  0.2080,  0.2295,  0.2168,  0.2217, -0.0801,\n",
      "         0.2012,  0.1992,  0.2402,  0.2119,  0.2158,  0.1533,  0.2119,  0.2061,\n",
      "         0.2158,  0.2080,  0.2109,  0.1895,  0.2031,  0.2100,  0.1895,  0.2197,\n",
      "         0.2109,  0.1875,  0.1719,  0.2305,  0.1826,  0.1953,  0.1533,  0.2324,\n",
      "         0.2178,  0.2178,  0.2139,  0.2070,  0.2100,  0.1758,  0.2246,  0.1475,\n",
      "         0.2051,  0.1963,  0.1592,  0.2188,  0.2139,  0.2207,  0.1914,  0.2256,\n",
      "         0.2432,  0.2158,  0.2227,  0.1934,  0.2109,  0.2080,  0.2070,  0.1992,\n",
      "         0.1914,  0.2051,  0.2070,  0.2061,  0.1992,  0.2100,  0.1572,  0.1494,\n",
      "         0.2031,  0.1504,  0.2324,  0.2051,  0.1914,  0.2256,  0.1709,  0.2168,\n",
      "         0.1992,  0.1875,  0.1816,  0.3105,  0.2441,  0.1934,  0.2148,  0.2207,\n",
      "         0.2021,  0.2109,  0.2080,  0.2217,  0.2041,  0.1875,  0.1279,  0.1973,\n",
      "         0.2334,  0.2246,  0.2002,  0.0330,  0.1553,  0.2188,  0.2246,  0.2080,\n",
      "         0.2168,  0.2109,  0.2139,  0.2158,  0.2178,  0.2070,  0.2207,  0.2178,\n",
      "         0.2031,  0.1924,  0.1904,  0.2295,  0.1289,  0.2070,  0.2100,  0.2012,\n",
      "         0.1816,  0.2148,  0.2275,  0.1885,  0.2119,  0.2041,  0.2012,  0.1113,\n",
      "         0.2441,  0.2139,  0.2148,  0.1953,  0.2227,  0.2061,  0.2100,  0.2314,\n",
      "         0.1963,  0.2012,  0.1982,  0.2178,  0.2158,  0.1943,  0.2158,  0.1396,\n",
      "         0.2080,  0.2217,  0.1982,  0.2061,  0.1973,  0.2100,  0.1777,  0.2100,\n",
      "         0.1846,  0.1865,  0.2021,  0.1992,  0.1602,  0.1992,  0.2139,  0.2080,\n",
      "         0.1494,  0.1865,  0.2080,  0.2041,  0.1523,  0.2021,  0.1201,  0.2080,\n",
      "         0.1992,  0.2305,  0.1377,  0.2324,  0.1895,  0.2090,  0.2188,  0.1157,\n",
      "         0.2012,  0.1621,  0.2197,  0.2129,  0.2002,  0.2041,  0.1963,  0.2295,\n",
      "         0.2109,  0.2041,  0.2061,  0.2295,  0.1826,  0.2217,  0.1875,  0.2080,\n",
      "         0.2061,  0.1631,  0.2207,  0.2197,  0.2012,  0.2070,  0.2188,  0.2266,\n",
      "         0.2256,  0.2227,  0.1719,  0.1865,  0.2002,  0.2041,  0.1953,  0.1943,\n",
      "         0.2314,  0.1914,  0.2246,  0.2080,  0.1592,  0.2080,  0.2129,  0.2246,\n",
      "         0.2197,  0.1963,  0.2129,  0.2070,  0.1963,  0.2061,  0.1953, -0.1021,\n",
      "         0.1758,  0.2139,  0.2188,  0.1602,  0.1387,  0.2109,  0.2021,  0.2002,\n",
      "         0.1807,  0.1973,  0.0967,  0.2002,  0.1885,  0.2041,  0.2051,  0.1855,\n",
      "         0.2100,  0.2031,  0.1504,  0.2129,  0.2393,  0.1035,  0.2012,  0.1953,\n",
      "         0.2002,  0.2217,  0.2295,  0.2168,  0.2236,  0.0708,  0.1963,  0.2021,\n",
      "         0.2285,  0.2090,  0.2129,  0.1787,  0.1826, -0.1133,  0.2285,  0.1777,\n",
      "         0.2217,  0.1787,  0.1465,  0.1992,  0.2109,  0.1436,  0.1904,  0.1924,\n",
      "         0.1846,  0.1982,  0.2178,  0.2119,  0.1982,  0.2002,  0.1021,  0.2080,\n",
      "         0.1729,  0.1670,  0.1973,  0.2188,  0.1943,  0.2090,  0.2578,  0.1982],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.4.layer.1.EncDecAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0510, -0.0427,  0.0188,  ..., -0.0040,  0.0444, -0.0884],\n",
      "        [-0.0325, -0.0332, -0.0298,  ...,  0.0023, -0.0249, -0.0088],\n",
      "        [ 0.0009, -0.0119, -0.0396,  ...,  0.0457, -0.0234,  0.0297],\n",
      "        ...,\n",
      "        [ 0.0234, -0.0352,  0.0459,  ...,  0.0176,  0.0018,  0.0291],\n",
      "        [ 0.0540,  0.0003,  0.0063,  ...,  0.0171, -0.0020, -0.0098],\n",
      "        [-0.1011, -0.0209, -0.0219,  ...,  0.0110,  0.0334,  0.0039]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.4.layer.1.EncDecAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.3867, -0.5820, -0.2080,  ..., -0.0757, -0.1094, -0.8711],\n",
      "        [ 0.1719,  0.1719,  0.1885,  ...,  0.2871, -0.3730,  0.2871],\n",
      "        [ 0.0767,  0.2695, -0.2344,  ..., -0.4531,  0.0757,  0.0417],\n",
      "        ...,\n",
      "        [-0.2832,  0.0364,  0.4102,  ...,  0.0106,  0.4199, -0.2334],\n",
      "        [ 0.1475,  0.1270,  0.3340,  ...,  0.1367, -0.0231,  0.3359],\n",
      "        [ 0.5820, -0.2676, -0.5781,  ..., -0.1631, -0.0403,  0.0845]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.4.layer.1.EncDecAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.4043, -0.3203, -0.0048,  ...,  0.3066,  0.5586, -0.2734],\n",
      "        [ 0.2656, -0.1201, -0.4570,  ..., -0.0171,  0.3262, -1.1406],\n",
      "        [ 0.2168, -0.2432, -0.2129,  ...,  0.1553, -0.8633, -0.2295],\n",
      "        ...,\n",
      "        [ 0.2490,  0.1602,  0.8516,  ..., -0.5391,  0.0238, -0.1250],\n",
      "        [-0.7539,  0.0630, -0.1602,  ..., -0.4453, -0.6992,  0.2676],\n",
      "        [ 0.2695,  0.3965, -0.1494,  ...,  0.3242, -0.8320,  0.1777]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.4.layer.1.EncDecAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.3164,  0.5117,  0.5039,  ...,  0.2578, -0.8516,  0.1777],\n",
      "        [ 0.3945,  0.9414, -0.7695,  ...,  0.0058,  0.2988, -0.0410],\n",
      "        [ 0.2754, -0.3125,  0.8125,  ...,  0.1260,  0.7500, -0.5039],\n",
      "        ...,\n",
      "        [ 0.1553,  0.1221,  0.4473,  ..., -0.8047,  0.0933,  0.1865],\n",
      "        [ 0.6367, -0.5352, -0.2041,  ...,  0.0757, -0.1338, -0.3984],\n",
      "        [-0.0049, -0.5430,  0.4238,  ...,  0.4082, -0.1045,  0.0442]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.4.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 7.2266e-02,  3.9062e-02,  4.7852e-02,  1.1035e-01,  7.4219e-02,\n",
      "         5.5176e-02,  9.5703e-02,  9.8145e-02,  7.5195e-02,  1.0107e-01,\n",
      "         5.9326e-02,  1.0352e-01,  5.4932e-02,  5.9814e-02,  8.5938e-02,\n",
      "         9.0332e-02,  8.5938e-02,  7.1289e-02,  7.9102e-02,  1.0059e-01,\n",
      "         8.6426e-02,  9.3262e-02,  9.5703e-02,  9.9609e-02,  8.9844e-02,\n",
      "         7.3730e-02,  8.8867e-02,  5.2734e-02,  8.1055e-02,  9.4238e-02,\n",
      "         1.0742e-01,  4.9561e-02,  1.0156e-01,  8.7891e-02,  9.3750e-02,\n",
      "         6.5918e-02,  9.3750e-02,  7.6660e-02,  8.0566e-02,  9.8145e-02,\n",
      "         8.9355e-02,  7.7637e-02,  8.8379e-02,  8.3496e-02,  5.9326e-02,\n",
      "         8.5938e-02,  1.1523e-01,  9.5703e-02,  9.9609e-02,  8.8867e-02,\n",
      "         8.5938e-02,  8.7402e-02,  7.7637e-02,  9.2773e-02,  1.2354e-01,\n",
      "         6.1279e-02,  7.8125e-02,  9.6191e-02,  9.5215e-02,  8.9355e-02,\n",
      "         7.5684e-02,  9.5215e-02,  9.1309e-02,  9.1309e-02,  9.6191e-02,\n",
      "         9.9609e-02,  7.9102e-02,  8.6914e-02,  4.6387e-02,  8.3008e-02,\n",
      "         9.4727e-02,  9.9121e-02,  8.3496e-02,  8.2520e-02,  8.1543e-02,\n",
      "         8.4473e-02,  5.4199e-02,  8.3008e-02,  8.8867e-02,  7.8125e-02,\n",
      "         8.5938e-02,  8.4961e-02,  8.8379e-02,  6.3965e-02,  8.4961e-02,\n",
      "         8.5938e-02,  6.6406e-02,  9.7656e-02,  8.9355e-02,  8.5938e-02,\n",
      "         7.2754e-02,  8.9355e-02,  8.7402e-02,  7.0312e-02,  8.8867e-02,\n",
      "         9.7168e-02,  8.9355e-02,  1.0205e-01,  9.3750e-02,  8.9844e-02,\n",
      "         7.0312e-02,  8.1543e-02,  4.1016e-02,  8.2520e-02,  7.6660e-02,\n",
      "         7.7637e-02,  9.4727e-02,  7.5195e-02,  9.3262e-02,  8.6426e-02,\n",
      "         8.8867e-02,  9.3750e-02,  7.5195e-02,  8.0566e-02,  7.9590e-02,\n",
      "         9.2285e-02,  7.7637e-02,  7.9590e-02, -7.5912e-04,  8.3008e-02,\n",
      "         7.9590e-02,  9.1309e-02,  6.7871e-02,  8.9355e-02,  8.7891e-02,\n",
      "         6.9336e-02,  8.3984e-02,  6.6895e-02,  7.7148e-02,  5.7861e-02,\n",
      "         4.7363e-02,  7.9102e-02,  8.3008e-02,  9.5215e-02,  5.5908e-02,\n",
      "         9.3750e-02,  8.2031e-02,  7.0801e-02,  7.8125e-02,  5.4932e-02,\n",
      "         6.9824e-02,  9.4727e-02,  7.8613e-02,  8.7891e-02,  8.7891e-02,\n",
      "         7.4219e-02,  8.4473e-02,  7.0312e-02,  8.1055e-02,  7.7148e-02,\n",
      "         9.4727e-02,  8.4473e-02,  9.4238e-02,  7.7637e-02,  9.4727e-02,\n",
      "         9.3262e-02,  9.4727e-02,  9.4238e-02,  1.1572e-01,  5.6641e-02,\n",
      "         8.2031e-02,  6.9824e-02,  7.6172e-02,  7.2266e-02,  6.7871e-02,\n",
      "         4.2236e-02,  9.1309e-02,  6.3477e-02,  3.5889e-02,  9.2773e-02,\n",
      "         3.8818e-02,  8.3008e-02,  1.1133e-01,  8.5938e-02,  9.0820e-02,\n",
      "         9.3262e-02,  8.3984e-02,  1.0010e-01,  3.8818e-02,  6.7871e-02,\n",
      "         8.7402e-02,  8.9844e-02,  1.0596e-01,  9.5215e-02,  6.9336e-02,\n",
      "         8.2520e-02,  8.7891e-02,  7.3730e-02,  8.2520e-02,  5.5908e-02,\n",
      "         9.6680e-02,  8.6426e-02,  8.2031e-02,  8.7891e-02,  7.5684e-02,\n",
      "         8.2031e-02,  8.1055e-02,  9.8145e-02,  8.9355e-02,  7.8125e-02,\n",
      "         9.3750e-02,  8.3008e-02,  8.6914e-02,  7.6660e-02,  7.8613e-02,\n",
      "         6.5430e-02,  1.0645e-01,  9.1309e-02,  8.4961e-02,  1.1426e-01,\n",
      "         8.7402e-02,  6.3477e-02,  7.9102e-02,  8.2031e-02,  1.0059e-01,\n",
      "         9.3262e-02,  8.2031e-02,  6.7383e-02,  8.9844e-02,  9.6321e-05,\n",
      "         5.2979e-02,  9.0332e-02,  7.3730e-02,  9.3262e-02,  7.2754e-02,\n",
      "         1.0645e-01,  1.2061e-01,  1.0938e-01,  8.2031e-02,  8.7402e-02,\n",
      "         8.2520e-02,  9.7656e-02,  7.7148e-02,  9.7656e-02,  8.6426e-02,\n",
      "         7.6660e-02,  7.6172e-02,  8.7402e-02,  5.4688e-02,  9.6680e-02,\n",
      "         6.3477e-02,  9.3750e-02,  8.2520e-02,  8.1055e-02,  1.0449e-01,\n",
      "         7.8613e-02,  8.9844e-02, -8.6426e-02,  8.5449e-02,  9.3750e-02,\n",
      "         8.1543e-02,  7.9102e-02,  9.0820e-02,  7.4707e-02,  8.6914e-02,\n",
      "         6.8848e-02,  8.3984e-02,  5.5664e-02,  9.0332e-02,  8.3496e-02,\n",
      "         1.2256e-01,  5.7617e-02,  1.0840e-01,  1.0156e-01,  8.9844e-02,\n",
      "         7.8125e-02,  9.6680e-02,  8.9844e-02,  8.5938e-02,  8.8867e-02,\n",
      "         9.2773e-02,  6.6895e-02,  1.0107e-01,  8.9355e-02,  3.8574e-02,\n",
      "         9.3750e-02,  8.9355e-02,  8.3984e-02,  9.1797e-02,  7.3242e-02,\n",
      "        -4.4405e-06,  5.1025e-02,  9.2285e-02,  7.5195e-02,  1.0205e-01,\n",
      "         8.8379e-02,  9.0332e-02,  1.0107e-01,  6.9824e-02,  5.8105e-02,\n",
      "         8.1055e-02,  7.6172e-02,  8.0566e-02,  9.6191e-02,  8.0566e-02,\n",
      "         9.1309e-02,  5.6396e-02,  8.8867e-02,  8.5449e-02,  1.0986e-01,\n",
      "         1.0156e-01,  8.2520e-02, -1.7624e-03,  7.7637e-02,  9.4727e-02,\n",
      "         8.2520e-02,  8.8379e-02,  9.5215e-02,  8.3984e-02,  8.9355e-02,\n",
      "         9.1797e-02,  8.7891e-02,  8.2520e-02,  9.4238e-02,  1.2695e-01,\n",
      "         8.0078e-02,  1.1230e-01,  9.4238e-02,  1.0498e-01,  9.8145e-02,\n",
      "         9.1309e-02,  9.8633e-02,  7.1289e-02,  8.9355e-02,  8.2520e-02,\n",
      "         7.7637e-02,  7.8125e-02,  8.5938e-02,  9.2773e-02,  8.4961e-02,\n",
      "         9.8633e-02,  6.9824e-02,  9.3750e-02,  7.7148e-02,  7.4707e-02,\n",
      "         8.1543e-02,  9.1309e-02,  1.0938e-01,  8.0078e-02,  7.0801e-02,\n",
      "         7.4707e-02,  8.8379e-02,  9.3750e-02,  8.8379e-02,  8.4961e-02,\n",
      "         7.0312e-02,  9.0332e-02,  6.4941e-02,  4.5410e-02,  7.4707e-02,\n",
      "         9.2285e-02,  7.6172e-02,  7.0801e-02,  7.9590e-02,  8.7891e-02,\n",
      "         9.6680e-02,  9.4727e-02,  6.5804e-05,  9.5215e-02,  1.0742e-01,\n",
      "         9.6191e-02,  6.3477e-02,  6.1768e-02,  7.6660e-02,  8.7891e-02,\n",
      "         1.0400e-01,  9.2773e-02,  7.5684e-02,  8.2031e-02,  8.4961e-02,\n",
      "         9.0332e-02,  8.6914e-02,  8.3984e-02,  7.8613e-02,  1.0303e-01,\n",
      "         7.8125e-02,  9.4727e-02,  9.5703e-02,  9.9121e-02,  6.0059e-02,\n",
      "         7.5195e-02,  7.3730e-02,  6.9336e-02,  8.5449e-02,  8.3984e-02,\n",
      "         9.1797e-02,  9.0820e-02,  9.6191e-02,  7.4219e-02,  9.1797e-02,\n",
      "         8.3984e-02,  7.3242e-02,  7.5195e-02,  9.0820e-02,  7.8125e-02,\n",
      "         8.7402e-02,  9.3750e-02,  7.6660e-02,  8.9355e-02,  7.9590e-02,\n",
      "         8.5449e-02,  8.4961e-02,  8.6914e-02,  9.7168e-02,  7.4707e-02,\n",
      "         7.7637e-02,  7.3242e-02,  8.2031e-02,  6.9824e-02,  8.3984e-02,\n",
      "         7.6660e-02,  8.2520e-02,  8.0566e-02,  9.7656e-02,  9.1309e-02,\n",
      "         1.0156e-01,  8.0078e-02,  9.2773e-02,  1.4160e-01,  8.1055e-02,\n",
      "        -7.4005e-04,  8.9844e-02,  8.9844e-02, -2.6367e-02,  1.0938e-01,\n",
      "         8.0566e-02,  7.4219e-02,  8.7402e-02,  6.9336e-02,  5.9082e-02,\n",
      "         9.9609e-02,  7.3730e-02,  1.0498e-01,  6.9824e-02,  7.4707e-02,\n",
      "         7.3242e-02,  8.9355e-02,  8.2031e-02,  8.3984e-02,  7.7148e-02,\n",
      "         8.3008e-02,  8.2031e-02, -7.0801e-02,  9.5215e-02,  9.8145e-02,\n",
      "         8.9844e-02,  9.3750e-02,  9.6191e-02,  1.4496e-04,  8.3008e-02,\n",
      "         9.1309e-02,  9.0332e-02,  8.0566e-02,  8.9844e-02,  8.2520e-02,\n",
      "         7.9102e-02,  1.2695e-01,  1.2500e-01,  8.3496e-02,  7.9102e-02,\n",
      "         8.8867e-02,  9.4238e-02,  9.1797e-02,  8.3496e-02,  8.8379e-02,\n",
      "         8.8379e-02,  7.1289e-02,  8.2031e-02,  7.6660e-02,  8.2520e-02,\n",
      "         9.8145e-02,  8.8379e-02,  3.1494e-02,  8.4961e-02,  9.0332e-02,\n",
      "         8.5938e-02,  9.2773e-02,  8.9844e-02,  8.1055e-02,  8.4961e-02,\n",
      "         6.2988e-02,  9.7656e-02,  1.0107e-01,  7.7148e-02,  1.8555e-02,\n",
      "         9.3750e-02,  8.7402e-02,  1.3672e-01,  9.2773e-02,  8.4473e-02,\n",
      "         9.0332e-02,  1.0498e-01,  8.6914e-02,  8.8379e-02,  1.0303e-01,\n",
      "         4.9561e-02,  7.9590e-02,  8.3984e-02,  9.2285e-02,  8.4473e-02,\n",
      "         7.6172e-02,  1.0107e-01,  8.8867e-02,  8.3008e-02,  7.7637e-02,\n",
      "         8.2520e-02,  9.1797e-02,  7.1289e-02,  7.8613e-02,  1.0254e-01,\n",
      "         8.4961e-02,  1.0596e-01,  8.3984e-02,  7.7148e-02,  7.2266e-02,\n",
      "         9.0820e-02,  8.2520e-02,  6.9336e-02,  5.9570e-02,  9.3750e-02,\n",
      "         8.4961e-02,  8.7402e-02,  9.4727e-02,  8.3496e-02,  8.2031e-02,\n",
      "         7.7637e-02,  9.2285e-02,  6.1768e-02,  7.3242e-02,  6.7383e-02,\n",
      "         5.7373e-02,  8.6426e-02,  8.7891e-02,  9.4727e-02,  8.1543e-02,\n",
      "         8.6426e-02,  1.0596e-01,  9.0820e-02,  9.6680e-02,  8.7891e-02,\n",
      "         8.7402e-02,  8.2520e-02,  9.0820e-02,  7.7637e-02,  8.1543e-02,\n",
      "         9.2773e-02,  8.8867e-02,  8.1543e-02,  9.3262e-02,  7.9590e-02,\n",
      "         6.5430e-02,  7.1289e-02,  8.1543e-02,  6.1523e-02,  9.1309e-02,\n",
      "         8.4961e-02,  7.8125e-02,  9.0332e-02,  6.4453e-02,  9.3262e-02,\n",
      "         8.5938e-02,  7.7637e-02,  5.0049e-02,  1.4941e-01,  9.9609e-02,\n",
      "         9.1309e-02,  9.4238e-02,  7.9590e-02,  8.8867e-02,  8.6426e-02,\n",
      "         9.1797e-02,  9.2773e-02,  8.1055e-02,  7.3730e-02,  5.4199e-02,\n",
      "         8.2520e-02,  9.3750e-02,  8.8379e-02,  7.6660e-02,  2.2030e-04,\n",
      "         5.9570e-02,  9.2285e-02,  8.7402e-02,  7.8613e-02,  8.7891e-02,\n",
      "         9.9609e-02,  8.7402e-02,  8.0566e-02,  8.7891e-02,  7.9590e-02,\n",
      "         9.3750e-02,  9.7168e-02,  7.8125e-02,  8.0566e-02,  7.6172e-02,\n",
      "         9.7168e-02,  5.2734e-02,  9.1797e-02,  9.3750e-02,  8.4473e-02,\n",
      "         6.6895e-02,  8.3496e-02,  9.7656e-02,  7.4707e-02,  8.3984e-02,\n",
      "         8.2031e-02,  9.4238e-02,  4.9561e-02,  9.9121e-02,  9.0820e-02,\n",
      "         8.7891e-02,  8.1543e-02,  9.6680e-02,  9.3262e-02,  7.9590e-02,\n",
      "         1.0352e-01,  7.0312e-02,  1.2207e-01,  8.7891e-02,  9.8145e-02,\n",
      "         8.0566e-02,  7.4219e-02,  8.3984e-02,  5.8594e-02,  9.1309e-02,\n",
      "         8.5938e-02,  8.0566e-02,  8.8379e-02,  8.4473e-02,  7.6172e-02,\n",
      "         7.8613e-02,  8.5449e-02,  6.7383e-02,  7.3242e-02,  8.0566e-02,\n",
      "         8.6914e-02,  6.8359e-02,  8.0078e-02,  8.4961e-02,  8.8379e-02,\n",
      "         6.3477e-02,  7.9590e-02,  9.3750e-02,  8.3984e-02,  5.5908e-02,\n",
      "         8.1543e-02,  5.1270e-02,  8.7402e-02,  8.3008e-02,  1.0010e-01,\n",
      "         6.2988e-02,  9.2285e-02,  7.7637e-02,  9.1309e-02,  9.1309e-02,\n",
      "         4.3457e-02,  7.2266e-02,  7.3242e-02,  9.4238e-02,  9.9609e-02,\n",
      "         8.6426e-02,  7.2266e-02,  6.7871e-02,  1.0498e-01,  9.4727e-02,\n",
      "         8.6914e-02,  8.8867e-02,  9.2773e-02,  8.9844e-02,  8.6426e-02,\n",
      "         7.4707e-02,  7.8125e-02,  8.3984e-02,  6.0059e-02,  9.7656e-02,\n",
      "         8.4473e-02,  7.9590e-02,  8.9355e-02,  8.1055e-02,  8.9355e-02,\n",
      "         9.1797e-02,  9.2773e-02,  6.6895e-02,  7.6660e-02,  7.8125e-02,\n",
      "         8.3984e-02,  8.9355e-02,  7.8613e-02,  9.7168e-02,  7.6660e-02,\n",
      "         8.4473e-02,  8.2031e-02,  5.8350e-02,  8.2520e-02,  8.9355e-02,\n",
      "         7.9102e-02,  1.0107e-01,  8.0078e-02,  8.8379e-02,  7.8613e-02,\n",
      "         7.8125e-02,  8.9355e-02,  8.0566e-02,  4.3213e-02,  8.2031e-02,\n",
      "         8.5938e-02,  9.0332e-02,  6.4941e-02,  6.2500e-02,  8.7891e-02,\n",
      "         7.6172e-02,  7.9102e-02,  8.9844e-02,  7.9102e-02, -3.9307e-02,\n",
      "         1.1182e-01,  7.6660e-02,  8.7402e-02,  8.5449e-02,  7.6660e-02,\n",
      "         1.0303e-01,  7.6660e-02,  6.0303e-02,  8.3008e-02,  8.6914e-02,\n",
      "         5.1270e-02,  8.3008e-02,  8.2031e-02,  8.0566e-02,  9.8145e-02,\n",
      "         9.5703e-02,  7.8613e-02,  9.4727e-02,  5.7129e-02,  8.5938e-02,\n",
      "         9.1797e-02,  1.0107e-01,  8.4961e-02,  9.3750e-02,  7.5684e-02,\n",
      "         8.3496e-02,  5.7373e-02,  8.8867e-02,  7.5684e-02,  9.8633e-02,\n",
      "         6.9824e-02,  6.2500e-02,  7.3242e-02,  8.4961e-02,  6.0547e-02,\n",
      "         8.2031e-02,  7.3242e-02,  8.0078e-02,  7.4707e-02,  8.9844e-02,\n",
      "         1.0449e-01,  8.5938e-02,  7.9590e-02,  6.3477e-02,  8.3008e-02,\n",
      "         7.3242e-02,  7.1289e-02,  7.8613e-02,  8.1055e-02,  8.5449e-02,\n",
      "         8.3496e-02,  1.0303e-01,  8.4961e-02], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.decoder.block.4.layer.2.DenseReluDense.wi.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.4609, -0.3242, -0.0461,  ..., -0.0096,  0.5469, -0.0398],\n",
      "        [-0.2676, -0.1318, -1.0469,  ..., -0.6992,  0.3691, -0.8398],\n",
      "        [ 0.2148,  0.7070, -1.3516,  ..., -0.8086, -0.2412, -0.0659],\n",
      "        ...,\n",
      "        [-0.4727,  0.0874, -0.1416,  ...,  0.0210,  0.6445,  1.1094],\n",
      "        [-0.7812, -0.3301, -1.1250,  ..., -0.5859,  0.3438, -1.5703],\n",
      "        [ 0.2734,  0.4434, -0.7188,  ...,  0.4629,  0.6133,  0.0315]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.4.layer.2.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0635,  0.2217,  0.1289,  ..., -0.3008,  0.2578, -0.3281],\n",
      "        [-0.3594,  0.0259,  0.1069,  ..., -0.2158, -0.2363,  0.5820],\n",
      "        [-0.1797,  0.1094,  0.1543,  ..., -0.1699, -0.5586,  0.1240],\n",
      "        ...,\n",
      "        [ 0.1826,  0.1582, -0.1963,  ...,  0.2334, -0.2656, -0.1099],\n",
      "        [-0.4375,  0.0908,  0.2061,  ..., -0.4121, -0.3223,  0.1099],\n",
      "        [ 0.0299, -0.5664,  0.1001,  ..., -0.2188, -0.2988, -0.0684]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.4.layer.2.layer_norm.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 2.3438,  1.7578,  1.6406,  2.2656,  2.3750,  2.6719,  2.0469,  2.6719,\n",
      "         1.8672,  2.1250,  3.7500,  2.5312,  1.9453,  2.8594,  2.2969,  2.3750,\n",
      "         2.2656,  1.7969,  2.2969,  2.3594,  2.3438,  2.4219,  2.2812,  2.3438,\n",
      "         2.5000,  1.9297,  2.3438,  2.5000,  2.3281,  2.4062,  2.5000,  1.6406,\n",
      "         2.5469,  2.2344,  2.5000,  1.9375,  2.3438,  1.4922,  2.1719,  2.4844,\n",
      "         2.0781,  2.3125,  2.2969,  2.4531,  1.5938,  2.1719,  2.2812,  2.4688,\n",
      "         2.3438,  2.6094,  2.1562,  2.3281,  2.3750,  2.2969,  2.9062,  2.5000,\n",
      "         2.2031,  2.3281,  2.2500,  2.3594,  2.1406,  2.5156,  2.3281,  2.4062,\n",
      "         2.4375,  1.1016,  2.2969,  2.3281,  1.9609,  2.2812,  2.1875,  2.2031,\n",
      "         2.1406,  2.2812,  2.3281,  2.1875,  2.4844,  2.4531,  2.3125,  2.2500,\n",
      "         2.2500,  2.2656,  2.4688,  1.8516,  2.1406,  2.3750,  2.3750,  2.3906,\n",
      "         2.3750,  2.4844,  2.0469,  2.1875,  2.2969,  2.5625,  2.5312,  2.3750,\n",
      "         2.3438,  2.2344,  2.4688,  2.2812,  2.7656,  1.9297,  2.6406,  2.4219,\n",
      "         2.2344,  2.0625,  2.3906,  2.0938,  2.0781,  2.2656,  2.2812,  2.4062,\n",
      "         2.2500,  2.1406,  2.1250,  2.1094,  2.3125,  2.7812,  0.0090,  2.2969,\n",
      "         2.3594,  1.9922,  1.8672,  2.1875,  2.1875,  2.2188,  2.4375,  2.3281,\n",
      "         2.3125,  2.8281,  2.5000,  2.0156,  2.4062,  2.3906,  1.5391,  2.2188,\n",
      "         2.2812,  2.1250,  2.9219,  2.9531,  2.0938,  2.2188,  2.8438,  2.2656,\n",
      "         2.2656,  2.4062,  2.1250,  1.9219,  2.1562,  2.6094,  2.2031,  2.1562,\n",
      "         2.1562,  1.9375,  2.3750,  2.3594,  2.3281,  2.4219,  1.8594,  2.7344,\n",
      "         2.2344,  2.0938,  2.1875,  2.1406,  1.8750,  1.6953,  2.4375,  1.7578,\n",
      "         1.6172,  2.2031,  2.8125,  2.1094,  2.2344,  2.1562,  2.3438,  2.2188,\n",
      "         2.1875,  2.0938,  2.5781,  1.8203,  2.4688,  2.3750,  2.4844,  2.5156,\n",
      "         2.5156,  2.1875,  2.2500,  1.3281,  2.2969,  2.5312,  2.1875,  2.2188,\n",
      "         2.0000,  2.6562,  2.1250,  2.0469,  2.3281,  2.2656,  2.3906,  2.0781,\n",
      "         2.4531,  2.3750,  2.3594,  2.0312,  2.3125,  2.3750,  2.3281,  2.1719,\n",
      "         2.2500,  2.4375,  1.8359,  3.0469,  2.1094,  2.1719,  2.3906,  2.4688,\n",
      "         2.4219,  2.0000,  2.5000,  0.8359,  2.5312,  2.1250,  2.0156,  2.2969,\n",
      "         2.2188,  2.3594,  0.6797,  2.2500,  2.3906,  2.2969,  1.9922,  2.3438,\n",
      "         2.0781,  2.3594,  2.2656,  2.1094,  2.4844,  2.4375,  2.9219,  2.7188,\n",
      "         3.7969,  2.2344,  2.1562,  2.1094,  2.3125,  2.2188,  2.0469,  2.6719,\n",
      "         2.1406,  2.1875,  2.2500,  2.3750,  2.5156,  2.2656,  2.1406,  1.8594,\n",
      "         2.3906,  2.7656,  2.3125,  2.0625,  2.4062,  3.1406,  2.4062,  2.6094,\n",
      "         2.2656,  2.2344,  2.1406,  2.1719,  2.3906,  2.4844,  2.2812,  1.5703,\n",
      "         2.4219,  2.2031,  2.8750,  2.3438,  2.3594,  2.6562,  2.3125,  2.3438,\n",
      "         0.4668,  1.9297,  2.3125,  2.1406,  2.2500,  2.3125,  2.4688,  2.5312,\n",
      "         1.9453,  2.8906,  2.1875,  2.2031,  2.2344,  2.4688,  2.3438,  2.3125,\n",
      "         1.6484,  2.3906,  2.3594,  2.6875,  2.2500,  2.1875,  0.3848,  2.2500,\n",
      "         2.4688,  2.2344,  2.3438,  2.4688,  2.4375,  2.1094,  1.9219,  2.3125,\n",
      "         2.2188,  2.1250,  1.5234,  2.4844,  1.9766,  2.3281,  2.3594,  2.2344,\n",
      "         2.1562,  2.2969,  2.5625,  2.2188,  1.8672,  2.0781,  2.3906,  2.1406,\n",
      "         2.3438,  2.3438,  2.2656,  2.1562,  2.2188,  2.0312,  2.2500,  2.4219,\n",
      "         2.2656,  2.2031,  2.1250,  3.5312,  2.3438,  2.2656,  2.0781,  2.2656,\n",
      "         2.2656,  2.0625,  2.3750,  2.9688,  1.5859,  2.4375,  2.1562,  2.0469,\n",
      "         1.9688,  2.1875,  2.3281,  2.3125,  2.5156,  0.5117,  2.4375,  2.5625,\n",
      "         2.4219,  1.8672,  2.6406,  2.1562,  2.1406,  2.1719,  2.3594,  2.0938,\n",
      "         2.2969,  2.5938,  2.3906,  2.1562,  2.0312,  2.2188,  2.2500,  2.4844,\n",
      "         2.2656,  1.9453,  2.3438,  1.9844,  2.1875,  2.0156,  2.6094,  2.3125,\n",
      "         2.3281,  2.3281,  2.2188,  2.5781,  2.1094,  2.2812,  2.2500,  2.1406,\n",
      "         2.2188,  2.6094,  2.2812,  2.3906,  2.3750,  2.2031,  2.1875,  2.7656,\n",
      "         2.1406,  2.2344,  2.3906,  2.4688,  2.2656,  2.2188,  1.9141,  2.2188,\n",
      "         2.1250,  2.2031,  2.1562,  2.1406,  2.2812,  2.2812,  2.3125,  2.2188,\n",
      "         2.1719,  2.3594,  1.1406,  2.2188,  0.9219,  2.2500,  2.3438,  1.9531,\n",
      "         2.4062,  2.1250,  2.3906,  2.3125,  2.2656,  3.5938,  2.2188,  2.2188,\n",
      "         2.3906,  2.1875,  2.1094,  2.1406,  2.4062,  2.1406,  2.3281,  2.1406,\n",
      "         2.2031,  2.1406,  1.7578,  2.3906,  2.2344,  2.1719,  2.1562,  2.2656,\n",
      "         0.9609,  2.2500,  2.2969,  2.2969,  2.2500,  2.3594,  2.1094,  2.4688,\n",
      "         1.7891,  1.0000,  2.2969,  2.2031,  2.4219,  2.2188,  2.2812,  2.2500,\n",
      "         2.3594,  2.4375,  2.5156,  2.3281,  2.2031,  2.3438,  2.3906,  2.1875,\n",
      "         1.2188,  2.3906,  2.2344,  2.1406,  2.3281,  2.2500,  2.3125,  2.3750,\n",
      "         3.1406,  2.5156,  2.3594,  2.0781,  0.5312,  2.4531,  2.1406,  0.8164,\n",
      "         2.2656,  2.3906,  2.3750,  2.0781,  2.3594,  2.3125,  2.3438,  2.6250,\n",
      "         2.1406,  2.2656,  2.5156,  2.4688,  2.2500,  1.7812,  2.2188,  2.3281,\n",
      "         2.2500,  2.1875,  2.1562,  2.3281,  2.1562,  2.3281,  2.1250,  2.4375,\n",
      "         2.3594,  2.3594,  1.8047,  2.4531,  2.2188,  2.0781,  2.8906,  2.3594,\n",
      "         2.2812,  2.3750,  2.2344,  2.2188,  2.3281,  2.0000,  2.5781,  1.6953,\n",
      "         2.3281,  2.2344,  1.8438,  2.2500,  2.2812,  2.2812,  2.0625,  2.3594,\n",
      "         2.4531,  2.1875,  2.2031,  2.2812,  2.1875,  2.4062,  2.1562,  2.1406,\n",
      "         2.2656,  2.3438,  2.1875,  2.2656,  2.1250,  2.1875,  2.1719,  2.1719,\n",
      "         2.1562,  1.9609,  2.2188,  2.1562,  2.1562,  2.5156,  2.0156,  2.3906,\n",
      "         2.1406,  2.1406,  0.9023,  2.7344,  2.3906,  2.0938,  2.2500,  2.2344,\n",
      "         2.5938,  2.2344,  2.1875,  2.3281,  2.2656,  2.8594,  3.0469,  2.0469,\n",
      "         2.3906,  2.2969,  2.1250, -0.0188,  1.8438,  2.3750,  2.5469,  2.2656,\n",
      "         2.2500,  2.2969,  2.4688,  2.4219,  2.4688,  2.1562,  2.3750,  2.3906,\n",
      "         2.3438,  2.0312,  2.1875,  2.4375,  1.8281,  2.2188,  2.9531,  2.3750,\n",
      "         2.0000,  2.2656,  2.2344,  2.1406,  2.3438,  2.2188,  2.1875,  1.4141,\n",
      "         2.4531,  2.3438,  2.2812,  2.4844,  2.3906,  2.2031,  2.1719,  2.4531,\n",
      "         2.3594,  1.1641,  2.0625,  2.2812,  2.2500,  2.1094,  2.2031,  3.0000,\n",
      "         2.5781,  2.2969,  2.5000,  2.2031,  2.2500,  2.1719,  1.9297,  2.2500,\n",
      "         1.9453,  2.1406,  2.1562,  2.1250,  3.0938,  2.3750,  2.2031,  2.2500,\n",
      "         2.2188,  1.9219,  2.3281,  2.2812,  1.9219,  2.5312,  1.6641,  2.2344,\n",
      "         2.2344,  2.3906,  1.6016,  2.4844,  2.1250,  2.4844,  2.3281,  2.6094,\n",
      "         2.1875,  2.6875,  2.5312,  2.6406,  2.2656,  2.2969,  1.9531,  2.2500,\n",
      "         2.2031,  2.1875,  2.3125,  2.4219,  2.7031,  2.3906,  2.1094,  2.3281,\n",
      "         2.4219,  1.8672,  2.2500,  2.3281,  2.0156,  2.1562,  2.2188,  2.4375,\n",
      "         2.2344,  2.3438,  2.3594,  2.0312,  2.2188,  2.2344,  2.0312,  2.1406,\n",
      "         2.4688,  2.1562,  2.3438,  2.2969,  2.0312,  2.1094,  2.3125,  2.1719,\n",
      "         2.3594,  2.1094,  2.4219,  2.3281,  2.1094,  2.2656,  2.1875,  2.1562,\n",
      "         1.9766,  2.3750,  2.2500,  2.7656,  2.0312,  2.2812,  2.1562,  2.1562,\n",
      "         2.0469,  2.0781,  2.1562,  1.6328,  2.2656,  1.9531,  2.1406,  2.5000,\n",
      "         2.2656,  2.2188,  2.0625,  2.2188,  2.2969,  1.6641,  1.9844,  2.1250,\n",
      "         2.1406,  2.2969,  2.1562,  2.4844,  2.2656,  1.7656,  1.9219,  2.2344,\n",
      "         2.3125,  2.1719,  2.2812,  2.1406,  2.2031,  2.6250,  2.4375,  2.4062,\n",
      "         2.4219,  2.0156,  2.1094,  2.0781,  2.3594,  2.7188,  2.6562,  2.0938,\n",
      "         2.3281,  2.0781,  2.3750,  2.1719,  2.3125,  2.1562,  1.9531,  2.3125,\n",
      "         1.9844,  2.0625,  2.3594,  2.5312,  2.0625,  2.2188,  2.6562,  2.6719],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.5.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0222,  0.0315, -0.0297,  ...,  0.0388,  0.0320,  0.0297],\n",
      "        [-0.0215,  0.0249, -0.0160,  ..., -0.0376, -0.0197, -0.0023],\n",
      "        [ 0.0332, -0.0425,  0.0004,  ...,  0.0031, -0.0240,  0.0096],\n",
      "        ...,\n",
      "        [ 0.0017, -0.0200,  0.0225,  ...,  0.0354, -0.0242, -0.0293],\n",
      "        [-0.0084, -0.0549,  0.0046,  ...,  0.0195,  0.0488, -0.0028],\n",
      "        [ 0.0422, -0.0056, -0.0299,  ..., -0.0100,  0.0041, -0.0569]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.5.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0571, -0.2988, -0.0150,  ..., -0.0564, -0.2139, -0.3027],\n",
      "        [ 0.1113, -0.1738, -0.1611,  ..., -0.1445, -0.2266,  0.4043],\n",
      "        [ 0.3184,  0.5547, -0.2490,  ...,  0.1787, -0.3535, -0.1108],\n",
      "        ...,\n",
      "        [ 0.5117,  0.0688, -0.2012,  ..., -0.5469, -0.0286,  0.4316],\n",
      "        [ 0.1553,  0.4512,  0.0996,  ..., -0.4961, -0.2090,  1.1328],\n",
      "        [ 0.4199, -0.2520,  0.2207,  ...,  0.7734,  0.2578,  0.0391]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.5.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[-1.5156,  0.3633, -0.4980,  ..., -0.0520, -0.3398,  0.1289],\n",
      "        [-0.5664,  0.8281,  0.2969,  ...,  0.8516,  0.1289, -1.2734],\n",
      "        [-0.2715, -0.0903, -0.6172,  ..., -0.5195, -0.8945, -0.3242],\n",
      "        ...,\n",
      "        [ 0.6797,  0.2637,  0.5078,  ..., -0.9492,  0.1885,  0.7461],\n",
      "        [ 1.5781,  0.3594,  0.0947,  ..., -0.8164, -0.7695, -0.4746],\n",
      "        [ 1.0469,  0.2227,  0.6719,  ..., -0.2246, -0.2812, -0.1963]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.5.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.8555,  0.8398, -0.7617,  ...,  0.1187,  0.2852,  0.5781],\n",
      "        [-0.4414, -0.1592,  0.7031,  ..., -0.0535, -0.3203,  0.2002],\n",
      "        [ 0.2178, -0.8789,  0.2676,  ...,  0.1768, -0.1177,  0.2676],\n",
      "        ...,\n",
      "        [-0.0084,  0.9922,  0.1650,  ..., -0.1680,  1.7969,  0.6172],\n",
      "        [ 1.1406,  0.3789, -1.1406,  ...,  0.8242, -0.0635,  0.0297],\n",
      "        [-1.2656, -0.7383, -1.3203,  ..., -0.0942, -0.4961,  0.5039]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.5.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 2.0703e-01,  1.0596e-01,  1.2988e-01,  2.3145e-01,  2.1777e-01,\n",
      "         1.5137e-01,  2.0898e-01,  2.4902e-01,  1.7676e-01,  2.1191e-01,\n",
      "        -1.4941e-01,  2.2559e-01,  1.1621e-01,  1.3965e-01,  2.0312e-01,\n",
      "         2.2363e-01,  1.9531e-01,  1.7969e-01,  1.7969e-01,  2.2656e-01,\n",
      "         2.0605e-01,  2.1484e-01,  2.1680e-01,  2.3340e-01,  2.0312e-01,\n",
      "         1.6602e-01,  2.0508e-01,  1.4453e-01,  2.1387e-01,  2.1387e-01,\n",
      "         2.2461e-01,  1.6211e-01,  2.2656e-01,  1.9824e-01,  2.1680e-01,\n",
      "         1.7188e-01,  2.2266e-01,  1.8848e-01,  2.0020e-01,  2.2461e-01,\n",
      "         2.1680e-01,  1.9629e-01,  2.2266e-01,  2.2266e-01,  1.4941e-01,\n",
      "         2.0312e-01,  2.2852e-01,  2.2754e-01,  2.3047e-01,  2.4023e-01,\n",
      "         2.0605e-01,  2.0996e-01,  2.0020e-01,  2.2363e-01,  2.8125e-01,\n",
      "         1.6797e-01,  1.8457e-01,  2.3145e-01,  2.2070e-01,  2.1582e-01,\n",
      "         1.9824e-01,  2.1973e-01,  2.1191e-01,  2.1582e-01,  2.1680e-01,\n",
      "         1.5723e-01,  2.0898e-01,  2.0312e-01,  1.2598e-01,  2.0801e-01,\n",
      "         2.2070e-01,  2.2656e-01,  2.1680e-01,  2.1387e-01,  2.0215e-01,\n",
      "         2.1094e-01,  1.3770e-01,  2.0020e-01,  2.0703e-01,  1.9531e-01,\n",
      "         2.0996e-01,  2.0215e-01,  2.2559e-01,  1.5820e-01,  1.9922e-01,\n",
      "         2.1484e-01,  1.1963e-01,  2.3535e-01,  2.1484e-01,  2.1680e-01,\n",
      "         1.8457e-01,  2.0703e-01,  2.0508e-01,  1.7480e-01,  2.2754e-01,\n",
      "         2.3047e-01,  2.1484e-01,  2.1191e-01,  2.2852e-01,  2.1582e-01,\n",
      "         1.8164e-01,  1.8359e-01,  1.1328e-01,  2.0996e-01,  1.9141e-01,\n",
      "         1.8164e-01,  2.0312e-01,  1.8848e-01,  2.0020e-01,  2.1289e-01,\n",
      "         2.2168e-01,  2.3047e-01,  2.0020e-01,  2.0801e-01,  1.9141e-01,\n",
      "         2.0996e-01,  1.9727e-01,  1.9824e-01,  4.3213e-02,  1.9531e-01,\n",
      "         2.1289e-01,  1.9238e-01,  1.6992e-01,  2.1387e-01,  2.2949e-01,\n",
      "         1.8945e-01,  2.1680e-01,  1.7188e-01,  1.9922e-01,  1.3281e-01,\n",
      "         1.3086e-01,  1.9043e-01,  1.9727e-01,  2.3340e-01,  1.1963e-01,\n",
      "         2.0996e-01,  2.0410e-01,  1.8945e-01,  1.9043e-01,  1.4258e-01,\n",
      "         1.6602e-01,  2.1191e-01,  2.1289e-01,  2.2266e-01,  1.9531e-01,\n",
      "         1.7969e-01,  1.9824e-01,  1.7480e-01,  1.9629e-01,  1.8945e-01,\n",
      "         2.2461e-01,  1.9629e-01,  2.0898e-01,  1.8945e-01,  2.2266e-01,\n",
      "         2.1973e-01,  2.0996e-01,  2.3047e-01, -5.5176e-02,  1.1377e-01,\n",
      "         2.0703e-01,  1.7480e-01,  1.8945e-01,  1.8555e-01,  1.8359e-01,\n",
      "         1.2891e-01,  2.1777e-01,  1.5918e-01,  1.2256e-01,  2.1191e-01,\n",
      "        -1.0791e-01,  2.1094e-01,  2.2852e-01,  2.2070e-01,  2.1582e-01,\n",
      "         2.3730e-01,  1.9727e-01,  2.1875e-01, -9.7656e-02,  1.6699e-01,\n",
      "         2.2266e-01,  2.0508e-01,  2.4023e-01,  2.3340e-01,  1.7969e-01,\n",
      "         1.8945e-01,  2.0703e-01,  3.2227e-02,  2.0605e-01,  1.3672e-01,\n",
      "         2.1777e-01,  2.1289e-01,  1.9141e-01,  2.3438e-01,  2.0215e-01,\n",
      "         1.9336e-01,  2.2070e-01,  2.2656e-01,  2.2266e-01,  1.9727e-01,\n",
      "         2.3242e-01,  2.0117e-01,  2.2461e-01,  1.9629e-01,  2.0020e-01,\n",
      "         1.6797e-01,  2.1289e-01,  2.0703e-01,  1.9922e-01,  2.5781e-01,\n",
      "         1.9629e-01,  1.5625e-01,  2.0703e-01,  1.9727e-01,  2.3145e-01,\n",
      "         2.2852e-01,  1.9531e-01,  1.8750e-01,  2.1875e-01,  2.3804e-02,\n",
      "         1.3965e-01,  1.9922e-01,  1.7871e-01,  1.9824e-01,  1.9141e-01,\n",
      "         2.2852e-01,  3.9062e-01,  2.3828e-01,  2.1875e-01,  2.2363e-01,\n",
      "         2.0312e-01,  2.3242e-01,  1.9043e-01,  2.3438e-01,  2.2266e-01,\n",
      "         1.8555e-01,  1.8652e-01,  1.9922e-01,  1.3086e-01,  2.1973e-01,\n",
      "         1.7676e-01,  2.2363e-01,  1.9629e-01,  1.9629e-01,  2.3633e-01,\n",
      "         2.1680e-01,  2.1094e-01,  2.0410e-01,  2.0117e-01,  2.2754e-01,\n",
      "         2.0312e-01,  2.0312e-01,  2.1973e-01,  2.0898e-01,  2.0703e-01,\n",
      "         1.8164e-01,  2.2949e-01, -1.3184e-01,  2.1777e-01,  1.9824e-01,\n",
      "         2.5391e-01,  1.3574e-01,  2.3340e-01,  2.2754e-01,  2.1387e-01,\n",
      "         2.0117e-01,  2.0898e-01,  2.0703e-01,  2.1875e-01,  2.1582e-01,\n",
      "         2.1484e-01,  1.3184e-01,  2.2168e-01,  2.0117e-01, -9.9121e-02,\n",
      "         2.3145e-01,  2.1484e-01,  2.1777e-01,  1.9629e-01,  1.7188e-01,\n",
      "         2.0996e-02,  1.3281e-01,  2.0508e-01,  1.9531e-01,  2.2656e-01,\n",
      "         2.2070e-01,  2.1875e-01,  2.3340e-01,  1.6016e-01,  1.4648e-01,\n",
      "         2.0312e-01,  1.9629e-01,  1.9727e-01,  2.1973e-01,  1.9531e-01,\n",
      "         2.0996e-01,  1.2109e-01,  2.1777e-01,  2.1582e-01,  2.3145e-01,\n",
      "         2.2559e-01,  1.9727e-01,  3.2959e-02,  2.1289e-01,  2.0801e-01,\n",
      "         1.9922e-01,  2.1680e-01,  2.1387e-01,  1.8750e-01,  1.9922e-01,\n",
      "         2.0703e-01,  2.1680e-01,  2.1289e-01,  2.1973e-01,  1.9824e-01,\n",
      "         2.0801e-01,  2.1191e-01,  2.2363e-01,  2.2168e-01,  2.0312e-01,\n",
      "         2.2070e-01,  2.1484e-01,  1.7480e-01,  2.1680e-01,  1.9434e-01,\n",
      "         1.9336e-01,  1.8848e-01,  2.0410e-01,  2.2363e-01,  2.2266e-01,\n",
      "         2.2852e-01,  1.7285e-01,  2.1582e-01,  1.8262e-01,  2.0312e-01,\n",
      "         2.0605e-01,  2.1777e-01,  2.2168e-01,  1.9238e-01,  1.7578e-01,\n",
      "         1.9238e-01,  2.0020e-01,  2.1777e-01,  2.1875e-01,  2.2168e-01,\n",
      "         1.9043e-01,  2.2461e-01,  1.5723e-01,  1.2451e-01,  2.0117e-01,\n",
      "         2.0605e-01,  1.8750e-01,  1.6602e-01,  2.0605e-01,  2.0703e-01,\n",
      "         2.2754e-01,  2.3242e-01,  5.2979e-02,  2.2266e-01,  2.3828e-01,\n",
      "         2.2754e-01,  1.6797e-01,  1.3477e-01,  1.8066e-01,  2.1289e-01,\n",
      "         2.1094e-01,  2.1680e-01,  1.9434e-01,  2.1094e-01,  2.0508e-01,\n",
      "         2.4512e-01,  2.2266e-01,  2.0117e-01,  2.0215e-01,  2.3145e-01,\n",
      "         2.0605e-01,  2.1680e-01,  2.1973e-01,  2.1289e-01,  1.5430e-01,\n",
      "         2.0215e-01,  1.9629e-01, -1.3574e-01,  2.0312e-01,  2.1582e-01,\n",
      "         2.3242e-01,  2.1289e-01,  2.1973e-01,  1.8164e-01,  2.2559e-01,\n",
      "         2.0020e-01,  2.0020e-01,  1.9141e-01,  2.1680e-01,  2.0312e-01,\n",
      "         2.1973e-01,  2.2949e-01,  1.8555e-01,  2.1484e-01,  2.0410e-01,\n",
      "         2.1484e-01,  2.2070e-01,  2.1191e-01,  2.2559e-01,  1.8457e-01,\n",
      "         2.1484e-01,  1.4453e-01,  2.0312e-01,  1.7773e-01,  2.2461e-01,\n",
      "         1.9434e-01,  2.0215e-01,  2.1094e-01,  2.0508e-01,  2.2949e-01,\n",
      "         2.2070e-01,  2.0312e-01,  2.0996e-01,  2.4316e-01,  2.0703e-01,\n",
      "        -1.9409e-02,  2.0605e-01,  2.2266e-01,  7.0801e-02,  2.0703e-01,\n",
      "         2.1094e-01,  1.8555e-01,  2.2949e-01,  1.8848e-01,  1.4355e-01,\n",
      "         2.1094e-01,  1.9922e-01,  2.2461e-01,  1.7578e-01,  1.8945e-01,\n",
      "         1.8750e-01,  2.1484e-01,  2.0410e-01,  2.1191e-01,  2.0215e-01,\n",
      "         2.1777e-01,  1.8750e-01,  1.7383e-01,  2.4316e-01,  2.2363e-01,\n",
      "         2.0508e-01,  2.0605e-01,  2.1875e-01, -2.8564e-02,  2.0215e-01,\n",
      "         2.2266e-01,  2.1484e-01,  2.0801e-01,  2.1973e-01,  2.0020e-01,\n",
      "         2.0020e-01,  1.9629e-01,  2.7734e-01,  2.0801e-01,  2.0020e-01,\n",
      "         2.2266e-01,  2.2754e-01,  2.1582e-01,  2.0312e-01,  2.0312e-01,\n",
      "         2.1875e-01,  1.5918e-01,  2.0605e-01,  1.9336e-01,  2.2656e-01,\n",
      "         2.1680e-01,  2.1582e-01,  1.0645e-01,  2.1777e-01,  2.1582e-01,\n",
      "         2.0605e-01,  2.1973e-01,  2.1484e-01,  2.1191e-01,  2.1973e-01,\n",
      "         1.5820e-01,  2.2070e-01,  2.1094e-01,  1.9336e-01,  7.0801e-02,\n",
      "         2.2363e-01,  2.0801e-01,  1.8978e-04,  2.0312e-01,  2.2754e-01,\n",
      "         2.2461e-01,  2.0410e-01,  2.2949e-01,  2.1973e-01,  2.1973e-01,\n",
      "         7.8613e-02,  2.0215e-01,  2.0215e-01,  2.3242e-01,  2.0605e-01,\n",
      "         2.0996e-01,  1.7480e-01,  2.0703e-01,  2.0312e-01,  1.9531e-01,\n",
      "         2.0898e-01,  2.1875e-01,  1.8457e-01,  2.0312e-01,  2.2363e-01,\n",
      "         2.0020e-01,  2.2949e-01,  2.0703e-01,  1.9043e-01,  1.7188e-01,\n",
      "         2.2949e-01,  1.8262e-01,  1.9238e-01,  1.5820e-01,  2.3633e-01,\n",
      "         2.1777e-01,  2.1387e-01,  2.0117e-01,  2.1387e-01,  1.9727e-01,\n",
      "         1.9434e-01,  2.1973e-01,  1.4746e-01,  2.0117e-01,  1.9238e-01,\n",
      "         1.6113e-01,  2.1973e-01,  2.0801e-01,  2.1484e-01,  2.0312e-01,\n",
      "         2.1484e-01,  2.3438e-01,  2.1777e-01,  2.1875e-01,  1.9141e-01,\n",
      "         2.1094e-01,  2.0898e-01,  2.0508e-01,  2.0410e-01,  1.9238e-01,\n",
      "         2.0996e-01,  2.0996e-01,  2.1680e-01,  2.0605e-01,  2.1191e-01,\n",
      "         1.6309e-01,  1.5918e-01,  2.0605e-01,  1.4746e-01,  2.1777e-01,\n",
      "         2.0801e-01,  1.8457e-01,  2.2461e-01,  1.7285e-01,  2.1875e-01,\n",
      "         1.9531e-01,  1.9434e-01,  1.6797e-01,  2.9688e-01,  2.5000e-01,\n",
      "         1.9922e-01,  2.1680e-01,  2.0801e-01,  2.0215e-01,  2.0898e-01,\n",
      "         2.0801e-01,  2.0898e-01,  2.0117e-01,  1.8164e-01,  1.3477e-01,\n",
      "         1.9629e-01,  2.2461e-01,  2.2461e-01,  2.0215e-01,  2.6611e-02,\n",
      "         1.5820e-01,  2.2168e-01,  2.2266e-01,  2.1289e-01,  2.1191e-01,\n",
      "         2.0410e-01,  1.9824e-01,  2.1387e-01,  2.2363e-01,  2.0117e-01,\n",
      "         2.2559e-01,  2.2949e-01,  2.1582e-01,  1.9629e-01,  1.9531e-01,\n",
      "         2.2852e-01,  1.3965e-01,  2.0801e-01,  2.1582e-01,  2.0117e-01,\n",
      "         1.7383e-01,  2.1973e-01,  2.2070e-01,  1.9141e-01,  2.0020e-01,\n",
      "         2.0508e-01,  2.0410e-01,  1.2402e-01,  2.4414e-01,  2.1289e-01,\n",
      "         2.0703e-01,  2.0703e-01,  2.0801e-01,  2.1582e-01,  2.0801e-01,\n",
      "         2.3340e-01,  1.9824e-01,  2.0801e-01,  2.0996e-01,  2.0898e-01,\n",
      "         2.0801e-01,  1.9824e-01,  2.0801e-01,  1.3672e-01,  1.9336e-01,\n",
      "         2.1973e-01,  2.0605e-01,  2.1191e-01,  2.0117e-01,  2.1094e-01,\n",
      "         1.7578e-01,  2.1094e-01,  1.7480e-01,  1.9531e-01,  1.9336e-01,\n",
      "         2.0312e-01,  1.6797e-01,  2.0996e-01,  2.1680e-01,  2.1094e-01,\n",
      "         1.5332e-01,  1.9434e-01,  2.1289e-01,  1.9531e-01,  1.4648e-01,\n",
      "         1.9434e-01,  1.1963e-01,  2.1289e-01,  2.0117e-01,  2.4414e-01,\n",
      "         1.3770e-01,  2.3145e-01,  1.9727e-01,  2.2363e-01,  2.2852e-01,\n",
      "         1.2061e-01,  1.9922e-01,  1.6406e-01,  2.2168e-01,  2.0117e-01,\n",
      "         2.2168e-01,  1.9434e-01,  2.0312e-01,  2.2168e-01,  2.0508e-01,\n",
      "         1.9434e-01,  2.2266e-01,  2.2070e-01,  1.9824e-01,  2.2266e-01,\n",
      "         1.8848e-01,  2.0801e-01,  2.0020e-01,  1.6602e-01,  2.1777e-01,\n",
      "         2.2266e-01,  1.8457e-01,  2.1387e-01,  2.1777e-01,  2.1973e-01,\n",
      "         2.1875e-01,  2.1875e-01,  1.7676e-01,  1.8164e-01,  1.9336e-01,\n",
      "         2.0703e-01,  2.0898e-01,  1.9141e-01,  2.2754e-01,  2.0020e-01,\n",
      "         2.3242e-01,  2.0605e-01,  1.5527e-01,  1.9922e-01,  2.2168e-01,\n",
      "         2.0312e-01,  2.2461e-01,  2.0410e-01,  2.1680e-01,  2.0508e-01,\n",
      "         1.9727e-01,  2.0996e-01,  1.9922e-01,  1.0498e-01,  1.8945e-01,\n",
      "         2.1777e-01,  2.1387e-01,  1.5527e-01,  1.3574e-01,  2.1387e-01,\n",
      "         1.9043e-01,  1.9434e-01,  1.8848e-01,  1.9336e-01,  9.7168e-02,\n",
      "         2.2363e-01,  1.9629e-01,  2.0508e-01,  2.0801e-01,  1.8652e-01,\n",
      "         2.2168e-01,  1.9727e-01,  1.5625e-01,  2.2070e-01,  2.2168e-01,\n",
      "         1.0986e-01,  1.9238e-01,  1.9336e-01,  1.9629e-01,  2.2363e-01,\n",
      "         2.2266e-01,  1.9824e-01,  2.1484e-01,  7.2754e-02,  2.0703e-01,\n",
      "         2.0215e-01,  2.2266e-01,  1.9824e-01,  2.1973e-01,  1.8555e-01,\n",
      "         1.9824e-01,  1.1084e-01,  2.2461e-01,  1.7285e-01,  2.3633e-01,\n",
      "         1.9531e-01,  1.5527e-01,  1.9629e-01,  2.1289e-01,  1.3477e-01,\n",
      "         1.8750e-01,  1.8262e-01,  1.8164e-01,  2.0312e-01,  2.2168e-01,\n",
      "         2.1387e-01,  2.0117e-01,  2.0898e-01,  1.1182e-01,  2.0410e-01,\n",
      "         1.7480e-01,  1.7188e-01,  1.9824e-01,  2.0508e-01,  2.0117e-01,\n",
      "         2.0215e-01,  2.5195e-01,  1.9434e-01], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.decoder.block.5.layer.1.EncDecAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0056, -0.0415,  0.0248,  ..., -0.0153, -0.0043,  0.0101],\n",
      "        [ 0.0238,  0.0337,  0.0251,  ...,  0.0043,  0.0388, -0.0215],\n",
      "        [-0.0664,  0.0136,  0.0327,  ...,  0.0125,  0.0601, -0.0317],\n",
      "        ...,\n",
      "        [ 0.0415, -0.0349,  0.0378,  ...,  0.0579, -0.0152, -0.0137],\n",
      "        [-0.0025, -0.0466, -0.0449,  ...,  0.0815, -0.0491,  0.0718],\n",
      "        [ 0.0825,  0.0981, -0.0698,  ...,  0.0294,  0.0092,  0.0239]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.5.layer.1.EncDecAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.4609, -0.3477, -0.0500,  ...,  0.1729,  0.1206,  0.1475],\n",
      "        [-0.3730,  0.0801,  0.0250,  ..., -0.0439, -0.0894, -0.2773],\n",
      "        [ 0.2285,  0.0232, -0.2021,  ..., -1.0234,  0.1338, -0.1445],\n",
      "        ...,\n",
      "        [-0.3945, -0.4414, -0.5938,  ...,  0.2617, -0.6680,  0.4785],\n",
      "        [ 0.1406, -0.7617, -0.1396,  ..., -0.0483,  0.1934,  0.0518],\n",
      "        [ 0.5547,  0.1963,  1.1172,  ..., -0.3203,  0.6484, -0.1348]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.5.layer.1.EncDecAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.9375,  0.4805, -0.4824,  ..., -0.1699,  0.0067, -0.2812],\n",
      "        [ 1.1875, -0.2969, -0.2910,  ...,  0.6523, -0.6289,  0.1260],\n",
      "        [ 0.5625, -0.2793, -0.2363,  ..., -0.6680,  0.1768, -0.4199],\n",
      "        ...,\n",
      "        [ 0.0967, -0.1895,  0.5078,  ..., -0.3301,  0.1387, -0.1914],\n",
      "        [ 0.2949,  0.1816, -0.6914,  ...,  0.0327, -0.0074, -0.5781],\n",
      "        [-0.3398, -0.1992, -0.5547,  ...,  0.2168,  0.2109,  0.0211]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.5.layer.1.EncDecAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1826, -0.0432,  0.3340,  ..., -0.4805, -0.1152,  0.1465],\n",
      "        [-0.2002, -0.3262, -0.1245,  ..., -0.3711,  0.3672,  0.3633],\n",
      "        [-0.0723, -0.2988,  0.3652,  ...,  0.4512, -0.5898, -0.3379],\n",
      "        ...,\n",
      "        [ 0.1865, -1.0156,  0.4336,  ...,  0.1367, -0.3965,  0.0371],\n",
      "        [ 0.0884,  0.6914,  0.4023,  ..., -0.1172,  0.7188, -1.0938],\n",
      "        [-0.0830,  0.2080, -0.1553,  ..., -0.1582,  0.5977,  0.1748]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.5.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 5.5420e-02,  3.8086e-02,  3.9062e-02,  7.1289e-02,  5.4932e-02,\n",
      "        -4.8584e-02,  7.0312e-02,  6.9336e-02,  5.4199e-02,  6.4941e-02,\n",
      "         4.9316e-02,  6.9824e-02,  4.5654e-02,  4.5654e-02,  5.8594e-02,\n",
      "         6.2988e-02,  5.8105e-02,  5.3711e-02,  5.7861e-02,  7.7148e-02,\n",
      "         5.8594e-02,  6.5918e-02,  6.7383e-02,  7.7637e-02,  6.3477e-02,\n",
      "         5.6396e-02,  5.8838e-02,  3.8086e-02,  6.4941e-02,  6.2988e-02,\n",
      "         7.4707e-02,  4.6875e-02,  6.9336e-02,  6.7871e-02,  6.4453e-02,\n",
      "         4.7852e-02,  7.0801e-02,  7.1777e-02,  5.4688e-02,  6.8848e-02,\n",
      "         6.2500e-02,  5.6152e-02,  6.2256e-02,  5.7861e-02,  4.9805e-02,\n",
      "         6.1768e-02,  8.5938e-02,  7.0801e-02,  7.0312e-02,  6.5918e-02,\n",
      "         6.1279e-02,  5.8105e-02,  5.9570e-02,  6.5918e-02,  8.3496e-02,\n",
      "         5.0049e-02,  5.2979e-02,  6.7383e-02,  7.3242e-02,  6.0791e-02,\n",
      "         5.4688e-02,  6.6406e-02,  6.9824e-02,  5.7129e-02,  6.3965e-02,\n",
      "         1.0889e-01,  6.2256e-02,  5.9326e-02,  3.6621e-02,  6.2256e-02,\n",
      "         6.2988e-02,  7.0801e-02,  6.5430e-02,  6.2500e-02,  6.0791e-02,\n",
      "         6.1768e-02,  4.3945e-02,  6.2256e-02,  6.4453e-02,  5.8594e-02,\n",
      "         7.2266e-02,  6.0059e-02,  6.3477e-02,  5.8594e-02,  6.4453e-02,\n",
      "         5.9570e-02,  3.8818e-02,  6.4453e-02,  6.0547e-02,  6.4453e-02,\n",
      "         5.5908e-02,  6.0547e-02,  6.0059e-02,  5.2734e-02,  6.6406e-02,\n",
      "         6.9824e-02,  6.3965e-02,  7.0801e-02,  7.0312e-02,  5.8594e-02,\n",
      "         5.1514e-02,  5.9570e-02,  3.2471e-02,  5.9570e-02,  5.7617e-02,\n",
      "         5.8350e-02,  6.7871e-02,  5.3467e-02,  6.8848e-02,  6.2500e-02,\n",
      "         6.5918e-02,  7.0312e-02,  5.2734e-02,  6.2500e-02,  5.6885e-02,\n",
      "         7.3242e-02,  5.5664e-02,  5.6885e-02, -2.2411e-04,  5.8105e-02,\n",
      "         5.8594e-02,  6.4453e-02,  5.0537e-02,  5.9814e-02,  6.4941e-02,\n",
      "         5.5420e-02,  6.3477e-02,  5.2246e-02,  5.6885e-02,  4.7852e-02,\n",
      "         3.6865e-02,  5.9326e-02,  5.8838e-02,  6.5918e-02,  3.9307e-02,\n",
      "         6.2988e-02,  6.3477e-02,  5.4199e-02,  6.2256e-02,  4.5654e-02,\n",
      "         5.0537e-02,  6.0547e-02,  5.7617e-02,  6.3965e-02,  5.8594e-02,\n",
      "         5.4443e-02,  6.4453e-02,  5.2734e-02,  6.2012e-02,  5.5420e-02,\n",
      "         7.0801e-02,  5.4443e-02,  6.3965e-02,  6.1768e-02,  6.9336e-02,\n",
      "         6.8359e-02,  6.2012e-02,  6.8848e-02,  1.1133e-01,  4.5898e-02,\n",
      "         6.4453e-02,  4.7852e-02,  5.6152e-02,  5.2734e-02,  5.4688e-02,\n",
      "         3.3936e-02,  7.0312e-02,  5.4688e-02,  3.4180e-02,  7.1289e-02,\n",
      "        -3.5156e-02,  5.9570e-02,  7.4219e-02,  6.2012e-02,  6.1035e-02,\n",
      "         6.3965e-02,  5.8350e-02,  6.4941e-02,  3.5889e-02,  5.8594e-02,\n",
      "         6.7383e-02,  6.9336e-02,  7.4219e-02,  6.9336e-02,  5.2002e-02,\n",
      "         5.9570e-02,  5.9570e-02,  6.9824e-02,  5.9814e-02,  4.5898e-02,\n",
      "         6.7871e-02,  6.2500e-02,  6.6406e-02,  6.4453e-02,  5.5664e-02,\n",
      "         6.2012e-02,  5.7861e-02,  7.1777e-02,  7.0312e-02,  5.7617e-02,\n",
      "         7.1777e-02,  6.2988e-02,  6.2256e-02,  5.7617e-02,  6.0059e-02,\n",
      "         5.2002e-02,  6.6895e-02,  7.2754e-02,  6.1035e-02,  7.6660e-02,\n",
      "         7.7637e-02,  4.6631e-02,  5.8105e-02,  5.8594e-02,  7.3242e-02,\n",
      "         6.1279e-02,  6.2012e-02,  5.5664e-02,  6.8359e-02,  7.8964e-04,\n",
      "         4.5166e-02,  6.8848e-02,  5.8105e-02,  6.6895e-02,  5.2002e-02,\n",
      "         7.1289e-02,  1.2793e-01,  7.6660e-02,  6.1523e-02,  6.2988e-02,\n",
      "         5.4932e-02,  6.2500e-02,  5.3223e-02,  6.6895e-02,  5.8594e-02,\n",
      "         5.2002e-02,  5.2246e-02,  5.7617e-02,  4.5654e-02,  6.5918e-02,\n",
      "         5.6396e-02,  6.2012e-02,  6.3477e-02,  5.8105e-02,  7.4707e-02,\n",
      "         5.8594e-02,  6.9336e-02,  5.7861e-02,  6.3965e-02,  6.2988e-02,\n",
      "         5.5908e-02,  5.9326e-02,  6.5918e-02,  5.9326e-02,  6.0059e-02,\n",
      "         5.2002e-02,  6.6406e-02,  4.1992e-02,  6.5430e-02,  6.4453e-02,\n",
      "         8.3008e-02, -4.9561e-02,  6.9336e-02,  6.2500e-02,  6.0791e-02,\n",
      "         5.8838e-02,  6.4453e-02,  6.9824e-02,  6.0059e-02,  6.0791e-02,\n",
      "         6.0303e-02,  7.8125e-02,  6.5918e-02,  6.1035e-02,  3.4424e-02,\n",
      "         6.2256e-02,  6.3965e-02,  5.7373e-02,  6.4453e-02,  5.1514e-02,\n",
      "        -2.3270e-04,  3.8574e-02,  6.6895e-02,  6.0791e-02,  6.9824e-02,\n",
      "         6.1768e-02,  6.6895e-02,  6.2988e-02,  4.5898e-02,  5.0049e-02,\n",
      "         5.9326e-02,  5.4932e-02,  5.6152e-02,  6.5918e-02,  5.9082e-02,\n",
      "         6.3477e-02,  5.0537e-02,  6.7871e-02,  6.1279e-02,  7.5195e-02,\n",
      "         7.2754e-02,  5.7129e-02, -1.3672e-02,  5.5420e-02,  6.6406e-02,\n",
      "         6.2988e-02,  5.6152e-02,  6.2988e-02,  6.1523e-02,  6.5430e-02,\n",
      "         6.7383e-02,  6.0547e-02,  5.7129e-02,  7.1777e-02,  1.1279e-01,\n",
      "         6.1523e-02,  8.0566e-02,  6.4453e-02,  7.8125e-02,  6.9824e-02,\n",
      "         6.7383e-02,  5.9326e-02,  5.3223e-02,  6.3965e-02,  6.3965e-02,\n",
      "         5.4932e-02,  5.3955e-02,  6.6406e-02,  7.1777e-02,  6.8848e-02,\n",
      "         6.5918e-02,  5.1025e-02,  6.7871e-02,  6.0059e-02,  5.8594e-02,\n",
      "         6.0547e-02,  6.3477e-02,  7.3730e-02,  6.0547e-02,  4.8340e-02,\n",
      "         5.7617e-02,  6.1035e-02,  6.6406e-02,  6.2500e-02,  6.4453e-02,\n",
      "         5.1758e-02,  6.4941e-02,  4.7852e-02,  4.0527e-02,  5.9814e-02,\n",
      "         6.4941e-02,  5.6885e-02,  5.0049e-02,  5.8350e-02,  5.7861e-02,\n",
      "         6.9824e-02,  6.8848e-02, -7.9155e-05,  6.7871e-02,  8.0566e-02,\n",
      "         7.2266e-02, -5.2734e-02,  5.7373e-02,  5.1025e-02,  6.2988e-02,\n",
      "         7.1289e-02,  6.2256e-02,  5.4688e-02,  5.7861e-02,  5.6641e-02,\n",
      "         6.1279e-02,  6.3477e-02,  6.2256e-02,  5.7129e-02,  8.3008e-02,\n",
      "         5.7373e-02,  6.5430e-02,  7.0801e-02,  7.0312e-02,  5.0293e-02,\n",
      "         5.2002e-02,  5.9814e-02,  4.6631e-02,  6.3477e-02,  6.2988e-02,\n",
      "         6.1035e-02,  7.0801e-02,  6.3965e-02,  5.4688e-02,  6.3965e-02,\n",
      "         6.2988e-02,  5.5908e-02,  5.5664e-02,  6.1768e-02,  5.9570e-02,\n",
      "         6.3477e-02,  6.5918e-02,  5.8594e-02,  6.3965e-02,  6.1035e-02,\n",
      "         6.0547e-02,  5.7129e-02,  6.3477e-02,  6.9336e-02,  5.6885e-02,\n",
      "         5.6885e-02,  5.4688e-02,  6.2012e-02,  5.1514e-02,  6.0059e-02,\n",
      "         5.6396e-02,  6.2256e-02,  5.6641e-02,  6.8848e-02,  7.2754e-02,\n",
      "         6.8848e-02,  6.0547e-02,  6.4453e-02,  1.4844e-01,  5.9570e-02,\n",
      "         3.8528e-04,  6.5918e-02,  7.4707e-02,  1.8066e-02,  7.1777e-02,\n",
      "         6.0547e-02,  5.3955e-02,  6.2256e-02,  5.2490e-02,  4.6631e-02,\n",
      "         6.9824e-02,  6.1035e-02,  7.9590e-02,  5.0537e-02,  5.7617e-02,\n",
      "         5.4688e-02,  6.7383e-02,  6.0059e-02,  6.1768e-02,  5.7373e-02,\n",
      "         6.2012e-02,  5.2490e-02,  5.5176e-02,  6.5430e-02,  6.9824e-02,\n",
      "         6.2988e-02,  6.1523e-02,  6.2256e-02,  1.5831e-04,  5.9326e-02,\n",
      "         6.8848e-02,  6.2988e-02,  6.0547e-02,  6.3965e-02,  5.9326e-02,\n",
      "         5.6152e-02,  1.0400e-01,  1.0352e-01,  5.6641e-02,  5.4688e-02,\n",
      "         5.9326e-02,  6.3965e-02,  6.4453e-02,  5.9570e-02,  6.5430e-02,\n",
      "         6.4453e-02,  4.4434e-02,  6.4941e-02,  5.8350e-02,  6.3477e-02,\n",
      "         6.6406e-02,  6.4453e-02,  2.8320e-02,  6.2012e-02,  6.2988e-02,\n",
      "         5.8350e-02,  6.6895e-02,  6.1035e-02,  6.2012e-02,  5.8838e-02,\n",
      "         5.1025e-02,  6.1279e-02,  7.0801e-02,  5.4932e-02, -1.2112e-04,\n",
      "         6.1279e-02,  6.7383e-02,  1.1914e-01,  6.1279e-02,  6.2500e-02,\n",
      "         6.2988e-02,  7.4219e-02,  6.5430e-02,  6.3965e-02,  6.4941e-02,\n",
      "         5.5420e-02,  5.2979e-02,  6.3477e-02,  7.3242e-02,  5.7373e-02,\n",
      "         5.7861e-02,  8.7402e-02,  6.2988e-02,  6.1035e-02,  5.8105e-02,\n",
      "         6.0059e-02,  6.5430e-02,  5.4199e-02,  5.9326e-02,  6.4453e-02,\n",
      "         6.0547e-02,  6.9824e-02,  5.7617e-02,  5.9082e-02,  5.7617e-02,\n",
      "         6.7383e-02,  5.8838e-02,  5.5664e-02,  4.0771e-02,  6.7383e-02,\n",
      "         5.9570e-02,  5.9326e-02,  6.8848e-02,  5.9326e-02,  5.7373e-02,\n",
      "         5.8594e-02,  6.3965e-02,  5.0781e-02,  5.7617e-02,  5.6885e-02,\n",
      "         4.3945e-02,  6.2988e-02,  5.9082e-02,  6.5430e-02,  6.7383e-02,\n",
      "         6.5918e-02,  6.6406e-02,  5.9082e-02,  6.2500e-02,  6.4453e-02,\n",
      "         6.0059e-02,  6.1035e-02,  5.7373e-02,  5.7373e-02,  5.5908e-02,\n",
      "         6.2988e-02,  6.2988e-02,  6.1279e-02,  6.1035e-02,  5.8350e-02,\n",
      "         5.1270e-02,  5.9082e-02,  5.8838e-02,  4.8584e-02,  6.3965e-02,\n",
      "         6.4941e-02,  5.0537e-02,  5.9570e-02,  4.8096e-02,  6.4941e-02,\n",
      "         6.1035e-02,  5.7129e-02,  3.4668e-02,  9.2285e-02,  6.6895e-02,\n",
      "         6.1279e-02,  6.7871e-02,  5.4932e-02,  5.7373e-02,  6.2500e-02,\n",
      "         6.8359e-02,  6.2012e-02,  6.0059e-02,  5.8105e-02,  4.2236e-02,\n",
      "         5.5420e-02,  7.1777e-02,  6.3965e-02,  6.1035e-02,  3.3760e-04,\n",
      "         4.7119e-02,  6.8848e-02,  6.0303e-02,  5.6641e-02,  6.3965e-02,\n",
      "         6.9824e-02,  5.5176e-02,  5.9326e-02,  6.4941e-02,  6.3477e-02,\n",
      "         6.3965e-02,  6.7383e-02,  6.3477e-02,  6.0547e-02,  5.8594e-02,\n",
      "         6.3477e-02,  4.3213e-02,  6.4941e-02,  5.7373e-02,  5.6152e-02,\n",
      "         4.8828e-02,  5.6641e-02,  6.7383e-02,  5.1025e-02,  5.7861e-02,\n",
      "         5.6396e-02,  6.7871e-02, -4.7607e-02,  6.6406e-02,  5.8838e-02,\n",
      "         6.5918e-02,  6.0059e-02,  6.7871e-02,  6.8848e-02,  6.0547e-02,\n",
      "         6.6895e-02,  5.7129e-02,  1.3965e-01,  6.7871e-02,  6.8848e-02,\n",
      "         6.2012e-02,  5.7373e-02,  5.6885e-02,  4.5654e-02,  6.8848e-02,\n",
      "         5.3955e-02,  6.3965e-02,  6.0547e-02,  6.2500e-02,  5.6885e-02,\n",
      "         6.1279e-02,  6.0059e-02,  5.1758e-02,  5.4688e-02,  6.0303e-02,\n",
      "         6.6406e-02,  5.5908e-02,  5.8105e-02,  6.3965e-02,  6.2500e-02,\n",
      "         4.6387e-02,  5.9326e-02,  6.9336e-02,  6.2500e-02,  3.7109e-02,\n",
      "         6.1523e-02,  4.3457e-02,  6.7871e-02,  6.1768e-02,  7.2266e-02,\n",
      "         5.0049e-02,  6.2500e-02,  5.7373e-02,  6.2256e-02,  6.5918e-02,\n",
      "         3.2715e-02,  5.2979e-02,  5.8838e-02,  6.6406e-02,  6.4453e-02,\n",
      "         6.6406e-02,  5.2002e-02, -5.1514e-02,  7.7637e-02,  6.4453e-02,\n",
      "         6.4453e-02,  6.4453e-02,  6.5430e-02,  6.2012e-02,  5.8594e-02,\n",
      "         5.8838e-02,  5.8105e-02,  5.5420e-02,  4.6387e-02,  6.9824e-02,\n",
      "         6.4941e-02,  6.2988e-02,  6.7383e-02,  5.4932e-02,  6.1035e-02,\n",
      "         6.3477e-02,  6.6406e-02,  4.8340e-02,  5.5908e-02,  5.7861e-02,\n",
      "         5.5908e-02,  6.3477e-02,  5.9570e-02,  6.2988e-02,  5.6152e-02,\n",
      "         6.3965e-02,  6.1035e-02,  4.5166e-02,  6.2500e-02,  6.3965e-02,\n",
      "         6.0547e-02,  6.8359e-02,  5.9326e-02,  6.2500e-02,  5.8105e-02,\n",
      "         5.7373e-02,  6.3477e-02,  5.5420e-02,  3.4180e-02,  6.3477e-02,\n",
      "         6.2500e-02,  6.1523e-02,  4.7119e-02,  5.8350e-02,  5.8350e-02,\n",
      "         5.6885e-02,  5.9082e-02,  6.6895e-02,  5.9326e-02,  3.1494e-02,\n",
      "         8.8379e-02,  5.5176e-02,  6.8359e-02,  6.9336e-02,  5.4688e-02,\n",
      "         6.9824e-02,  5.5176e-02,  4.8828e-02,  6.5918e-02,  6.5430e-02,\n",
      "         4.5654e-02,  5.6885e-02,  5.7373e-02,  5.5420e-02,  7.2754e-02,\n",
      "         7.0801e-02,  5.9082e-02,  6.5430e-02,  4.3701e-02,  7.0801e-02,\n",
      "         6.2500e-02,  6.9824e-02,  6.4453e-02,  6.5430e-02,  5.0781e-02,\n",
      "         6.1523e-02,  4.9805e-02,  6.1279e-02,  5.2246e-02,  6.8359e-02,\n",
      "         5.5176e-02,  5.0293e-02,  6.1279e-02,  5.8594e-02,  4.3945e-02,\n",
      "         5.4932e-02,  5.6396e-02,  5.4443e-02,  5.9082e-02,  6.6895e-02,\n",
      "         6.9824e-02,  6.1035e-02,  6.1523e-02,  4.5654e-02,  6.3965e-02,\n",
      "         5.3223e-02,  5.0293e-02,  5.2246e-02,  5.7129e-02,  5.9326e-02,\n",
      "         5.9082e-02,  6.9824e-02,  5.7129e-02], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.decoder.block.5.layer.2.DenseReluDense.wi.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.2949,  0.0381, -0.4609,  ..., -0.8398, -1.1875, -0.6914],\n",
      "        [ 0.4258,  0.8125, -0.5625,  ..., -0.8320, -0.3691, -0.0938],\n",
      "        [ 0.8672,  0.3398, -0.2988,  ..., -0.0095,  0.1924,  0.5664],\n",
      "        ...,\n",
      "        [ 0.2051,  0.0879, -0.4980,  ..., -0.2617,  0.3594,  0.1924],\n",
      "        [-0.1367,  0.5820, -0.2559,  ..., -1.1719,  0.0518,  0.7812],\n",
      "        [ 0.6719,  0.9375,  0.8125,  ..., -1.0547, -0.2773,  0.3945]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.5.layer.2.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[-1.9775e-02,  2.5195e-01,  1.0303e-01,  ...,  2.8125e-01,\n",
      "         -3.0078e-01,  2.4316e-01],\n",
      "        [-4.5312e-01, -4.6680e-01, -1.9824e-01,  ..., -1.0938e-01,\n",
      "          3.0664e-01,  5.1172e-01],\n",
      "        [-2.9883e-01,  5.0293e-02,  9.7656e-03,  ...,  2.1289e-01,\n",
      "          1.5625e-01,  2.4902e-01],\n",
      "        ...,\n",
      "        [ 2.6758e-01,  1.4062e-01, -3.1641e-01,  ..., -2.0142e-02,\n",
      "          2.5977e-01, -1.2793e-01],\n",
      "        [ 2.1973e-01,  2.3242e-01, -4.5508e-01,  ...,  2.6562e-01,\n",
      "         -4.4922e-02, -3.3789e-01],\n",
      "        [ 7.2266e-01,  3.7109e-01, -6.0059e-02,  ...,  3.1853e-04,\n",
      "          1.7578e-01, -1.6895e-01]], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.5.layer.2.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 2.5469,  1.9844,  1.9141,  2.3438,  2.6250,  2.6719,  2.1875,  2.8438,\n",
      "         2.0312,  2.4844,  3.7188,  2.7031,  2.3594,  3.1250,  2.4219,  2.6562,\n",
      "         2.4688,  2.0156,  2.5469,  2.6250,  2.5625,  2.7812,  2.4375,  2.5469,\n",
      "         2.6250,  2.2188,  2.5781,  2.6875,  2.6094,  2.7188,  2.7031,  2.1719,\n",
      "         2.7656,  2.4375,  2.7969,  2.2656,  2.5781,  1.7891,  2.3750,  2.7031,\n",
      "         2.3438,  2.4062,  2.4688,  2.6875,  1.9688,  2.4844,  2.4844,  2.6250,\n",
      "         2.5781,  2.7969,  2.3281,  2.4531,  2.5781,  2.5781,  3.1250,  2.6406,\n",
      "         2.5469,  2.5938,  2.5781,  2.7188,  2.4375,  2.6250,  2.5156,  2.5625,\n",
      "         2.6250,  1.3828,  2.5625,  2.6406,  2.4375,  2.5469,  2.4688,  2.5156,\n",
      "         2.4219,  2.4219,  2.7344,  2.4375,  2.7188,  2.6094,  2.5781,  2.4844,\n",
      "         2.5625,  2.4531,  2.5938,  2.1719,  2.4375,  2.5312,  2.3125,  2.6719,\n",
      "         2.6094,  2.6875,  2.2812,  2.4844,  2.5469,  2.7656,  2.7500,  2.6094,\n",
      "         2.6250,  2.5156,  2.6250,  2.4688,  2.9062,  2.1719,  2.8281,  2.6406,\n",
      "         2.4062,  2.4219,  2.5625,  2.4219,  2.3594,  2.5938,  2.5469,  2.6875,\n",
      "         2.5625,  2.3750,  2.4062,  2.3438,  2.5156,  2.9375,  0.0271,  2.4375,\n",
      "         2.6094,  2.2031,  2.2500,  2.4375,  2.5156,  2.3594,  2.5781,  2.5156,\n",
      "         2.6406,  3.0312,  2.7188,  2.3125,  2.6562,  2.5625,  1.7031,  2.4688,\n",
      "         2.4219,  2.3281,  2.8750,  2.9062,  2.4688,  2.4219,  2.9688,  2.5781,\n",
      "         2.4062,  2.6094,  2.2969,  2.1250,  2.5000,  2.7812,  2.4062,  2.3281,\n",
      "         2.3281,  2.1875,  2.5938,  2.5938,  2.5469,  2.6719,  2.0000,  2.7656,\n",
      "         2.4688,  2.3281,  2.3750,  2.5156,  2.1406,  2.0781,  2.6406,  2.2031,\n",
      "         2.0781,  2.4531,  3.0625,  2.3438,  2.4688,  2.4219,  2.5312,  2.4531,\n",
      "         2.4688,  2.3438,  2.6406,  2.1719,  2.6875,  2.5938,  2.6875,  2.6719,\n",
      "         2.7656,  2.4375,  2.5000,  1.4375,  2.5312,  2.5312,  2.4062,  2.4219,\n",
      "         2.3438,  2.8750,  2.3750,  2.2812,  2.5156,  2.4688,  2.6406,  2.3438,\n",
      "         2.6562,  2.5938,  2.6250,  2.2812,  2.5000,  2.5156,  2.5000,  2.4531,\n",
      "         2.4062,  2.6562,  2.1406,  2.9688,  2.4219,  2.5000,  2.6406,  2.6719,\n",
      "         2.7188,  2.2188,  2.6875,  0.8086,  2.7656,  2.2500,  2.3594,  2.5000,\n",
      "         2.4844,  2.5625,  1.2656,  2.4531,  2.5469,  2.5625,  2.3438,  2.6875,\n",
      "         2.3750,  2.7344,  2.4531,  2.3594,  2.7500,  2.6406,  3.0625,  2.8281,\n",
      "         3.8594,  2.5938,  2.4531,  2.4219,  2.6094,  2.4219,  2.2969,  2.7188,\n",
      "         2.3594,  2.5156,  2.4062,  2.6250,  2.6406,  2.5469,  2.4375,  2.1094,\n",
      "         2.5000,  3.0312,  2.4219,  2.4062,  2.6406,  3.2188,  2.5938,  2.8594,\n",
      "         2.4219,  2.5625,  2.3750,  2.3125,  2.5938,  2.6875,  2.4688,  1.9844,\n",
      "         2.6250,  2.2969,  2.8281,  2.6406,  2.6875,  2.7969,  2.5781,  2.5781,\n",
      "         0.7070,  2.2656,  2.5156,  2.3594,  2.5312,  2.4688,  2.5625,  2.7969,\n",
      "         2.2656,  3.0469,  2.4219,  2.4375,  2.3906,  2.6406,  2.4375,  2.5625,\n",
      "         1.9766,  2.5625,  2.6250,  2.8594,  2.5312,  2.5156,  0.5898,  2.4688,\n",
      "         2.6719,  2.5781,  2.5938,  2.6094,  2.7031,  2.3125,  2.1875,  2.5000,\n",
      "         2.4844,  2.3594,  1.7734,  2.7188,  2.2812,  2.7031,  2.5000,  2.4219,\n",
      "         2.5469,  2.5781,  2.7031,  2.4062,  2.0938,  2.2969,  2.5469,  2.4219,\n",
      "         2.4844,  2.4688,  2.5312,  2.4062,  2.4219,  2.3750,  2.4844,  2.7969,\n",
      "         2.5156,  2.4062,  2.2812,  3.3125,  2.6406,  2.4219,  2.3281,  2.5625,\n",
      "         2.4844,  2.2344,  2.5312,  3.1719,  1.8750,  2.7812,  2.2656,  2.2344,\n",
      "         2.1719,  2.3125,  2.5938,  2.5469,  2.7969,  0.0125,  2.6562,  2.7500,\n",
      "         2.5625,  2.1562,  2.9062,  2.4219,  2.4531,  2.4062,  2.6719,  2.4219,\n",
      "         2.5000,  2.7812,  2.6406,  2.3594,  2.3125,  2.4219,  2.6250,  2.6875,\n",
      "         2.5156,  2.2344,  2.5469,  2.2500,  2.4219,  2.2812,  2.7344,  2.4531,\n",
      "         2.6875,  2.5938,  2.4844,  2.6719,  2.3906,  2.5469,  2.4219,  2.3750,\n",
      "         2.4375,  2.6875,  2.5156,  2.4688,  2.6250,  2.5000,  2.4062,  2.9375,\n",
      "         2.4219,  2.6094,  2.6250,  2.5781,  2.4375,  2.4688,  2.2656,  2.4531,\n",
      "         2.4219,  2.5000,  2.4062,  2.3125,  2.5156,  2.5312,  2.5625,  2.4375,\n",
      "         2.3906,  2.4844,  1.4531,  2.3594,  0.8398,  2.5625,  2.5781,  1.8125,\n",
      "         2.6875,  2.4375,  2.7188,  2.5938,  2.6094,  3.3750,  2.4062,  2.4219,\n",
      "         2.6875,  2.3438,  2.3125,  2.3594,  2.5781,  2.3594,  2.5312,  2.4062,\n",
      "         2.3906,  2.4219,  2.0312,  2.5781,  2.5000,  2.3906,  2.4062,  2.5469,\n",
      "         0.9531,  2.3438,  2.5000,  2.5625,  2.5000,  2.5938,  2.3750,  2.6719,\n",
      "         2.0625,  1.3672,  2.5312,  2.4062,  2.7031,  2.4531,  2.5938,  2.4219,\n",
      "         2.4844,  2.7188,  2.5781,  2.4844,  2.4062,  2.5625,  2.5625,  2.4219,\n",
      "         1.4375,  2.7031,  2.4844,  2.4375,  2.6094,  2.4375,  2.5469,  2.5781,\n",
      "         3.1719,  2.7500,  2.5312,  2.2812,  0.5234,  2.6250,  2.4062, -0.5547,\n",
      "         2.5000,  2.6406,  2.5781,  2.3438,  2.5156,  2.4688,  2.6094,  3.1719,\n",
      "         2.4688,  2.5312,  2.6875,  2.6562,  2.4844,  2.0781,  2.4688,  2.5469,\n",
      "         2.4531,  2.5156,  2.4531,  2.6406,  2.3750,  2.6562,  2.3281,  2.6250,\n",
      "         2.6094,  2.4219,  2.0781,  2.5781,  2.4688,  2.2812,  2.9688,  2.6406,\n",
      "         2.5625,  2.5781,  2.4844,  2.4688,  2.5469,  2.2969,  2.8906,  2.0625,\n",
      "         2.5000,  2.3438,  2.1250,  2.3594,  2.4375,  2.4531,  2.2969,  2.5625,\n",
      "         2.7188,  2.4219,  2.5625,  2.5938,  2.3125,  2.6094,  2.4844,  2.4844,\n",
      "         2.5000,  2.5781,  2.4375,  2.5000,  2.3750,  2.4531,  2.3125,  2.5312,\n",
      "         2.4375,  2.1875,  2.4531,  2.4688,  2.3750,  2.7188,  2.2344,  2.5000,\n",
      "         2.4062,  2.4219,  1.1250,  3.0469,  2.6406,  2.4062,  2.4688,  2.4062,\n",
      "         2.6094,  2.3594,  2.4531,  2.5156,  2.5000,  2.8438,  3.0938,  2.2969,\n",
      "         2.5781,  2.4844,  2.4688, -0.0045,  2.1094,  2.6250,  2.7031,  2.5312,\n",
      "         2.5625,  2.4531,  2.5938,  2.5469,  2.6250,  2.4844,  2.4688,  2.6875,\n",
      "         2.5312,  2.3750,  2.2812,  2.6250,  2.2031,  2.5156,  3.3438,  2.5938,\n",
      "         2.1875,  2.4844,  2.4531,  2.4375,  2.5312,  2.6250,  2.2656,  1.8203,\n",
      "         2.7344,  2.5312,  2.4688,  2.6094,  2.4844,  2.4062,  2.5938,  2.6875,\n",
      "         2.5469,  1.2422,  2.2812,  2.6094,  2.4062,  2.4219,  2.4844,  3.2031,\n",
      "         2.6094,  2.6250,  2.7656,  2.4531,  2.5156,  2.4062,  2.1094,  2.5156,\n",
      "         2.2344,  2.5625,  2.4219,  2.4219,  3.0312,  2.6094,  2.4688,  2.5000,\n",
      "         2.3281,  2.1875,  2.5312,  2.4844,  2.0781,  2.7500,  2.0781,  2.4375,\n",
      "         2.4844,  2.5938,  1.9062,  2.8125,  2.4219,  2.7656,  2.4688,  2.6719,\n",
      "         2.3906,  2.7500,  2.6562,  2.9844,  2.4844,  2.4375,  2.1094,  2.5312,\n",
      "         2.5000,  2.4688,  2.5000,  2.6719,  2.8906,  2.6250,  2.2812,  2.6250,\n",
      "         2.7344,  2.0938,  2.5156,  2.5938,  2.2969,  2.4844,  2.4844,  2.6719,\n",
      "         2.5312,  2.5000,  2.5625,  2.2656,  2.4062,  2.4688,  2.3281,  2.3438,\n",
      "         2.5781,  2.4062,  2.6406,  2.5781,  2.2188,  2.3906,  2.5625,  2.3438,\n",
      "         2.5312,  2.3438,  2.7031,  2.5156,  2.2812,  2.3750,  2.2969,  2.3281,\n",
      "         2.3281,  2.6562,  2.5312,  2.7656,  2.2969,  2.5000,  2.3750,  2.4062,\n",
      "         2.3750,  2.4219,  2.2188,  1.9297,  2.4062,  2.2969,  2.3750,  2.7344,\n",
      "         2.3906,  2.5156,  2.5156,  2.4375,  2.5000,  1.9922,  2.3281,  2.3906,\n",
      "         2.4062,  2.5938,  2.4688,  2.6875,  2.5156,  2.0156,  2.1875,  2.4844,\n",
      "         2.5156,  2.4375,  2.3906,  2.5000,  2.5000,  2.7344,  2.5156,  2.7188,\n",
      "         2.5938,  2.2656,  2.3125,  2.2969,  2.5781,  2.7812,  2.8594,  2.3438,\n",
      "         2.5312,  2.3281,  2.5781,  2.4375,  2.4531,  2.3906,  2.2031,  2.5156,\n",
      "         2.2344,  2.2656,  2.5781,  2.6562,  2.3438,  2.2969,  2.8906,  2.9062],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.6.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0091, -0.0439,  0.0128,  ...,  0.0250,  0.0447,  0.0015],\n",
      "        [-0.0391, -0.0004, -0.0095,  ..., -0.0454,  0.0364, -0.0077],\n",
      "        [ 0.0178,  0.0231, -0.0664,  ...,  0.0281, -0.0289,  0.0134],\n",
      "        ...,\n",
      "        [-0.0025,  0.0001,  0.0054,  ...,  0.0039, -0.0134,  0.0216],\n",
      "        [ 0.0143, -0.0520, -0.0244,  ...,  0.0282,  0.0070, -0.0334],\n",
      "        [ 0.0139,  0.0138, -0.0085,  ...,  0.0277,  0.0369, -0.0062]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.6.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.3574, -0.1631, -0.0933,  ...,  0.3164, -0.2715, -0.1196],\n",
      "        [-0.0115,  0.1226,  0.1069,  ..., -0.2012, -0.2891,  0.2109],\n",
      "        [-0.2383, -0.0923,  0.3398,  ...,  0.2539, -0.2334, -0.4277],\n",
      "        ...,\n",
      "        [ 0.1875, -0.6250,  0.4883,  ..., -0.2344, -0.3262,  0.1050],\n",
      "        [ 0.2227,  0.3027, -0.0012,  ..., -0.2617, -0.2910, -0.1436],\n",
      "        [-0.6523,  0.0325,  0.2715,  ...,  0.0420, -0.6367,  0.4043]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.6.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.7695, -0.3848,  0.5586,  ..., -0.7383, -1.0547, -0.2793],\n",
      "        [ 0.1396, -0.5430, -0.9023,  ..., -0.5625,  0.0371, -0.8789],\n",
      "        [-0.2012, -0.5586,  1.0000,  ..., -0.1279, -0.1777, -0.3047],\n",
      "        ...,\n",
      "        [-0.6250, -0.1045,  0.3750,  ...,  0.1885,  0.0674,  0.9062],\n",
      "        [-0.3789, -0.8008,  0.1875,  ..., -0.3457, -1.3047,  0.7695],\n",
      "        [-0.2910,  0.7305, -0.1650,  ...,  0.8164,  1.5000, -0.0708]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.6.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.4473,  0.6836,  0.6992,  ..., -0.2344,  0.2852,  0.3203],\n",
      "        [-0.4590, -0.0184,  0.4277,  ...,  0.4355,  0.0947,  0.4004],\n",
      "        [ 1.5156, -0.9609,  1.1328,  ..., -0.0747, -0.7422, -0.5586],\n",
      "        ...,\n",
      "        [-0.1426,  0.4805, -1.3203,  ...,  1.4297, -0.5039, -0.6602],\n",
      "        [ 0.0378, -0.7461,  1.7031,  ..., -0.5039,  0.4434, -1.1094],\n",
      "        [ 0.8984,  0.2969, -1.3125,  ..., -0.2012, -0.7812,  1.2812]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.6.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 2.2559e-01,  1.2695e-01,  1.6699e-01,  2.2070e-01,  2.3926e-01,\n",
      "         2.0020e-01,  2.2559e-01,  2.6172e-01,  1.8945e-01,  2.4316e-01,\n",
      "         1.7773e-01,  2.5000e-01,  1.3574e-01,  1.5820e-01,  2.3438e-01,\n",
      "         2.4805e-01,  2.1582e-01,  2.0605e-01,  1.9824e-01,  2.5586e-01,\n",
      "         2.2852e-01,  2.4023e-01,  2.3145e-01,  2.4121e-01,  2.4121e-01,\n",
      "         1.9629e-01,  2.3730e-01,  1.7090e-01,  2.2852e-01,  2.4219e-01,\n",
      "         2.5195e-01,  2.0801e-01,  2.5586e-01,  2.3438e-01,  2.4512e-01,\n",
      "         2.0508e-01,  2.5391e-01,  1.9336e-01,  2.3730e-01,  2.6562e-01,\n",
      "         2.1973e-01,  2.0605e-01,  2.3242e-01,  2.3438e-01,  1.8262e-01,\n",
      "         2.3145e-01,  2.4707e-01,  2.4707e-01,  2.4707e-01,  2.6367e-01,\n",
      "         2.2363e-01,  2.3926e-01,  2.2363e-01,  2.4609e-01,  2.9297e-01,\n",
      "         1.8457e-01,  2.0312e-01,  2.5195e-01,  2.4707e-01,  2.3047e-01,\n",
      "         2.0996e-01,  2.5000e-01,  2.3438e-01,  2.3926e-01,  2.4219e-01,\n",
      "         1.8555e-01,  2.3438e-01,  2.3438e-01,  1.6504e-01,  2.3340e-01,\n",
      "         2.5195e-01,  2.5000e-01,  2.4121e-01,  2.3633e-01,  2.3340e-01,\n",
      "         2.4023e-01, -1.6895e-01,  2.1484e-01,  2.4023e-01,  2.3340e-01,\n",
      "         2.5000e-01,  2.2266e-01,  2.4609e-01,  1.7871e-01,  2.2461e-01,\n",
      "         2.4609e-01,  1.1328e-01,  2.4805e-01,  2.4609e-01,  2.3438e-01,\n",
      "         2.1973e-01,  2.3438e-01,  2.1875e-01,  2.0117e-01,  2.5977e-01,\n",
      "         2.5977e-01,  2.3535e-01,  2.4121e-01,  2.6953e-01,  2.3438e-01,\n",
      "         1.9336e-01,  2.1094e-01,  1.4355e-01,  2.3340e-01,  2.1973e-01,\n",
      "         1.9238e-01,  2.3730e-01,  2.2070e-01,  2.3828e-01,  2.4023e-01,\n",
      "         2.4512e-01,  2.5391e-01,  2.1680e-01,  2.3145e-01,  2.1875e-01,\n",
      "         2.3926e-01,  2.3145e-01,  2.1777e-01,  4.6387e-02,  2.5195e-01,\n",
      "         2.4414e-01,  2.2656e-01,  1.9922e-01,  2.3242e-01,  2.3828e-01,\n",
      "         2.1680e-01,  2.3926e-01,  2.0215e-01,  2.0801e-01,  1.6309e-01,\n",
      "         1.5625e-01,  2.2363e-01,  2.2363e-01,  2.5195e-01,  1.2354e-01,\n",
      "         2.3047e-01,  2.2656e-01,  2.0801e-01,  2.0508e-01,  1.6406e-01,\n",
      "         1.9238e-01,  2.2559e-01,  2.2949e-01,  2.4805e-01,  2.2559e-01,\n",
      "         1.9922e-01,  2.2949e-01,  1.9141e-01,  2.3828e-01,  2.0508e-01,\n",
      "         2.5586e-01,  2.2363e-01,  2.4414e-01,  2.0605e-01,  2.4707e-01,\n",
      "         2.5000e-01,  2.4219e-01,  2.6562e-01,  5.8838e-02, -1.3477e-01,\n",
      "         2.3047e-01,  1.8848e-01,  2.0508e-01,  2.1484e-01,  2.1582e-01,\n",
      "         1.4941e-01,  2.4219e-01,  1.7969e-01,  1.6113e-01,  2.4707e-01,\n",
      "         1.3086e-01,  2.2559e-01,  2.5391e-01,  2.4316e-01,  2.5195e-01,\n",
      "         2.4023e-01,  2.1875e-01,  2.3047e-01,  1.1133e-01,  1.9629e-01,\n",
      "         2.5195e-01,  2.2363e-01,  2.5781e-01,  2.5781e-01,  1.9629e-01,\n",
      "         2.1875e-01,  2.3340e-01,  3.4668e-02,  2.2754e-01,  1.6602e-01,\n",
      "         2.3340e-01,  2.3730e-01,  2.2559e-01,  2.5781e-01,  2.2266e-01,\n",
      "         2.1680e-01,  2.3633e-01,  2.5781e-01,  2.4316e-01,  2.1484e-01,\n",
      "         2.5977e-01,  2.2168e-01,  2.3730e-01,  2.2070e-01,  2.3242e-01,\n",
      "         1.9141e-01,  2.3145e-01,  2.2461e-01,  2.2949e-01,  2.6758e-01,\n",
      "         2.0312e-01,  1.8652e-01,  2.2559e-01,  2.3535e-01,  2.4805e-01,\n",
      "         2.5391e-01,  2.3047e-01,  2.1191e-01,  2.5195e-01,  1.9287e-02,\n",
      "         1.6895e-01,  2.1973e-01,  2.1582e-01,  2.2656e-01,  2.1289e-01,\n",
      "         2.4805e-01,  4.1406e-01,  2.4512e-01,  2.4316e-01,  2.4805e-01,\n",
      "         2.2852e-01,  2.4902e-01,  2.1973e-01,  2.5391e-01,  2.4707e-01,\n",
      "         2.0312e-01,  1.9727e-01,  2.1973e-01,  1.5625e-01,  2.3438e-01,\n",
      "         2.1289e-01,  2.4316e-01,  2.1680e-01,  2.3145e-01,  2.5586e-01,\n",
      "         2.4316e-01,  2.3535e-01,  2.2852e-01,  2.2266e-01,  2.4023e-01,\n",
      "         2.1777e-01,  2.3438e-01,  2.3633e-01,  2.2754e-01,  2.4023e-01,\n",
      "         1.9824e-01,  2.5586e-01, -1.5039e-01,  2.2656e-01,  2.1191e-01,\n",
      "         2.4609e-01,  1.5527e-01,  2.6758e-01,  2.5391e-01,  2.4316e-01,\n",
      "         2.3535e-01,  2.3535e-01,  2.3535e-01,  2.4219e-01,  2.4316e-01,\n",
      "         2.3438e-01,  1.5332e-01,  2.3730e-01,  2.2461e-01,  1.1670e-01,\n",
      "         2.5000e-01,  2.4805e-01,  2.3828e-01,  2.2949e-01,  1.9727e-01,\n",
      "         3.7842e-02,  1.6309e-01,  2.3340e-01,  2.2461e-01,  2.4609e-01,\n",
      "         2.3926e-01,  2.5000e-01,  2.6172e-01,  1.6699e-01,  1.6797e-01,\n",
      "         2.2168e-01,  2.1973e-01,  2.2363e-01,  2.4902e-01,  2.1582e-01,\n",
      "         2.5000e-01,  1.4746e-01,  2.4316e-01,  2.4707e-01,  2.6758e-01,\n",
      "         2.4414e-01,  2.3242e-01, -3.3203e-02,  2.3535e-01,  2.3438e-01,\n",
      "         2.4316e-01,  2.4121e-01,  2.4414e-01,  2.1582e-01,  2.1582e-01,\n",
      "         2.2461e-01,  2.3340e-01,  2.2266e-01,  2.3535e-01,  2.2949e-01,\n",
      "         2.1484e-01,  2.3242e-01,  2.5000e-01,  2.5391e-01,  2.2949e-01,\n",
      "         2.3926e-01,  2.3828e-01,  1.8848e-01,  2.3828e-01,  1.9824e-01,\n",
      "         2.2070e-01,  2.1387e-01,  2.3535e-01,  2.2852e-01,  2.4219e-01,\n",
      "         2.3926e-01,  2.0215e-01,  2.2754e-01,  2.1387e-01,  2.4219e-01,\n",
      "         2.3730e-01,  2.4805e-01,  2.3730e-01,  2.1777e-01,  1.8848e-01,\n",
      "         2.2168e-01,  2.3438e-01,  2.3730e-01,  2.3828e-01,  2.3730e-01,\n",
      "         2.1875e-01,  2.4512e-01,  1.9141e-01,  1.4453e-01,  2.1680e-01,\n",
      "         2.1875e-01,  2.1777e-01,  1.9531e-01,  2.3145e-01,  2.2266e-01,\n",
      "         2.5000e-01,  2.5000e-01,  6.3477e-02,  2.4805e-01,  2.5977e-01,\n",
      "         2.5781e-01,  1.8359e-01,  1.5332e-01,  2.1289e-01,  2.4414e-01,\n",
      "         2.4512e-01,  2.3340e-01,  2.1973e-01,  2.4316e-01,  2.3047e-01,\n",
      "         2.6562e-01,  2.3242e-01,  2.2852e-01,  2.2168e-01,  2.4805e-01,\n",
      "         2.3242e-01,  2.3926e-01,  2.4121e-01,  2.4121e-01,  1.7090e-01,\n",
      "         2.2168e-01,  2.0410e-01,  1.5527e-01,  2.2754e-01,  2.3828e-01,\n",
      "         2.5391e-01,  2.4414e-01,  2.5977e-01,  2.2070e-01,  2.4707e-01,\n",
      "         2.3730e-01,  2.4023e-01,  2.0898e-01,  2.2168e-01,  2.3340e-01,\n",
      "         2.4512e-01,  2.5000e-01,  2.1289e-01,  2.2656e-01,  2.4121e-01,\n",
      "         2.3438e-01,  2.4316e-01,  2.4121e-01,  2.4219e-01,  2.1973e-01,\n",
      "         2.3145e-01,  1.7090e-01,  2.2461e-01,  2.1973e-01,  2.5586e-01,\n",
      "         2.1484e-01,  2.2656e-01,  2.2266e-01,  2.2754e-01,  2.4805e-01,\n",
      "         2.3926e-01,  2.2559e-01,  2.3730e-01,  2.4805e-01,  2.3535e-01,\n",
      "         1.5991e-02,  2.2070e-01,  2.4414e-01,  8.2031e-02,  2.4219e-01,\n",
      "         2.3633e-01,  1.9922e-01,  2.3926e-01,  2.2266e-01,  1.7480e-01,\n",
      "         2.3047e-01,  2.3242e-01,  2.4512e-01,  2.1875e-01,  2.1484e-01,\n",
      "         2.1973e-01,  2.3438e-01,  2.3633e-01,  2.4316e-01,  2.2266e-01,\n",
      "         2.4316e-01,  2.1875e-01,  1.9434e-01,  2.6367e-01,  2.4219e-01,\n",
      "         2.2852e-01,  2.4609e-01,  2.4121e-01, -3.7598e-02,  2.2363e-01,\n",
      "         2.3145e-01,  2.5195e-01,  2.3438e-01,  2.4707e-01,  2.2266e-01,\n",
      "         2.1094e-01,  2.0703e-01,  3.7500e-01,  2.2363e-01,  2.2949e-01,\n",
      "         2.5195e-01,  2.3145e-01,  2.4219e-01,  2.3047e-01,  2.2266e-01,\n",
      "         2.5781e-01,  1.8164e-01,  2.2949e-01,  2.1777e-01,  2.4023e-01,\n",
      "         2.4902e-01,  2.3633e-01,  1.4453e-01,  2.5195e-01,  2.4219e-01,\n",
      "         2.3828e-01,  2.3633e-01,  2.4023e-01,  2.4805e-01,  2.4023e-01,\n",
      "         1.9043e-01,  2.4316e-01,  2.4707e-01,  2.1973e-01,  7.8125e-02,\n",
      "         2.4707e-01,  2.2168e-01, -2.9945e-04,  2.5195e-01,  2.3438e-01,\n",
      "         2.4414e-01,  2.2266e-01,  2.4805e-01,  2.4121e-01,  2.4805e-01,\n",
      "         8.0566e-02,  2.2266e-01,  2.3145e-01,  2.5586e-01,  2.2559e-01,\n",
      "         2.2852e-01,  2.0801e-01,  2.3828e-01,  2.3242e-01,  2.2266e-01,\n",
      "         2.3438e-01,  2.2949e-01,  2.0703e-01,  2.3730e-01,  2.3828e-01,\n",
      "         2.1387e-01,  2.5586e-01,  2.2656e-01,  2.2363e-01,  2.0312e-01,\n",
      "         2.5195e-01,  2.0117e-01,  2.1094e-01,  1.7773e-01,  2.5000e-01,\n",
      "         2.1387e-01,  2.4219e-01,  2.1875e-01,  2.3828e-01,  2.2949e-01,\n",
      "         2.1582e-01,  2.4219e-01,  1.9434e-01,  2.2949e-01,  2.1484e-01,\n",
      "         1.8848e-01,  2.3828e-01,  2.3730e-01,  2.4609e-01,  2.1973e-01,\n",
      "         2.4219e-01,  2.5391e-01,  2.3145e-01,  2.3242e-01,  2.1680e-01,\n",
      "         2.3438e-01,  2.4219e-01,  2.3047e-01,  2.1484e-01,  2.1289e-01,\n",
      "         2.3145e-01,  2.3340e-01,  2.3730e-01,  2.2363e-01,  2.3535e-01,\n",
      "         1.7676e-01,  1.8457e-01,  2.2559e-01,  1.8555e-01,  2.4023e-01,\n",
      "         2.3535e-01,  2.0117e-01,  2.4805e-01,  2.0117e-01,  2.3242e-01,\n",
      "         2.1875e-01,  2.0801e-01,  1.7773e-01,  2.8906e-01,  2.5195e-01,\n",
      "         2.3047e-01,  2.3633e-01,  2.3633e-01,  2.1484e-01,  2.3926e-01,\n",
      "         2.3633e-01,  2.4023e-01,  2.2461e-01,  2.0996e-01,  1.6504e-01,\n",
      "         2.1777e-01,  2.5977e-01,  2.4805e-01,  2.3535e-01,  2.4319e-04,\n",
      "         1.9043e-01,  2.4805e-01,  2.3047e-01,  2.2754e-01,  2.3438e-01,\n",
      "         2.3926e-01,  2.2656e-01,  2.3047e-01,  2.4121e-01,  2.2754e-01,\n",
      "         2.4316e-01,  2.5195e-01,  2.3828e-01,  2.2168e-01,  2.1973e-01,\n",
      "         2.4902e-01,  1.7188e-01,  2.3633e-01,  2.3047e-01,  2.4414e-01,\n",
      "         1.9238e-01,  2.4512e-01,  2.5391e-01,  2.1191e-01,  2.2461e-01,\n",
      "         2.3340e-01,  2.3926e-01,  1.4355e-01,  2.6367e-01,  2.3242e-01,\n",
      "         2.4512e-01,  2.2559e-01,  2.4219e-01,  2.3828e-01,  2.2949e-01,\n",
      "         2.5391e-01,  2.1582e-01,  2.2656e-01,  2.4121e-01,  2.3145e-01,\n",
      "         2.3145e-01,  2.2363e-01,  2.4121e-01,  1.6602e-01,  2.2559e-01,\n",
      "         2.4121e-01,  2.3047e-01,  2.2852e-01,  2.3828e-01,  2.3438e-01,\n",
      "         1.9531e-01,  2.3242e-01,  2.0898e-01,  2.2363e-01,  2.2559e-01,\n",
      "         2.3633e-01,  2.0117e-01,  2.2754e-01,  2.4414e-01,  2.3242e-01,\n",
      "         1.8066e-01,  2.2070e-01,  2.3828e-01,  2.3242e-01,  1.7285e-01,\n",
      "         2.2461e-01,  1.4844e-01,  2.3926e-01,  2.1387e-01,  2.5391e-01,\n",
      "         1.5625e-01,  2.6367e-01,  2.2168e-01,  2.2852e-01,  2.4414e-01,\n",
      "         1.4062e-01,  2.2070e-01,  1.8164e-01,  2.4414e-01,  2.2559e-01,\n",
      "         2.5000e-01,  2.0020e-01,  2.1484e-01,  2.3730e-01,  2.2852e-01,\n",
      "         2.1777e-01,  2.4707e-01,  2.5586e-01,  2.1973e-01,  2.4609e-01,\n",
      "         2.2363e-01,  2.4512e-01,  2.4316e-01,  1.9531e-01,  2.3926e-01,\n",
      "         2.5000e-01,  2.1875e-01,  2.4609e-01,  2.3047e-01,  2.5000e-01,\n",
      "         2.3535e-01,  2.4121e-01,  2.0703e-01,  2.0801e-01,  2.2266e-01,\n",
      "         2.2070e-01,  2.3145e-01,  2.1484e-01,  2.5977e-01,  2.2559e-01,\n",
      "         2.4902e-01,  2.4121e-01,  1.7285e-01,  2.1582e-01,  2.3828e-01,\n",
      "         2.2754e-01,  2.5000e-01,  2.3438e-01,  2.2949e-01,  2.2949e-01,\n",
      "         2.1973e-01,  2.2852e-01,  2.1191e-01,  1.2256e-01,  2.2070e-01,\n",
      "         2.4316e-01,  2.4805e-01,  1.7383e-01,  1.6699e-01,  2.3730e-01,\n",
      "         2.1387e-01,  2.1875e-01,  2.0215e-01,  2.2461e-01,  1.1865e-01,\n",
      "         2.2754e-01,  2.1582e-01,  2.2070e-01,  2.2949e-01,  1.9824e-01,\n",
      "         2.4219e-01,  2.2363e-01,  1.9141e-01,  2.5195e-01,  2.4121e-01,\n",
      "         1.3574e-01,  2.1777e-01,  2.1875e-01,  2.2949e-01,  2.3730e-01,\n",
      "         2.4609e-01,  2.3633e-01,  2.3633e-01,  8.0566e-02,  2.1973e-01,\n",
      "         2.3340e-01,  2.3047e-01,  2.2754e-01,  2.4707e-01,  2.1680e-01,\n",
      "         2.2070e-01,  1.3379e-01,  2.4609e-01,  1.9434e-01,  2.5781e-01,\n",
      "         2.0801e-01,  1.7578e-01,  2.2656e-01,  2.2559e-01,  1.7090e-01,\n",
      "         2.1484e-01,  2.1094e-01,  1.8359e-01,  2.2559e-01,  2.4902e-01,\n",
      "         2.5000e-01,  2.0996e-01,  2.1387e-01,  1.2354e-01,  2.2461e-01,\n",
      "         1.9824e-01,  1.9531e-01,  2.1973e-01,  2.2266e-01,  2.3047e-01,\n",
      "         2.2070e-01,  2.6953e-01,  2.1484e-01], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.decoder.block.6.layer.1.EncDecAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0820, -0.0050,  0.0713,  ...,  0.0315,  0.0669,  0.0070],\n",
      "        [ 0.0140, -0.0187,  0.0317,  ..., -0.0309, -0.0425, -0.0006],\n",
      "        [-0.0074,  0.0317, -0.0140,  ..., -0.0684, -0.0391,  0.0374],\n",
      "        ...,\n",
      "        [ 0.0417,  0.0659, -0.0496,  ...,  0.0216,  0.0107, -0.1133],\n",
      "        [ 0.0195,  0.0107,  0.0107,  ...,  0.0815, -0.0723,  0.0503],\n",
      "        [-0.0251,  0.0664, -0.0141,  ...,  0.0018, -0.0039,  0.0332]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.6.layer.1.EncDecAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[-1.9238e-01,  6.8359e-01,  8.9453e-01,  ..., -1.6895e-01,\n",
      "          2.1582e-01, -2.5977e-01],\n",
      "        [ 2.9663e-02,  2.0996e-02,  9.6512e-04,  ..., -4.4727e-01,\n",
      "         -2.3438e-02, -1.7285e-01],\n",
      "        [ 2.2461e-01, -6.7188e-01,  2.0703e-01,  ...,  1.1377e-01,\n",
      "         -5.6250e-01, -8.4305e-04],\n",
      "        ...,\n",
      "        [ 5.3516e-01,  1.5430e-01, -3.9258e-01,  ..., -5.1270e-02,\n",
      "          4.1992e-01,  2.3730e-01],\n",
      "        [-5.6250e-01, -3.0664e-01,  3.8605e-03,  ...,  1.5625e-01,\n",
      "         -3.7695e-01, -4.3555e-01],\n",
      "        [ 6.4062e-01,  2.8125e-01, -6.0547e-01,  ..., -1.9824e-01,\n",
      "         -6.5231e-04,  3.4766e-01]], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.6.layer.1.EncDecAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1357,  0.5820, -0.5391,  ...,  0.5117, -1.2031, -0.0137],\n",
      "        [ 1.1641,  0.0942,  0.1523,  ..., -0.0674, -0.2500,  0.2129],\n",
      "        [-0.0410,  0.4824,  0.8242,  ...,  0.3789,  0.2070,  0.2656],\n",
      "        ...,\n",
      "        [-0.0170, -1.1797, -0.5898,  ..., -0.2451,  0.6719,  1.5391],\n",
      "        [-0.7383, -0.1289,  0.8711,  ...,  0.9883, -0.9844,  1.2422],\n",
      "        [ 0.1177, -0.6719,  0.5078,  ..., -0.9023,  0.7539, -0.5977]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.6.layer.1.EncDecAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-2.2168e-01,  4.9023e-01, -1.3477e-01,  ..., -8.9453e-01,\n",
      "          1.2207e-01,  7.2266e-01],\n",
      "        [ 3.9551e-02, -1.5381e-02, -4.1016e-01,  ...,  1.0010e-01,\n",
      "         -9.9609e-02,  3.1250e-01],\n",
      "        [ 1.6016e-01,  4.0820e-01, -3.0518e-02,  ...,  1.2512e-03,\n",
      "          1.3574e-01, -2.1191e-01],\n",
      "        ...,\n",
      "        [ 3.5352e-01, -1.8457e-01, -8.5547e-01,  ..., -8.6328e-01,\n",
      "          4.4141e-01,  4.0430e-01],\n",
      "        [-4.2383e-01,  1.1084e-01, -1.4297e+00,  ..., -4.0625e-01,\n",
      "         -5.0000e-01,  3.7695e-01],\n",
      "        [ 9.7168e-02,  1.5234e-01,  1.4062e-01,  ...,  3.4570e-01,\n",
      "          7.0801e-02,  1.9629e-01]], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.6.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 5.4199e-02,  3.9551e-02,  4.2969e-02, -6.0059e-02,  4.9561e-02,\n",
      "         4.9561e-02,  6.8359e-02,  6.5430e-02,  5.5908e-02,  6.6895e-02,\n",
      "         5.0049e-02,  6.5918e-02,  5.5908e-02,  4.9561e-02,  5.5176e-02,\n",
      "         6.2256e-02,  5.6885e-02,  5.2490e-02,  5.5176e-02,  8.0078e-02,\n",
      "         5.7861e-02,  5.8594e-02,  6.2988e-02,  7.3730e-02,  6.2012e-02,\n",
      "         6.0547e-02,  5.7373e-02,  4.0039e-02,  6.0303e-02,  5.9814e-02,\n",
      "         6.8848e-02,  5.4688e-02,  6.3477e-02,  6.6406e-02,  6.3477e-02,\n",
      "         5.1514e-02,  7.4219e-02,  6.7871e-02,  5.1758e-02,  6.2256e-02,\n",
      "         5.4688e-02,  5.2002e-02,  6.2500e-02,  5.6641e-02,  5.2734e-02,\n",
      "         6.1279e-02,  8.1543e-02,  7.6172e-02,  6.3477e-02,  6.1523e-02,\n",
      "         6.4941e-02,  5.8105e-02,  5.9326e-02,  5.9570e-02,  7.8613e-02,\n",
      "         4.7607e-02,  5.1758e-02,  5.9326e-02,  7.0801e-02,  5.7373e-02,\n",
      "         5.3711e-02,  6.3477e-02,  6.3477e-02,  6.0059e-02,  6.2500e-02,\n",
      "         1.1816e-01,  5.9570e-02,  5.9082e-02,  4.1748e-02,  5.8350e-02,\n",
      "         6.7383e-02,  6.9824e-02,  6.4453e-02,  5.7617e-02,  6.2988e-02,\n",
      "         5.8105e-02,  4.5410e-02,  5.9082e-02,  6.2500e-02,  6.0303e-02,\n",
      "         6.5430e-02,  5.5420e-02, -5.5664e-02,  5.6396e-02,  6.8848e-02,\n",
      "         6.0547e-02, -2.9785e-02,  6.3965e-02,  6.2500e-02,  6.1523e-02,\n",
      "         5.1025e-02,  5.9814e-02,  5.2734e-02,  5.0049e-02,  6.3477e-02,\n",
      "         6.5918e-02,  6.0303e-02,  7.4219e-02,  6.6895e-02,  5.6641e-02,\n",
      "         5.0537e-02,  5.7373e-02,  3.5889e-02,  5.5176e-02,  5.5908e-02,\n",
      "         5.3467e-02,  6.6406e-02,  5.3467e-02,  6.6406e-02,  6.4453e-02,\n",
      "         6.5430e-02,  6.3965e-02,  5.3711e-02,  5.7373e-02,  5.3711e-02,\n",
      "         8.2031e-02,  5.4199e-02,  5.5664e-02, -1.7834e-04,  5.5908e-02,\n",
      "         5.8105e-02,  6.3965e-02,  5.3467e-02,  5.9570e-02,  5.7129e-02,\n",
      "         5.0293e-02,  6.1279e-02,  5.0781e-02,  5.3711e-02,  4.9561e-02,\n",
      "         4.1992e-02,  5.4932e-02,  6.4941e-02,  6.5918e-02,  4.0039e-02,\n",
      "         6.0059e-02,  5.8105e-02,  5.3467e-02,  5.7617e-02,  4.9805e-02,\n",
      "         5.6396e-02,  6.2988e-02,  5.4443e-02,  6.0303e-02,  5.9082e-02,\n",
      "         5.0537e-02,  6.4941e-02,  4.9072e-02,  6.1523e-02,  5.5420e-02,\n",
      "         7.1777e-02,  5.5664e-02,  6.7871e-02,  5.7129e-02,  6.7383e-02,\n",
      "         6.6895e-02,  5.9570e-02,  6.7871e-02,  1.1816e-01,  4.8584e-02,\n",
      "         5.8838e-02,  4.8340e-02,  5.1514e-02,  5.6641e-02,  5.7861e-02,\n",
      "         3.6377e-02,  6.6406e-02,  5.5420e-02,  3.4180e-02,  6.2988e-02,\n",
      "         3.6621e-02,  6.1279e-02,  6.7383e-02,  6.1279e-02,  5.8838e-02,\n",
      "         6.2500e-02, -5.2979e-02,  6.6406e-02,  3.6377e-02,  5.6152e-02,\n",
      "         5.8594e-02,  6.8848e-02,  6.5918e-02,  6.4941e-02,  5.3467e-02,\n",
      "         5.7373e-02,  6.0059e-02,  6.8848e-02,  5.5908e-02,  4.5166e-02,\n",
      "         6.5918e-02,  5.4443e-02,  6.1523e-02,  5.8594e-02,  4.7607e-02,\n",
      "         5.8105e-02,  5.7373e-02,  7.3242e-02,  6.3477e-02,  5.8838e-02,\n",
      "         7.0312e-02,  5.5908e-02,  6.1279e-02,  5.5420e-02,  5.7129e-02,\n",
      "         4.7363e-02,  5.9814e-02,  6.8359e-02,  6.1523e-02,  6.6895e-02,\n",
      "         7.7148e-02,  4.9805e-02,  5.5420e-02,  6.2012e-02,  6.9824e-02,\n",
      "         6.0547e-02,  5.6641e-02,  5.9082e-02,  6.0791e-02,  3.1853e-04,\n",
      "         4.7363e-02,  7.1289e-02,  5.6641e-02,  6.4453e-02,  5.2002e-02,\n",
      "         6.4453e-02,  1.5137e-01,  7.1777e-02,  5.6885e-02,  6.6406e-02,\n",
      "         5.9326e-02,  6.3965e-02,  5.1758e-02,  6.8848e-02,  5.8350e-02,\n",
      "         5.3467e-02,  5.1270e-02,  5.4932e-02,  4.7607e-02,  6.1523e-02,\n",
      "        -6.2500e-02,  6.3477e-02,  6.1768e-02,  5.5420e-02,  6.6895e-02,\n",
      "         5.6152e-02,  6.5918e-02,  5.1514e-02,  6.1523e-02,  5.8594e-02,\n",
      "         5.3955e-02,  5.9326e-02,  6.0791e-02,  5.8350e-02,  6.2256e-02,\n",
      "         4.9561e-02,  6.3965e-02, -4.1992e-02,  6.5430e-02,  6.4453e-02,\n",
      "         7.2754e-02,  4.8828e-02,  6.6406e-02,  5.8105e-02,  5.8838e-02,\n",
      "         6.0059e-02,  6.0547e-02,  6.4453e-02,  5.5664e-02,  6.1035e-02,\n",
      "         6.0059e-02,  7.7637e-02,  6.4453e-02,  5.6885e-02,  3.7598e-02,\n",
      "         5.7861e-02,  6.4941e-02,  5.7373e-02,  6.4941e-02,  5.2979e-02,\n",
      "         2.9945e-04,  4.4434e-02,  6.3965e-02,  5.7861e-02,  6.5918e-02,\n",
      "         5.8350e-02,  5.9082e-02,  6.5430e-02,  4.5166e-02,  4.6875e-02,\n",
      "         5.7617e-02,  5.4688e-02,  5.8105e-02,  6.2500e-02,  5.6152e-02,\n",
      "         6.0059e-02,  5.2246e-02,  6.2256e-02,  6.2988e-02,  7.2754e-02,\n",
      "         7.2266e-02,  5.9082e-02,  2.1118e-02,  5.3223e-02,  5.7861e-02,\n",
      "         6.2256e-02,  5.7129e-02,  6.4453e-02,  6.1035e-02,  6.2988e-02,\n",
      "         6.7383e-02,  5.8594e-02,  5.3955e-02,  6.2988e-02,  1.2354e-01,\n",
      "         6.4453e-02,  7.9102e-02,  6.2988e-02,  6.8359e-02,  6.5918e-02,\n",
      "         6.2988e-02,  6.2988e-02,  5.1025e-02,  6.3965e-02,  6.0303e-02,\n",
      "         5.6396e-02,  5.2490e-02,  6.4941e-02,  6.4453e-02,  6.3965e-02,\n",
      "         6.4453e-02,  4.9316e-02,  6.7871e-02,  5.9082e-02,  5.9570e-02,\n",
      "         5.8838e-02,  6.3477e-02,  7.5684e-02,  6.0303e-02,  4.4189e-02,\n",
      "         5.6396e-02,  5.8105e-02,  6.1523e-02,  5.8838e-02,  6.3477e-02,\n",
      "         5.6641e-02,  6.1768e-02,  5.1025e-02,  4.1016e-02,  5.4688e-02,\n",
      "         5.9082e-02,  5.2979e-02,  4.8096e-02,  6.0303e-02,  5.3223e-02,\n",
      "         7.0801e-02,  6.5918e-02,  5.1880e-04,  6.2256e-02,  7.6172e-02,\n",
      "         6.3965e-02,  5.3711e-02,  6.1035e-02,  5.0537e-02,  6.4453e-02,\n",
      "         7.4707e-02,  6.2256e-02,  5.3467e-02,  5.7861e-02,  5.9814e-02,\n",
      "         6.2500e-02,  6.5430e-02,  5.8594e-02,  5.8838e-02,  7.1777e-02,\n",
      "         5.9326e-02,  6.1768e-02,  7.3730e-02,  6.2988e-02,  4.4678e-02,\n",
      "         5.6152e-02,  5.7861e-02,  4.5166e-02,  6.3477e-02,  6.0791e-02,\n",
      "         6.4453e-02,  6.8359e-02,  6.5430e-02,  5.2490e-02,  6.2256e-02,\n",
      "         5.8105e-02,  5.4443e-02,  5.1025e-02,  5.4199e-02,  5.8105e-02,\n",
      "         6.3477e-02,  6.2256e-02,  5.4932e-02,  5.6152e-02,  6.0547e-02,\n",
      "         5.7129e-02,  5.8838e-02,  6.0059e-02,  6.5430e-02,  5.9814e-02,\n",
      "         5.8594e-02,  5.9570e-02,  5.5420e-02,  5.3467e-02,  6.0303e-02,\n",
      "         5.4932e-02,  6.1035e-02,  5.9082e-02,  6.5918e-02,  6.9824e-02,\n",
      "         6.2988e-02,  5.9570e-02,  6.4941e-02,  1.5723e-01,  5.8838e-02,\n",
      "        -1.6594e-04,  6.4453e-02,  6.6895e-02, -1.7212e-02,  6.5918e-02,\n",
      "         6.2500e-02,  4.7607e-02,  6.3965e-02,  5.2979e-02,  4.7119e-02,\n",
      "         6.6406e-02,  5.6641e-02,  7.0312e-02,  5.1025e-02,  5.9326e-02,\n",
      "         5.4688e-02,  6.2500e-02,  5.6885e-02,  6.4453e-02,  5.3711e-02,\n",
      "         6.4453e-02,  5.8350e-02,  5.3711e-02,  6.4453e-02,  6.4453e-02,\n",
      "         6.1523e-02,  5.8594e-02,  6.2012e-02,  1.8406e-04,  6.2500e-02,\n",
      "         6.7383e-02,  6.2500e-02,  5.8350e-02,  6.2500e-02,  5.6641e-02,\n",
      "         4.8340e-02,  1.0840e-01,  1.0938e-01,  5.2734e-02,  5.4688e-02,\n",
      "         6.1768e-02,  6.2988e-02,  6.2500e-02,  6.2988e-02,  5.8350e-02,\n",
      "         6.1523e-02,  4.2969e-02,  5.9082e-02,  5.6152e-02,  5.7617e-02,\n",
      "         6.5430e-02,  6.0059e-02,  3.1494e-02,  6.5430e-02,  5.3467e-02,\n",
      "         6.3965e-02,  6.2988e-02,  6.0791e-02,  6.1523e-02,  5.2246e-02,\n",
      "         5.0049e-02,  5.9570e-02,  6.4941e-02,  5.2979e-02,  2.6941e-05,\n",
      "         5.5908e-02,  6.3965e-02,  1.0938e-01,  5.9082e-02,  5.9326e-02,\n",
      "         6.4453e-02,  7.7637e-02,  6.5918e-02,  6.6406e-02,  6.1279e-02,\n",
      "         6.9824e-02,  5.3223e-02,  6.2012e-02,  6.6895e-02,  5.9814e-02,\n",
      "         5.7617e-02,  9.4727e-02,  6.2988e-02,  5.6641e-02,  5.7373e-02,\n",
      "         5.9082e-02,  6.2256e-02,  5.0781e-02,  5.7861e-02,  6.0303e-02,\n",
      "         6.1279e-02,  6.5918e-02,  5.1758e-02,  5.9570e-02,  6.2012e-02,\n",
      "         6.4941e-02,  5.8838e-02,  5.2734e-02,  4.0039e-02,  7.2754e-02,\n",
      "         5.6641e-02,  5.8838e-02,  6.5430e-02,  6.2988e-02,  6.0059e-02,\n",
      "         5.7129e-02,  5.9814e-02,  5.2490e-02,  5.7861e-02,  5.5176e-02,\n",
      "         4.2480e-02,  6.2012e-02,  5.7129e-02,  6.2500e-02,  6.5430e-02,\n",
      "         6.1035e-02,  5.9326e-02,  6.1523e-02,  5.8105e-02,  6.3965e-02,\n",
      "         6.0303e-02,  6.3477e-02,  5.0293e-02,  5.4688e-02,  5.6152e-02,\n",
      "         5.7129e-02,  6.2012e-02,  5.3955e-02,  5.4932e-02,  5.5420e-02,\n",
      "        -5.4443e-02,  6.3965e-02,  5.6885e-02,  5.9814e-02,  5.9814e-02,\n",
      "         6.0547e-02,  4.8096e-02,  5.7861e-02,  4.8584e-02,  6.4453e-02,\n",
      "         5.8594e-02,  5.4932e-02,  3.2959e-02,  7.8125e-02,  6.2988e-02,\n",
      "         6.4941e-02,  6.4941e-02,  5.3467e-02,  5.5420e-02,  6.2988e-02,\n",
      "         6.7383e-02,  5.3223e-02,  5.5664e-02,  5.8350e-02,  4.5654e-02,\n",
      "         5.3223e-02,  6.7383e-02,  6.6406e-02,  6.2500e-02,  2.1267e-04,\n",
      "         4.7607e-02,  6.7383e-02,  5.7129e-02,  5.5908e-02,  6.2988e-02,\n",
      "         6.2988e-02,  5.6152e-02,  6.0791e-02,  5.9570e-02,  5.4199e-02,\n",
      "         5.9814e-02,  6.4453e-02,  6.5430e-02,  5.7861e-02,  5.4688e-02,\n",
      "         6.1768e-02,  5.3467e-02,  6.1523e-02,  5.5176e-02,  5.8350e-02,\n",
      "         4.6875e-02,  5.9082e-02,  6.4453e-02,  5.1025e-02,  5.4932e-02,\n",
      "         5.2734e-02,  6.8359e-02, -4.5898e-02,  6.1035e-02,  5.2734e-02,\n",
      "         5.8594e-02,  5.8838e-02,  6.2012e-02,  6.7871e-02,  5.4443e-02,\n",
      "         6.5430e-02,  5.0537e-02,  1.5332e-01,  6.6406e-02,  6.7383e-02,\n",
      "         5.8838e-02,  5.3223e-02,  5.7373e-02,  4.8096e-02,  6.4941e-02,\n",
      "         5.2979e-02,  6.0547e-02,  5.7129e-02,  5.7861e-02,  5.1758e-02,\n",
      "         5.8105e-02,  6.1279e-02,  5.1514e-02,  6.3477e-02,  5.6885e-02,\n",
      "         6.1035e-02,  5.7861e-02,  5.5176e-02,  6.0059e-02,  5.8105e-02,\n",
      "         4.5898e-02,  5.7617e-02,  6.7871e-02,  5.9326e-02,  3.8818e-02,\n",
      "         5.6641e-02,  4.3213e-02,  6.4941e-02,  6.2012e-02,  6.8848e-02,\n",
      "         5.2246e-02,  6.5430e-02,  5.5908e-02,  5.7861e-02,  6.6895e-02,\n",
      "         3.6377e-02,  5.3955e-02,  5.9082e-02,  6.2500e-02,  6.5430e-02,\n",
      "         6.4453e-02,  4.4678e-02,  5.2246e-02,  8.1055e-02,  6.2500e-02,\n",
      "         5.8105e-02,  6.3965e-02,  6.1279e-02,  6.7871e-02,  5.9570e-02,\n",
      "         5.8350e-02,  6.0303e-02,  5.8838e-02,  4.8340e-02,  6.9824e-02,\n",
      "         6.7383e-02,  5.9570e-02,  6.9336e-02,  5.3955e-02,  5.8105e-02,\n",
      "         6.3477e-02,  6.1035e-02,  5.0781e-02,  5.5664e-02,  5.5908e-02,\n",
      "         5.3467e-02,  5.8594e-02,  5.2490e-02,  6.2988e-02,  5.5908e-02,\n",
      "         6.0059e-02,  5.8838e-02, -4.7119e-02,  5.8105e-02,  5.9082e-02,\n",
      "         5.6885e-02,  6.2988e-02,  6.2500e-02,  5.5908e-02,  5.3955e-02,\n",
      "         5.7129e-02,  6.8359e-02,  5.2734e-02, -3.4912e-02,  6.4453e-02,\n",
      "         6.1523e-02,  5.5664e-02, -4.6143e-02,  6.4941e-02,  5.6396e-02,\n",
      "         5.5908e-02,  5.2979e-02,  6.6895e-02,  5.6152e-02, -3.6133e-02,\n",
      "         9.7168e-02,  5.4199e-02,  6.9336e-02,  6.3477e-02,  5.0781e-02,\n",
      "         7.1289e-02,  5.5420e-02,  4.9561e-02,  6.4941e-02,  6.1523e-02,\n",
      "         5.4443e-02,  5.9326e-02,  5.7129e-02,  5.6885e-02,  6.6895e-02,\n",
      "         7.0312e-02,  5.9814e-02,  6.4941e-02,  5.2002e-02,  7.0801e-02,\n",
      "         6.0059e-02,  6.6895e-02,  5.9814e-02,  6.0059e-02,  5.1758e-02,\n",
      "         5.7129e-02,  4.9316e-02,  6.0791e-02,  5.0293e-02,  6.5430e-02,\n",
      "         5.2246e-02,  5.1514e-02,  5.6396e-02,  5.7373e-02, -4.2969e-02,\n",
      "         5.3467e-02,  5.2246e-02,  5.2002e-02,  5.8350e-02,  5.8838e-02,\n",
      "         7.2266e-02,  5.7617e-02,  5.4199e-02,  5.3223e-02,  5.8105e-02,\n",
      "         4.9561e-02,  4.8340e-02,  4.9316e-02,  5.6396e-02,  5.9326e-02,\n",
      "         5.3955e-02,  6.6406e-02,  5.2246e-02], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.decoder.block.6.layer.2.DenseReluDense.wi.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.4434,  0.4434, -0.3652,  ...,  0.1060, -0.8555, -0.1182],\n",
      "        [ 0.1709,  0.0233,  0.7031,  ...,  0.3945,  0.0796,  0.1016],\n",
      "        [-0.1069,  0.2324, -0.7891,  ..., -0.0776, -0.3477,  0.6328],\n",
      "        ...,\n",
      "        [ 0.6172,  0.0099,  0.8984,  ..., -0.8164, -0.7031,  0.1157],\n",
      "        [-0.1963,  0.7188, -0.3906,  ..., -1.4375,  0.3672,  0.3516],\n",
      "        [ 0.0479,  0.2480, -0.0432,  ..., -0.4062,  0.2656,  0.5039]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.6.layer.2.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.2891,  0.2432,  0.1836,  ..., -0.0674,  0.3984, -0.2041],\n",
      "        [-0.0298,  0.2227, -0.1514,  ...,  0.0569,  0.0006,  0.1055],\n",
      "        [ 0.1641,  0.0466, -0.1846,  ..., -0.3418, -0.2275, -0.0654],\n",
      "        ...,\n",
      "        [ 0.1504,  0.3477, -0.0175,  ...,  0.1406, -0.1143, -0.1670],\n",
      "        [ 0.0090, -0.0178,  0.4316,  ..., -0.2021, -0.2852, -0.2490],\n",
      "        [ 0.1245,  0.0366,  0.0106,  ..., -0.1201, -0.1240, -0.3750]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.6.layer.2.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 2.7969e+00,  2.5938e+00,  2.4062e+00,  2.3438e+00,  2.8750e+00,\n",
      "         2.7344e+00,  2.4531e+00,  3.2656e+00,  2.4531e+00,  2.7656e+00,\n",
      "         3.7031e+00,  2.8750e+00,  3.0000e+00,  3.2500e+00,  2.7656e+00,\n",
      "         3.0625e+00,  2.8594e+00,  2.3594e+00,  2.9219e+00,  2.9688e+00,\n",
      "         2.9062e+00,  3.0469e+00,  2.7188e+00,  2.7969e+00,  2.9688e+00,\n",
      "         2.5625e+00,  2.7656e+00,  3.0469e+00,  2.9375e+00,  2.9688e+00,\n",
      "         3.1094e+00,  2.9062e+00,  3.0469e+00,  2.6406e+00,  3.0625e+00,\n",
      "         2.6406e+00,  2.8125e+00,  2.3125e+00,  2.7656e+00,  3.0000e+00,\n",
      "         2.6250e+00,  2.7188e+00,  2.7344e+00,  2.8438e+00,  2.5781e+00,\n",
      "         2.7500e+00,  2.9062e+00,  2.7656e+00,  3.0469e+00,  3.1094e+00,\n",
      "         2.6562e+00,  2.7969e+00,  3.0625e+00,  2.8906e+00,  3.5156e+00,\n",
      "         2.8281e+00,  2.7812e+00,  2.8438e+00,  2.8125e+00,  2.9062e+00,\n",
      "         2.7031e+00,  2.8750e+00,  2.9531e+00,  2.8281e+00,  2.8125e+00,\n",
      "         2.0781e+00,  2.8281e+00,  2.9062e+00,  3.0000e+00,  2.8125e+00,\n",
      "         2.7812e+00,  2.7812e+00,  2.7031e+00,  2.7031e+00,  3.0781e+00,\n",
      "         2.7969e+00,  2.9219e+00,  2.9219e+00,  2.9531e+00,  2.6406e+00,\n",
      "         2.7656e+00,  2.7188e+00,  2.7500e+00,  2.5312e+00,  2.7188e+00,\n",
      "         2.7500e+00,  2.0625e+00,  2.8750e+00,  2.9219e+00,  3.0312e+00,\n",
      "         2.6094e+00,  2.7031e+00,  2.8438e+00,  3.0312e+00,  3.0469e+00,\n",
      "         2.8438e+00,  2.9219e+00,  2.7031e+00,  2.9375e+00,  2.7656e+00,\n",
      "         3.3281e+00,  2.5781e+00,  3.2500e+00,  2.9062e+00,  2.7500e+00,\n",
      "         2.4531e+00,  2.8438e+00,  2.6406e+00,  2.6250e+00,  2.6875e+00,\n",
      "         2.8125e+00,  2.8281e+00,  2.9844e+00,  2.6719e+00,  2.7188e+00,\n",
      "         2.5938e+00,  2.8438e+00,  3.0469e+00, -1.1414e-02,  2.7812e+00,\n",
      "         2.9531e+00,  2.5469e+00,  2.6562e+00,  2.6250e+00,  2.7812e+00,\n",
      "         2.6250e+00,  2.9531e+00,  2.7188e+00,  3.0938e+00,  3.0781e+00,\n",
      "         3.4219e+00,  2.6875e+00,  2.9688e+00,  2.7969e+00,  2.0625e+00,\n",
      "         2.6719e+00,  2.7031e+00,  2.7500e+00,  3.0312e+00,  3.0312e+00,\n",
      "         2.8438e+00,  2.8125e+00,  3.0312e+00,  2.8750e+00,  2.8125e+00,\n",
      "         2.7344e+00,  2.6406e+00,  2.5938e+00,  2.7656e+00,  3.0000e+00,\n",
      "         2.8125e+00,  2.8438e+00,  2.7969e+00,  2.5938e+00,  2.8750e+00,\n",
      "         2.9531e+00,  2.8438e+00,  2.9375e+00,  2.2812e+00,  2.9531e+00,\n",
      "         2.7188e+00,  2.6719e+00,  2.7344e+00,  2.8750e+00,  2.5312e+00,\n",
      "         2.5469e+00,  3.0000e+00,  2.5781e+00,  2.6406e+00,  2.8594e+00,\n",
      "         3.2812e+00,  2.7031e+00,  2.7344e+00,  2.7031e+00,  2.7500e+00,\n",
      "         2.7812e+00,  2.7500e+00,  2.5625e+00,  2.9375e+00,  2.3906e+00,\n",
      "         2.9062e+00,  3.0156e+00,  3.0000e+00,  2.7500e+00,  3.2656e+00,\n",
      "         2.8594e+00,  2.8594e+00,  1.5781e+00,  2.7969e+00,  2.7344e+00,\n",
      "         2.7344e+00,  2.6875e+00,  2.5938e+00,  3.0938e+00,  2.5781e+00,\n",
      "         2.6406e+00,  2.7500e+00,  2.7812e+00,  2.9062e+00,  2.6094e+00,\n",
      "         3.0312e+00,  2.7969e+00,  2.9219e+00,  2.5781e+00,  2.7188e+00,\n",
      "         2.7188e+00,  2.6719e+00,  2.7656e+00,  2.6562e+00,  2.9219e+00,\n",
      "         2.5625e+00,  3.1719e+00,  2.5781e+00,  2.7500e+00,  2.8906e+00,\n",
      "         2.9062e+00,  3.0312e+00,  2.5625e+00,  2.9375e+00,  7.5391e-01,\n",
      "         3.0156e+00,  2.6562e+00,  2.7188e+00,  2.7344e+00,  2.8125e+00,\n",
      "         2.7031e+00,  1.4219e+00,  2.7500e+00,  2.9688e+00,  2.9375e+00,\n",
      "         2.6406e+00,  2.9844e+00,  2.7344e+00,  2.8906e+00,  2.8438e+00,\n",
      "         2.7188e+00,  3.0312e+00,  2.8750e+00,  3.3906e+00,  3.0469e+00,\n",
      "         4.0625e+00,  2.8125e+00,  2.7812e+00,  2.5781e+00,  2.9375e+00,\n",
      "         2.6875e+00,  2.6562e+00,  2.8750e+00,  2.6094e+00,  2.8906e+00,\n",
      "         2.7656e+00,  2.8594e+00,  2.9531e+00,  2.7656e+00,  2.8594e+00,\n",
      "         2.5000e+00,  2.8125e+00,  3.1719e+00,  2.7500e+00,  2.7500e+00,\n",
      "         2.8906e+00,  3.1719e+00,  2.8906e+00,  3.0625e+00,  2.7969e+00,\n",
      "         2.9062e+00,  2.6719e+00,  2.5938e+00,  2.8594e+00,  2.9375e+00,\n",
      "         2.7344e+00,  2.6562e+00,  2.8438e+00,  2.3906e+00,  2.9844e+00,\n",
      "         2.8906e+00,  2.9844e+00,  3.0938e+00,  2.7500e+00,  2.8281e+00,\n",
      "         6.0156e-01,  2.7656e+00,  2.9219e+00,  2.6875e+00,  2.9219e+00,\n",
      "         2.7344e+00,  2.8281e+00,  3.0469e+00,  2.7500e+00,  3.1250e+00,\n",
      "         2.6875e+00,  2.6562e+00,  2.6562e+00,  2.8750e+00,  2.6562e+00,\n",
      "         2.9688e+00,  2.4531e+00,  2.8906e+00,  2.9688e+00,  3.0312e+00,\n",
      "         2.7031e+00,  2.8438e+00,  7.6172e-01,  2.8281e+00,  2.8438e+00,\n",
      "         2.7969e+00,  2.9219e+00,  3.0781e+00,  2.9688e+00,  2.6250e+00,\n",
      "         2.5000e+00,  2.6875e+00,  2.8125e+00,  2.7500e+00,  2.1094e+00,\n",
      "         2.9531e+00,  2.4844e+00,  3.0156e+00,  2.9219e+00,  2.7969e+00,\n",
      "         2.6875e+00,  2.8438e+00,  2.8750e+00,  2.8125e+00,  2.5312e+00,\n",
      "         2.7188e+00,  2.7969e+00,  2.7031e+00,  2.9688e+00,  2.8594e+00,\n",
      "         2.8594e+00,  2.8906e+00,  2.7188e+00,  2.7031e+00,  2.7500e+00,\n",
      "         3.0156e+00,  2.7344e+00,  2.7188e+00,  2.4688e+00,  3.1562e+00,\n",
      "         2.9531e+00,  2.7812e+00,  2.6250e+00,  2.7344e+00,  2.9219e+00,\n",
      "         2.7344e+00,  2.9375e+00,  3.1875e+00,  2.3594e+00,  3.0312e+00,\n",
      "         2.6406e+00,  2.5156e+00,  2.5156e+00,  2.6406e+00,  2.8750e+00,\n",
      "         2.9375e+00,  2.9688e+00,  1.7212e-02,  2.9375e+00,  3.2500e+00,\n",
      "         2.9375e+00,  2.4844e+00,  3.2344e+00,  2.7188e+00,  2.7500e+00,\n",
      "         2.8125e+00,  3.0156e+00,  2.6094e+00,  2.8750e+00,  2.9688e+00,\n",
      "         2.9219e+00,  2.7812e+00,  2.6562e+00,  2.7344e+00,  2.8906e+00,\n",
      "         2.8906e+00,  2.7500e+00,  2.5469e+00,  2.8594e+00,  2.6406e+00,\n",
      "         2.7500e+00,  2.5469e+00,  3.1250e+00,  2.7188e+00,  2.9219e+00,\n",
      "         2.8906e+00,  2.8594e+00,  2.9375e+00,  2.5938e+00,  2.7031e+00,\n",
      "         2.7500e+00,  2.6562e+00,  2.7500e+00,  2.8750e+00,  2.7969e+00,\n",
      "         2.8594e+00,  3.0000e+00,  2.7031e+00,  2.6875e+00,  3.1094e+00,\n",
      "         2.7188e+00,  2.8750e+00,  2.8594e+00,  3.0469e+00,  2.6250e+00,\n",
      "         2.7344e+00,  2.5625e+00,  2.7656e+00,  2.6562e+00,  2.8125e+00,\n",
      "         2.6875e+00,  2.6562e+00,  2.7500e+00,  2.7188e+00,  2.7656e+00,\n",
      "         2.7500e+00,  2.8281e+00,  2.7344e+00,  1.7891e+00,  2.6406e+00,\n",
      "         7.5391e-01,  2.7188e+00,  2.8750e+00,  1.7812e+00,  3.1094e+00,\n",
      "         2.7500e+00,  2.8906e+00,  2.8750e+00,  2.8438e+00,  3.4219e+00,\n",
      "         2.6875e+00,  2.7500e+00,  3.1094e+00,  2.5938e+00,  2.6094e+00,\n",
      "         2.6562e+00,  2.8438e+00,  2.7031e+00,  2.9062e+00,  2.6250e+00,\n",
      "         2.7969e+00,  2.8594e+00,  2.2812e+00,  2.8438e+00,  2.6406e+00,\n",
      "         2.6094e+00,  2.6562e+00,  2.8594e+00,  7.9688e-01,  2.7969e+00,\n",
      "         2.7188e+00,  2.7969e+00,  2.7188e+00,  2.8594e+00,  2.6562e+00,\n",
      "         2.8594e+00,  2.7188e+00,  1.8125e+00,  2.7344e+00,  2.8438e+00,\n",
      "         2.9375e+00,  2.7500e+00,  2.7812e+00,  2.7969e+00,  2.7812e+00,\n",
      "         3.0781e+00,  2.8594e+00,  2.8594e+00,  2.6562e+00,  2.7656e+00,\n",
      "         2.8750e+00,  2.5938e+00,  2.0156e+00,  2.8906e+00,  2.7500e+00,\n",
      "         2.6875e+00,  2.7344e+00,  2.7812e+00,  2.8750e+00,  2.7500e+00,\n",
      "         3.0000e+00,  2.9844e+00,  2.8281e+00,  2.7500e+00,  3.5400e-02,\n",
      "         2.8438e+00,  2.6406e+00,  6.4844e-01,  2.9062e+00,  2.8125e+00,\n",
      "         2.9219e+00,  2.6719e+00,  2.7188e+00,  2.7500e+00,  2.8906e+00,\n",
      "         3.9062e+00,  2.8594e+00,  2.8125e+00,  3.0000e+00,  3.0000e+00,\n",
      "         2.8125e+00,  2.3906e+00,  2.9062e+00,  2.8750e+00,  2.7031e+00,\n",
      "         2.6719e+00,  2.8125e+00,  2.7500e+00,  2.8438e+00,  2.9062e+00,\n",
      "         2.6719e+00,  2.8906e+00,  3.0781e+00,  2.7031e+00,  2.4688e+00,\n",
      "         2.8438e+00,  2.7188e+00,  2.5938e+00,  2.9688e+00,  2.9062e+00,\n",
      "         2.8281e+00,  2.7656e+00,  2.7812e+00,  2.7656e+00,  2.9062e+00,\n",
      "         2.6406e+00,  3.0312e+00,  2.6562e+00,  2.8906e+00,  2.6250e+00,\n",
      "         2.4844e+00,  2.7031e+00,  2.7344e+00,  2.6875e+00,  2.6406e+00,\n",
      "         2.7188e+00,  2.9062e+00,  2.8125e+00,  2.7656e+00,  2.7188e+00,\n",
      "         2.6562e+00,  2.9688e+00,  2.7031e+00,  2.6719e+00,  2.6719e+00,\n",
      "         2.7812e+00,  2.7812e+00,  2.8281e+00,  2.7188e+00,  2.8594e+00,\n",
      "         2.6094e+00,  2.8906e+00,  2.7656e+00,  2.6562e+00,  2.7656e+00,\n",
      "         2.7344e+00,  2.6406e+00,  2.9531e+00,  2.5781e+00,  2.7656e+00,\n",
      "         2.7344e+00,  2.7812e+00,  1.0859e+00,  3.6719e+00,  2.8125e+00,\n",
      "         2.7500e+00,  2.8281e+00,  2.6250e+00,  2.8906e+00,  2.7031e+00,\n",
      "         2.6719e+00,  2.7344e+00,  2.7969e+00,  3.0625e+00,  3.2500e+00,\n",
      "         2.5938e+00,  2.7969e+00,  2.8438e+00,  2.8594e+00,  1.3504e-03,\n",
      "         2.5000e+00,  2.7812e+00,  2.8750e+00,  2.7656e+00,  2.9688e+00,\n",
      "         2.8125e+00,  2.8594e+00,  2.7500e+00,  2.9688e+00,  2.7500e+00,\n",
      "         2.8594e+00,  2.8125e+00,  2.8594e+00,  2.6094e+00,  2.5781e+00,\n",
      "         2.8594e+00,  2.8281e+00,  2.8125e+00,  3.8281e+00,  3.0625e+00,\n",
      "         2.4531e+00,  2.7812e+00,  2.6875e+00,  2.6719e+00,  2.9219e+00,\n",
      "         2.8125e+00,  2.7656e+00,  2.3750e+00,  3.0938e+00,  2.9375e+00,\n",
      "         2.8438e+00,  2.7656e+00,  2.7344e+00,  2.6719e+00,  2.8281e+00,\n",
      "         3.0000e+00,  2.8750e+00,  1.8516e+00,  2.6094e+00,  2.8594e+00,\n",
      "         2.7344e+00,  2.6094e+00,  2.8594e+00,  3.3438e+00,  2.8750e+00,\n",
      "         2.9375e+00,  3.1094e+00,  2.7812e+00,  2.8594e+00,  2.5781e+00,\n",
      "         2.4688e+00,  2.7500e+00,  2.5312e+00,  3.2031e+00,  2.8281e+00,\n",
      "         2.7188e+00,  3.0781e+00,  2.8906e+00,  2.7812e+00,  2.7656e+00,\n",
      "         2.7344e+00,  2.5000e+00,  2.6875e+00,  2.8125e+00,  2.5156e+00,\n",
      "         2.9375e+00,  2.9062e+00,  2.7656e+00,  2.9062e+00,  2.8750e+00,\n",
      "         2.2344e+00,  3.0625e+00,  2.7812e+00,  3.0000e+00,  3.0156e+00,\n",
      "         2.9531e+00,  2.6406e+00,  3.0000e+00,  2.9531e+00,  3.2344e+00,\n",
      "         2.7188e+00,  2.6875e+00,  2.3906e+00,  2.7188e+00,  2.7188e+00,\n",
      "         2.7188e+00,  2.8750e+00,  3.0156e+00,  2.9688e+00,  2.9062e+00,\n",
      "         2.5938e+00,  2.8906e+00,  3.1094e+00,  2.3906e+00,  2.7500e+00,\n",
      "         2.9062e+00,  2.5312e+00,  2.7812e+00,  2.7344e+00,  3.0000e+00,\n",
      "         2.8438e+00,  2.8125e+00,  3.0781e+00,  2.5469e+00,  2.6875e+00,\n",
      "         2.7188e+00,  2.5469e+00,  2.6250e+00,  2.9688e+00,  2.6250e+00,\n",
      "         2.9531e+00,  2.9062e+00,  2.4688e+00,  2.5781e+00,  2.7031e+00,\n",
      "         2.5156e+00,  2.8906e+00,  2.7812e+00,  3.0312e+00,  2.7344e+00,\n",
      "         2.5469e+00,  2.6562e+00,  2.5312e+00,  2.6094e+00,  2.6094e+00,\n",
      "         2.9531e+00,  2.8125e+00,  2.8906e+00,  2.8438e+00,  2.7812e+00,\n",
      "         2.6406e+00,  2.6406e+00,  2.7188e+00,  2.6406e+00,  2.2969e+00,\n",
      "         2.3125e+00,  2.6875e+00,  2.6250e+00,  2.6094e+00,  2.9219e+00,\n",
      "         2.7656e+00,  2.9375e+00,  3.2188e+00,  2.8906e+00,  2.7500e+00,\n",
      "         2.3594e+00,  2.7188e+00,  2.6719e+00,  2.6250e+00,  2.8438e+00,\n",
      "         2.7188e+00,  2.9375e+00,  2.7812e+00,  2.4062e+00,  2.5938e+00,\n",
      "         2.7188e+00,  2.7188e+00,  2.7188e+00,  2.7031e+00,  2.7656e+00,\n",
      "         2.7344e+00,  2.9531e+00,  3.0312e+00,  3.0938e+00,  2.7812e+00,\n",
      "         2.4844e+00,  2.6875e+00,  2.6406e+00,  2.7344e+00,  3.1250e+00,\n",
      "         3.0000e+00,  2.6562e+00,  2.9062e+00,  2.6875e+00,  2.9688e+00,\n",
      "         2.7812e+00,  2.7812e+00,  2.5312e+00,  2.5625e+00,  2.7812e+00,\n",
      "         2.7031e+00,  2.5938e+00,  2.7344e+00,  2.9375e+00,  2.5938e+00,\n",
      "         2.6719e+00,  3.0781e+00,  3.2031e+00], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.decoder.block.7.layer.0.SelfAttention.q.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0566,  0.0062, -0.0300,  ...,  0.0083, -0.0011,  0.0172],\n",
      "        [ 0.0043,  0.0137,  0.0447,  ...,  0.0593,  0.0157,  0.0173],\n",
      "        [-0.0260, -0.0359,  0.0728,  ...,  0.0197,  0.0190,  0.0071],\n",
      "        ...,\n",
      "        [-0.0469, -0.0192,  0.0009,  ...,  0.0292, -0.0024, -0.0425],\n",
      "        [-0.0635, -0.0259, -0.0065,  ...,  0.0048, -0.0047, -0.0267],\n",
      "        [-0.0082,  0.0199, -0.0588,  ...,  0.0267, -0.0192,  0.0049]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.7.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.2373, -0.3965,  0.0195,  ..., -0.1230,  0.3672, -0.1426],\n",
      "        [-0.1289, -0.0874,  0.0143,  ...,  0.0491,  0.0615,  0.1035],\n",
      "        [-0.2930,  0.3594, -0.1797,  ...,  0.0262,  0.1631,  0.1089],\n",
      "        ...,\n",
      "        [ 0.0219, -0.4336, -0.0070,  ...,  0.0801,  0.0270, -0.3086],\n",
      "        [-0.2832, -0.0549, -0.1602,  ..., -0.0859, -0.3633,  0.1138],\n",
      "        [-0.3867, -0.8398, -0.0098,  ...,  0.0640, -0.3652, -0.0420]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.7.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.3477,  0.0898, -0.5195,  ..., -1.0703, -0.1934,  0.7734],\n",
      "        [-0.3105, -0.0981, -0.5625,  ...,  1.4844, -1.1172, -0.7656],\n",
      "        [-1.6562,  0.7578,  0.5508,  ..., -1.5703, -0.8711,  0.0815],\n",
      "        ...,\n",
      "        [ 0.0903, -0.0903,  0.8398,  ...,  0.0135,  0.2217, -0.2793],\n",
      "        [-0.6211, -0.0854,  0.1494,  ..., -0.3848, -0.2578, -0.3223],\n",
      "        [-0.1719, -0.3750,  0.7109,  ...,  0.6758, -0.4668,  0.6016]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.7.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.6680, -0.4355, -2.1719,  ...,  0.7227, -1.0469, -0.5820],\n",
      "        [-0.5664, -0.8828,  0.9805,  ...,  0.3262, -0.3555, -0.3457],\n",
      "        [-1.5469,  0.7969,  0.3340,  ...,  0.4219,  0.5898, -0.2656],\n",
      "        ...,\n",
      "        [-0.9180,  0.1650, -3.8125,  ...,  0.0601, -1.6484, -0.8438],\n",
      "        [-3.1719,  0.5430, -0.1738,  ..., -0.3711, -0.1475, -1.1953],\n",
      "        [-0.2471,  1.2812,  1.2812,  ...,  0.9727, -1.2891,  0.9375]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.7.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 2.1777e-01,  1.5430e-01,  1.8555e-01,  1.7188e-01,  2.5586e-01,\n",
      "         1.8945e-01,  2.4512e-01,  2.5586e-01,  1.9238e-01,  2.5195e-01,\n",
      "        -1.7285e-01,  2.5391e-01,  1.4355e-01,  1.7383e-01,  2.4805e-01,\n",
      "         2.5195e-01,  2.1484e-01,  2.1973e-01,  1.9238e-01,  2.6367e-01,\n",
      "         2.3535e-01,  2.5000e-01,  2.3145e-01,  2.5391e-01,  2.4609e-01,\n",
      "         2.2070e-01,  2.4121e-01,  1.7090e-01,  2.5781e-01,  2.5781e-01,\n",
      "         2.5391e-01,  2.3047e-01,  2.5977e-01,  2.5586e-01,  2.3633e-01,\n",
      "         2.2070e-01,  2.5977e-01,  2.2070e-01,  2.4805e-01,  2.5391e-01,\n",
      "         2.2461e-01,  2.2949e-01,  2.3242e-01,  2.3730e-01,  2.0898e-01,\n",
      "         2.3047e-01,  2.5195e-01,  2.4609e-01,  2.5391e-01,  2.5195e-01,\n",
      "         2.3242e-01,  2.3340e-01,  2.3438e-01,  2.5781e-01,  2.9297e-01,\n",
      "         2.0020e-01,  2.0020e-01,  2.6172e-01,  2.5586e-01,  2.4023e-01,\n",
      "         2.2949e-01,  2.6758e-01,  2.4609e-01,  2.4805e-01,  2.5977e-01,\n",
      "         2.2656e-01,  2.4121e-01,  2.3535e-01,  1.8555e-01,  2.4512e-01,\n",
      "         2.6172e-01,  2.5586e-01,  2.5781e-01,  2.4609e-01,  2.4609e-01,\n",
      "         2.5000e-01,  1.7090e-01,  2.2461e-01,  2.5781e-01,  2.4902e-01,\n",
      "         2.5586e-01,  2.2266e-01,  2.5586e-01,  2.0215e-01,  2.4805e-01,\n",
      "         2.5781e-01, -9.7656e-02,  2.4219e-01,  2.6562e-01,  2.4512e-01,\n",
      "         2.2559e-01,  2.3828e-01,  2.2656e-01,  1.9922e-01,  2.5586e-01,\n",
      "         2.6562e-01,  2.5195e-01,  2.4609e-01,  2.8320e-01,  2.3730e-01,\n",
      "         2.0410e-01,  2.2656e-01,  1.6309e-01,  2.5195e-01,  2.4023e-01,\n",
      "         1.8652e-01,  2.4414e-01,  2.3828e-01,  2.5977e-01,  2.5000e-01,\n",
      "         2.6367e-01,  2.5977e-01,  2.1484e-01,  2.3242e-01,  2.3828e-01,\n",
      "         2.5586e-01,  2.3730e-01,  2.2949e-01,  4.6387e-02,  2.5781e-01,\n",
      "         2.6562e-01,  2.5000e-01,  2.2266e-01,  2.5195e-01,  2.5977e-01,\n",
      "         2.3633e-01,  2.4316e-01,  2.1094e-01,  2.1973e-01,  1.7676e-01,\n",
      "         1.7285e-01,  2.3242e-01,  2.3730e-01,  2.4902e-01,  1.3281e-01,\n",
      "         2.2070e-01,  2.3926e-01,  2.1777e-01,  2.1777e-01,  1.9141e-01,\n",
      "         2.1387e-01,  2.5391e-01,  2.2559e-01,  2.3926e-01,  2.0996e-01,\n",
      "         2.0117e-01,  2.4707e-01,  2.1289e-01,  2.5195e-01,  2.1387e-01,\n",
      "         2.7148e-01,  2.3438e-01,  2.7148e-01,  2.1680e-01,  2.5977e-01,\n",
      "         2.7344e-01,  2.4219e-01,  2.7148e-01,  7.0801e-02,  1.5039e-01,\n",
      "         2.4219e-01,  2.1191e-01,  2.0703e-01,  2.2070e-01,  2.3438e-01,\n",
      "         1.7676e-01,  2.6758e-01,  1.9336e-01,  1.8848e-01,  2.4805e-01,\n",
      "         1.4941e-01,  2.4512e-01,  2.6367e-01,  2.3730e-01,  2.4805e-01,\n",
      "         2.4414e-01,  2.3926e-01,  2.3926e-01,  1.3477e-01,  2.2363e-01,\n",
      "         2.4805e-01,  2.3730e-01,  2.6758e-01,  2.6562e-01,  2.0801e-01,\n",
      "         2.3730e-01,  2.4121e-01,  4.3457e-02,  2.3828e-01, -1.8164e-01,\n",
      "         2.3535e-01,  2.4316e-01,  2.3926e-01,  2.5391e-01,  2.3535e-01,\n",
      "         2.2754e-01,  2.4121e-01,  2.6367e-01,  2.5977e-01,  2.2266e-01,\n",
      "         2.6562e-01,  2.2168e-01,  2.5000e-01,  2.3340e-01,  2.4902e-01,\n",
      "         1.8652e-01,  2.3340e-01,  2.4219e-01,  2.3535e-01,  2.7539e-01,\n",
      "         2.3730e-01,  1.9531e-01,  2.1875e-01,  2.4609e-01,  2.4902e-01,\n",
      "         2.5586e-01,  2.3535e-01,  2.3828e-01,  2.4805e-01,  6.4392e-03,\n",
      "         1.7480e-01,  2.5000e-01,  2.4316e-01,  2.3633e-01,  2.1875e-01,\n",
      "         2.5781e-01,  4.5117e-01,  2.5000e-01,  2.3828e-01,  2.6562e-01,\n",
      "         2.3145e-01,  2.3340e-01,  2.3438e-01,  2.7148e-01,  2.4805e-01,\n",
      "         2.1484e-01,  1.9629e-01,  2.1582e-01,  1.6895e-01,  2.4121e-01,\n",
      "        -2.2070e-01,  2.4121e-01,  2.3926e-01,  2.3633e-01,  2.5391e-01,\n",
      "         2.3535e-01,  2.4805e-01,  2.2363e-01,  2.5391e-01,  2.4707e-01,\n",
      "         2.2559e-01,  2.4023e-01,  2.4902e-01,  2.5391e-01,  2.4902e-01,\n",
      "         2.2754e-01,  2.7539e-01, -1.6406e-01,  2.3145e-01,  2.2656e-01,\n",
      "         2.4805e-01,  1.7871e-01,  2.6367e-01,  2.6758e-01,  2.4316e-01,\n",
      "         2.3633e-01,  2.4609e-01,  2.5391e-01,  2.4219e-01,  2.4512e-01,\n",
      "         2.3730e-01,  2.1094e-01,  2.4512e-01,  2.3047e-01,  1.2354e-01,\n",
      "         2.4316e-01,  2.4609e-01,  2.3340e-01,  2.3828e-01,  1.9727e-01,\n",
      "         3.8818e-02,  1.8457e-01,  2.5000e-01,  2.3926e-01,  2.4512e-01,\n",
      "         2.3730e-01,  2.4609e-01,  2.7930e-01,  1.8359e-01,  1.8750e-01,\n",
      "         2.3633e-01,  2.5781e-01,  2.4219e-01,  2.5391e-01,  2.3535e-01,\n",
      "         2.6953e-01,  1.7480e-01,  2.5586e-01,  2.5391e-01,  2.6758e-01,\n",
      "         2.3438e-01,  2.4219e-01, -3.9307e-02,  2.3242e-01,  2.4707e-01,\n",
      "         2.6172e-01,  2.1289e-01,  2.5195e-01,  2.2852e-01,  2.2949e-01,\n",
      "         2.4414e-01,  2.4219e-01,  2.3535e-01,  2.2363e-01,  2.6758e-01,\n",
      "         2.2754e-01,  2.5195e-01,  2.4023e-01,  2.6172e-01,  2.4414e-01,\n",
      "         2.5195e-01,  2.4902e-01,  2.0508e-01,  2.6367e-01,  2.2949e-01,\n",
      "         2.2559e-01,  2.1875e-01,  2.4219e-01,  2.3438e-01,  2.4121e-01,\n",
      "         2.4219e-01,  2.0898e-01,  2.4121e-01,  2.3340e-01,  2.5000e-01,\n",
      "         2.4609e-01,  2.6758e-01,  2.8125e-01,  2.2754e-01,  1.7090e-01,\n",
      "         2.2070e-01,  2.4414e-01,  2.4121e-01,  2.4023e-01,  2.5195e-01,\n",
      "         2.3242e-01,  2.5977e-01,  2.0801e-01,  1.5918e-01,  2.3438e-01,\n",
      "         2.3926e-01,  2.4609e-01,  2.1289e-01,  2.4121e-01,  2.2363e-01,\n",
      "         2.6758e-01,  2.5195e-01,  8.2031e-02,  2.4707e-01,  2.5000e-01,\n",
      "         2.5195e-01,  2.0410e-01,  1.6992e-01,  2.1875e-01,  2.5977e-01,\n",
      "         2.5781e-01,  2.3242e-01,  2.3730e-01,  2.4414e-01,  2.2363e-01,\n",
      "         2.6758e-01,  2.4316e-01,  2.4805e-01,  2.3340e-01,  2.5000e-01,\n",
      "         2.4902e-01,  2.4707e-01,  2.7734e-01,  2.5195e-01,  1.7871e-01,\n",
      "         2.3145e-01,  2.2461e-01, -1.6602e-01,  2.2754e-01,  2.6367e-01,\n",
      "         2.5586e-01,  2.5781e-01,  2.5000e-01,  2.3730e-01,  2.5000e-01,\n",
      "         2.4902e-01,  2.3340e-01,  2.1094e-01,  2.0801e-01,  2.3730e-01,\n",
      "         2.6172e-01,  2.7148e-01,  2.3145e-01,  2.3730e-01,  2.3535e-01,\n",
      "         2.4805e-01,  2.3828e-01,  2.6367e-01,  2.6367e-01,  2.2266e-01,\n",
      "         2.5000e-01,  1.7578e-01,  2.3242e-01,  2.4316e-01,  2.5000e-01,\n",
      "         2.4707e-01,  2.4414e-01,  2.4023e-01,  2.3438e-01,  2.5195e-01,\n",
      "         2.5391e-01,  2.3340e-01,  2.4707e-01,  3.0859e-01,  2.5977e-01,\n",
      "         4.7112e-04,  2.4512e-01,  2.5391e-01,  8.4961e-02,  2.5195e-01,\n",
      "         2.6172e-01,  1.9629e-01,  2.3828e-01,  2.3242e-01,  1.9141e-01,\n",
      "         2.3438e-01,  2.4414e-01,  2.5195e-01,  2.2559e-01,  2.2754e-01,\n",
      "         2.2363e-01,  2.4609e-01,  2.6172e-01,  2.5195e-01,  2.3242e-01,\n",
      "         2.5391e-01,  2.3145e-01,  2.1289e-01,  2.6562e-01,  2.4512e-01,\n",
      "         2.3340e-01,  2.4121e-01,  2.4805e-01, -2.6978e-02,  2.3535e-01,\n",
      "         2.3438e-01,  2.5781e-01,  2.4902e-01,  2.6172e-01,  2.3242e-01,\n",
      "         2.1387e-01,  2.0801e-01,  4.5312e-01,  2.2559e-01,  2.4512e-01,\n",
      "         2.5195e-01,  2.5391e-01,  2.4707e-01,  2.3730e-01,  2.2754e-01,\n",
      "         2.6172e-01,  1.8066e-01,  2.4512e-01,  2.3828e-01,  2.4414e-01,\n",
      "         2.5195e-01,  2.4316e-01,  1.6016e-01,  2.7148e-01,  2.4707e-01,\n",
      "         2.5781e-01,  2.3340e-01,  2.7148e-01,  2.4805e-01,  2.3340e-01,\n",
      "         1.9531e-01,  2.4121e-01,  2.4902e-01,  2.3828e-01,  9.1797e-02,\n",
      "         2.3633e-01,  2.2949e-01, -7.4387e-04,  2.5781e-01,  2.3145e-01,\n",
      "         2.5000e-01,  2.4414e-01,  2.4219e-01,  2.5781e-01,  2.4414e-01,\n",
      "         9.6680e-02,  2.3828e-01,  2.3242e-01,  2.6758e-01,  2.4219e-01,\n",
      "         2.5000e-01,  2.3145e-01,  2.5195e-01,  2.4414e-01,  2.3633e-01,\n",
      "         2.4512e-01,  2.5195e-01,  2.0312e-01,  2.5195e-01,  2.5195e-01,\n",
      "         2.2461e-01,  2.5781e-01,  2.2852e-01,  2.3535e-01,  2.1680e-01,\n",
      "         2.6562e-01,  2.1289e-01,  2.2656e-01, -1.7383e-01,  2.5195e-01,\n",
      "         2.3438e-01,  2.3047e-01,  2.4805e-01,  2.5781e-01,  2.3633e-01,\n",
      "         2.3828e-01,  2.2949e-01,  2.2070e-01,  2.4512e-01,  2.3340e-01,\n",
      "         2.0020e-01,  2.3535e-01,  2.2070e-01,  2.5781e-01,  2.3242e-01,\n",
      "         2.5781e-01,  2.5000e-01,  2.4414e-01,  2.4316e-01,  2.2949e-01,\n",
      "         2.3633e-01,  2.5977e-01,  2.1875e-01,  2.2656e-01,  2.2266e-01,\n",
      "         2.5586e-01,  2.3340e-01,  2.4609e-01,  2.4023e-01,  2.2754e-01,\n",
      "         1.9434e-01,  2.1191e-01,  2.3926e-01,  2.0801e-01,  2.5195e-01,\n",
      "         2.5000e-01,  2.1191e-01,  2.5586e-01,  2.0801e-01,  2.3633e-01,\n",
      "         2.4023e-01,  2.2656e-01,  1.3086e-01,  2.5586e-01,  2.5195e-01,\n",
      "         2.3926e-01,  2.4805e-01,  2.3047e-01,  2.0996e-01,  2.5195e-01,\n",
      "         2.4512e-01,  2.4805e-01,  2.4609e-01,  2.3145e-01,  1.8066e-01,\n",
      "         2.2168e-01,  2.7930e-01,  2.5977e-01,  2.5000e-01,  2.6512e-04,\n",
      "         2.1582e-01,  2.7148e-01,  2.5586e-01,  2.4023e-01,  2.5000e-01,\n",
      "         2.4609e-01,  2.3828e-01,  2.3828e-01,  2.5195e-01,  2.5195e-01,\n",
      "         2.5781e-01,  2.5586e-01,  2.6367e-01,  2.2559e-01,  2.3242e-01,\n",
      "         2.6172e-01,  2.0117e-01,  2.4316e-01,  2.0996e-01,  2.4512e-01,\n",
      "         1.9629e-01,  2.5586e-01,  2.4219e-01,  2.1680e-01,  2.3535e-01,\n",
      "         2.5391e-01,  2.6172e-01,  1.6504e-01,  2.4805e-01,  2.3340e-01,\n",
      "         2.2559e-01,  2.4512e-01,  2.3730e-01,  2.5000e-01,  2.2656e-01,\n",
      "         2.4609e-01,  2.3926e-01,  2.6758e-01,  2.5781e-01,  2.4219e-01,\n",
      "         2.3730e-01,  2.3438e-01,  2.5391e-01,  1.8359e-01,  2.2949e-01,\n",
      "         2.4219e-01,  2.3340e-01,  2.2461e-01,  2.4219e-01,  2.2656e-01,\n",
      "         2.2168e-01,  2.4707e-01,  2.1680e-01,  2.3730e-01,  2.5195e-01,\n",
      "         2.4219e-01,  2.1680e-01,  2.4023e-01,  2.5195e-01,  2.3730e-01,\n",
      "         1.8652e-01,  2.3535e-01,  2.5586e-01,  2.5781e-01,  1.7480e-01,\n",
      "         2.2363e-01,  1.7090e-01,  2.5977e-01,  2.1973e-01,  2.7148e-01,\n",
      "         1.7090e-01,  2.6758e-01,  2.2852e-01,  2.2949e-01,  2.5781e-01,\n",
      "         1.6016e-01,  2.3047e-01,  1.8457e-01,  2.3535e-01,  2.2168e-01,\n",
      "         2.5977e-01,  2.0312e-01,  2.1387e-01,  2.4219e-01,  2.3242e-01,\n",
      "         2.2754e-01,  2.5391e-01,  2.6758e-01,  2.3438e-01,  2.6172e-01,\n",
      "         2.4609e-01,  2.4707e-01,  2.5000e-01,  2.0801e-01,  2.4219e-01,\n",
      "         2.6562e-01,  2.2559e-01,  2.5586e-01,  2.3633e-01,  2.5391e-01,\n",
      "         2.5781e-01,  2.5195e-01,  2.1484e-01,  2.3145e-01,  2.3242e-01,\n",
      "         2.2168e-01,  2.3145e-01,  2.3340e-01,  2.5977e-01,  2.3242e-01,\n",
      "         2.5977e-01,  2.4609e-01,  1.8262e-01,  2.4023e-01,  2.5391e-01,\n",
      "         2.4512e-01,  2.5391e-01,  2.6953e-01,  2.4902e-01,  2.3340e-01,\n",
      "         2.4219e-01,  2.4414e-01,  2.2754e-01,  1.3184e-01,  2.4121e-01,\n",
      "         2.5195e-01,  2.4512e-01,  1.6992e-01,  1.9629e-01,  2.5586e-01,\n",
      "         2.2559e-01,  2.4316e-01,  2.0996e-01,  2.3633e-01,  1.3477e-01,\n",
      "         2.6953e-01,  2.2461e-01,  2.5000e-01,  2.3242e-01,  2.0605e-01,\n",
      "         2.4902e-01,  2.3047e-01,  1.9629e-01,  2.6562e-01,  2.4609e-01,\n",
      "         1.5625e-01,  2.1875e-01,  2.3535e-01,  2.4609e-01,  2.4707e-01,\n",
      "         2.5977e-01,  2.4219e-01,  2.5586e-01,  9.4727e-02,  2.5000e-01,\n",
      "         2.4121e-01,  2.5195e-01,  2.2754e-01,  2.4023e-01,  2.1875e-01,\n",
      "         2.4316e-01,  1.5332e-01,  2.5391e-01,  1.9727e-01,  2.6562e-01,\n",
      "         2.1582e-01,  1.9238e-01,  2.4707e-01,  2.3145e-01,  1.6602e-01,\n",
      "         2.1777e-01,  2.2363e-01,  1.9336e-01,  2.4219e-01,  2.5977e-01,\n",
      "         2.6562e-01,  2.2363e-01,  2.2949e-01,  1.4160e-01,  2.3535e-01,\n",
      "         2.0312e-01,  2.0801e-01,  2.2461e-01,  2.4023e-01,  2.3828e-01,\n",
      "         2.3145e-01,  2.7148e-01,  2.1973e-01], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.decoder.block.7.layer.1.EncDecAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0481,  0.0061, -0.0332,  ...,  0.0742,  0.0102,  0.0037],\n",
      "        [ 0.0732,  0.0170,  0.0014,  ...,  0.0240,  0.0583, -0.0430],\n",
      "        [ 0.0137,  0.0012, -0.0469,  ...,  0.0225, -0.0325, -0.0325],\n",
      "        ...,\n",
      "        [ 0.0332,  0.0096,  0.0312,  ...,  0.0048,  0.0776, -0.0347],\n",
      "        [-0.0303, -0.0398,  0.0300,  ..., -0.0635, -0.0068, -0.0091],\n",
      "        [-0.0142,  0.0640,  0.0435,  ...,  0.0115, -0.0649, -0.0212]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.7.layer.1.EncDecAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.4707, -0.4883,  0.3652,  ..., -0.1426, -0.0713,  0.2402],\n",
      "        [-0.3965, -0.2461, -0.5820,  ...,  0.0933,  0.8086, -0.1445],\n",
      "        [-0.2441,  0.5117, -0.2422,  ...,  0.3926, -0.0869,  0.8711],\n",
      "        ...,\n",
      "        [-0.4629, -0.4980,  0.7266,  ...,  0.5664,  0.2539, -0.4570],\n",
      "        [-0.4316,  0.0043,  0.2266,  ..., -0.1475, -0.2041, -0.1245],\n",
      "        [-0.2129, -0.0474,  0.4316,  ..., -0.2656, -0.2354,  0.2832]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.7.layer.1.EncDecAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.2041,  0.1562, -0.4219,  ..., -0.5391,  0.8359,  0.4766],\n",
      "        [ 0.0442, -0.1240,  0.9023,  ..., -0.5742, -0.1582,  0.2217],\n",
      "        [-0.6367, -0.0654,  0.7617,  ..., -0.3672,  0.1992,  0.7305],\n",
      "        ...,\n",
      "        [-0.6758, -0.3125, -0.6094,  ...,  0.7969, -0.1250, -0.4062],\n",
      "        [ 1.0234,  0.3477,  0.4688,  ..., -0.4766,  0.5820, -0.1064],\n",
      "        [-0.0231,  1.2656,  0.6680,  ...,  0.4219, -0.6562, -0.1865]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.7.layer.1.EncDecAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.1089, -0.3184,  0.0669,  ..., -0.9453,  0.9570,  0.8516],\n",
      "        [-0.4551,  0.6016, -0.0223,  ...,  0.1011, -0.2852,  1.1562],\n",
      "        [ 0.1768, -0.1982, -0.2441,  ...,  0.7891,  0.5664,  0.9688],\n",
      "        ...,\n",
      "        [-0.3750,  0.3965,  0.9453,  ..., -0.3203,  0.2236,  0.3770],\n",
      "        [ 1.3906,  0.0549, -0.4844,  ..., -0.7656, -0.5508, -0.7969],\n",
      "        [ 0.5703, -0.1982,  0.5195,  ...,  0.3516,  0.7383, -1.3594]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.7.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 5.2246e-02,  4.7852e-02,  5.0049e-02, -4.9072e-02,  5.3955e-02,\n",
      "         4.8096e-02,  7.0801e-02,  6.4453e-02,  5.5908e-02,  6.2988e-02,\n",
      "         4.4922e-02,  6.8359e-02,  4.8340e-02,  4.2725e-02,  5.5176e-02,\n",
      "         6.0547e-02,  5.7373e-02,  5.6885e-02,  4.8096e-02,  7.6172e-02,\n",
      "         5.6641e-02,  5.9082e-02,  6.6406e-02,  6.5918e-02,  6.1523e-02,\n",
      "         5.8105e-02,  5.8838e-02,  3.7842e-02,  6.0791e-02,  6.0303e-02,\n",
      "         6.8359e-02,  6.0791e-02,  6.0059e-02,  6.5918e-02,  6.1768e-02,\n",
      "         5.6885e-02,  7.0801e-02,  6.3965e-02,  5.4688e-02,  6.2988e-02,\n",
      "         5.3467e-02,  5.1025e-02,  6.2988e-02,  5.2002e-02,  6.2256e-02,\n",
      "         5.7373e-02,  7.8125e-02,  7.0801e-02,  6.3477e-02,  5.9082e-02,\n",
      "         6.0791e-02,  5.8594e-02,  6.0059e-02,  6.1523e-02,  7.4219e-02,\n",
      "         4.6387e-02,  4.7119e-02,  6.0303e-02,  6.8359e-02,  5.9814e-02,\n",
      "         5.4199e-02,  6.3477e-02,  6.2012e-02,  5.9814e-02,  6.2500e-02,\n",
      "         1.0352e-01,  5.6152e-02,  5.5664e-02,  4.0039e-02,  5.8594e-02,\n",
      "         6.4453e-02,  6.9824e-02,  6.3477e-02,  5.9570e-02,  6.2256e-02,\n",
      "         5.3467e-02, -4.1748e-02,  5.5908e-02,  6.0791e-02,  6.1035e-02,\n",
      "         5.9082e-02,  5.3467e-02,  5.8838e-02,  4.9561e-02,  6.2988e-02,\n",
      "         5.8838e-02, -2.1729e-02,  6.1279e-02,  5.6152e-02,  6.0791e-02,\n",
      "         5.4688e-02,  6.0547e-02,  5.6641e-02,  4.7852e-02,  5.7373e-02,\n",
      "         6.5430e-02,  5.9082e-02,  6.9824e-02,  6.8848e-02,  5.5664e-02,\n",
      "         4.5410e-02,  6.0059e-02,  3.7842e-02,  5.7129e-02,  5.8105e-02,\n",
      "        -4.6875e-02,  6.1768e-02,  5.1025e-02,  7.0312e-02,  6.2988e-02,\n",
      "         6.2988e-02,  6.2012e-02,  5.0537e-02,  5.7617e-02,  5.5908e-02,\n",
      "         7.3242e-02,  5.2490e-02,  5.3223e-02,  4.5776e-05,  5.7373e-02,\n",
      "         5.9814e-02,  6.3477e-02,  5.4688e-02,  5.8105e-02,  6.2500e-02,\n",
      "         5.6152e-02,  5.9326e-02,  4.8828e-02,  4.9805e-02, -4.5410e-02,\n",
      "        -4.2725e-02,  5.8105e-02,  6.0059e-02,  6.0303e-02,  4.2480e-02,\n",
      "         5.7617e-02,  5.6641e-02,  5.3711e-02,  5.1514e-02,  4.0771e-02,\n",
      "         5.4688e-02,  6.3965e-02, -5.0049e-02,  5.5908e-02,  5.5176e-02,\n",
      "         5.0293e-02,  6.2988e-02,  5.2002e-02,  6.2988e-02,  4.9072e-02,\n",
      "         7.4707e-02,  5.6396e-02,  7.2754e-02,  5.2734e-02,  6.7871e-02,\n",
      "         6.7383e-02,  5.5176e-02,  6.9336e-02,  9.6191e-02,  4.7119e-02,\n",
      "         5.8838e-02,  4.9561e-02,  5.2490e-02,  5.5908e-02,  6.2500e-02,\n",
      "         4.2236e-02,  6.6895e-02,  5.0537e-02,  4.4922e-02,  6.4453e-02,\n",
      "         3.7354e-02,  5.7617e-02,  7.1289e-02,  6.0303e-02,  6.0059e-02,\n",
      "         6.3477e-02,  5.4688e-02,  6.7383e-02,  3.7354e-02,  5.6885e-02,\n",
      "         6.0791e-02,  6.3965e-02,  6.6895e-02,  6.5430e-02,  4.8340e-02,\n",
      "         5.9814e-02,  5.5176e-02,  6.3965e-02,  5.4932e-02, -4.2969e-02,\n",
      "         6.5918e-02,  5.7617e-02,  6.2988e-02,  5.6396e-02,  5.4443e-02,\n",
      "         5.7373e-02,  5.5176e-02,  6.7383e-02,  6.8848e-02,  5.8105e-02,\n",
      "         6.2256e-02,  5.2490e-02,  5.7617e-02,  6.0547e-02,  5.5908e-02,\n",
      "        -4.6631e-02,  6.2988e-02,  6.3965e-02,  6.1279e-02,  6.5430e-02,\n",
      "         7.5684e-02,  4.5410e-02,  5.3711e-02,  6.0303e-02,  6.3965e-02,\n",
      "         6.1768e-02,  5.9326e-02,  6.0547e-02,  5.8350e-02, -4.7302e-04,\n",
      "         4.1504e-02,  6.8359e-02,  6.0303e-02,  5.9814e-02,  5.4932e-02,\n",
      "         6.6406e-02,  1.5430e-01,  7.2266e-02,  6.0059e-02,  6.6895e-02,\n",
      "         6.1523e-02,  6.0059e-02,  5.0537e-02,  6.4453e-02,  6.1035e-02,\n",
      "         5.3955e-02,  4.5654e-02,  5.2490e-02, -4.3457e-02,  5.8350e-02,\n",
      "         4.8828e-02,  6.1035e-02,  6.1768e-02,  5.8350e-02,  6.5918e-02,\n",
      "         5.2490e-02,  7.0312e-02,  5.1514e-02,  5.9326e-02,  5.3711e-02,\n",
      "         5.5420e-02,  5.5908e-02,  6.3965e-02,  5.4443e-02,  5.9570e-02,\n",
      "         5.0293e-02,  6.2256e-02, -4.3945e-02,  6.2500e-02,  6.1279e-02,\n",
      "         6.9336e-02,  4.4922e-02,  6.4941e-02,  6.4453e-02,  6.2012e-02,\n",
      "         5.8838e-02,  6.4941e-02,  6.4453e-02,  5.8594e-02,  5.5420e-02,\n",
      "         5.7129e-02,  6.0059e-02,  6.1035e-02,  5.8350e-02, -3.9062e-02,\n",
      "         6.0791e-02,  6.9336e-02,  5.0537e-02,  6.5430e-02,  4.6387e-02,\n",
      "         8.9111e-03,  4.8828e-02,  6.1523e-02,  5.9326e-02,  6.2500e-02,\n",
      "         5.6885e-02,  5.8105e-02,  6.3965e-02,  5.0049e-02,  4.3945e-02,\n",
      "         5.9814e-02,  5.5908e-02,  5.5176e-02,  5.7373e-02,  5.3467e-02,\n",
      "         6.3477e-02,  5.8838e-02,  6.4941e-02,  6.1035e-02,  7.0801e-02,\n",
      "         6.8359e-02,  5.9814e-02, -1.7090e-02,  5.3711e-02,  6.0547e-02,\n",
      "         6.5430e-02,  5.1758e-02,  6.0059e-02,  5.5176e-02,  6.5918e-02,\n",
      "         7.2266e-02,  5.9326e-02,  5.2979e-02,  6.1279e-02,  1.3086e-01,\n",
      "         5.6396e-02,  8.3008e-02,  5.6885e-02,  6.3477e-02,  6.4941e-02,\n",
      "         5.9326e-02,  6.2256e-02, -4.8584e-02,  6.4453e-02,  6.3965e-02,\n",
      "         5.6641e-02,  5.1758e-02,  6.3477e-02,  5.9326e-02,  6.3965e-02,\n",
      "         6.1523e-02,  4.5898e-02,  6.7383e-02,  6.1768e-02,  6.0059e-02,\n",
      "         5.5908e-02,  6.1279e-02,  7.6660e-02,  6.0547e-02,  3.5889e-02,\n",
      "         5.1514e-02,  5.8838e-02,  6.2012e-02,  5.4443e-02,  6.4453e-02,\n",
      "         5.5908e-02,  6.4941e-02,  4.7119e-02,  5.0049e-02,  5.2979e-02,\n",
      "         5.5420e-02,  5.5664e-02,  5.0781e-02,  6.3965e-02,  5.4443e-02,\n",
      "         6.9824e-02,  6.9336e-02, -3.5048e-05,  6.2988e-02, -7.0801e-02,\n",
      "         6.4453e-02,  5.2734e-02,  5.1758e-02,  5.5664e-02,  6.4941e-02,\n",
      "         7.1777e-02,  5.5908e-02,  5.5420e-02,  5.7129e-02,  5.1514e-02,\n",
      "         6.5918e-02,  6.2988e-02,  6.3477e-02,  5.5420e-02,  7.0801e-02,\n",
      "         6.1035e-02,  5.8594e-02,  7.6172e-02,  6.3965e-02,  4.7363e-02,\n",
      "         5.3223e-02,  5.8594e-02,  4.1504e-02,  5.7373e-02,  5.5176e-02,\n",
      "         6.2988e-02,  6.1523e-02,  6.7383e-02,  5.3467e-02,  6.1768e-02,\n",
      "         6.1523e-02,  5.3467e-02,  5.2246e-02,  5.1025e-02,  5.8594e-02,\n",
      "         5.8594e-02,  6.2988e-02,  5.4688e-02,  5.5420e-02,  5.6885e-02,\n",
      "         5.5908e-02,  5.8594e-02,  5.8105e-02,  6.2988e-02,  5.5908e-02,\n",
      "         6.0059e-02,  4.9561e-02,  5.4199e-02,  5.3467e-02,  5.9326e-02,\n",
      "         5.6152e-02,  6.0303e-02,  5.6885e-02,  6.1768e-02,  6.4453e-02,\n",
      "         6.8359e-02,  5.6396e-02,  6.2988e-02,  1.7383e-01,  6.0303e-02,\n",
      "        -1.3256e-04,  6.4941e-02,  6.6406e-02, -1.5198e-02,  6.5918e-02,\n",
      "         6.1768e-02,  4.8340e-02,  6.6895e-02,  5.2246e-02,  4.3457e-02,\n",
      "         6.5430e-02,  5.3467e-02,  6.3965e-02,  5.0781e-02,  5.3223e-02,\n",
      "         5.7129e-02,  5.9082e-02,  6.2988e-02,  6.0791e-02,  5.3955e-02,\n",
      "         6.6895e-02,  5.5908e-02,  5.5420e-02,  6.4941e-02,  6.3965e-02,\n",
      "         6.2012e-02,  5.9570e-02,  6.2988e-02,  5.4169e-04,  5.6885e-02,\n",
      "        -5.7129e-02,  6.5430e-02,  5.7373e-02,  6.2988e-02,  5.7373e-02,\n",
      "         5.1025e-02,  7.2266e-02,  1.2500e-01,  5.0049e-02,  5.4688e-02,\n",
      "         5.9814e-02,  6.3965e-02,  6.3965e-02,  6.1768e-02,  5.7617e-02,\n",
      "         6.2988e-02, -4.1748e-02,  5.9082e-02,  5.4688e-02,  5.7861e-02,\n",
      "         6.3477e-02,  6.2988e-02,  5.4199e-02,  6.0303e-02,  5.6152e-02,\n",
      "         6.2012e-02,  5.8838e-02,  6.3477e-02,  5.8594e-02,  5.2979e-02,\n",
      "         4.7607e-02,  6.1523e-02,  6.8359e-02,  5.5420e-02,  2.4605e-04,\n",
      "         5.6641e-02,  6.2500e-02,  8.3008e-02,  6.4941e-02,  5.5664e-02,\n",
      "         6.2256e-02,  7.6172e-02,  6.3477e-02,  6.3965e-02,  6.2500e-02,\n",
      "         6.0791e-02,  5.2002e-02,  5.5420e-02,  6.2500e-02,  5.6885e-02,\n",
      "         5.9814e-02,  8.7402e-02,  6.4453e-02,  5.3223e-02,  5.7373e-02,\n",
      "         5.8838e-02,  6.5918e-02,  5.0537e-02,  5.8105e-02,  6.2988e-02,\n",
      "         5.8838e-02,  6.6406e-02,  5.2734e-02,  5.7861e-02,  6.0059e-02,\n",
      "         6.3477e-02,  5.2979e-02,  5.8594e-02, -3.6865e-02,  6.6406e-02,\n",
      "         6.2988e-02,  5.6885e-02,  6.6895e-02,  5.9326e-02,  5.6885e-02,\n",
      "         6.2988e-02,  5.6641e-02,  5.8594e-02,  5.8594e-02,  5.8594e-02,\n",
      "         4.5410e-02,  5.5176e-02,  5.4443e-02,  5.8594e-02,  6.3477e-02,\n",
      "         6.2256e-02,  5.9326e-02,  5.8105e-02,  5.6641e-02,  5.6396e-02,\n",
      "         5.9570e-02,  5.9326e-02,  5.3955e-02,  5.1025e-02,  5.3223e-02,\n",
      "         5.4443e-02,  5.8350e-02,  5.4688e-02,  5.9814e-02,  5.5908e-02,\n",
      "         4.7119e-02,  6.0547e-02,  5.8838e-02,  5.2490e-02,  5.8350e-02,\n",
      "         5.9326e-02,  4.4434e-02,  5.8105e-02,  5.4932e-02,  6.1035e-02,\n",
      "         5.7129e-02,  5.5176e-02,  1.9409e-02, -6.6406e-02,  6.4941e-02,\n",
      "         6.5918e-02,  6.7871e-02,  5.3711e-02,  5.0537e-02,  6.2988e-02,\n",
      "         6.9824e-02,  5.4443e-02,  6.0303e-02,  5.2734e-02,  4.3945e-02,\n",
      "         5.4932e-02,  7.0312e-02,  6.3965e-02,  6.2012e-02,  1.6594e-04,\n",
      "         5.5176e-02,  6.9336e-02,  5.7617e-02,  5.4443e-02,  6.2988e-02,\n",
      "         6.5918e-02,  5.5420e-02,  5.4443e-02,  5.9082e-02,  6.0303e-02,\n",
      "         6.1035e-02,  6.5430e-02,  6.5430e-02,  5.2734e-02,  5.6152e-02,\n",
      "         6.2988e-02,  4.8340e-02,  6.3477e-02, -5.2246e-02,  5.6885e-02,\n",
      "         4.8096e-02,  6.0303e-02,  5.8594e-02,  5.1025e-02,  5.5908e-02,\n",
      "         5.4932e-02,  6.8848e-02,  5.3467e-02,  5.9814e-02,  5.5420e-02,\n",
      "         5.2734e-02,  6.0303e-02,  5.8838e-02,  6.8359e-02,  5.1025e-02,\n",
      "         6.5430e-02,  5.4199e-02,  1.4160e-01,  6.7383e-02,  6.0791e-02,\n",
      "         5.9814e-02,  5.5664e-02,  5.9570e-02,  4.2236e-02,  5.9326e-02,\n",
      "         5.8350e-02,  5.6885e-02,  5.8594e-02,  5.8594e-02,  5.3711e-02,\n",
      "         5.9570e-02,  6.4941e-02,  5.2002e-02,  5.4688e-02,  5.4688e-02,\n",
      "         5.7373e-02,  5.2490e-02,  5.6641e-02,  6.2012e-02,  6.2500e-02,\n",
      "         4.6143e-02,  5.9570e-02,  6.5430e-02,  6.2500e-02,  4.0039e-02,\n",
      "         5.1270e-02,  4.2236e-02,  6.4453e-02,  5.6152e-02,  6.6895e-02,\n",
      "         5.5664e-02,  6.3477e-02,  5.6885e-02,  5.6152e-02,  6.4941e-02,\n",
      "         3.6133e-02,  5.3955e-02,  4.8340e-02,  5.5664e-02,  5.9570e-02,\n",
      "         6.5918e-02,  4.5166e-02,  4.6875e-02,  8.0566e-02,  5.9570e-02,\n",
      "         5.4443e-02,  6.2256e-02,  5.7373e-02,  6.1279e-02,  6.0059e-02,\n",
      "         6.2500e-02,  5.7617e-02,  5.5664e-02,  4.8340e-02,  6.9824e-02,\n",
      "         6.1768e-02,  6.1279e-02,  6.8848e-02,  5.1758e-02,  5.6641e-02,\n",
      "         6.4941e-02,  6.5430e-02,  4.9072e-02,  5.6641e-02,  5.6396e-02,\n",
      "         5.1758e-02,  5.9814e-02,  5.3467e-02,  6.2500e-02,  5.3467e-02,\n",
      "         6.1279e-02,  5.5176e-02, -4.2725e-02,  5.8594e-02,  5.7373e-02,\n",
      "         5.5664e-02,  6.6406e-02,  6.0303e-02,  5.9814e-02,  5.6641e-02,\n",
      "         5.6641e-02,  6.4453e-02,  5.4932e-02,  3.7842e-02,  6.9336e-02,\n",
      "         6.0791e-02,  5.7129e-02, -3.8086e-02,  5.8350e-02,  6.1523e-02,\n",
      "         5.6152e-02,  5.4688e-02,  6.6895e-02,  6.0059e-02, -3.3447e-02,\n",
      "         1.0498e-01,  5.3223e-02,  6.9824e-02,  6.2988e-02, -4.7607e-02,\n",
      "         6.8848e-02,  5.5176e-02,  4.8584e-02,  6.3965e-02,  5.9570e-02,\n",
      "         4.4189e-02,  5.3223e-02,  5.7373e-02,  5.5664e-02,  6.2988e-02,\n",
      "         6.1768e-02,  5.7617e-02,  6.5430e-02,  4.9316e-02,  7.5684e-02,\n",
      "         6.0059e-02,  6.4453e-02,  5.8350e-02,  5.9570e-02,  5.3955e-02,\n",
      "         5.4932e-02,  4.6143e-02,  6.0059e-02,  4.8096e-02,  6.2988e-02,\n",
      "         5.3955e-02,  5.0537e-02,  5.8838e-02,  5.6885e-02,  4.0771e-02,\n",
      "        -4.7119e-02,  4.9561e-02,  5.1270e-02,  5.9814e-02,  6.2988e-02,\n",
      "         7.4707e-02,  5.7617e-02,  5.2490e-02,  5.2002e-02,  5.7861e-02,\n",
      "         5.1025e-02,  5.2490e-02,  4.9316e-02,  5.6396e-02,  6.0303e-02,\n",
      "         5.2979e-02,  6.2500e-02,  5.3223e-02], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.decoder.block.7.layer.2.DenseReluDense.wi.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.7266,  0.0771,  0.6719,  ...,  0.0566, -0.0991, -0.5703],\n",
      "        [ 1.1094,  0.2637,  0.1025,  ...,  0.9453,  0.0021,  0.3750],\n",
      "        [ 0.4629,  0.0703,  0.2598,  ..., -0.7188, -0.1299, -0.0388],\n",
      "        ...,\n",
      "        [-0.2461, -0.2236, -0.0981,  ...,  0.3574,  0.2637, -0.0172],\n",
      "        [ 0.5000,  0.4160, -0.1099,  ..., -0.0344,  0.6328,  0.0549],\n",
      "        [-0.0771,  0.0718, -0.0074,  ...,  0.6367, -0.5625, -0.4453]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.7.layer.2.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.1216, -0.0879,  0.2969,  ..., -0.0574,  0.0933,  0.2812],\n",
      "        [-0.2002,  0.0309,  0.0072,  ...,  0.1680,  0.0669, -0.2383],\n",
      "        [ 0.2539,  0.0796,  0.0737,  ..., -0.1963, -0.0388,  0.1602],\n",
      "        ...,\n",
      "        [ 0.6523, -0.1147,  0.5469,  ..., -0.3281, -0.0942,  0.3320],\n",
      "        [ 0.0320,  0.0215,  0.1050,  ...,  0.1934, -0.0544,  0.3945],\n",
      "        [-0.2139, -0.0167,  0.0135,  ..., -0.0291,  0.1621,  0.2578]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.7.layer.2.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 3.5000,  3.3281,  3.3594,  2.7969,  3.5000,  3.2969,  3.2812,  3.8750,\n",
      "         3.1562,  3.4375,  3.8594,  3.4844,  4.0938,  3.8125,  3.4219,  3.8594,\n",
      "         3.3438,  2.9688,  3.4844,  3.6406,  3.4844,  3.4688,  3.5000,  3.3750,\n",
      "         3.5000,  3.2031,  3.3594,  3.5938,  3.5312,  3.5000,  3.6094,  4.3125,\n",
      "         3.6406,  3.2812,  3.8594,  3.5000,  3.5625,  3.3438,  3.3125,  3.7344,\n",
      "         3.2188,  3.3594,  3.4219,  3.5000,  3.5469,  3.5312,  3.4688,  3.4062,\n",
      "         3.6094,  3.4219,  3.2344,  3.3750,  3.6875,  3.3750,  4.5938,  3.6250,\n",
      "         3.5469,  3.4062,  3.4531,  3.5781,  3.4062,  3.5469,  3.5156,  3.2656,\n",
      "         3.5000,  4.2812,  3.5000,  3.5781,  4.0938,  3.3906,  3.5000,  3.5625,\n",
      "         3.3906,  3.3594,  3.6719,  3.4062,  3.4531,  3.5469,  3.5625,  3.4844,\n",
      "         3.4531,  3.2500,  3.3438,  3.2812,  3.4219,  3.4844,  1.8750,  3.5781,\n",
      "         3.5312,  3.6406,  3.2500,  3.2188,  3.3438,  3.4375,  3.6562,  3.6562,\n",
      "         3.4531,  3.4062,  3.5000,  3.3594,  3.8125,  3.3594,  4.3125,  3.4375,\n",
      "         3.3750,  3.1562,  3.4375,  3.3438,  3.2500,  3.5156,  3.4844,  3.5312,\n",
      "         3.4531,  3.4219,  3.3594,  3.3594,  3.4531,  3.6250, -0.0065,  3.3750,\n",
      "         3.4844,  3.0625,  3.3906,  3.3906,  3.4844,  3.2188,  3.4844,  3.2656,\n",
      "         3.6875,  3.6094,  4.4062,  3.3750,  3.5625,  3.4219,  2.7969,  3.2656,\n",
      "         3.4688,  3.4531,  3.3125,  3.5000,  3.5781,  3.5000,  3.5938,  3.5781,\n",
      "         3.3750,  3.3906,  3.2969,  3.2812,  3.4531,  3.4844,  3.4844,  3.3281,\n",
      "         3.4688,  3.2188,  3.5156,  3.4688,  3.4062,  3.6094,  2.8750,  3.3281,\n",
      "         3.2031,  3.3750,  3.4062,  3.6250,  3.2344,  3.5312,  3.8281,  3.4688,\n",
      "         3.8438,  3.2812,  4.0625,  3.3438,  3.2656,  3.2500,  3.4219,  3.2812,\n",
      "         3.2656,  3.1719,  3.5938,  3.0469,  3.4531,  3.7031,  3.4844,  3.5312,\n",
      "         3.9531,  3.5469,  3.2812,  1.9219,  3.3750,  3.1719,  3.3438,  3.2500,\n",
      "         3.2500,  3.5312,  3.1094,  3.3125,  3.3281,  3.5156,  3.7500,  3.1719,\n",
      "         3.5000,  3.3438,  3.5781,  3.3594,  3.4688,  3.2031,  3.2812,  3.4062,\n",
      "         3.2344,  3.6250,  3.4531,  3.6406,  3.2500,  3.4688,  3.4062,  3.6406,\n",
      "         3.7656,  3.3125,  3.5469,  0.8789,  3.8750,  3.3125,  3.4375,  3.2500,\n",
      "         3.4375,  3.3906,  2.6562,  3.4375,  3.4688,  3.5000,  3.3594,  3.6406,\n",
      "         3.1875,  3.5312,  3.4688,  3.4375,  3.6875,  3.3281,  3.8906,  3.6875,\n",
      "         5.0000,  3.5156,  3.3438,  3.2969,  3.5781,  3.2812,  3.2500,  3.5156,\n",
      "         3.3281,  3.4688,  3.3906,  3.4375,  3.6094,  3.4531,  3.4219,  3.1562,\n",
      "         3.5625,  3.6406,  3.4219,  3.5156,  3.7188,  3.8438,  3.5156,  3.6406,\n",
      "         3.4219,  3.6875,  3.2969,  3.3281,  3.3594,  3.6406,  3.4219,  3.9219,\n",
      "         3.5000,  3.1250,  3.7969,  3.5156,  3.4688,  3.5469,  3.5312,  3.4375,\n",
      "         0.5938,  3.6250,  3.5469,  3.3438,  3.4375,  3.3438,  3.3594,  3.6094,\n",
      "         3.6562,  3.5312,  3.5156,  3.3125,  3.1875,  3.4688,  3.3906,  3.4219,\n",
      "         3.2656,  3.5469,  3.7500,  3.9219,  3.3594,  3.4219,  1.0469,  3.4219,\n",
      "         3.3750,  3.5156,  3.5625,  3.5312,  3.6875,  3.3438,  3.1094,  3.4375,\n",
      "         3.3906,  3.2188,  2.9219,  3.5156,  3.2500,  3.7969,  3.4375,  3.4375,\n",
      "         3.1875,  3.5000,  3.3438,  3.5000,  3.2031,  3.3750,  3.4062,  3.3750,\n",
      "         3.5156,  3.4062,  3.4219,  3.7500,  3.3750,  3.3281,  3.4062,  3.5312,\n",
      "         3.3750,  3.3750,  3.2969,  3.3594,  3.7031,  3.3594,  3.3125,  3.2812,\n",
      "         3.4844,  3.2500,  3.5156,  3.8750,  3.2969,  3.8906,  3.1719,  3.1250,\n",
      "         3.0938,  3.3750,  3.4531,  3.4531,  3.6562,  0.0058,  3.5781,  3.8281,\n",
      "         3.5156,  3.1562,  3.9375,  3.2656,  3.3281,  3.4688,  3.7344,  3.2812,\n",
      "         3.3594,  3.4531,  3.7188,  3.3281,  3.4688,  3.4531,  3.5312,  3.4219,\n",
      "         3.1719,  3.3281,  3.4844,  3.5000,  3.4531,  3.1719,  3.8750,  3.2656,\n",
      "         3.6719,  3.6719,  3.5312,  3.7188,  3.3281,  3.3594,  3.4844,  3.2188,\n",
      "         3.2969,  3.7500,  3.6562,  3.4219,  3.5469,  3.2031,  3.2812,  3.6719,\n",
      "         3.3906,  3.6094,  3.3906,  3.5938,  3.1719,  3.2344,  3.2500,  3.3750,\n",
      "         3.3438,  3.2969,  3.2969,  3.2344,  3.4219,  3.2656,  3.4531,  3.3438,\n",
      "         3.4531,  3.4688,  3.0469,  3.3438,  0.6719,  3.3594,  3.4062,  1.9766,\n",
      "         3.6719,  3.5156,  3.4688,  3.4375,  3.5312,  3.7188,  3.3125,  3.4531,\n",
      "         3.6562,  3.1094,  3.2656,  3.2656,  3.4531,  3.3281,  3.4688,  3.5625,\n",
      "         3.2812,  3.7031,  3.0312,  3.5469,  3.2500,  3.1875,  3.2812,  3.3438,\n",
      "         0.6680,  3.4531,  3.3281,  3.2969,  3.4375,  3.4219,  3.3281,  3.4688,\n",
      "         3.4531,  3.6406,  3.4375,  3.4844,  3.4688,  3.4688,  3.6719,  3.6094,\n",
      "         3.4844,  3.7656,  3.4062,  3.4531,  3.2188,  3.2969,  3.5156,  3.2656,\n",
      "         3.8594,  3.5469,  3.2812,  3.3125,  3.2656,  3.5469,  3.5312,  3.2656,\n",
      "         3.4688,  3.5312,  3.4375,  3.2969,  0.1001,  3.3594,  3.2344,  0.5820,\n",
      "         3.4688,  3.4219,  3.6562,  3.4375,  3.4531,  3.4219,  3.7188,  5.5312,\n",
      "         3.5156,  3.3906,  3.4531,  3.6250,  3.4219,  3.2969,  3.4531,  3.5312,\n",
      "         3.4688,  3.4844,  3.6250,  3.3438,  3.4531,  3.5000,  3.1875,  3.4531,\n",
      "         3.6562,  3.4688,  3.2344,  3.4219,  3.2969,  3.1250,  3.6406,  3.4531,\n",
      "         3.3281,  3.2656,  3.5625,  3.3594,  3.5469,  3.2344,  3.6562,  3.5781,\n",
      "         3.6719,  3.1719,  3.1562,  3.2969,  3.2812,  3.3281,  3.3750,  3.5312,\n",
      "         3.5312,  3.3594,  3.3281,  3.4844,  3.2031,  3.7188,  3.2812,  3.2500,\n",
      "         3.3125,  3.4531,  3.3438,  3.4844,  3.4375,  3.4375,  3.3281,  3.5625,\n",
      "         3.3906,  3.6875,  3.3750,  3.4219,  3.0781,  3.6562,  3.2344,  3.4375,\n",
      "         3.4375,  3.4688,  0.8984,  4.7500,  3.2656,  3.5938,  3.4844,  3.1094,\n",
      "         3.5469,  3.2969,  3.3906,  3.3281,  3.4844,  3.5625,  3.7812,  3.3750,\n",
      "         3.3906,  3.4375,  3.5625, -0.0236,  3.3906,  3.5156,  3.3750,  3.4219,\n",
      "         3.7188,  3.3438,  3.4531,  3.5781,  3.4688,  3.4375,  3.4844,  3.4688,\n",
      "         3.5312,  3.3594,  3.1875,  3.4062,  3.8281,  3.5469,  4.6250,  3.6719,\n",
      "         3.1406,  3.4219,  3.3125,  3.1875,  3.5938,  3.2656,  3.2500,  3.7188,\n",
      "         3.6250,  3.5781,  3.3906,  3.3906,  3.5000,  3.3438,  3.3438,  3.5469,\n",
      "         3.4531,  3.0625,  3.3750,  3.3438,  3.2656,  3.1406,  3.5312,  3.8906,\n",
      "         3.4531,  3.4688,  3.6719,  3.5156,  3.4844,  3.2656,  3.1094,  3.4688,\n",
      "         3.2656,  4.4688,  3.3438,  3.4531,  3.7188,  3.5938,  3.3125,  3.3594,\n",
      "         3.5000,  3.3438,  3.4375,  3.6094,  3.2656,  3.7344,  4.3125,  3.2812,\n",
      "         3.5781,  3.6562,  2.9531,  3.5625,  3.2656,  3.5938,  3.5000,  3.6562,\n",
      "         3.2344,  3.5000,  3.5156,  3.8281,  3.4844,  3.2188,  3.1250,  3.4844,\n",
      "         3.3281,  3.3906,  3.4531,  3.4844,  3.5156,  3.4531,  3.3125,  3.6094,\n",
      "         3.8594,  3.0625,  3.4688,  3.5000,  3.3125,  3.4531,  3.3906,  3.5469,\n",
      "         3.3125,  3.3750,  3.6250,  3.1562,  3.3281,  3.4062,  3.1562,  3.2344,\n",
      "         3.5000,  3.3438,  3.7500,  3.6250,  3.0938,  3.3125,  3.2344,  3.3438,\n",
      "         3.4844,  3.5625,  3.6406,  3.2812,  3.1875,  3.2500,  3.2500,  3.2031,\n",
      "         3.3281,  3.5469,  3.4062,  3.3438,  3.8281,  3.4062,  3.4062,  3.4219,\n",
      "         3.4219,  3.4844,  2.9531,  3.1406,  3.3438,  3.4219,  3.2031,  3.3906,\n",
      "         3.3750,  3.5156,  4.2188,  3.4531,  3.4531,  3.2500,  3.2656,  3.3125,\n",
      "         3.2344,  3.5469,  3.3125,  3.4844,  3.4375,  3.1094,  3.2656,  3.4062,\n",
      "         3.2969,  3.4375,  3.3125,  3.5312,  3.3125,  3.5781,  3.4375,  3.7344,\n",
      "         3.4375,  3.1562,  3.4375,  3.2812,  3.4062,  3.8281,  3.5000,  3.2812,\n",
      "         3.6094,  3.3125,  3.6406,  3.5469,  3.4531,  3.2812,  3.4531,  3.4219,\n",
      "         3.5156,  3.2031,  3.2500,  3.5156,  3.2344,  3.2031,  3.5469,  3.8281],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.8.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[ 1.3184e-02,  6.4941e-02, -4.0283e-03,  ..., -2.2339e-02,\n",
      "         -7.0190e-03, -1.0437e-02],\n",
      "        [ 2.1973e-02,  5.8594e-02,  8.7280e-03,  ..., -3.2959e-02,\n",
      "         -2.3804e-02, -9.7046e-03],\n",
      "        [ 3.3203e-02,  5.0659e-03, -1.5991e-02,  ...,  5.9204e-03,\n",
      "          1.1414e-02,  1.0803e-02],\n",
      "        ...,\n",
      "        [-1.2354e-01, -1.8239e-05,  9.7656e-03,  ...,  1.5991e-02,\n",
      "          8.3618e-03, -2.8687e-02],\n",
      "        [-2.6733e-02, -3.7109e-02, -1.3794e-02,  ...,  4.3488e-04,\n",
      "         -4.4678e-02, -3.6621e-02],\n",
      "        [ 8.3984e-02, -5.6152e-03,  5.5664e-02,  ..., -5.8105e-02,\n",
      "         -4.6875e-02, -6.2256e-03]], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.8.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.2168, -0.0962, -0.1680,  ...,  0.3496,  0.3887,  0.1475],\n",
      "        [ 0.4023,  0.3125, -0.2119,  ..., -0.4961, -0.0664,  0.0635],\n",
      "        [-0.3828, -0.3613,  0.0178,  ...,  0.2695,  0.1250,  0.4785],\n",
      "        ...,\n",
      "        [-0.3770, -0.1592, -0.4258,  ..., -0.0879,  0.1699, -0.2158],\n",
      "        [ 0.2305, -0.0287, -0.1084,  ..., -0.3789, -0.4238, -0.4277],\n",
      "        [-0.6133, -0.1973, -0.0535,  ...,  0.0952,  0.4297, -0.2773]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.8.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.8906,  0.2246, -0.3438,  ...,  0.0168,  0.7031,  1.0703],\n",
      "        [-1.0547, -0.2773,  0.0830,  ..., -0.6602,  0.5352, -0.5820],\n",
      "        [-0.8008,  0.0483,  0.7344,  ..., -0.7852, -1.3984,  0.6289],\n",
      "        ...,\n",
      "        [-0.2910, -0.4395, -0.4688,  ...,  0.0708,  0.4805,  1.4922],\n",
      "        [ 0.7969, -0.9727, -0.1338,  ...,  0.1475, -0.2363,  0.0776],\n",
      "        [-0.0444,  1.0547, -0.1025,  ..., -0.6875, -1.0391,  0.4766]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.8.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 2.4844, -0.4082, -0.2910,  ..., -0.8242,  0.5898, -1.1719],\n",
      "        [ 0.9375, -1.2188, -0.6680,  ..., -0.0791,  0.3555,  0.4668],\n",
      "        [-0.4297,  0.4453,  0.1934,  ...,  0.3965,  0.0615,  0.8164],\n",
      "        ...,\n",
      "        [-0.4160, -2.4688, -1.6562,  ..., -0.4160,  0.6250, -1.1562],\n",
      "        [ 1.5156,  2.7812, -1.2500,  ..., -0.4160, -0.6445, -1.1562],\n",
      "        [ 1.0781,  0.4258,  1.8594,  ...,  0.0723, -0.0129, -0.5742]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.8.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 2.2266e-01,  1.8555e-01,  2.3535e-01,  1.6504e-01,  2.4023e-01,\n",
      "         2.1094e-01,  2.4609e-01,  2.6172e-01,  2.2266e-01,  2.4707e-01,\n",
      "         2.0215e-01,  2.5391e-01,  1.9434e-01,  2.0703e-01,  2.3242e-01,\n",
      "         2.4902e-01,  2.2852e-01,  2.2754e-01,  2.0801e-01,  2.6953e-01,\n",
      "         2.3926e-01,  2.3633e-01,  2.2559e-01,  2.3828e-01,  2.3535e-01,\n",
      "         2.6953e-01,  2.4316e-01,  2.2070e-01,  2.5000e-01,  2.5391e-01,\n",
      "         2.5000e-01,  2.7930e-01,  2.4219e-01,  2.4805e-01,  2.5195e-01,\n",
      "         2.4805e-01,  2.6367e-01,  2.3242e-01,  2.4414e-01,  2.5000e-01,\n",
      "         2.1777e-01,  2.3145e-01,  2.5195e-01,  2.4023e-01,  2.4707e-01,\n",
      "         2.4316e-01,  2.4805e-01,  2.5391e-01,  2.5195e-01,  2.4121e-01,\n",
      "         2.4902e-01,  2.3730e-01,  2.3730e-01,  2.3926e-01,  3.1641e-01,\n",
      "         2.0703e-01,  2.2266e-01,  2.4219e-01,  2.4707e-01,  2.4121e-01,\n",
      "         2.3340e-01,  2.4902e-01,  2.4609e-01,  2.5195e-01,  2.4902e-01,\n",
      "         2.5000e-01,  2.4023e-01,  2.3828e-01,  2.3730e-01,  2.3730e-01,\n",
      "         2.5977e-01,  2.5195e-01,  2.4707e-01,  2.4512e-01,  2.6758e-01,\n",
      "         2.4414e-01, -2.0703e-01,  2.3633e-01,  2.5781e-01,  2.5781e-01,\n",
      "         2.5195e-01,  2.2168e-01,  2.2461e-01,  2.6562e-01,  2.6953e-01,\n",
      "         2.6562e-01, -9.7168e-02,  2.3730e-01,  2.5195e-01,  2.5000e-01,\n",
      "         2.2949e-01,  2.4316e-01,  2.3438e-01,  2.0898e-01,  2.5586e-01,\n",
      "         2.5195e-01,  2.4609e-01,  2.5000e-01,  2.6953e-01,  2.3340e-01,\n",
      "         2.2949e-01,  2.3242e-01,  2.2949e-01,  2.5195e-01,  2.3633e-01,\n",
      "         2.3047e-01,  2.4414e-01,  2.4512e-01,  2.5391e-01,  2.4609e-01,\n",
      "         2.5195e-01,  2.4609e-01,  2.1484e-01,  2.4414e-01,  2.3535e-01,\n",
      "         2.7734e-01,  2.2461e-01,  2.3926e-01,  6.7871e-02,  2.5977e-01,\n",
      "         2.5000e-01,  2.2852e-01,  2.7344e-01,  2.4023e-01,  2.5586e-01,\n",
      "         2.3730e-01,  2.4023e-01,  2.1289e-01,  2.3926e-01,  2.2070e-01,\n",
      "         2.1582e-01,  2.4414e-01,  2.4609e-01,  2.4805e-01,  1.6016e-01,\n",
      "         2.2168e-01,  2.3145e-01,  2.3438e-01,  2.2070e-01,  2.3047e-01,\n",
      "         2.3926e-01,  2.4902e-01,  2.5000e-01,  2.4512e-01,  2.1777e-01,\n",
      "         2.1680e-01,  2.5195e-01,  2.3438e-01,  2.5977e-01,  2.1289e-01,\n",
      "         2.7930e-01,  2.3340e-01,  2.5391e-01,  2.3535e-01,  2.4512e-01,\n",
      "         2.5781e-01,  2.3242e-01,  2.5977e-01,  9.5215e-02,  1.8652e-01,\n",
      "         2.4609e-01,  2.2949e-01,  2.1094e-01,  2.5781e-01,  2.3633e-01,\n",
      "         2.3926e-01,  2.5391e-01,  2.3730e-01,  2.6758e-01,  2.5977e-01,\n",
      "        -1.8359e-01,  2.4121e-01,  2.5000e-01,  2.3926e-01,  2.4512e-01,\n",
      "         2.2168e-01,  2.2754e-01,  2.4805e-01,  1.5527e-01,  2.3438e-01,\n",
      "         2.4707e-01,  2.4707e-01,  2.5781e-01,  2.3926e-01,  2.3926e-01,\n",
      "         2.7734e-01,  2.3535e-01,  5.6885e-02,  2.3828e-01,  2.0898e-01,\n",
      "         2.3145e-01,  2.3730e-01,  2.4023e-01,  2.4219e-01,  2.3535e-01,\n",
      "         2.2852e-01,  2.2949e-01,  2.4707e-01,  2.4902e-01,  2.2949e-01,\n",
      "         2.4414e-01,  2.1582e-01,  2.4414e-01,  2.4707e-01,  2.3438e-01,\n",
      "         2.0898e-01,  2.2363e-01,  2.4414e-01,  2.3633e-01,  2.5781e-01,\n",
      "         2.6953e-01,  2.1680e-01,  2.1973e-01,  2.6953e-01,  2.4707e-01,\n",
      "         2.5781e-01,  2.5000e-01,  2.3730e-01,  2.4023e-01,  1.9897e-02,\n",
      "         2.2070e-01,  2.6562e-01,  2.6367e-01,  2.4023e-01,  2.2754e-01,\n",
      "         2.6758e-01,  5.7812e-01,  2.5391e-01,  2.2266e-01,  2.6172e-01,\n",
      "         2.3828e-01,  2.3926e-01,  2.2559e-01,  2.5977e-01,  2.5195e-01,\n",
      "         2.2363e-01,  2.2363e-01,  2.1973e-01,  2.2656e-01,  2.5391e-01,\n",
      "         2.9688e-01,  2.4121e-01,  2.3730e-01,  2.3242e-01,  2.4707e-01,\n",
      "         2.2754e-01,  2.5586e-01,  2.2363e-01,  2.4316e-01,  2.4023e-01,\n",
      "         2.2461e-01,  2.4512e-01,  2.2949e-01,  2.3926e-01,  2.3242e-01,\n",
      "         2.4219e-01,  2.6562e-01, -1.8359e-01,  2.3926e-01,  2.3535e-01,\n",
      "         2.3242e-01,  2.0410e-01,  2.5391e-01,  2.5195e-01,  2.4902e-01,\n",
      "         2.3047e-01,  2.3828e-01,  2.5586e-01,  2.3535e-01,  2.4805e-01,\n",
      "         2.3926e-01,  3.1641e-01,  2.2949e-01,  2.4316e-01, -1.5723e-01,\n",
      "         2.4707e-01,  2.5977e-01,  2.3047e-01,  2.5977e-01,  2.1680e-01,\n",
      "         5.2734e-02,  2.4414e-01,  2.2656e-01,  2.3047e-01,  2.2168e-01,\n",
      "         2.2852e-01,  2.4707e-01,  2.5391e-01,  2.2363e-01,  2.0703e-01,\n",
      "         2.4023e-01,  2.4023e-01,  2.2363e-01,  2.5195e-01,  2.4316e-01,\n",
      "         2.5195e-01,  2.0801e-01,  2.5586e-01,  2.6367e-01,  2.6758e-01,\n",
      "         2.4219e-01,  2.4805e-01,  6.7871e-02,  2.3340e-01,  2.5391e-01,\n",
      "         2.5195e-01,  2.2559e-01,  2.3926e-01,  2.4414e-01,  2.3535e-01,\n",
      "         2.4707e-01,  2.5586e-01,  2.2949e-01,  2.1191e-01,  2.7734e-01,\n",
      "         2.3633e-01,  2.5977e-01,  2.6562e-01,  2.4609e-01,  2.4414e-01,\n",
      "         2.2168e-01,  2.5195e-01,  2.2852e-01,  2.6953e-01,  2.4219e-01,\n",
      "         2.3145e-01,  2.2070e-01,  2.5977e-01,  2.4609e-01,  2.3926e-01,\n",
      "         2.3340e-01,  2.3340e-01,  2.3145e-01,  2.5391e-01,  2.5195e-01,\n",
      "         2.2656e-01,  2.5391e-01,  2.5391e-01,  2.4023e-01,  1.9727e-01,\n",
      "         2.4121e-01,  2.4316e-01,  2.3145e-01,  2.2070e-01,  2.4316e-01,\n",
      "         2.4023e-01,  2.4316e-01,  2.2754e-01,  1.9824e-01,  2.2949e-01,\n",
      "         2.4707e-01,  2.5391e-01,  2.2168e-01,  2.4512e-01,  2.2461e-01,\n",
      "         2.6367e-01,  2.5195e-01,  1.1035e-01,  2.4805e-01,  2.5391e-01,\n",
      "         2.5195e-01,  2.0801e-01,  1.7480e-01,  2.2070e-01,  2.5586e-01,\n",
      "         2.8906e-01,  2.4512e-01,  2.3340e-01,  2.5000e-01,  2.3730e-01,\n",
      "         2.6172e-01,  2.4121e-01,  2.5781e-01,  2.3340e-01,  2.4316e-01,\n",
      "         2.5781e-01,  2.3242e-01,  2.5195e-01,  2.4902e-01,  2.0703e-01,\n",
      "         2.4805e-01,  2.2168e-01, -2.1680e-01,  2.4902e-01,  2.4707e-01,\n",
      "         2.4805e-01,  2.5195e-01,  2.5000e-01,  2.3828e-01,  2.4609e-01,\n",
      "         2.6953e-01,  2.4219e-01,  2.2363e-01,  2.3242e-01,  2.4414e-01,\n",
      "         2.5195e-01,  2.7148e-01,  2.2852e-01,  2.2168e-01,  2.4414e-01,\n",
      "         2.4316e-01,  2.4512e-01,  2.3535e-01,  2.5000e-01,  2.3047e-01,\n",
      "         2.4023e-01,  1.9824e-01,  2.3438e-01,  2.5195e-01,  2.4512e-01,\n",
      "         2.4219e-01,  2.4512e-01,  2.5195e-01,  2.3438e-01,  2.3633e-01,\n",
      "         2.3145e-01,  2.4219e-01,  2.4121e-01,  2.5977e-01,  2.5391e-01,\n",
      "        -2.5391e-02,  2.5977e-01,  2.4219e-01,  1.2451e-01,  2.5781e-01,\n",
      "         2.4609e-01,  1.9727e-01,  2.5195e-01,  2.4316e-01,  2.1582e-01,\n",
      "         2.4609e-01,  2.3340e-01,  2.4609e-01,  2.2559e-01,  2.3438e-01,\n",
      "         2.3438e-01,  2.4805e-01,  2.5586e-01,  2.5391e-01,  2.3438e-01,\n",
      "         2.6758e-01,  2.4902e-01,  2.2754e-01,  2.6172e-01,  2.3926e-01,\n",
      "         2.3828e-01,  2.2461e-01,  2.3340e-01, -5.0049e-02,  2.5391e-01,\n",
      "         2.2852e-01,  2.4414e-01,  2.3828e-01,  2.6172e-01,  2.3730e-01,\n",
      "         2.0996e-01,  2.5195e-01,  4.7852e-01,  2.1484e-01,  2.5195e-01,\n",
      "         2.5195e-01,  2.4707e-01,  2.5000e-01,  2.4707e-01,  2.3145e-01,\n",
      "         2.5586e-01,  2.1191e-01,  2.3633e-01,  2.3730e-01,  2.3145e-01,\n",
      "         2.5391e-01,  2.4805e-01,  2.3535e-01,  2.4805e-01,  2.3242e-01,\n",
      "         2.5195e-01,  2.4316e-01,  2.5391e-01,  2.4707e-01,  2.2754e-01,\n",
      "         2.2656e-01,  2.4316e-01,  2.4512e-01,  2.3535e-01,  1.2061e-01,\n",
      "         2.3535e-01,  2.3828e-01,  3.5858e-04,  2.5391e-01,  2.2949e-01,\n",
      "         2.3633e-01,  2.4707e-01,  2.3438e-01,  2.4414e-01,  2.5586e-01,\n",
      "        -1.6504e-01,  2.3730e-01,  2.3535e-01,  2.4121e-01,  2.2070e-01,\n",
      "         2.4219e-01,  2.5781e-01,  2.4121e-01,  2.4219e-01,  2.3340e-01,\n",
      "         2.3730e-01,  2.4902e-01,  2.2559e-01,  2.6953e-01,  2.5977e-01,\n",
      "         2.4023e-01,  2.4902e-01,  2.3438e-01,  2.3242e-01,  2.4707e-01,\n",
      "         2.5195e-01,  2.2461e-01,  2.3730e-01,  2.1094e-01,  2.4316e-01,\n",
      "         2.2461e-01,  2.3047e-01,  2.5195e-01,  2.5586e-01,  2.6367e-01,\n",
      "         2.2754e-01,  2.2266e-01,  2.8320e-01,  2.5195e-01,  2.3438e-01,\n",
      "         2.1387e-01,  2.4902e-01,  2.1973e-01,  2.5000e-01,  2.4609e-01,\n",
      "         2.6367e-01,  2.3145e-01,  2.4219e-01,  2.4023e-01,  2.3730e-01,\n",
      "         2.4121e-01,  2.5000e-01,  2.2070e-01,  2.1875e-01,  2.2852e-01,\n",
      "         2.3438e-01,  2.3633e-01,  2.3828e-01,  2.2656e-01,  2.4219e-01,\n",
      "         2.3242e-01,  2.5195e-01,  2.4121e-01,  2.6953e-01,  2.4121e-01,\n",
      "         2.3828e-01,  2.4023e-01,  2.4121e-01,  2.3145e-01,  2.3145e-01,\n",
      "         2.4316e-01,  2.2070e-01,  8.8867e-02,  2.8320e-01,  2.5195e-01,\n",
      "         2.8320e-01,  2.4707e-01,  2.2754e-01,  2.4316e-01,  2.4609e-01,\n",
      "         2.4805e-01,  2.4316e-01,  2.5195e-01,  2.3535e-01,  2.2070e-01,\n",
      "         2.3438e-01,  2.5781e-01,  2.7148e-01,  2.5977e-01,  7.4863e-05,\n",
      "         2.4316e-01,  2.5586e-01,  2.4609e-01,  2.3438e-01,  2.3828e-01,\n",
      "         2.3828e-01,  2.3047e-01,  2.3926e-01,  2.4414e-01,  2.2949e-01,\n",
      "         2.5781e-01,  2.6172e-01,  2.6367e-01,  2.3535e-01,  2.2363e-01,\n",
      "         2.4219e-01,  2.6562e-01,  2.4414e-01,  2.3438e-01,  2.4414e-01,\n",
      "         2.0605e-01,  2.4902e-01,  2.5000e-01,  2.2461e-01,  2.3535e-01,\n",
      "         2.4316e-01,  2.5586e-01,  2.0996e-01,  2.3047e-01,  2.3828e-01,\n",
      "         2.2559e-01,  2.4902e-01,  2.4609e-01,  2.5977e-01,  2.2754e-01,\n",
      "         2.4414e-01,  2.3535e-01,  2.9492e-01,  2.6562e-01,  2.3047e-01,\n",
      "         2.4023e-01,  2.3438e-01,  2.4512e-01,  2.1973e-01,  2.3730e-01,\n",
      "         2.4707e-01,  2.2656e-01,  2.2559e-01,  2.2949e-01,  2.3340e-01,\n",
      "         2.3438e-01,  2.4414e-01,  2.4219e-01,  3.2617e-01,  2.3926e-01,\n",
      "         2.3730e-01,  2.4902e-01,  2.4707e-01,  2.3340e-01,  2.2754e-01,\n",
      "         2.0996e-01,  2.5391e-01,  2.3535e-01,  2.4805e-01,  2.1777e-01,\n",
      "         2.3242e-01,  2.3145e-01,  2.3535e-01,  2.3242e-01,  2.7148e-01,\n",
      "         2.0801e-01,  2.5195e-01,  2.3242e-01,  2.3145e-01,  2.5586e-01,\n",
      "         2.1484e-01,  2.3730e-01,  2.2754e-01,  2.2656e-01,  2.3926e-01,\n",
      "         2.6758e-01,  1.9922e-01,  2.2852e-01,  2.6562e-01,  2.2070e-01,\n",
      "         2.2070e-01,  2.4707e-01,  2.5977e-01,  2.4316e-01,  2.5195e-01,\n",
      "         2.4609e-01,  2.5586e-01,  2.5195e-01,  2.0801e-01,  2.4707e-01,\n",
      "         2.4707e-01,  2.2852e-01,  2.5391e-01,  2.3633e-01,  2.4414e-01,\n",
      "         2.5195e-01,  2.4512e-01,  2.4805e-01,  2.2461e-01,  2.4316e-01,\n",
      "         2.2559e-01,  2.4414e-01,  2.2168e-01,  2.6172e-01,  2.4609e-01,\n",
      "         2.5781e-01,  2.3438e-01,  1.9824e-01,  2.4316e-01,  2.4902e-01,\n",
      "         2.4023e-01,  2.4805e-01,  2.6367e-01,  2.5586e-01,  2.3438e-01,\n",
      "         2.2559e-01,  2.3633e-01,  2.3340e-01, -1.7090e-01,  2.5781e-01,\n",
      "         2.5195e-01,  2.4609e-01, -2.1191e-01,  2.8516e-01,  2.5195e-01,\n",
      "         2.4121e-01,  2.4512e-01,  2.2949e-01,  2.4316e-01,  2.0117e-01,\n",
      "         2.8125e-01,  2.3340e-01,  2.6367e-01,  2.2559e-01,  2.2266e-01,\n",
      "         2.4707e-01,  2.4121e-01,  2.3340e-01,  2.4609e-01,  2.3828e-01,\n",
      "         2.4316e-01,  2.1582e-01,  2.3145e-01,  2.3633e-01,  2.2168e-01,\n",
      "         2.5195e-01,  2.4316e-01,  2.6367e-01,  1.2793e-01,  2.5195e-01,\n",
      "         2.3340e-01,  2.5195e-01,  2.3926e-01,  2.2754e-01,  2.3047e-01,\n",
      "         2.4414e-01,  1.7285e-01,  2.4512e-01,  2.2852e-01,  2.5195e-01,\n",
      "         2.2363e-01,  2.0898e-01,  2.4902e-01,  2.3730e-01,  2.0215e-01,\n",
      "         2.2266e-01,  2.3438e-01,  2.0996e-01,  2.3633e-01,  2.5391e-01,\n",
      "         2.6758e-01,  2.5000e-01,  2.2461e-01,  1.9238e-01,  2.5586e-01,\n",
      "         2.1875e-01,  2.1387e-01,  2.1973e-01,  2.2559e-01,  2.2363e-01,\n",
      "         2.3340e-01,  2.3438e-01,  2.3633e-01], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.decoder.block.8.layer.1.EncDecAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0466,  0.1099,  0.0299,  ...,  0.0095,  0.0454, -0.0273],\n",
      "        [-0.0405,  0.0097, -0.0073,  ...,  0.0588,  0.0089, -0.0413],\n",
      "        [ 0.0129,  0.0178,  0.0332,  ...,  0.0245, -0.0386, -0.0123],\n",
      "        ...,\n",
      "        [ 0.0549,  0.0654,  0.0066,  ...,  0.0508,  0.0121,  0.0305],\n",
      "        [ 0.0155, -0.0291, -0.0172,  ...,  0.0049,  0.0034,  0.0042],\n",
      "        [ 0.0059, -0.0322, -0.0317,  ...,  0.0206, -0.0056,  0.0623]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.8.layer.1.EncDecAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0061,  0.0354, -0.0747,  ...,  0.1328,  0.5625, -0.1660],\n",
      "        [-0.4082, -0.0059, -0.0574,  ..., -0.5234, -0.1035, -0.2500],\n",
      "        [ 0.0019,  0.3281,  0.0486,  ..., -0.0942, -0.6836, -0.3379],\n",
      "        ...,\n",
      "        [-0.3066, -0.0221, -0.3359,  ...,  0.2656, -0.1592,  0.2490],\n",
      "        [-0.4160, -0.4844,  0.6484,  ..., -0.4414, -0.2930, -0.0530],\n",
      "        [-0.1011, -0.0023,  0.1904,  ..., -0.4805,  0.0508, -0.1133]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.8.layer.1.EncDecAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.5820,  0.1934,  0.3887,  ...,  0.7305,  0.9180, -0.7656],\n",
      "        [ 0.3613,  0.3164, -0.9609,  ..., -0.2793,  0.0064,  0.2949],\n",
      "        [-1.0234,  0.3574, -0.7109,  ...,  0.6250,  1.4141, -0.4180],\n",
      "        ...,\n",
      "        [ 0.5391, -0.3027, -0.2432,  ...,  0.4434, -0.4844,  0.7617],\n",
      "        [ 0.5078, -0.1504,  0.3535,  ...,  0.8320,  0.7266, -0.2334],\n",
      "        [-0.0089,  0.2793,  0.3359,  ..., -0.4180,  0.4043,  0.1504]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.8.layer.1.EncDecAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.5156,  1.5938, -0.8984,  ..., -0.4141,  0.9727, -0.3828],\n",
      "        [-1.1719,  0.0811, -0.6719,  ...,  0.0435, -0.1768,  0.6719],\n",
      "        [-0.3027, -0.5508,  0.7188,  ...,  0.8594, -0.5117,  1.0781],\n",
      "        ...,\n",
      "        [ 0.6016,  0.4453,  0.3086,  ...,  0.3867, -0.6133, -0.4473],\n",
      "        [ 2.3438,  0.5703, -1.0938,  ..., -0.6836,  0.9297,  0.4023],\n",
      "        [ 0.4688,  0.2266, -0.1914,  ...,  0.3457, -1.4922,  0.2070]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.8.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 6.3965e-02,  6.2256e-02,  6.3965e-02,  5.1514e-02,  6.2988e-02,\n",
      "         5.2979e-02,  9.5703e-02,  7.4219e-02,  6.2256e-02,  7.3242e-02,\n",
      "         5.0049e-02,  7.8613e-02,  6.3477e-02,  4.9072e-02,  6.9336e-02,\n",
      "         7.2266e-02,  6.2500e-02,  6.7871e-02,  5.6885e-02,  9.4727e-02,\n",
      "         6.7383e-02,  7.0801e-02,  7.4707e-02,  8.0078e-02,  7.5684e-02,\n",
      "         7.5195e-02,  6.9824e-02,  4.9805e-02,  7.9102e-02,  7.1777e-02,\n",
      "         7.4219e-02,  7.1777e-02,  6.8359e-02,  8.0078e-02,  7.7148e-02,\n",
      "         7.0801e-02,  8.6914e-02,  7.2754e-02,  6.6895e-02,  7.1777e-02,\n",
      "         5.8838e-02,  6.5918e-02,  7.0312e-02,  6.6406e-02,  7.7148e-02,\n",
      "         7.0312e-02,  9.1309e-02,  8.8379e-02,  7.8125e-02,  6.8359e-02,\n",
      "         8.3008e-02,  6.6406e-02,  7.0801e-02,  7.1289e-02,  8.5449e-02,\n",
      "         5.1514e-02,  5.6396e-02,  6.7871e-02,  9.1797e-02,  6.7383e-02,\n",
      "         6.2256e-02,  7.4707e-02,  7.8125e-02,  7.1289e-02,  8.0566e-02,\n",
      "         9.7656e-02,  7.0312e-02,  6.5430e-02,  5.4932e-02,  6.7871e-02,\n",
      "         7.8613e-02,  8.5938e-02,  8.4961e-02,  6.7383e-02,  7.6660e-02,\n",
      "         6.5918e-02, -5.3467e-02,  6.1768e-02,  7.8125e-02,  7.9102e-02,\n",
      "         7.7637e-02,  6.5430e-02,  6.6406e-02,  6.5918e-02,  8.6914e-02,\n",
      "         7.3242e-02,  2.0874e-02,  6.8359e-02,  7.7637e-02,  7.0801e-02,\n",
      "         6.4941e-02,  7.1289e-02,  6.5430e-02,  4.9561e-02,  7.0312e-02,\n",
      "         7.6172e-02,  7.2266e-02,  8.0566e-02,  7.9590e-02,  5.9570e-02,\n",
      "         5.1025e-02,  7.2266e-02,  5.4932e-02,  6.7871e-02,  6.8359e-02,\n",
      "         5.9570e-02,  7.3242e-02,  6.4941e-02,  8.5938e-02,  7.6660e-02,\n",
      "         8.1055e-02,  7.3730e-02,  6.0059e-02,  6.6895e-02,  6.2500e-02,\n",
      "         1.0156e-01,  6.5918e-02,  6.0791e-02, -7.3433e-05,  6.7383e-02,\n",
      "         6.7871e-02,  7.7148e-02,  7.4219e-02,  6.4941e-02,  7.9102e-02,\n",
      "         6.4453e-02,  7.0312e-02,  5.3955e-02,  6.2500e-02, -5.3955e-02,\n",
      "         5.4443e-02,  7.0801e-02,  6.8359e-02,  7.3242e-02,  5.3223e-02,\n",
      "        -6.0547e-02,  6.1523e-02,  6.1035e-02,  5.8594e-02,  5.4688e-02,\n",
      "         6.4941e-02,  7.8125e-02,  5.4443e-02,  6.4453e-02,  6.2988e-02,\n",
      "         5.2490e-02,  8.3008e-02,  6.2500e-02,  7.0801e-02,  5.6641e-02,\n",
      "         9.7168e-02,  6.4453e-02,  9.8633e-02, -6.4453e-02,  8.1543e-02,\n",
      "         8.7402e-02,  6.7383e-02,  8.4961e-02,  1.2305e-01,  5.3711e-02,\n",
      "         6.4941e-02,  5.9570e-02,  5.8350e-02,  6.8359e-02,  7.6172e-02,\n",
      "         5.2734e-02,  8.3984e-02,  6.4453e-02,  5.7373e-02,  7.2754e-02,\n",
      "        -4.7119e-02,  7.1777e-02,  8.4473e-02,  7.2754e-02,  7.5195e-02,\n",
      "         7.0312e-02,  6.0791e-02,  8.5449e-02,  4.6631e-02,  7.4707e-02,\n",
      "         7.2754e-02,  7.5684e-02,  8.0566e-02,  8.1055e-02,  6.0791e-02,\n",
      "         7.7637e-02,  7.3730e-02,  8.0078e-02,  6.6895e-02,  5.2490e-02,\n",
      "         7.8613e-02,  6.8848e-02,  7.6660e-02,  6.8359e-02,  6.6895e-02,\n",
      "         7.3242e-02,  6.8359e-02,  8.6914e-02,  7.4707e-02,  7.0801e-02,\n",
      "         7.8613e-02, -5.5908e-02,  7.2754e-02,  7.3730e-02,  6.4453e-02,\n",
      "         5.3223e-02,  6.5430e-02,  7.3242e-02,  6.7871e-02,  7.7637e-02,\n",
      "         8.5938e-02,  5.6152e-02,  6.2988e-02,  7.7148e-02,  7.4219e-02,\n",
      "         7.7637e-02,  6.3477e-02,  7.2266e-02,  6.6406e-02, -7.4506e-06,\n",
      "         5.7617e-02,  9.6680e-02,  7.2754e-02,  6.8848e-02,  6.1768e-02,\n",
      "         7.9102e-02,  2.1191e-01,  8.2520e-02,  6.3965e-02,  7.5195e-02,\n",
      "         7.4219e-02,  6.5918e-02,  6.7383e-02,  7.6660e-02,  7.4707e-02,\n",
      "         6.4453e-02,  5.3467e-02,  5.7861e-02,  5.7129e-02,  6.7383e-02,\n",
      "         6.5430e-02,  7.7148e-02,  7.1289e-02,  6.4941e-02,  7.9102e-02,\n",
      "         6.0791e-02,  8.8379e-02,  5.6885e-02,  7.4219e-02,  6.5918e-02,\n",
      "         5.6641e-02,  6.8848e-02,  7.1289e-02,  6.7383e-02,  7.4707e-02,\n",
      "         6.2988e-02,  7.4707e-02,  5.1514e-02,  7.7148e-02,  8.0078e-02,\n",
      "        -6.9824e-02, -5.4199e-02,  7.2754e-02,  7.5195e-02,  7.5195e-02,\n",
      "         6.3965e-02,  7.6172e-02,  8.1543e-02,  6.7383e-02,  6.0547e-02,\n",
      "         7.1289e-02,  8.3008e-02,  7.6172e-02,  7.1777e-02, -4.9805e-02,\n",
      "         6.9824e-02,  7.9590e-02,  5.7617e-02,  7.8613e-02, -5.3955e-02,\n",
      "        -2.9373e-04,  6.3477e-02,  6.7871e-02,  6.5918e-02,  6.7871e-02,\n",
      "         6.2988e-02,  6.9336e-02,  7.6172e-02,  5.9082e-02,  4.6631e-02,\n",
      "         6.8848e-02,  6.8848e-02,  6.2500e-02,  7.5195e-02,  6.6895e-02,\n",
      "         7.9590e-02,  6.5918e-02,  7.5684e-02,  7.4707e-02,  9.1797e-02,\n",
      "         8.4473e-02,  6.9336e-02, -2.5024e-02,  5.7373e-02,  7.0312e-02,\n",
      "         7.6172e-02,  6.0303e-02,  6.9824e-02,  6.1279e-02,  7.3730e-02,\n",
      "         9.3262e-02,  7.7637e-02,  6.7383e-02,  6.1768e-02,  2.0508e-01,\n",
      "         7.1777e-02,  1.1084e-01,  6.4453e-02,  7.4219e-02,  8.5449e-02,\n",
      "         6.7383e-02,  8.0566e-02, -5.2979e-02,  8.3008e-02,  6.9336e-02,\n",
      "         6.8848e-02,  6.0547e-02,  8.4961e-02,  7.1777e-02,  6.7383e-02,\n",
      "         7.5195e-02,  6.2256e-02,  7.7637e-02,  7.3242e-02,  7.5195e-02,\n",
      "         6.8359e-02,  7.5195e-02,  9.6680e-02,  7.3730e-02, -4.1748e-02,\n",
      "         6.1279e-02,  7.1289e-02,  7.4219e-02,  6.4453e-02,  8.5449e-02,\n",
      "         7.2266e-02,  8.0078e-02,  6.0059e-02,  5.7373e-02,  5.9082e-02,\n",
      "         6.7871e-02,  7.2754e-02,  5.6885e-02,  8.7402e-02,  5.9326e-02,\n",
      "         8.7891e-02,  7.5195e-02,  1.3638e-04,  7.9102e-02,  8.2031e-02,\n",
      "         7.4219e-02,  6.3477e-02,  5.5908e-02,  5.8838e-02,  8.0566e-02,\n",
      "         9.7656e-02,  6.5918e-02,  5.9570e-02,  6.7871e-02,  6.5430e-02,\n",
      "         7.3242e-02,  8.0566e-02,  7.8125e-02, -6.3477e-02,  7.6660e-02,\n",
      "         6.9336e-02,  6.2256e-02,  9.4238e-02,  7.6172e-02,  5.2734e-02,\n",
      "         6.4453e-02,  6.6895e-02,  5.3711e-02,  7.0312e-02,  7.1777e-02,\n",
      "         7.9102e-02,  7.5684e-02,  7.5195e-02,  6.4453e-02,  7.3730e-02,\n",
      "         7.7637e-02,  6.4941e-02,  5.5908e-02,  5.6885e-02,  6.9824e-02,\n",
      "         7.1777e-02,  8.1055e-02,  6.4453e-02,  6.7871e-02,  6.6895e-02,\n",
      "         5.8594e-02,  7.2754e-02,  7.3242e-02,  7.7637e-02,  6.3965e-02,\n",
      "         7.0312e-02,  5.7129e-02,  6.7383e-02,  6.4453e-02,  7.0312e-02,\n",
      "         7.1777e-02,  7.5195e-02,  6.6895e-02,  7.0801e-02,  7.7637e-02,\n",
      "         8.5449e-02,  7.2266e-02,  7.6172e-02,  2.3633e-01,  7.2754e-02,\n",
      "        -2.1935e-04,  7.3730e-02,  7.9590e-02,  1.9165e-02,  7.6660e-02,\n",
      "         7.7637e-02,  5.1270e-02,  7.5195e-02,  6.0547e-02, -4.8340e-02,\n",
      "         8.1055e-02,  6.8848e-02,  7.4707e-02,  5.5908e-02,  6.5430e-02,\n",
      "         6.6406e-02,  7.4707e-02,  7.1289e-02,  6.8359e-02,  6.1279e-02,\n",
      "         8.4473e-02,  6.4941e-02,  6.7871e-02,  8.3008e-02,  7.4707e-02,\n",
      "         7.0312e-02,  7.2266e-02,  6.7871e-02,  1.1110e-04,  6.9336e-02,\n",
      "         6.6895e-02,  7.3242e-02,  7.0312e-02,  7.2266e-02,  6.8359e-02,\n",
      "         5.7373e-02,  7.4219e-02,  1.6211e-01,  5.6885e-02,  6.9336e-02,\n",
      "         7.2754e-02,  7.9590e-02,  7.8613e-02,  7.2754e-02,  6.6895e-02,\n",
      "         7.6660e-02, -4.7607e-02,  6.4453e-02,  6.4453e-02,  6.2988e-02,\n",
      "         7.2754e-02,  7.4219e-02,  8.3008e-02,  7.8125e-02,  6.5918e-02,\n",
      "         7.1777e-02,  7.5195e-02,  8.3008e-02,  6.7871e-02,  6.3477e-02,\n",
      "        -5.4932e-02,  6.5430e-02,  8.5449e-02,  6.8848e-02, -1.1873e-04,\n",
      "         6.9824e-02,  7.0312e-02,  8.6914e-02,  7.6660e-02,  6.4453e-02,\n",
      "         7.0312e-02,  9.4727e-02,  7.4219e-02,  8.1543e-02,  8.5938e-02,\n",
      "         1.0449e-01,  6.5918e-02,  6.7383e-02,  7.2754e-02,  6.7871e-02,\n",
      "         7.3242e-02,  1.2012e-01,  7.7637e-02,  6.3477e-02,  7.0312e-02,\n",
      "         6.6406e-02,  7.6172e-02, -5.8350e-02,  6.8359e-02,  7.3242e-02,\n",
      "         6.8359e-02,  7.6172e-02,  5.8105e-02,  7.2754e-02,  7.7148e-02,\n",
      "         7.3242e-02,  6.2256e-02,  6.4941e-02,  4.7607e-02,  7.8125e-02,\n",
      "         7.1289e-02,  6.6406e-02,  8.2520e-02,  7.6660e-02,  7.0801e-02,\n",
      "         7.2266e-02,  6.6406e-02,  7.7148e-02,  6.7383e-02,  6.7383e-02,\n",
      "         5.6152e-02,  7.3242e-02,  6.5430e-02,  6.9824e-02,  7.6172e-02,\n",
      "         7.7148e-02,  6.7871e-02,  6.4941e-02,  6.6406e-02,  6.6895e-02,\n",
      "         7.7637e-02,  7.9590e-02, -5.6396e-02,  5.9814e-02,  6.2500e-02,\n",
      "         7.2754e-02,  7.2266e-02,  6.0791e-02,  7.1777e-02,  6.4941e-02,\n",
      "         5.6641e-02,  8.7891e-02,  6.7871e-02,  7.1777e-02,  7.5684e-02,\n",
      "         7.5195e-02,  5.4932e-02,  6.6406e-02,  6.3477e-02,  7.2266e-02,\n",
      "         7.2754e-02,  6.2012e-02,  4.5776e-05,  7.1289e-02,  7.2266e-02,\n",
      "         9.0332e-02,  8.0078e-02,  6.3477e-02,  6.2012e-02,  7.6172e-02,\n",
      "         8.8379e-02,  6.2988e-02,  7.1777e-02,  6.2012e-02,  5.1758e-02,\n",
      "         6.2988e-02,  7.9590e-02,  8.1055e-02,  8.0078e-02,  2.9182e-04,\n",
      "         6.5918e-02,  8.4473e-02,  6.8359e-02,  6.6895e-02,  7.1289e-02,\n",
      "         8.1543e-02,  6.7871e-02,  6.2500e-02,  7.1777e-02,  7.2266e-02,\n",
      "         7.4707e-02,  7.9590e-02,  8.4961e-02,  6.5918e-02,  6.2256e-02,\n",
      "         7.3242e-02,  7.4219e-02,  7.4707e-02, -5.4932e-02,  6.5918e-02,\n",
      "         5.2734e-02,  7.5684e-02,  7.2754e-02,  6.5430e-02,  6.4453e-02,\n",
      "         6.5918e-02,  8.0566e-02,  6.5918e-02,  6.9824e-02,  6.8359e-02,\n",
      "         6.3965e-02,  6.9336e-02,  6.8848e-02,  8.4961e-02,  6.3477e-02,\n",
      "         8.2031e-02,  6.2256e-02,  1.7480e-01,  8.4961e-02,  7.0801e-02,\n",
      "         7.5195e-02,  6.6895e-02,  6.8848e-02,  5.1025e-02,  7.1289e-02,\n",
      "         6.4453e-02,  6.4941e-02,  6.7383e-02,  6.8359e-02,  6.6406e-02,\n",
      "         7.0801e-02,  7.5684e-02,  6.4453e-02,  8.2031e-02,  6.7871e-02,\n",
      "         6.6406e-02,  7.0312e-02,  6.7383e-02,  7.0801e-02,  6.9336e-02,\n",
      "        -5.3467e-02,  7.0801e-02,  7.8125e-02,  7.3242e-02,  5.4199e-02,\n",
      "         6.4941e-02,  5.2979e-02,  7.1777e-02,  6.6406e-02,  9.0332e-02,\n",
      "         6.9824e-02,  7.4219e-02,  6.1035e-02,  6.6895e-02,  7.8613e-02,\n",
      "         4.2480e-02,  6.2012e-02, -5.7373e-02,  6.4941e-02,  6.7871e-02,\n",
      "         8.5938e-02, -5.2490e-02,  5.3223e-02,  1.0840e-01,  6.8848e-02,\n",
      "         6.0059e-02,  7.8125e-02,  7.2754e-02,  7.1777e-02,  7.5684e-02,\n",
      "         7.1289e-02,  7.3242e-02,  6.7871e-02,  5.4443e-02,  8.2031e-02,\n",
      "         7.6660e-02,  6.9336e-02,  8.8379e-02,  6.2988e-02,  6.9336e-02,\n",
      "         7.4219e-02,  7.8613e-02,  6.3965e-02,  6.6406e-02,  6.1035e-02,\n",
      "         5.9082e-02,  6.8848e-02,  6.0303e-02,  7.8125e-02,  6.2988e-02,\n",
      "         7.1289e-02,  6.5430e-02,  5.2734e-02,  6.4941e-02,  6.9336e-02,\n",
      "         6.4941e-02,  8.2031e-02,  7.8613e-02,  7.4219e-02,  6.6895e-02,\n",
      "         6.7871e-02,  8.2520e-02,  6.5918e-02,  5.1514e-02,  8.3008e-02,\n",
      "         7.2266e-02,  7.0312e-02, -4.6875e-02,  7.9590e-02,  7.1289e-02,\n",
      "         6.5918e-02,  6.6895e-02,  7.7637e-02,  7.2266e-02,  4.2969e-02,\n",
      "         1.4746e-01,  6.2988e-02,  9.6680e-02,  6.8359e-02,  5.3223e-02,\n",
      "         8.8867e-02,  6.7871e-02,  5.5420e-02,  7.7148e-02,  7.2266e-02,\n",
      "         6.7871e-02,  6.0791e-02,  7.0801e-02,  6.7383e-02, -6.9824e-02,\n",
      "         7.7637e-02,  6.6895e-02,  8.3496e-02,  6.6406e-02,  1.0449e-01,\n",
      "         7.4707e-02,  7.8125e-02,  7.1777e-02,  6.8359e-02,  6.1768e-02,\n",
      "         7.1777e-02,  5.2979e-02,  7.1289e-02,  5.3955e-02,  7.8125e-02,\n",
      "         6.0791e-02,  5.6396e-02,  8.0078e-02,  6.5918e-02,  4.7363e-02,\n",
      "         5.6396e-02,  6.2500e-02, -6.0303e-02,  7.0312e-02,  7.2754e-02,\n",
      "         9.0820e-02,  6.8848e-02,  6.2500e-02,  7.1777e-02,  6.8848e-02,\n",
      "         6.2012e-02,  5.9570e-02,  5.8594e-02,  5.9814e-02,  7.3730e-02,\n",
      "         6.3477e-02,  7.0801e-02,  5.6885e-02], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.decoder.block.8.layer.2.DenseReluDense.wi.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.2227,  0.0405,  0.0286,  ..., -0.3535, -0.2461, -0.1089],\n",
      "        [ 0.4883,  0.3359,  0.3398,  ...,  0.1787, -0.2207,  0.5625],\n",
      "        [ 0.1050,  0.9766, -0.7266,  ..., -0.0664,  0.6055, -0.0112],\n",
      "        ...,\n",
      "        [ 0.1748,  0.9453, -0.7656,  ...,  0.5352,  0.2002, -0.5703],\n",
      "        [-0.2500,  0.2178, -0.4824,  ...,  0.0991, -0.0251,  0.1245],\n",
      "        [ 0.2227,  0.1670,  0.0957,  ...,  0.4336,  0.4707,  1.0547]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.8.layer.2.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.2559,  0.1572,  0.2100,  ..., -0.7461,  0.1523,  0.1426],\n",
      "        [ 0.1348, -0.1338, -0.2471,  ..., -0.1963,  0.1143,  0.4219],\n",
      "        [-0.0214,  0.1904, -0.1748,  ..., -0.0806,  0.1611,  0.3047],\n",
      "        ...,\n",
      "        [-0.0077, -0.1309, -0.4258,  ...,  0.4785, -0.0192, -0.4434],\n",
      "        [-0.5078,  0.0776,  0.0383,  ...,  0.0894,  0.0302,  0.0277],\n",
      "        [ 0.3867, -0.4961,  0.1475,  ..., -0.0801,  0.1455,  0.6016]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.8.layer.2.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 4.3438e+00,  4.5938e+00,  4.5312e+00,  3.3438e+00,  4.3125e+00,\n",
      "         4.0312e+00,  4.2500e+00,  4.9688e+00,  4.0938e+00,  4.2188e+00,\n",
      "         4.4062e+00,  4.4688e+00,  5.2812e+00,  4.6562e+00,  4.5625e+00,\n",
      "         4.4375e+00,  4.4062e+00,  4.0625e+00,  4.2188e+00,  4.5312e+00,\n",
      "         4.3125e+00,  4.3750e+00,  4.4688e+00,  4.1562e+00,  4.2500e+00,\n",
      "         4.1875e+00,  4.2812e+00,  4.6875e+00,  4.6250e+00,  4.2812e+00,\n",
      "         4.4062e+00,  5.5938e+00,  4.2812e+00,  4.1562e+00,  4.6875e+00,\n",
      "         4.6875e+00,  4.4688e+00,  4.7812e+00,  4.0938e+00,  4.3438e+00,\n",
      "         4.0000e+00,  4.1562e+00,  4.3750e+00,  4.5000e+00,  4.7500e+00,\n",
      "         4.3438e+00,  4.4375e+00,  4.6875e+00,  4.4375e+00,  4.4688e+00,\n",
      "         4.2812e+00,  4.1875e+00,  4.6875e+00,  4.2188e+00,  5.8125e+00,\n",
      "         4.2188e+00,  4.2188e+00,  4.2500e+00,  4.4688e+00,  4.2500e+00,\n",
      "         4.3750e+00,  4.3125e+00,  4.5625e+00,  4.3438e+00,  4.1875e+00,\n",
      "         6.5625e+00,  4.1875e+00,  4.7188e+00,  5.5938e+00,  4.2188e+00,\n",
      "         4.5312e+00,  4.3750e+00,  4.4688e+00,  4.2500e+00,  4.8750e+00,\n",
      "         4.2812e+00,  4.3438e+00,  4.3125e+00,  4.4688e+00,  4.5938e+00,\n",
      "         4.2500e+00,  4.1250e+00,  4.2812e+00,  4.4062e+00,  4.5938e+00,\n",
      "         4.5000e+00,  1.5859e+00,  4.1250e+00,  4.1250e+00,  4.5312e+00,\n",
      "         4.3438e+00,  4.1875e+00,  4.1250e+00,  4.2812e+00,  4.6250e+00,\n",
      "         4.1875e+00,  4.2812e+00,  4.2812e+00,  4.2812e+00,  4.0938e+00,\n",
      "         4.7188e+00,  4.1875e+00,  5.8750e+00,  4.2812e+00,  4.2500e+00,\n",
      "         3.9844e+00,  4.2500e+00,  4.3438e+00,  4.3125e+00,  4.5312e+00,\n",
      "         4.3750e+00,  4.1875e+00,  4.2188e+00,  4.1875e+00,  4.5000e+00,\n",
      "         4.3438e+00,  4.4062e+00,  4.5000e+00, -3.7537e-03,  4.4375e+00,\n",
      "         4.1250e+00,  4.0000e+00,  4.5312e+00,  4.0312e+00,  4.4375e+00,\n",
      "         4.1562e+00,  4.2812e+00,  3.9531e+00,  4.5000e+00,  4.4375e+00,\n",
      "         5.5938e+00,  4.4062e+00,  4.2500e+00,  4.1250e+00,  3.6250e+00,\n",
      "         4.1250e+00,  4.2812e+00,  4.6250e+00,  4.3438e+00,  4.1250e+00,\n",
      "         4.8438e+00,  4.3750e+00,  4.3438e+00,  4.5625e+00,  4.3125e+00,\n",
      "         4.2188e+00,  4.3438e+00,  4.4375e+00,  4.3438e+00,  4.1562e+00,\n",
      "         4.5312e+00,  4.4375e+00,  4.7812e+00,  4.3438e+00,  4.3438e+00,\n",
      "         4.3125e+00,  4.3438e+00,  4.3438e+00,  3.7500e+00,  4.3438e+00,\n",
      "         4.1250e+00,  4.4062e+00,  4.2500e+00,  4.7188e+00,  4.3438e+00,\n",
      "         4.5625e+00,  4.7500e+00,  4.4375e+00,  4.9688e+00,  4.3750e+00,\n",
      "         5.2188e+00,  4.3125e+00,  4.3750e+00,  4.2812e+00,  4.2812e+00,\n",
      "         3.9375e+00,  4.1875e+00,  4.1875e+00,  4.8750e+00,  4.1250e+00,\n",
      "         4.4062e+00,  4.6562e+00,  4.1875e+00,  4.5938e+00,  4.7188e+00,\n",
      "         4.8125e+00,  4.1875e+00,  2.8281e+00,  4.4375e+00,  4.0625e+00,\n",
      "         4.2812e+00,  4.2500e+00,  4.2188e+00,  4.2812e+00,  3.9688e+00,\n",
      "         4.3125e+00,  4.0312e+00,  4.4375e+00,  4.6250e+00,  4.0938e+00,\n",
      "         4.4062e+00,  4.0938e+00,  4.5000e+00,  4.4062e+00,  4.1875e+00,\n",
      "         4.0000e+00,  3.8438e+00,  4.3438e+00,  4.3750e+00,  4.3438e+00,\n",
      "         4.6875e+00,  4.2188e+00,  4.0312e+00,  4.5938e+00,  4.0625e+00,\n",
      "         4.5312e+00,  4.7812e+00,  4.3438e+00,  4.3438e+00,  9.2578e-01,\n",
      "         5.0312e+00,  4.2500e+00,  4.7500e+00,  4.2188e+00,  4.5625e+00,\n",
      "         4.2188e+00,  3.8750e+00,  4.3125e+00,  4.2188e+00,  4.6562e+00,\n",
      "         4.4688e+00,  4.5000e+00,  4.1875e+00,  4.4062e+00,  4.4375e+00,\n",
      "         4.4375e+00,  4.4688e+00,  3.9531e+00,  4.7812e+00,  4.5000e+00,\n",
      "         6.0938e+00,  4.1875e+00,  4.3438e+00,  4.4062e+00,  4.2812e+00,\n",
      "         4.0000e+00,  4.3125e+00,  4.0938e+00,  4.2500e+00,  4.2188e+00,\n",
      "         4.4688e+00,  4.5312e+00,  4.4375e+00,  4.4375e+00,  4.5625e+00,\n",
      "         4.3125e+00,  4.5938e+00,  4.7812e+00,  4.2812e+00,  4.5938e+00,\n",
      "         4.8750e+00,  4.6875e+00,  4.5000e+00,  4.3125e+00,  4.3438e+00,\n",
      "         4.3438e+00,  4.5000e+00,  4.3125e+00,  4.2188e+00,  4.7500e+00,\n",
      "         4.2500e+00,  5.5625e+00,  4.3438e+00,  4.3750e+00,  5.0938e+00,\n",
      "         4.2812e+00,  4.5312e+00,  4.2812e+00,  4.5625e+00,  4.4062e+00,\n",
      "         7.7344e-01,  5.3438e+00,  4.5000e+00,  4.2812e+00,  4.2500e+00,\n",
      "         4.2812e+00,  4.2188e+00,  4.5625e+00,  4.5312e+00,  4.2812e+00,\n",
      "         4.5000e+00,  4.2500e+00,  4.0000e+00,  4.3125e+00,  4.2500e+00,\n",
      "         4.5625e+00,  4.4688e+00,  4.4688e+00,  4.6250e+00,  4.8750e+00,\n",
      "         4.3125e+00,  4.3438e+00,  1.5234e+00,  4.2188e+00,  4.2188e+00,\n",
      "         4.4062e+00,  4.3438e+00,  4.2188e+00,  4.6562e+00,  4.1875e+00,\n",
      "         4.1250e+00,  4.5000e+00,  4.0312e+00,  4.0625e+00,  4.1562e+00,\n",
      "         4.5000e+00,  4.2500e+00,  4.6875e+00,  4.4688e+00,  4.4062e+00,\n",
      "         3.8750e+00,  4.5938e+00,  4.1562e+00,  4.5000e+00,  4.2188e+00,\n",
      "         4.3750e+00,  4.0625e+00,  4.5625e+00,  4.3750e+00,  4.1562e+00,\n",
      "         4.3438e+00,  4.7812e+00,  4.1562e+00,  4.2188e+00,  4.4375e+00,\n",
      "         4.2812e+00,  4.2812e+00,  4.5000e+00,  4.1875e+00,  3.9844e+00,\n",
      "         4.5000e+00,  4.5312e+00,  4.2500e+00,  3.9219e+00,  4.3125e+00,\n",
      "         4.2812e+00,  4.1562e+00,  4.5000e+00,  4.4375e+00,  4.6562e+00,\n",
      "         4.1562e+00,  4.2188e+00,  4.0625e+00,  4.2188e+00,  4.1562e+00,\n",
      "         4.4062e+00,  4.5938e+00,  2.6245e-02,  4.5000e+00,  4.7500e+00,\n",
      "         4.4688e+00,  4.1562e+00,  5.5625e+00,  4.0625e+00,  4.4062e+00,\n",
      "         4.3750e+00,  4.7188e+00,  4.0938e+00,  4.2812e+00,  4.3750e+00,\n",
      "         4.6875e+00,  4.2500e+00,  4.4375e+00,  4.3125e+00,  4.3438e+00,\n",
      "         4.3750e+00,  4.0625e+00,  4.4375e+00,  4.6562e+00,  4.4688e+00,\n",
      "         4.1562e+00,  4.2500e+00,  4.6875e+00,  4.0625e+00,  4.6562e+00,\n",
      "         4.5938e+00,  4.1875e+00,  4.4688e+00,  4.2500e+00,  4.0938e+00,\n",
      "         4.6875e+00,  4.0938e+00,  4.2812e+00,  4.3750e+00,  4.5625e+00,\n",
      "         4.4688e+00,  4.6250e+00,  4.1562e+00,  4.3125e+00,  4.5625e+00,\n",
      "         4.2812e+00,  4.4688e+00,  4.2500e+00,  4.6562e+00,  4.0938e+00,\n",
      "         4.2500e+00,  4.3125e+00,  4.1250e+00,  4.0938e+00,  4.2188e+00,\n",
      "         4.4375e+00,  4.3750e+00,  4.2188e+00,  4.1562e+00,  4.3750e+00,\n",
      "         4.2500e+00,  4.7500e+00,  4.4062e+00,  4.6250e+00,  4.4375e+00,\n",
      "         7.1094e-01,  4.3125e+00,  4.3125e+00,  2.4219e+00,  4.5938e+00,\n",
      "         4.3438e+00,  4.1250e+00,  4.2188e+00,  4.5625e+00,  4.5312e+00,\n",
      "         4.2500e+00,  4.4375e+00,  4.4688e+00,  4.2812e+00,  4.0938e+00,\n",
      "         4.0938e+00,  4.1562e+00,  4.5938e+00,  4.6250e+00,  4.6562e+00,\n",
      "         4.5938e+00,  5.0938e+00,  4.0312e+00,  4.4375e+00,  4.1250e+00,\n",
      "         4.2500e+00,  4.1562e+00,  4.2812e+00,  7.2266e-01,  4.2812e+00,\n",
      "         4.2500e+00,  4.2188e+00,  4.4375e+00,  4.4375e+00,  4.4688e+00,\n",
      "         4.3438e+00,  4.5938e+00,  4.7500e+00,  4.1875e+00,  4.3750e+00,\n",
      "         4.4688e+00,  4.5312e+00,  4.5312e+00,  4.5312e+00,  4.5938e+00,\n",
      "         4.5312e+00,  4.3125e+00,  4.2500e+00,  4.0625e+00,  4.5312e+00,\n",
      "         4.4688e+00,  4.0625e+00,  5.8125e+00,  4.3750e+00,  4.2188e+00,\n",
      "         4.3750e+00,  4.0938e+00,  4.4062e+00,  4.4375e+00,  4.3438e+00,\n",
      "         4.0938e+00,  4.4375e+00,  4.3438e+00,  4.4375e+00,  7.2266e-01,\n",
      "         4.4375e+00,  4.0625e+00,  8.8672e-01,  4.4375e+00,  4.2812e+00,\n",
      "         4.5625e+00,  4.3750e+00,  4.3438e+00,  4.4062e+00,  4.5938e+00,\n",
      "         8.5625e+00,  4.5938e+00,  4.4375e+00,  4.3750e+00,  4.2812e+00,\n",
      "         4.4062e+00,  4.3750e+00,  4.3750e+00,  4.4375e+00,  4.2500e+00,\n",
      "         4.4375e+00,  4.5938e+00,  4.0938e+00,  4.2500e+00,  4.5625e+00,\n",
      "         4.1875e+00,  4.2500e+00,  4.7812e+00,  4.4062e+00,  4.2812e+00,\n",
      "         4.4375e+00,  4.3125e+00,  4.0312e+00,  4.6250e+00,  4.3125e+00,\n",
      "         4.0938e+00,  4.2812e+00,  4.5000e+00,  4.3438e+00,  4.6250e+00,\n",
      "         4.2188e+00,  4.4688e+00,  4.8125e+00,  4.7812e+00,  4.1562e+00,\n",
      "         4.2188e+00,  4.2500e+00,  4.1250e+00,  4.1250e+00,  4.3750e+00,\n",
      "         4.4062e+00,  4.2188e+00,  4.2188e+00,  4.1875e+00,  4.3125e+00,\n",
      "         4.3438e+00,  4.5938e+00,  3.9688e+00,  4.0312e+00,  4.1562e+00,\n",
      "         4.3438e+00,  4.3125e+00,  4.5625e+00,  4.2500e+00,  4.5000e+00,\n",
      "         4.4688e+00,  4.9062e+00,  4.0938e+00,  4.8750e+00,  4.3750e+00,\n",
      "         4.2188e+00,  4.1875e+00,  4.3750e+00,  4.3438e+00,  4.2500e+00,\n",
      "         4.3438e+00,  4.3750e+00,  6.9141e-01,  5.8125e+00,  4.2500e+00,\n",
      "         4.8750e+00,  4.3750e+00,  3.8906e+00,  4.4375e+00,  4.0625e+00,\n",
      "         4.3125e+00,  4.3125e+00,  4.3750e+00,  4.3438e+00,  4.6875e+00,\n",
      "         4.4062e+00,  4.3125e+00,  4.5938e+00,  4.5938e+00,  3.4961e-01,\n",
      "         4.6562e+00,  4.3438e+00,  4.2812e+00,  4.4062e+00,  4.6250e+00,\n",
      "         4.3438e+00,  4.3750e+00,  4.3750e+00,  4.2188e+00,  4.2500e+00,\n",
      "         4.4375e+00,  4.5312e+00,  4.5312e+00,  4.2188e+00,  3.9688e+00,\n",
      "         4.3438e+00,  5.3438e+00,  4.3438e+00,  5.0312e+00,  4.6875e+00,\n",
      "         4.0938e+00,  4.3125e+00,  4.2188e+00,  4.2188e+00,  4.5625e+00,\n",
      "         4.2812e+00,  4.2812e+00,  5.1250e+00,  4.3125e+00,  4.3125e+00,\n",
      "         4.1250e+00,  4.3125e+00,  4.1250e+00,  4.4062e+00,  4.3438e+00,\n",
      "         4.4375e+00,  4.1250e+00,  4.7500e+00,  4.4375e+00,  4.0000e+00,\n",
      "         3.9844e+00,  4.2188e+00,  4.4062e+00,  4.5625e+00,  4.2188e+00,\n",
      "         4.1875e+00,  4.3750e+00,  4.2812e+00,  4.3438e+00,  4.1250e+00,\n",
      "         4.3438e+00,  4.4375e+00,  4.3438e+00,  5.6875e+00,  4.2812e+00,\n",
      "         4.4375e+00,  4.6562e+00,  4.4375e+00,  4.2812e+00,  4.1562e+00,\n",
      "         4.5625e+00,  4.1250e+00,  4.3750e+00,  4.5312e+00,  4.7188e+00,\n",
      "         4.6875e+00,  5.8125e+00,  4.0938e+00,  4.4062e+00,  4.8438e+00,\n",
      "         3.9062e+00,  4.3750e+00,  4.2500e+00,  4.5000e+00,  4.5312e+00,\n",
      "         4.9375e+00,  4.0938e+00,  4.2188e+00,  4.1562e+00,  4.6250e+00,\n",
      "         4.3125e+00,  3.9219e+00,  4.0000e+00,  4.8125e+00,  4.0938e+00,\n",
      "         4.3125e+00,  4.4688e+00,  4.6250e+00,  4.2188e+00,  4.4375e+00,\n",
      "         4.0938e+00,  4.4062e+00,  4.8125e+00,  4.0312e+00,  4.3125e+00,\n",
      "         4.2812e+00,  4.1562e+00,  4.4062e+00,  4.1562e+00,  4.3438e+00,\n",
      "         4.1250e+00,  4.2500e+00,  5.0312e+00,  4.1250e+00,  4.0312e+00,\n",
      "         4.1875e+00,  4.1562e+00,  4.2500e+00,  4.4062e+00,  4.3125e+00,\n",
      "         4.5312e+00,  4.3750e+00,  3.9531e+00,  4.1250e+00,  4.0938e+00,\n",
      "         4.1250e+00,  4.5625e+00,  4.4688e+00,  4.5938e+00,  4.2500e+00,\n",
      "         4.0938e+00,  4.1875e+00,  4.1875e+00,  4.4062e+00,  4.2188e+00,\n",
      "         4.4375e+00,  4.3750e+00,  4.4062e+00,  5.1562e+00,  4.2500e+00,\n",
      "         4.3750e+00,  4.3438e+00,  4.7188e+00,  4.5000e+00,  3.9688e+00,\n",
      "         4.3438e+00,  4.2812e+00,  4.3125e+00,  3.9219e+00,  4.1875e+00,\n",
      "         4.1875e+00,  4.3125e+00,  5.4375e+00,  4.3750e+00,  4.2500e+00,\n",
      "         4.5312e+00,  4.2500e+00,  4.2812e+00,  4.0938e+00,  4.3125e+00,\n",
      "         4.3438e+00,  4.4375e+00,  4.5000e+00,  4.5938e+00,  4.3125e+00,\n",
      "         4.3438e+00,  4.2812e+00,  4.1250e+00,  4.1562e+00,  4.5625e+00,\n",
      "         4.3750e+00,  4.6875e+00,  4.3750e+00,  4.6875e+00,  4.2500e+00,\n",
      "         3.9375e+00,  4.4688e+00,  4.2812e+00,  4.3750e+00,  4.5938e+00,\n",
      "         4.2500e+00,  4.2812e+00,  4.3438e+00,  4.2500e+00,  4.5938e+00,\n",
      "         4.5312e+00,  4.5312e+00,  4.2188e+00,  4.5938e+00,  4.1875e+00,\n",
      "         4.4688e+00,  4.0625e+00,  4.0938e+00,  4.0625e+00,  4.1562e+00,\n",
      "         4.2188e+00,  4.3125e+00,  4.5625e+00], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.decoder.block.9.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0144, -0.0447, -0.0366,  ..., -0.0186, -0.0233, -0.0245],\n",
      "        [-0.0210, -0.0361,  0.0055,  ..., -0.0096,  0.0084, -0.0265],\n",
      "        [-0.0369,  0.0157, -0.0486,  ...,  0.0131,  0.0004, -0.0557],\n",
      "        ...,\n",
      "        [-0.0054, -0.0767,  0.0090,  ...,  0.0143,  0.0549, -0.0464],\n",
      "        [-0.0112, -0.0430,  0.0267,  ..., -0.0583, -0.0188,  0.0903],\n",
      "        [ 0.0073, -0.0064,  0.0396,  ...,  0.0383, -0.0557,  0.0266]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.9.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1113,  0.2578, -0.5195,  ..., -0.1865,  0.1787,  0.4941],\n",
      "        [-0.6992,  0.3320, -0.1660,  ...,  0.2451, -0.3223, -0.1797],\n",
      "        [ 0.0177,  0.2578, -0.2988,  ..., -0.1006,  0.8516, -0.2988],\n",
      "        ...,\n",
      "        [ 0.3320, -0.3203, -0.3711,  ..., -0.7344, -0.3027, -0.0635],\n",
      "        [-0.0127,  0.0791, -0.2656,  ...,  0.5078, -0.3730, -0.6562],\n",
      "        [-0.0723, -0.4570,  0.0500,  ..., -0.2637,  0.1797, -0.1104]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.9.layer.0.SelfAttention.v.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.6641, -1.5234, -0.0693,  ...,  1.0859,  0.6016, -0.7461],\n",
      "        [ 0.7539,  0.8242,  0.0082,  ...,  0.1377, -0.9570, -0.9688],\n",
      "        [-0.8789,  0.5195,  0.4844,  ..., -0.7695, -0.2910,  0.7344],\n",
      "        ...,\n",
      "        [-0.0762, -0.9023, -0.8086,  ...,  0.6797, -0.1543,  0.2246],\n",
      "        [ 0.4492, -0.3848, -0.7617,  ...,  0.7305,  0.6992,  0.2031],\n",
      "        [-0.8242, -0.2617,  0.2793,  ...,  0.0601,  0.2227,  0.1914]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.9.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0457, -0.8672, -0.0591,  ...,  0.6641, -0.4688,  0.2734],\n",
      "        [-0.1377, -0.3984,  0.5234,  ...,  0.8281,  0.4707,  0.4199],\n",
      "        [ 0.0684,  1.0469,  0.9570,  ..., -0.1660,  0.7188, -0.3027],\n",
      "        ...,\n",
      "        [ 1.5938,  0.7578,  0.2539,  ..., -0.8906, -1.4453, -0.0952],\n",
      "        [ 0.4180,  0.4121,  1.3906,  ...,  0.1426,  0.5312,  0.8633],\n",
      "        [-1.6641,  1.6484,  0.9883,  ...,  0.7812, -0.1494, -0.1631]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.9.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 2.4316e-01,  2.3340e-01,  3.0469e-01,  1.7383e-01,  2.5781e-01,\n",
      "         2.5586e-01,  2.9883e-01,  2.6367e-01,  2.6367e-01,  2.6758e-01,\n",
      "         2.3926e-01,  2.7539e-01,  2.5000e-01,  2.3926e-01,  2.7539e-01,\n",
      "         2.6953e-01,  2.6172e-01,  2.7930e-01,  2.3730e-01,  2.8320e-01,\n",
      "         2.6172e-01,  2.6172e-01,  2.6172e-01,  2.6758e-01,  2.7930e-01,\n",
      "         3.1836e-01,  2.7734e-01,  2.7148e-01,  2.8320e-01,  2.7539e-01,\n",
      "         2.8320e-01,  3.1055e-01,  2.7539e-01,  2.7344e-01,  2.7734e-01,\n",
      "         2.6953e-01,  2.7539e-01,  3.3594e-01,  2.5391e-01,  2.6758e-01,\n",
      "         2.4902e-01,  2.7344e-01,  2.7148e-01,  2.7734e-01,  3.0078e-01,\n",
      "         2.7539e-01,  2.7734e-01,  2.7930e-01,  2.8320e-01,  2.6172e-01,\n",
      "         3.0273e-01,  2.6758e-01,  2.7539e-01,  2.5781e-01,  4.0625e-01,\n",
      "         2.3730e-01,  2.6367e-01,  2.8125e-01,  2.6758e-01,  2.5195e-01,\n",
      "         2.7148e-01,  2.6562e-01,  2.6758e-01,  2.6562e-01,  2.5977e-01,\n",
      "         3.8867e-01,  2.5781e-01,  2.7734e-01,  3.1445e-01,  2.6172e-01,\n",
      "         2.6953e-01,  2.8516e-01,  2.8125e-01,  2.7148e-01,  2.9883e-01,\n",
      "         2.5977e-01,  2.5195e-01,  2.6172e-01,  2.7734e-01,  3.0664e-01,\n",
      "         2.8711e-01,  2.6172e-01,  2.4805e-01,  3.3008e-01,  3.0273e-01,\n",
      "         2.8906e-01,  7.6660e-02,  2.6367e-01,  2.7344e-01,  2.7344e-01,\n",
      "         2.6758e-01,  2.5977e-01,  2.5586e-01,  2.5000e-01,  2.8906e-01,\n",
      "         2.6953e-01,  2.7539e-01,  2.6367e-01,  2.8906e-01,  2.6172e-01,\n",
      "         2.6953e-01,  2.6758e-01,  3.1055e-01,  2.5586e-01,  2.6562e-01,\n",
      "         2.6562e-01,  2.7148e-01,  2.7148e-01,  2.8516e-01,  2.6953e-01,\n",
      "         2.9102e-01,  2.6953e-01,  2.5195e-01,  2.6758e-01,  2.5781e-01,\n",
      "         2.9688e-01,  2.7734e-01,  2.6562e-01,  7.4219e-02,  2.8711e-01,\n",
      "         2.5781e-01,  2.5391e-01,  3.2031e-01,  2.5391e-01,  2.7930e-01,\n",
      "         2.5977e-01,  2.5000e-01,  2.5586e-01,  2.5586e-01,  2.7148e-01,\n",
      "         2.7930e-01,  2.7344e-01,  2.6953e-01,  2.7148e-01,  2.1289e-01,\n",
      "         2.6172e-01,  2.5977e-01,  2.7539e-01,  2.4609e-01,  2.6562e-01,\n",
      "         2.8906e-01,  2.8125e-01,  2.7148e-01,  2.7344e-01,  2.5781e-01,\n",
      "         2.4023e-01,  2.7734e-01,  2.6367e-01,  2.9297e-01,  2.3926e-01,\n",
      "         3.0664e-01,  2.6953e-01,  2.7734e-01,  2.9883e-01,  2.7148e-01,\n",
      "         2.8516e-01,  2.5391e-01,  2.8125e-01,  1.1670e-01,  2.2168e-01,\n",
      "         2.5977e-01,  2.5586e-01,  2.5781e-01,  3.1055e-01,  2.7539e-01,\n",
      "         3.0273e-01,  2.8906e-01,  3.0664e-01,  3.6523e-01,  2.6367e-01,\n",
      "        -2.3145e-01,  2.7344e-01,  2.8125e-01,  2.8711e-01,  2.7344e-01,\n",
      "         2.5195e-01,  2.6172e-01,  2.7539e-01,  2.2266e-01,  2.8125e-01,\n",
      "         2.7344e-01,  2.8125e-01,  2.7148e-01,  2.7930e-01,  2.7930e-01,\n",
      "         2.9883e-01,  2.6758e-01,  8.8379e-02,  2.7148e-01,  2.6367e-01,\n",
      "         2.5977e-01,  2.7148e-01,  2.6953e-01,  2.7148e-01,  2.4805e-01,\n",
      "         2.7930e-01,  2.5391e-01,  2.8320e-01,  2.7539e-01,  2.7148e-01,\n",
      "         2.5781e-01,  2.5195e-01,  2.7930e-01,  2.8125e-01,  2.6172e-01,\n",
      "         2.5586e-01,  2.5000e-01,  2.6953e-01,  2.6758e-01,  2.8516e-01,\n",
      "         3.1836e-01,  2.5586e-01,  2.4609e-01,  2.9102e-01,  2.6758e-01,\n",
      "         2.7539e-01,  2.6562e-01,  2.7539e-01,  2.6562e-01, -2.7008e-03,\n",
      "         2.8125e-01,  2.9297e-01,  2.7148e-01,  2.6758e-01,  2.5586e-01,\n",
      "         2.7930e-01,  8.3594e-01,  2.7930e-01,  2.5391e-01,  2.8516e-01,\n",
      "         2.9688e-01,  2.6562e-01,  2.6953e-01,  2.6562e-01,  2.7734e-01,\n",
      "         2.5586e-01,  2.7344e-01,  2.3828e-01,  2.8320e-01,  2.5586e-01,\n",
      "         3.6719e-01,  2.6953e-01,  2.7344e-01,  2.8320e-01,  2.5586e-01,\n",
      "         2.5977e-01,  2.7930e-01,  2.4805e-01,  2.6758e-01,  2.6758e-01,\n",
      "         2.6172e-01,  2.6758e-01,  2.6562e-01,  2.6367e-01,  2.6953e-01,\n",
      "         2.7930e-01,  2.8125e-01,  2.3047e-01,  2.7539e-01,  2.7148e-01,\n",
      "         2.5391e-01,  2.5000e-01,  2.8320e-01,  2.7148e-01,  2.7539e-01,\n",
      "         2.6758e-01,  2.7148e-01,  2.8125e-01,  2.6758e-01,  2.8320e-01,\n",
      "         2.6758e-01,  4.4141e-01,  2.6562e-01,  2.7148e-01, -2.0410e-01,\n",
      "         2.5195e-01,  2.8320e-01,  2.5781e-01,  2.8320e-01,  2.4121e-01,\n",
      "         6.5918e-02,  2.9297e-01,  2.7734e-01,  2.6953e-01,  2.4902e-01,\n",
      "         2.5586e-01,  2.5781e-01,  2.7148e-01,  2.8516e-01,  2.4707e-01,\n",
      "         2.8711e-01,  2.7539e-01,  2.5000e-01,  2.6758e-01,  2.5781e-01,\n",
      "         2.6758e-01,  2.6758e-01,  2.7539e-01,  2.7539e-01,  2.9492e-01,\n",
      "         2.7344e-01,  2.6758e-01, -7.1777e-02,  2.6953e-01,  2.6758e-01,\n",
      "         2.8711e-01,  2.5195e-01,  2.5977e-01,  2.8125e-01,  3.0078e-01,\n",
      "         2.8516e-01,  2.9102e-01,  2.5000e-01,  2.5586e-01,  3.5352e-01,\n",
      "         2.5977e-01,  2.8320e-01,  3.0664e-01,  2.7539e-01,  2.7539e-01,\n",
      "         2.5195e-01,  2.6367e-01,  2.5977e-01,  2.9102e-01,  2.8516e-01,\n",
      "         2.7344e-01,  2.5977e-01,  2.8906e-01,  2.6758e-01,  2.5391e-01,\n",
      "         2.5977e-01,  2.6172e-01,  2.6953e-01,  2.8516e-01,  2.7148e-01,\n",
      "         2.6172e-01,  2.7734e-01,  2.8125e-01,  2.6758e-01,  1.8555e-01,\n",
      "         2.5391e-01,  2.8125e-01,  2.6172e-01,  2.4121e-01,  2.8125e-01,\n",
      "         2.8125e-01,  2.8320e-01,  2.5391e-01,  2.2852e-01,  2.5977e-01,\n",
      "         2.8125e-01,  2.7539e-01,  2.5781e-01,  2.7734e-01,  2.5195e-01,\n",
      "         2.9297e-01,  2.6562e-01,  1.2305e-01,  2.7734e-01,  2.8906e-01,\n",
      "         2.6758e-01,  2.5586e-01,  2.4512e-01,  2.5586e-01,  2.9297e-01,\n",
      "         3.0469e-01,  2.7930e-01,  2.6953e-01,  2.6758e-01,  2.5195e-01,\n",
      "         2.7344e-01,  2.7148e-01,  2.9492e-01,  2.6172e-01,  2.7930e-01,\n",
      "         2.6367e-01,  2.6172e-01,  2.9297e-01,  2.9297e-01,  2.3145e-01,\n",
      "         2.7344e-01,  2.6758e-01,  2.5000e-01,  2.7734e-01,  2.7344e-01,\n",
      "         2.7148e-01,  2.7148e-01,  2.7148e-01,  2.6758e-01,  2.7148e-01,\n",
      "         2.9102e-01,  2.7734e-01,  2.5586e-01, -2.6367e-01,  2.7734e-01,\n",
      "         2.7734e-01,  2.9102e-01,  2.5781e-01,  2.6758e-01,  2.7148e-01,\n",
      "         2.7539e-01,  2.6953e-01,  2.5781e-01,  2.7734e-01,  2.6367e-01,\n",
      "         2.7148e-01,  2.7148e-01,  2.5977e-01,  2.8125e-01,  2.7148e-01,\n",
      "         2.7539e-01,  2.7539e-01,  2.8906e-01,  2.5000e-01,  2.7344e-01,\n",
      "         2.6562e-01,  2.7148e-01,  2.7539e-01,  4.0234e-01,  2.7930e-01,\n",
      "         3.5286e-04,  2.8516e-01,  2.6562e-01, -1.5137e-01,  2.7539e-01,\n",
      "         2.7148e-01,  2.3242e-01,  2.6562e-01,  2.6172e-01,  2.5391e-01,\n",
      "         2.7148e-01,  2.5781e-01,  2.7539e-01,  2.7930e-01,  2.7930e-01,\n",
      "         2.7344e-01,  2.6953e-01,  2.6758e-01,  2.9102e-01,  2.9297e-01,\n",
      "         2.8906e-01,  2.9297e-01,  2.7148e-01,  2.7930e-01,  2.6562e-01,\n",
      "         2.6562e-01,  2.5000e-01,  2.6758e-01, -5.8105e-02,  2.8711e-01,\n",
      "         2.6367e-01,  2.8125e-01,  2.6758e-01,  2.7539e-01,  2.8516e-01,\n",
      "         2.4707e-01,  2.8516e-01,  6.0547e-01,  2.4023e-01,  2.7930e-01,\n",
      "         2.8320e-01,  2.7148e-01,  2.7539e-01,  2.7930e-01,  2.7539e-01,\n",
      "         2.8320e-01,  2.5195e-01,  2.5586e-01,  2.6562e-01,  2.5781e-01,\n",
      "         2.6367e-01,  2.6367e-01,  3.4961e-01,  2.8906e-01,  2.6562e-01,\n",
      "         2.7344e-01,  2.5195e-01,  2.8320e-01,  2.7344e-01,  2.5195e-01,\n",
      "         2.6367e-01,  2.5977e-01,  2.7148e-01,  2.7344e-01,  1.4160e-01,\n",
      "         2.6758e-01,  2.6367e-01, -1.0586e-04,  2.7148e-01,  2.4609e-01,\n",
      "         2.7930e-01,  3.0078e-01,  2.7344e-01,  2.6562e-01,  2.9297e-01,\n",
      "        -2.1484e-01,  2.7148e-01,  2.5977e-01,  2.5391e-01,  2.6562e-01,\n",
      "         2.7930e-01,  3.4570e-01,  2.7344e-01,  2.5781e-01,  2.5000e-01,\n",
      "         2.7734e-01,  2.7344e-01,  2.5781e-01,  2.8711e-01,  2.7930e-01,\n",
      "         2.6172e-01,  2.6953e-01,  2.6758e-01,  2.7148e-01,  2.7148e-01,\n",
      "         2.7539e-01,  2.5781e-01,  2.6953e-01,  2.5586e-01,  2.7734e-01,\n",
      "         2.6562e-01,  2.6758e-01,  2.7539e-01,  2.9688e-01,  2.6758e-01,\n",
      "         2.5977e-01,  2.6172e-01,  3.3789e-01,  2.7539e-01,  2.7734e-01,\n",
      "         2.7539e-01,  2.7148e-01,  2.5195e-01,  2.6562e-01,  2.8125e-01,\n",
      "         2.8516e-01,  2.4219e-01,  2.5977e-01,  2.7344e-01,  2.6367e-01,\n",
      "         2.8516e-01,  2.7148e-01,  2.5391e-01,  2.5781e-01,  2.6562e-01,\n",
      "         2.5586e-01,  2.6758e-01,  2.6562e-01,  2.5977e-01,  2.7344e-01,\n",
      "         2.8711e-01,  2.9297e-01,  2.5195e-01,  3.0664e-01,  2.7344e-01,\n",
      "         2.7148e-01,  2.8320e-01,  2.6758e-01,  2.7344e-01,  2.7148e-01,\n",
      "         2.7344e-01,  2.7539e-01,  8.0566e-02,  3.1055e-01,  2.6953e-01,\n",
      "         3.4180e-01,  2.7148e-01,  2.4902e-01,  2.7344e-01,  2.7148e-01,\n",
      "         2.7344e-01,  2.6172e-01,  2.8125e-01,  2.6953e-01,  2.5586e-01,\n",
      "         2.8320e-01,  2.7344e-01,  2.7930e-01,  2.9492e-01,  1.4801e-03,\n",
      "         3.0078e-01,  2.7734e-01,  2.5781e-01,  2.7148e-01,  2.6953e-01,\n",
      "         2.6367e-01,  2.6758e-01,  2.6758e-01,  2.6758e-01,  2.7344e-01,\n",
      "         2.5781e-01,  2.9102e-01,  3.0078e-01,  2.5781e-01,  2.5195e-01,\n",
      "         2.5781e-01,  3.4570e-01,  2.7734e-01,  1.9238e-01,  2.7539e-01,\n",
      "         2.5000e-01,  2.6367e-01,  2.7344e-01,  2.5977e-01,  2.8711e-01,\n",
      "         2.5586e-01,  2.7734e-01,  3.2812e-01,  2.6367e-01,  2.6172e-01,\n",
      "         2.5391e-01,  2.8125e-01,  2.6172e-01,  2.7539e-01,  2.5781e-01,\n",
      "         2.7344e-01,  2.6758e-01,  4.1406e-01,  3.0273e-01,  2.5586e-01,\n",
      "         2.6367e-01,  2.6562e-01,  2.7344e-01, -2.6953e-01,  2.6758e-01,\n",
      "         2.6758e-01,  2.4707e-01,  2.6172e-01,  2.7148e-01,  2.7930e-01,\n",
      "         2.8711e-01,  2.7344e-01,  3.2031e-01,  3.5547e-01,  2.6562e-01,\n",
      "         2.7539e-01,  2.8320e-01,  2.7539e-01,  2.7344e-01,  2.5195e-01,\n",
      "         2.5391e-01,  2.8906e-01,  2.6758e-01,  2.7344e-01,  2.7734e-01,\n",
      "         2.6953e-01,  3.2227e-01,  2.6562e-01,  2.6172e-01,  2.7930e-01,\n",
      "         2.7539e-01,  2.8516e-01,  2.5977e-01,  2.6953e-01,  2.6953e-01,\n",
      "         2.7344e-01,  2.7539e-01,  2.5977e-01,  2.6367e-01,  2.6562e-01,\n",
      "         2.9883e-01,  2.3340e-01,  2.6172e-01,  3.2617e-01,  2.6562e-01,\n",
      "         2.6367e-01,  2.7930e-01,  2.7344e-01,  2.6758e-01,  2.8125e-01,\n",
      "         2.6953e-01,  2.6758e-01,  2.8711e-01,  2.5781e-01,  2.7539e-01,\n",
      "         2.7930e-01,  2.5195e-01,  2.8711e-01,  2.7539e-01,  2.6172e-01,\n",
      "         2.7930e-01,  2.7148e-01,  2.8711e-01,  2.5586e-01,  2.5586e-01,\n",
      "         2.4316e-01,  2.6758e-01,  2.4512e-01,  2.8125e-01,  2.7148e-01,\n",
      "         2.7930e-01,  2.5977e-01,  2.5391e-01,  2.6562e-01,  2.6562e-01,\n",
      "         2.7930e-01,  2.8125e-01,  2.9102e-01,  2.7539e-01,  2.6172e-01,\n",
      "         2.6562e-01,  2.6953e-01,  2.5781e-01,  2.1973e-01,  3.0664e-01,\n",
      "         2.7344e-01,  2.5781e-01,  2.4902e-01,  3.3398e-01,  2.7539e-01,\n",
      "         2.7539e-01,  2.7539e-01,  2.6758e-01,  2.7148e-01,  2.5391e-01,\n",
      "         3.5938e-01,  2.7539e-01,  3.1445e-01,  2.6953e-01,  2.5000e-01,\n",
      "         2.6758e-01,  2.7344e-01,  2.7344e-01,  2.6758e-01,  2.6367e-01,\n",
      "         3.2031e-01,  2.4316e-01,  2.7344e-01,  2.6758e-01,  2.5195e-01,\n",
      "         2.8320e-01,  2.5391e-01,  2.7930e-01,  1.6602e-01,  3.0078e-01,\n",
      "         2.8125e-01,  2.8125e-01,  2.5781e-01,  2.5195e-01,  2.6953e-01,\n",
      "         2.7930e-01,  2.3047e-01,  2.6367e-01,  2.7344e-01,  3.0273e-01,\n",
      "         2.6172e-01,  2.4609e-01,  2.7344e-01,  2.7344e-01,  2.2754e-01,\n",
      "         2.4316e-01,  2.6953e-01,  2.5391e-01,  2.7930e-01,  2.8320e-01,\n",
      "         2.9688e-01,  2.7344e-01,  2.6172e-01,  2.5781e-01,  2.5977e-01,\n",
      "         2.4707e-01,  2.4707e-01,  2.4023e-01,  2.3926e-01,  2.6758e-01,\n",
      "         2.5195e-01,  2.5000e-01,  2.6172e-01], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.decoder.block.9.layer.1.EncDecAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-3.0396e-02, -3.5645e-02,  3.9551e-02,  ...,  3.7354e-02,\n",
      "         -5.8899e-03, -1.3611e-02],\n",
      "        [-4.4678e-02,  3.2715e-02, -3.4485e-03,  ..., -2.3560e-02,\n",
      "         -2.7222e-02,  4.3701e-02],\n",
      "        [ 3.8605e-03,  5.6152e-02, -3.4424e-02,  ..., -1.8066e-02,\n",
      "          8.1787e-03, -3.4912e-02],\n",
      "        ...,\n",
      "        [ 2.1362e-02, -3.9307e-02,  2.9297e-02,  ..., -4.3335e-03,\n",
      "         -8.2031e-02, -1.9653e-02],\n",
      "        [ 4.6631e-02,  2.8564e-02, -7.3547e-03,  ...,  5.4688e-02,\n",
      "          2.2217e-02,  2.3193e-02],\n",
      "        [-1.1780e-02,  7.8125e-02,  6.6280e-05,  ...,  1.9531e-03,\n",
      "         -1.8555e-02, -1.7944e-02]], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.9.layer.1.EncDecAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.2324, -0.0026,  0.2773,  ..., -0.5469, -0.0204,  0.5859],\n",
      "        [ 0.1758,  0.7227, -0.1445,  ...,  0.3906, -0.0635, -0.0752],\n",
      "        [-0.4824,  0.0147,  0.6719,  ..., -0.3887,  0.3848, -0.2539],\n",
      "        ...,\n",
      "        [ 0.2578, -0.2217, -0.1494,  ...,  0.2793, -0.1064,  1.1172],\n",
      "        [-0.0776, -0.1924,  0.3066,  ...,  0.7812, -0.1533,  0.1533],\n",
      "        [-0.1069, -0.2070, -0.0120,  ..., -0.7578,  0.1230, -0.5547]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.9.layer.1.EncDecAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.2969,  0.2559, -0.6328,  ...,  0.5781,  0.7539,  0.9297],\n",
      "        [-0.9492,  0.5117, -0.1855,  ...,  0.1953,  0.6797,  0.4160],\n",
      "        [ 0.6016, -0.3828, -0.3457,  ..., -0.7930, -0.0417,  0.5430],\n",
      "        ...,\n",
      "        [-0.3613,  0.7109, -0.1699,  ..., -0.9609, -0.9688, -0.0028],\n",
      "        [-1.6562,  0.7539, -0.3906,  ...,  0.8828, -0.2197,  1.5625],\n",
      "        [-0.4961,  0.0986, -1.3672,  ...,  1.3203, -1.2422,  0.7109]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.9.layer.1.EncDecAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.6211, -1.2500, -0.4863,  ..., -1.0781, -1.9531, -1.6797],\n",
      "        [-0.2988,  0.0996,  0.3086,  ..., -0.0048, -0.8281,  0.2617],\n",
      "        [ 0.0469, -0.5156, -0.1973,  ...,  0.4238, -0.5078, -0.4316],\n",
      "        ...,\n",
      "        [ 0.7891,  0.0415, -1.2578,  ..., -1.4297,  0.8320,  0.1572],\n",
      "        [-0.4590,  0.4258,  1.0000,  ..., -1.4922, -0.4414, -1.5078],\n",
      "        [-0.2363,  1.1484,  1.4609,  ...,  0.7617,  2.2188,  0.8633]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.9.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 5.0537e-02,  6.9336e-02,  6.2988e-02,  3.4912e-02,  5.5176e-02,\n",
      "         5.2246e-02,  9.3262e-02,  6.6406e-02,  5.8105e-02,  6.9824e-02,\n",
      "         4.5410e-02,  7.8613e-02,  6.5430e-02,  4.4922e-02,  7.0801e-02,\n",
      "         6.4941e-02,  5.8350e-02,  6.7871e-02,  5.1514e-02,  8.1055e-02,\n",
      "         6.5918e-02,  6.3965e-02,  6.5918e-02,  6.7871e-02,  6.5430e-02,\n",
      "         7.4219e-02,  6.8848e-02,  5.2734e-02,  6.9336e-02,  7.5195e-02,\n",
      "         7.2266e-02,  6.4453e-02,  6.4941e-02,  7.9590e-02,  6.3477e-02,\n",
      "         6.6895e-02,  7.8125e-02,  5.8105e-02,  5.8350e-02,  6.5918e-02,\n",
      "         5.1514e-02,  6.0059e-02,  6.5430e-02,  5.8350e-02,  8.0078e-02,\n",
      "         6.5430e-02,  7.8125e-02,  8.5938e-02,  6.9336e-02,  6.0547e-02,\n",
      "         7.6172e-02,  6.5918e-02,  6.0059e-02,  6.3965e-02,  7.7148e-02,\n",
      "         4.9072e-02,  4.9805e-02,  6.3477e-02,  8.4961e-02,  6.0059e-02,\n",
      "         6.2500e-02,  6.4453e-02,  7.1289e-02,  6.6895e-02,  7.2266e-02,\n",
      "         6.9336e-02,  6.2500e-02,  6.2988e-02,  5.9082e-02,  5.8350e-02,\n",
      "         8.4473e-02,  7.8613e-02,  7.2754e-02,  6.4941e-02,  6.6406e-02,\n",
      "         6.1279e-02, -4.8340e-02,  5.8838e-02,  6.8359e-02,  7.7148e-02,\n",
      "         6.6895e-02,  6.1768e-02,  6.0303e-02,  6.1035e-02,  7.7637e-02,\n",
      "         7.0801e-02,  1.2146e-02,  6.1035e-02,  7.0312e-02,  6.5918e-02,\n",
      "         6.4453e-02,  6.8848e-02,  5.8594e-02,  5.0293e-02,  6.7383e-02,\n",
      "         7.1777e-02,  6.4941e-02,  7.2266e-02,  7.6172e-02,  6.0303e-02,\n",
      "         4.7607e-02,  7.1777e-02,  5.5176e-02,  5.9326e-02,  6.2012e-02,\n",
      "         5.2002e-02,  6.5918e-02,  5.9082e-02,  8.5938e-02,  7.1777e-02,\n",
      "         7.7148e-02,  6.5918e-02,  4.4189e-02,  6.6406e-02,  6.5430e-02,\n",
      "         1.0205e-01,  5.7129e-02,  5.5908e-02,  3.0327e-04,  6.4941e-02,\n",
      "         5.7861e-02,  7.9102e-02,  7.1777e-02,  6.1523e-02,  7.1289e-02,\n",
      "         6.6406e-02,  6.3965e-02, -5.0781e-02,  5.5664e-02,  4.8096e-02,\n",
      "         5.3711e-02,  6.7871e-02,  6.2988e-02,  6.5430e-02,  5.3467e-02,\n",
      "         5.3711e-02,  5.9814e-02,  5.5664e-02,  5.4688e-02, -4.8096e-02,\n",
      "         6.6895e-02,  7.2266e-02,  5.3223e-02,  6.0547e-02,  5.4199e-02,\n",
      "        -5.0293e-02,  7.9590e-02,  5.6885e-02,  7.0801e-02, -4.9561e-02,\n",
      "         8.9355e-02,  5.7129e-02,  9.5703e-02,  6.0059e-02,  7.4219e-02,\n",
      "         7.9590e-02,  5.9326e-02,  7.4707e-02,  8.4961e-02,  5.0537e-02,\n",
      "         6.1523e-02,  5.3711e-02,  5.1758e-02,  5.8105e-02,  7.2754e-02,\n",
      "         5.3711e-02,  7.5684e-02,  5.6641e-02,  5.3955e-02,  6.8848e-02,\n",
      "        -5.2734e-02,  6.3477e-02,  8.4473e-02,  6.8848e-02,  7.1289e-02,\n",
      "         6.5918e-02,  6.0303e-02,  8.6914e-02,  4.7852e-02,  7.0801e-02,\n",
      "         6.8848e-02,  6.7871e-02,  6.7871e-02,  7.1777e-02,  5.7861e-02,\n",
      "         7.1777e-02,  6.7383e-02,  6.2500e-02,  6.4453e-02,  5.1514e-02,\n",
      "         7.1289e-02,  6.8359e-02,  7.2266e-02,  6.2256e-02,  5.7617e-02,\n",
      "         6.5430e-02,  6.7383e-02,  8.3984e-02,  6.4941e-02,  6.3965e-02,\n",
      "         6.7383e-02, -4.8096e-02,  6.8359e-02,  7.4707e-02,  6.4941e-02,\n",
      "        -4.5166e-02,  5.7617e-02,  6.8359e-02,  6.3965e-02,  7.0801e-02,\n",
      "         7.8613e-02,  5.0293e-02,  5.4199e-02,  6.9824e-02,  6.3477e-02,\n",
      "         6.8359e-02,  5.9082e-02,  7.4219e-02,  5.7861e-02,  9.5215e-03,\n",
      "         5.2246e-02,  9.3750e-02,  7.0801e-02,  6.2988e-02,  5.9326e-02,\n",
      "         7.4707e-02,  2.0801e-01,  8.1055e-02,  5.5664e-02,  7.5684e-02,\n",
      "         7.5195e-02,  5.5664e-02,  6.2988e-02,  7.0312e-02,  7.0801e-02,\n",
      "        -6.2012e-02, -4.8584e-02,  5.3223e-02,  5.1514e-02,  6.0791e-02,\n",
      "        -6.1768e-02,  6.8359e-02,  6.9824e-02,  6.5430e-02,  6.2500e-02,\n",
      "         5.7373e-02,  9.3262e-02, -5.0781e-02,  6.8359e-02,  5.8594e-02,\n",
      "         5.5664e-02,  6.3477e-02,  7.3242e-02,  6.1035e-02,  6.8848e-02,\n",
      "         6.1523e-02,  6.5430e-02,  4.8096e-02,  7.1777e-02,  7.2266e-02,\n",
      "        -6.2988e-02,  5.1514e-02,  7.1777e-02,  6.6406e-02,  7.0801e-02,\n",
      "         5.4932e-02,  6.8359e-02,  6.8848e-02,  6.3965e-02,  5.8105e-02,\n",
      "         6.5430e-02,  6.8359e-02,  6.3477e-02,  6.7383e-02,  4.7119e-02,\n",
      "         6.0059e-02,  7.3730e-02,  5.7617e-02,  7.5684e-02,  5.1514e-02,\n",
      "        -2.3842e-04,  6.1523e-02,  6.6895e-02,  6.1279e-02,  5.9082e-02,\n",
      "         6.1279e-02,  6.3477e-02,  7.2266e-02,  5.3223e-02, -4.3457e-02,\n",
      "         6.4453e-02,  6.8359e-02,  5.8105e-02,  6.7871e-02,  6.1279e-02,\n",
      "         6.8359e-02,  6.1768e-02,  7.0801e-02,  7.0312e-02,  7.8125e-02,\n",
      "         8.3496e-02,  6.6895e-02, -2.3315e-02,  5.7617e-02,  6.1768e-02,\n",
      "         6.5918e-02,  5.4932e-02,  6.1035e-02,  5.3711e-02,  6.7871e-02,\n",
      "         9.4238e-02,  7.0801e-02,  6.0547e-02, -5.5664e-02,  2.2461e-01,\n",
      "         6.4453e-02,  1.0742e-01,  5.7129e-02,  6.8359e-02,  7.9102e-02,\n",
      "         6.6895e-02,  7.1289e-02, -5.2002e-02,  7.7148e-02,  5.7617e-02,\n",
      "         6.1035e-02,  5.4199e-02,  8.2520e-02,  5.9082e-02,  6.5430e-02,\n",
      "         6.8848e-02,  5.6885e-02,  6.9824e-02,  7.0801e-02,  7.2754e-02,\n",
      "         6.4941e-02,  7.3730e-02,  8.8867e-02,  6.7871e-02, -3.7598e-02,\n",
      "         6.1523e-02,  6.8848e-02,  7.6172e-02,  5.4932e-02,  7.4707e-02,\n",
      "         6.6895e-02,  7.3242e-02,  5.1270e-02,  5.4932e-02,  5.1758e-02,\n",
      "         6.2500e-02,  6.7383e-02,  6.1279e-02,  7.4707e-02,  6.0547e-02,\n",
      "         8.9355e-02,  6.6895e-02, -7.3242e-04,  6.9336e-02,  6.8359e-02,\n",
      "         6.1523e-02,  5.9570e-02,  4.7363e-02,  5.7373e-02,  7.7148e-02,\n",
      "         8.7891e-02,  5.5176e-02,  6.1279e-02,  5.8350e-02,  5.6396e-02,\n",
      "         6.8848e-02,  7.7148e-02,  7.3242e-02,  5.8594e-02,  7.2266e-02,\n",
      "         6.5918e-02,  6.2988e-02,  9.0820e-02,  7.2754e-02,  5.1758e-02,\n",
      "         5.8838e-02,  5.9082e-02,  4.6387e-02,  6.6406e-02,  6.8848e-02,\n",
      "         6.9824e-02,  6.4941e-02,  6.5918e-02,  6.0547e-02,  6.3965e-02,\n",
      "         7.2754e-02,  6.1523e-02,  5.5908e-02, -4.9805e-02,  6.9336e-02,\n",
      "        -6.1035e-02,  7.5684e-02,  5.9814e-02,  6.3477e-02,  5.5908e-02,\n",
      "         5.9082e-02,  6.6406e-02,  6.5430e-02,  7.0312e-02,  5.6152e-02,\n",
      "         6.7383e-02,  5.2734e-02,  6.2256e-02,  6.2012e-02,  6.7383e-02,\n",
      "         6.9336e-02,  7.0801e-02,  6.0791e-02,  6.1523e-02,  7.2754e-02,\n",
      "         7.6660e-02,  6.7871e-02,  6.1035e-02,  2.3438e-01,  7.0312e-02,\n",
      "        -1.8311e-04,  6.0547e-02,  6.6406e-02,  2.0386e-02,  6.7871e-02,\n",
      "         7.1289e-02,  5.3467e-02,  7.0312e-02,  5.6396e-02,  5.1758e-02,\n",
      "         7.1289e-02,  6.2012e-02,  6.3965e-02,  5.7617e-02,  6.3477e-02,\n",
      "         6.5918e-02,  6.4941e-02,  6.8848e-02,  6.5430e-02,  6.3477e-02,\n",
      "         8.5449e-02,  6.4453e-02,  6.6406e-02,  7.1777e-02,  6.8359e-02,\n",
      "         6.2988e-02,  6.5918e-02,  6.1523e-02, -2.2316e-04,  6.2988e-02,\n",
      "         5.8594e-02,  7.2266e-02,  6.3965e-02,  7.0801e-02,  6.5918e-02,\n",
      "         5.1270e-02,  5.9570e-02,  1.5332e-01,  5.2002e-02,  6.3965e-02,\n",
      "         6.5430e-02,  7.3242e-02,  6.7871e-02,  6.2256e-02,  6.2500e-02,\n",
      "         7.1289e-02, -4.1992e-02,  5.9326e-02,  6.3965e-02,  5.9570e-02,\n",
      "         6.9824e-02,  6.9336e-02,  8.9844e-02,  7.1777e-02,  5.9814e-02,\n",
      "         6.9824e-02,  5.9570e-02,  7.5195e-02,  6.1768e-02,  5.7129e-02,\n",
      "        -4.9561e-02,  6.2988e-02,  7.6172e-02,  5.8838e-02, -5.2929e-05,\n",
      "         6.7383e-02,  6.9336e-02,  4.8340e-02,  7.4707e-02,  5.5420e-02,\n",
      "         6.2988e-02,  9.2773e-02,  6.9336e-02,  7.8613e-02,  7.9102e-02,\n",
      "         9.7656e-02,  6.0547e-02,  6.2500e-02,  6.2988e-02,  6.1523e-02,\n",
      "         6.6406e-02,  1.1816e-01,  6.7871e-02,  6.0547e-02,  6.5918e-02,\n",
      "         6.2988e-02,  6.8359e-02,  5.4688e-02,  6.1279e-02,  7.2754e-02,\n",
      "         6.2256e-02,  7.0801e-02,  5.5420e-02,  7.2266e-02,  7.6172e-02,\n",
      "         7.1777e-02, -5.6885e-02,  6.3965e-02,  4.4922e-02,  7.0312e-02,\n",
      "         6.2988e-02,  6.5430e-02,  7.5195e-02,  7.3730e-02,  6.7383e-02,\n",
      "         6.8359e-02,  5.1514e-02,  7.9102e-02,  6.2988e-02,  6.1279e-02,\n",
      "         5.4932e-02,  6.9824e-02,  6.1279e-02,  6.5918e-02,  7.0801e-02,\n",
      "         7.2266e-02,  6.2500e-02,  6.0059e-02, -6.0547e-02,  6.3965e-02,\n",
      "         7.3730e-02,  6.8359e-02,  5.3467e-02, -5.4199e-02,  5.9570e-02,\n",
      "         6.4453e-02,  6.8359e-02,  6.3477e-02,  6.4941e-02,  6.2988e-02,\n",
      "         5.2246e-02,  8.6426e-02,  6.6406e-02,  7.0801e-02,  6.9336e-02,\n",
      "         7.1777e-02,  5.4932e-02,  6.1523e-02,  6.0547e-02,  6.4453e-02,\n",
      "         7.3242e-02,  5.7373e-02, -2.5749e-04,  5.1025e-02,  6.6895e-02,\n",
      "         8.5449e-02,  7.6660e-02,  5.6641e-02,  5.2002e-02,  7.1289e-02,\n",
      "         7.7148e-02,  5.8105e-02,  6.0791e-02,  5.9326e-02,  5.2490e-02,\n",
      "         6.6895e-02,  7.3242e-02,  7.6172e-02,  8.2031e-02,  1.0315e-02,\n",
      "         6.8359e-02,  8.2031e-02,  6.0791e-02,  6.1279e-02,  6.2012e-02,\n",
      "         7.2266e-02,  6.2500e-02,  6.4453e-02,  6.5430e-02,  6.9336e-02,\n",
      "         6.4941e-02,  7.6172e-02,  7.4219e-02,  5.9570e-02, -5.7373e-02,\n",
      "         6.6895e-02,  6.8848e-02,  6.9824e-02, -4.3945e-02,  6.3965e-02,\n",
      "         4.9072e-02,  6.9336e-02,  6.2256e-02,  6.2988e-02,  5.9326e-02,\n",
      "         6.0303e-02,  7.5195e-02,  5.9082e-02,  5.9570e-02,  6.2500e-02,\n",
      "        -5.4443e-02,  6.2500e-02,  6.2988e-02,  8.6426e-02,  5.8594e-02,\n",
      "         6.9336e-02,  5.7129e-02,  1.5430e-01,  8.1055e-02,  6.4453e-02,\n",
      "         6.7383e-02,  6.1768e-02,  6.3477e-02,  4.1016e-02,  6.2500e-02,\n",
      "         6.0303e-02,  5.4688e-02,  5.9570e-02,  6.6895e-02,  6.3965e-02,\n",
      "         6.5918e-02,  7.3242e-02,  6.2988e-02,  7.5195e-02,  5.9082e-02,\n",
      "         6.0791e-02,  6.1768e-02,  6.2012e-02,  6.5430e-02,  6.4453e-02,\n",
      "         4.8828e-02,  7.1289e-02,  7.0312e-02,  6.6406e-02,  5.2734e-02,\n",
      "         6.0303e-02,  5.6396e-02,  6.7383e-02,  5.6641e-02,  8.1055e-02,\n",
      "         6.1768e-02,  6.5918e-02,  5.6396e-02,  5.9082e-02,  7.0312e-02,\n",
      "         4.3213e-02,  5.7861e-02,  5.2490e-02,  6.1279e-02,  5.7373e-02,\n",
      "         8.8867e-02, -4.9072e-02,  5.3955e-02,  1.1182e-01,  6.1523e-02,\n",
      "         5.8350e-02,  7.2754e-02,  6.4453e-02,  6.6406e-02,  6.3965e-02,\n",
      "         7.6172e-02,  6.7871e-02,  6.4453e-02,  5.4688e-02, -7.5684e-02,\n",
      "         7.3730e-02,  6.2256e-02,  8.1055e-02,  5.7373e-02,  6.0547e-02,\n",
      "         6.7871e-02,  7.6172e-02,  6.0059e-02,  5.8594e-02,  5.5420e-02,\n",
      "         5.6885e-02,  6.0547e-02,  5.8105e-02,  7.5195e-02,  5.9326e-02,\n",
      "         6.5918e-02,  6.0547e-02, -4.9072e-02,  5.9082e-02,  6.2500e-02,\n",
      "         5.9814e-02,  7.5684e-02,  7.2754e-02,  6.0791e-02,  6.0059e-02,\n",
      "         6.1768e-02,  7.8125e-02,  5.6152e-02,  4.9316e-02,  8.5449e-02,\n",
      "         7.5684e-02,  6.6406e-02, -4.5654e-02,  7.0801e-02,  6.4941e-02,\n",
      "         6.3965e-02,  6.2256e-02,  7.0312e-02,  6.9336e-02,  4.5410e-02,\n",
      "         1.4551e-01,  5.9326e-02,  8.4961e-02,  6.0547e-02, -5.0537e-02,\n",
      "         7.9102e-02,  6.4941e-02,  5.2734e-02,  6.7383e-02,  6.7383e-02,\n",
      "         6.9336e-02,  5.8594e-02,  6.7383e-02,  6.3477e-02,  6.1768e-02,\n",
      "         6.7871e-02,  6.1523e-02,  7.7637e-02,  6.7871e-02,  1.0352e-01,\n",
      "         7.0312e-02,  6.9336e-02,  6.1035e-02,  6.3965e-02,  5.8105e-02,\n",
      "         6.2988e-02,  4.8096e-02,  6.6406e-02,  5.7617e-02,  6.8359e-02,\n",
      "         5.7617e-02,  5.0537e-02,  7.0801e-02,  6.2500e-02,  4.7119e-02,\n",
      "        -5.1758e-02,  5.8838e-02,  5.3711e-02,  6.7871e-02,  6.3477e-02,\n",
      "         8.4961e-02,  5.9082e-02,  5.9814e-02,  6.6895e-02,  6.4941e-02,\n",
      "         5.4688e-02,  5.6396e-02,  5.4443e-02, -5.3223e-02,  6.8359e-02,\n",
      "         6.1035e-02,  6.2988e-02,  5.1025e-02], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.decoder.block.9.layer.2.DenseReluDense.wi.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.2168,  0.0894,  0.6289,  ...,  0.4941,  0.1025, -0.0903],\n",
      "        [ 0.4668, -0.1494,  0.0981,  ..., -0.5859, -0.3027,  0.2734],\n",
      "        [-0.1729, -0.3691,  0.2432,  ..., -0.4238,  0.1357,  0.1436],\n",
      "        ...,\n",
      "        [ 0.3789,  0.0542, -0.2871,  ..., -0.6289,  0.2754,  0.6953],\n",
      "        [ 0.1748,  0.4102,  0.2441,  ..., -0.4453,  0.0986,  0.7109],\n",
      "        [-0.3027,  0.1436, -0.5312,  ..., -0.1748, -0.0874,  0.7422]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.9.layer.2.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.5625, -0.0684,  0.1465,  ..., -0.0483,  0.2656, -0.1025],\n",
      "        [ 0.1768,  0.2852,  0.1084,  ...,  0.2695,  0.2441,  0.1377],\n",
      "        [ 0.2285,  0.0781,  0.0483,  ..., -0.3457,  0.0569, -0.0674],\n",
      "        ...,\n",
      "        [ 0.2949,  0.0464,  0.2197,  ...,  0.3926, -0.3066, -0.0723],\n",
      "        [-0.0466,  0.3359,  0.2168,  ...,  0.1416,  0.0374,  0.0262],\n",
      "        [ 0.6719, -0.0068,  0.0413,  ..., -0.0713,  0.2256,  0.4609]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.9.layer.2.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 4.6250e+00,  5.3438e+00,  5.4062e+00,  3.0156e+00,  4.6250e+00,\n",
      "         4.3125e+00,  4.7188e+00,  5.0625e+00,  4.8750e+00,  4.5312e+00,\n",
      "         4.6250e+00,  4.7188e+00,  6.1250e+00,  4.9375e+00,  4.8125e+00,\n",
      "         4.4375e+00,  4.6875e+00,  4.7188e+00,  4.6250e+00,  4.8125e+00,\n",
      "         4.6562e+00,  4.6250e+00,  4.7500e+00,  3.9375e+00,  4.3438e+00,\n",
      "         4.8125e+00,  4.7500e+00,  5.4688e+00,  5.0312e+00,  4.6250e+00,\n",
      "         4.6562e+00,  6.0625e+00,  4.5312e+00,  4.5000e+00,  5.1250e+00,\n",
      "         5.1875e+00,  4.7812e+00,  5.9375e+00,  4.4062e+00,  4.5625e+00,\n",
      "         4.2500e+00,  4.8438e+00,  4.7812e+00,  4.8125e+00,  5.7188e+00,\n",
      "         4.5000e+00,  4.9375e+00,  4.9062e+00,  4.4375e+00,  4.7500e+00,\n",
      "         5.0312e+00,  4.4062e+00,  4.9062e+00,  4.4688e+00,  6.9062e+00,\n",
      "         4.5312e+00,  4.9375e+00,  4.5000e+00,  4.7500e+00,  4.5000e+00,\n",
      "         4.9062e+00,  4.5000e+00,  4.7812e+00,  4.9688e+00,  4.5312e+00,\n",
      "         8.5000e+00,  4.4062e+00,  5.1250e+00,  7.1875e+00,  4.4062e+00,\n",
      "         5.2500e+00,  4.5312e+00,  4.8750e+00,  4.7188e+00,  5.1562e+00,\n",
      "         4.3750e+00,  4.9375e+00,  4.3750e+00,  4.7188e+00,  5.2812e+00,\n",
      "         4.4062e+00,  4.5312e+00,  4.3750e+00,  5.2188e+00,  4.9688e+00,\n",
      "         5.0000e+00,  1.3594e+00,  4.4688e+00,  4.3438e+00,  4.5312e+00,\n",
      "         4.6250e+00,  4.5625e+00,  4.2500e+00,  4.5312e+00,  4.9062e+00,\n",
      "         4.5312e+00,  4.5625e+00,  4.4375e+00,  4.7500e+00,  4.2188e+00,\n",
      "         4.8125e+00,  4.5000e+00,  6.6562e+00,  4.6875e+00,  4.6250e+00,\n",
      "         4.5312e+00,  4.3750e+00,  4.7188e+00,  4.7812e+00,  5.0312e+00,\n",
      "         4.7188e+00,  4.6875e+00,  4.3125e+00,  4.5312e+00,  4.8750e+00,\n",
      "         5.2188e+00,  4.6250e+00,  4.6875e+00,  2.0264e-02,  4.8125e+00,\n",
      "         4.5312e+00,  4.4375e+00,  5.4688e+00,  4.4062e+00,  4.8438e+00,\n",
      "         4.5625e+00,  4.5312e+00,  4.4062e+00,  4.6250e+00,  4.9688e+00,\n",
      "         6.1250e+00,  4.8750e+00,  4.6562e+00,  4.5312e+00,  4.4375e+00,\n",
      "         4.7500e+00,  4.2812e+00,  5.4375e+00,  4.4062e+00,  4.7500e+00,\n",
      "         5.9062e+00,  4.6562e+00,  4.3750e+00,  4.8750e+00,  4.6250e+00,\n",
      "         4.4688e+00,  4.8750e+00,  5.2188e+00,  4.7188e+00,  4.3750e+00,\n",
      "         4.6875e+00,  4.6875e+00,  5.7812e+00,  5.1250e+00,  4.6875e+00,\n",
      "         4.8750e+00,  4.4062e+00,  4.8125e+00,  4.4375e+00,  4.5625e+00,\n",
      "         4.4062e+00,  4.6875e+00,  4.6875e+00,  5.3438e+00,  4.6875e+00,\n",
      "         5.2812e+00,  4.9062e+00,  5.0625e+00,  6.3438e+00,  4.6250e+00,\n",
      "         6.4375e+00,  4.5625e+00,  5.0312e+00,  4.8438e+00,  4.7188e+00,\n",
      "         4.2812e+00,  4.6875e+00,  4.7812e+00,  6.0938e+00,  4.7812e+00,\n",
      "         5.0000e+00,  4.6562e+00,  4.5938e+00,  4.8125e+00,  4.8750e+00,\n",
      "         5.4062e+00,  4.5625e+00,  3.0312e+00,  4.8750e+00,  4.5938e+00,\n",
      "         4.7812e+00,  4.5938e+00,  4.8438e+00,  4.4375e+00,  4.2812e+00,\n",
      "         4.7812e+00,  4.4375e+00,  4.9375e+00,  4.5625e+00,  4.5312e+00,\n",
      "         4.5625e+00,  4.4375e+00,  4.7188e+00,  4.8438e+00,  4.6875e+00,\n",
      "         4.2188e+00,  4.2188e+00,  4.5938e+00,  4.6875e+00,  4.5312e+00,\n",
      "         5.3125e+00,  4.4688e+00,  4.3438e+00,  5.0000e+00,  4.3438e+00,\n",
      "         5.0938e+00,  4.8750e+00,  4.7500e+00,  4.4375e+00,  1.4922e+00,\n",
      "         5.7812e+00,  4.8438e+00,  5.1875e+00,  4.3438e+00,  4.9375e+00,\n",
      "         4.6250e+00,  4.2500e+00,  4.5625e+00,  4.5625e+00,  5.0000e+00,\n",
      "         4.6250e+00,  4.4375e+00,  4.8750e+00,  4.3125e+00,  4.7812e+00,\n",
      "         4.5625e+00,  4.9062e+00,  4.2500e+00,  5.1875e+00,  4.4688e+00,\n",
      "         7.4688e+00,  4.5000e+00,  4.5625e+00,  4.5000e+00,  4.0000e+00,\n",
      "         4.2500e+00,  4.6562e+00,  4.1562e+00,  4.4688e+00,  4.3750e+00,\n",
      "         5.0000e+00,  4.6562e+00,  4.7500e+00,  4.9375e+00,  4.7188e+00,\n",
      "         4.9688e+00,  4.8438e+00,  5.3438e+00,  4.4688e+00,  4.9375e+00,\n",
      "         5.1875e+00,  5.2812e+00,  4.4688e+00,  4.3125e+00,  4.9375e+00,\n",
      "         4.4688e+00,  4.7812e+00,  4.6875e+00,  4.4062e+00,  4.8750e+00,\n",
      "         4.5625e+00,  8.2500e+00,  4.5312e+00,  4.8438e+00,  5.9688e+00,\n",
      "         4.2812e+00,  4.9375e+00,  4.6250e+00,  4.7812e+00,  4.6250e+00,\n",
      "         1.2266e+00,  6.4062e+00,  4.6250e+00,  4.5938e+00,  4.2188e+00,\n",
      "         4.5625e+00,  4.4062e+00,  4.8125e+00,  5.5938e+00,  4.7500e+00,\n",
      "         4.8125e+00,  4.6562e+00,  4.4688e+00,  4.5625e+00,  4.6250e+00,\n",
      "         4.7500e+00,  5.5000e+00,  4.6875e+00,  4.9375e+00,  4.9688e+00,\n",
      "         5.0000e+00,  4.7500e+00,  2.0781e+00,  4.4062e+00,  4.5312e+00,\n",
      "         4.7500e+00,  4.3750e+00,  4.4375e+00,  4.8750e+00,  4.5625e+00,\n",
      "         4.7500e+00,  4.9062e+00,  4.2500e+00,  4.3438e+00,  5.1562e+00,\n",
      "         4.7500e+00,  4.8750e+00,  4.9688e+00,  4.5938e+00,  4.5625e+00,\n",
      "         4.2188e+00,  5.0938e+00,  4.3438e+00,  4.8125e+00,  4.7500e+00,\n",
      "         4.7188e+00,  4.2812e+00,  5.5312e+00,  4.6250e+00,  4.4688e+00,\n",
      "         4.7500e+00,  5.4062e+00,  4.5312e+00,  4.5000e+00,  4.8125e+00,\n",
      "         4.6250e+00,  4.9375e+00,  4.7500e+00,  4.6250e+00,  4.3750e+00,\n",
      "         4.7188e+00,  5.0625e+00,  4.5938e+00,  4.1875e+00,  4.7500e+00,\n",
      "         4.8438e+00,  4.6562e+00,  4.8438e+00,  5.2812e+00,  5.0625e+00,\n",
      "         4.5312e+00,  4.5000e+00,  4.5000e+00,  4.6562e+00,  4.3438e+00,\n",
      "         4.7500e+00,  4.5938e+00, -1.8921e-02,  4.7500e+00,  4.6250e+00,\n",
      "         4.6875e+00,  4.6562e+00,  6.9688e+00,  4.3750e+00,  4.7500e+00,\n",
      "         4.8125e+00,  4.9688e+00,  4.6562e+00,  4.6562e+00,  4.6562e+00,\n",
      "         4.9062e+00,  4.5625e+00,  5.0938e+00,  4.3750e+00,  4.5000e+00,\n",
      "         4.5938e+00,  4.5625e+00,  4.8750e+00,  5.2500e+00,  5.2188e+00,\n",
      "         4.6562e+00,  4.6562e+00,  5.4688e+00,  4.3750e+00,  4.8750e+00,\n",
      "         4.3750e+00,  4.4375e+00,  4.5625e+00,  4.5938e+00,  4.5625e+00,\n",
      "         5.0312e+00,  4.6562e+00,  4.6250e+00,  4.7188e+00,  4.7500e+00,\n",
      "         4.3750e+00,  5.1562e+00,  4.6562e+00,  4.4375e+00,  4.6562e+00,\n",
      "         4.4375e+00,  4.9688e+00,  4.4688e+00,  4.6250e+00,  4.1875e+00,\n",
      "         4.7500e+00,  5.0312e+00,  4.4062e+00,  4.7812e+00,  4.4062e+00,\n",
      "         4.9375e+00,  4.7812e+00,  4.5312e+00,  4.4688e+00,  4.8438e+00,\n",
      "         4.7500e+00,  5.2812e+00,  4.4688e+00,  6.0000e+00,  4.8438e+00,\n",
      "         1.4526e-02,  4.4062e+00,  4.5625e+00,  3.2500e+00,  4.5938e+00,\n",
      "         4.8750e+00,  4.3125e+00,  4.4688e+00,  4.9062e+00,  4.9062e+00,\n",
      "         4.5938e+00,  4.9375e+00,  4.2812e+00,  4.8125e+00,  4.5938e+00,\n",
      "         4.7500e+00,  4.4062e+00,  5.1250e+00,  5.2188e+00,  5.3750e+00,\n",
      "         5.2812e+00,  5.9688e+00,  4.7500e+00,  4.6875e+00,  4.5000e+00,\n",
      "         4.6250e+00,  4.3750e+00,  4.4062e+00,  5.4321e-03,  4.6875e+00,\n",
      "         4.6250e+00,  4.6562e+00,  4.5000e+00,  4.8438e+00,  4.7812e+00,\n",
      "         4.7188e+00,  5.4375e+00,  5.5938e+00,  4.5938e+00,  4.8125e+00,\n",
      "         4.7500e+00,  4.5312e+00,  4.7500e+00,  4.9688e+00,  4.7812e+00,\n",
      "         4.8125e+00,  4.7500e+00,  4.5000e+00,  4.2188e+00,  4.6875e+00,\n",
      "         4.4062e+00,  4.5312e+00,  8.1250e+00,  4.8125e+00,  4.5312e+00,\n",
      "         4.9062e+00,  4.3750e+00,  5.0000e+00,  4.8125e+00,  4.3750e+00,\n",
      "         4.4688e+00,  4.8438e+00,  4.6875e+00,  4.6562e+00,  1.7812e+00,\n",
      "         4.7188e+00,  4.5000e+00,  1.2109e+00,  5.0938e+00,  4.2812e+00,\n",
      "         4.6250e+00,  4.6875e+00,  4.6562e+00,  4.6562e+00,  4.8125e+00,\n",
      "         1.1812e+01,  5.0312e+00,  4.5625e+00,  4.3750e+00,  4.5000e+00,\n",
      "         4.6875e+00,  5.2500e+00,  4.6250e+00,  4.7500e+00,  4.6875e+00,\n",
      "         4.8750e+00,  4.9375e+00,  4.3438e+00,  4.6562e+00,  4.7500e+00,\n",
      "         4.5938e+00,  4.5312e+00,  4.9375e+00,  4.8125e+00,  4.9688e+00,\n",
      "         4.6562e+00,  4.3438e+00,  4.3750e+00,  5.1250e+00,  4.5625e+00,\n",
      "         4.4688e+00,  4.4375e+00,  4.5000e+00,  4.9062e+00,  4.8750e+00,\n",
      "         4.7812e+00,  4.5625e+00,  5.8750e+00,  4.9062e+00,  4.6250e+00,\n",
      "         4.7812e+00,  4.6562e+00,  4.3125e+00,  4.3125e+00,  4.6875e+00,\n",
      "         4.8125e+00,  4.3750e+00,  4.5000e+00,  4.5000e+00,  4.3438e+00,\n",
      "         4.6562e+00,  4.8438e+00,  4.3125e+00,  4.5625e+00,  4.4062e+00,\n",
      "         4.5625e+00,  5.0312e+00,  4.6875e+00,  4.7188e+00,  4.8125e+00,\n",
      "         5.2188e+00,  5.6250e+00,  4.3750e+00,  6.0938e+00,  4.4688e+00,\n",
      "         4.7812e+00,  4.8750e+00,  4.5000e+00,  4.6250e+00,  4.5312e+00,\n",
      "         4.6562e+00,  5.0625e+00,  9.8047e-01,  5.4062e+00,  4.5312e+00,\n",
      "         5.9375e+00,  4.7188e+00,  4.4062e+00,  4.7812e+00,  4.4688e+00,\n",
      "         4.7500e+00,  4.5000e+00,  4.6562e+00,  4.5625e+00,  4.7812e+00,\n",
      "         4.9375e+00,  4.5625e+00,  4.8750e+00,  4.9688e+00,  1.0078e+00,\n",
      "         5.0000e+00,  4.7188e+00,  4.6875e+00,  4.9688e+00,  4.3438e+00,\n",
      "         4.6250e+00,  4.8438e+00,  4.5625e+00,  4.4688e+00,  4.6562e+00,\n",
      "         4.6562e+00,  4.9062e+00,  5.2500e+00,  4.6562e+00,  4.2188e+00,\n",
      "         4.5938e+00,  6.2812e+00,  4.6250e+00,  4.4375e+00,  5.0312e+00,\n",
      "         4.5625e+00,  4.6562e+00,  4.2812e+00,  4.5938e+00,  4.6250e+00,\n",
      "         4.6562e+00,  4.6250e+00,  6.2188e+00,  4.3125e+00,  4.4688e+00,\n",
      "         4.5938e+00,  4.8438e+00,  4.3438e+00,  4.9062e+00,  4.5000e+00,\n",
      "         4.5000e+00,  4.4688e+00,  6.4688e+00,  4.9688e+00,  4.3750e+00,\n",
      "         4.5000e+00,  4.6250e+00,  4.9375e+00,  5.5000e+00,  4.3750e+00,\n",
      "         4.7188e+00,  4.4062e+00,  4.2812e+00,  4.7812e+00,  4.5625e+00,\n",
      "         4.6875e+00,  4.6250e+00,  5.0312e+00,  7.0625e+00,  4.7500e+00,\n",
      "         4.9375e+00,  4.7812e+00,  4.7188e+00,  4.6875e+00,  4.5000e+00,\n",
      "         4.9688e+00,  4.9062e+00,  4.4688e+00,  5.1562e+00,  5.5938e+00,\n",
      "         4.6250e+00,  7.0625e+00,  4.4375e+00,  4.0625e+00,  5.0000e+00,\n",
      "         4.6250e+00,  4.6250e+00,  4.4062e+00,  4.4062e+00,  4.9375e+00,\n",
      "         5.9062e+00,  4.5625e+00,  4.5000e+00,  4.1875e+00,  4.6250e+00,\n",
      "         4.9688e+00,  4.7500e+00,  4.4375e+00,  5.6250e+00,  4.5938e+00,\n",
      "         4.6250e+00,  4.6562e+00,  4.7500e+00,  4.4375e+00,  4.4688e+00,\n",
      "         4.8750e+00,  4.5312e+00,  4.7812e+00,  4.3750e+00,  4.6562e+00,\n",
      "         4.5625e+00,  4.5938e+00,  4.8125e+00,  4.7812e+00,  4.5312e+00,\n",
      "         4.5938e+00,  4.7812e+00,  6.0000e+00,  4.5000e+00,  4.4375e+00,\n",
      "         4.4688e+00,  4.3125e+00,  4.4375e+00,  4.7188e+00,  4.6250e+00,\n",
      "         4.6562e+00,  4.5000e+00,  4.6875e+00,  4.5938e+00,  4.6875e+00,\n",
      "         4.3438e+00,  4.7500e+00,  5.0000e+00,  4.9375e+00,  4.5625e+00,\n",
      "         4.3125e+00,  4.6562e+00,  4.7188e+00,  5.1250e+00,  4.9062e+00,\n",
      "         4.6250e+00,  4.5938e+00,  5.0000e+00,  6.3750e+00,  4.5312e+00,\n",
      "         4.9375e+00,  4.7812e+00,  4.7812e+00,  5.1562e+00,  5.1875e+00,\n",
      "         5.0625e+00,  4.7188e+00,  5.0938e+00,  4.2188e+00,  4.4375e+00,\n",
      "         4.5312e+00,  4.6562e+00,  6.0312e+00,  4.7188e+00,  4.5000e+00,\n",
      "         5.6875e+00,  4.6250e+00,  4.7812e+00,  4.5625e+00,  4.4375e+00,\n",
      "         4.3750e+00,  4.5000e+00,  4.9062e+00,  5.8750e+00,  4.7500e+00,\n",
      "         4.7500e+00,  4.4062e+00,  4.4688e+00,  4.3125e+00,  4.9688e+00,\n",
      "         4.8750e+00,  5.1562e+00,  4.8438e+00,  5.5000e+00,  4.7188e+00,\n",
      "         4.6250e+00,  5.0625e+00,  4.5625e+00,  4.6562e+00,  5.0000e+00,\n",
      "         4.1875e+00,  4.5312e+00,  4.6562e+00,  4.5938e+00,  4.8438e+00,\n",
      "         4.9688e+00,  4.5312e+00,  4.5000e+00,  5.2188e+00,  4.4688e+00,\n",
      "         5.0625e+00,  4.5625e+00,  4.1562e+00,  4.2500e+00,  4.6562e+00,\n",
      "         4.4062e+00,  4.1562e+00,  4.5000e+00], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.decoder.block.10.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0071, -0.0295, -0.0222,  ...,  0.0337, -0.0172, -0.0116],\n",
      "        [-0.0020, -0.0041,  0.0317,  ..., -0.0461, -0.0361,  0.0732],\n",
      "        [-0.0330, -0.0339, -0.0108,  ..., -0.0195,  0.0437,  0.0204],\n",
      "        ...,\n",
      "        [ 0.0151,  0.0064, -0.0486,  ..., -0.0074,  0.0031, -0.0447],\n",
      "        [ 0.0752, -0.0138, -0.0330,  ..., -0.0439,  0.0299,  0.0095],\n",
      "        [ 0.0291, -0.0469,  0.0088,  ...,  0.0050, -0.0159, -0.0024]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.10.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0189,  0.1865, -0.0282,  ...,  0.1328,  0.1172, -0.2676],\n",
      "        [-0.2988,  0.3027,  0.3184,  ...,  0.2871, -0.0669,  0.1318],\n",
      "        [-0.0023,  0.1748,  0.1621,  ...,  0.1299,  0.1128,  0.0322],\n",
      "        ...,\n",
      "        [-0.2129,  0.0437, -0.3691,  ..., -0.3652, -0.2246, -0.3262],\n",
      "        [ 0.2451, -0.5000, -0.1191,  ..., -0.3105,  0.1523, -0.2158],\n",
      "        [ 0.1885, -0.5312,  0.0339,  ..., -0.4785,  0.0535, -0.0938]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.10.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 9.2285e-02, -1.5527e-01,  7.9297e-01,  ...,  3.0664e-01,\n",
      "         -4.2915e-04,  3.7500e-01],\n",
      "        [-7.7734e-01,  1.2266e+00, -1.2031e+00,  ..., -5.3125e-01,\n",
      "         -3.1641e-01,  3.7354e-02],\n",
      "        [-1.2891e+00, -6.4844e-01,  4.0430e-01,  ..., -4.2188e-01,\n",
      "          6.7969e-01,  9.1797e-01],\n",
      "        ...,\n",
      "        [-7.3828e-01, -1.3750e+00,  4.4922e-01,  ..., -8.0078e-01,\n",
      "         -2.8198e-02, -1.3438e+00],\n",
      "        [-5.5469e-01,  1.1719e+00,  1.7188e+00,  ...,  8.1641e-01,\n",
      "          3.3691e-02,  6.8750e-01],\n",
      "        [-9.4604e-04, -1.5469e+00,  4.3359e-01,  ...,  4.8242e-01,\n",
      "          1.0547e+00, -9.4531e-01]], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.10.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.5391, -1.8438,  0.1279,  ...,  0.3691,  0.5117, -0.2432],\n",
      "        [ 0.2432, -0.0199, -0.1079,  ..., -0.1006, -0.6250,  0.2539],\n",
      "        [-0.1328, -0.9062, -0.0053,  ...,  0.2852,  0.8359, -0.8320],\n",
      "        ...,\n",
      "        [-0.5352, -1.0234, -0.5742,  ..., -0.7617, -1.0781,  1.1719],\n",
      "        [ 0.0400, -1.5078,  1.7891,  ..., -0.6641, -0.2812, -1.1953],\n",
      "        [ 0.3965,  1.6328,  1.5469,  ...,  0.1807, -1.6484,  0.1787]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.10.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 0.2871,  0.3047,  0.3906, -0.1523,  0.3145,  0.2754,  0.3418,  0.3125,\n",
      "         0.3340,  0.2969,  0.2734,  0.3223,  0.2559,  0.2891,  0.3008,  0.2930,\n",
      "         0.2988,  0.3398, -0.2656,  0.3066,  0.3008,  0.3047,  0.2891,  0.2812,\n",
      "         0.2852,  0.3535,  0.3105,  0.3105,  0.3340,  0.3145,  0.3164,  0.3301,\n",
      "         0.3008,  0.3223,  0.3125,  0.3398,  0.3184,  0.3223,  0.3125,  0.3105,\n",
      "         0.2754,  0.3281,  0.3164,  0.3125,  0.4082,  0.3105,  0.3008,  0.3164,\n",
      "         0.3105,  0.3027,  0.3535,  0.3008,  0.3262,  0.2988,  0.4551,  0.2852,\n",
      "         0.3145,  0.3086,  0.3203,  0.3027,  0.3242,  0.3008,  0.2988,  0.3184,\n",
      "         0.3086,  0.3086,  0.3086,  0.3184,  0.3086,  0.2910,  0.3145,  0.3281,\n",
      "         0.3301,  0.3105,  0.3516,  0.3047, -0.2871,  0.2852,  0.3340,  0.3516,\n",
      "         0.3086,  0.2930,  0.2910,  0.3789,  0.3340,  0.3398,  0.0713,  0.3047,\n",
      "         0.2969,  0.3008,  0.3164,  0.3027,  0.2871,  0.2715,  0.3242,  0.3086,\n",
      "         0.3086,  0.3105,  0.3301,  0.2930,  0.2871,  0.3008,  0.3652,  0.3066,\n",
      "         0.3262,  0.3340,  0.2910,  0.3262,  0.3398,  0.3164,  0.3223,  0.3125,\n",
      "         0.2754,  0.3145,  0.3242,  0.3613,  0.3281,  0.2754, -0.0713,  0.3340,\n",
      "         0.3027,  0.3145,  0.3926,  0.3203,  0.3320,  0.3164,  0.2930,  0.2930,\n",
      "         0.2852,  0.3008, -0.3164,  0.3398,  0.3086,  0.2969,  0.2441,  0.2793,\n",
      "         0.2969,  0.3320,  0.2930,  0.3066,  0.3379,  0.3203,  0.2812,  0.3359,\n",
      "         0.2969,  0.2852,  0.3438,  0.3203,  0.3398,  0.2832,  0.3379,  0.3105,\n",
      "         0.3652,  0.3535,  0.3242,  0.3281,  0.2773,  0.3164,  0.1191,  0.2637,\n",
      "         0.2969,  0.3047,  0.2852,  0.3711,  0.3438,  0.3945,  0.3145,  0.3418,\n",
      "         0.4316,  0.3164,  0.2832,  0.3262,  0.3125,  0.3320,  0.3105,  0.2969,\n",
      "         0.2910,  0.3359, -0.2520,  0.3242,  0.3184,  0.2930,  0.2910,  0.3047,\n",
      "         0.3027,  0.3535,  0.3066,  0.0771,  0.3242,  0.3027,  0.3008,  0.3027,\n",
      "         0.3340,  0.2891,  0.3145,  0.3105,  0.3086,  0.3320,  0.3105,  0.3184,\n",
      "         0.2969,  0.2812,  0.3203,  0.3379,  0.3203,  0.2852,  0.2852,  0.2988,\n",
      "         0.3203,  0.3203,  0.3691,  0.2832,  0.2871,  0.3477,  0.2871,  0.3262,\n",
      "         0.3184,  0.3164,  0.2988,  0.0031,  0.3340,  0.3359,  0.3379,  0.2969,\n",
      "         0.3203,  0.3223,  0.7500,  0.3164,  0.3008,  0.3301,  0.3398,  0.2656,\n",
      "         0.3164,  0.3105,  0.3262,  0.2930,  0.2988,  0.2832,  0.3262,  0.2949,\n",
      "         0.4531,  0.3027,  0.3145,  0.3262,  0.2871,  0.2891,  0.3379,  0.2773,\n",
      "         0.3340,  0.2988,  0.3105,  0.2969,  0.3027,  0.3105,  0.3242,  0.3320,\n",
      "         0.3281,  0.2617,  0.3164,  0.3086, -0.2598,  0.2812,  0.3086,  0.2988,\n",
      "         0.3320,  0.3086,  0.3105,  0.3242,  0.3086,  0.3145,  0.2930,  0.5195,\n",
      "         0.3047,  0.3203, -0.2412,  0.2891,  0.3242,  0.2930,  0.3242,  0.2891,\n",
      "         0.0654,  0.4023,  0.3320,  0.3145,  0.2773,  0.2949,  0.3086,  0.3184,\n",
      "         0.3457,  0.2949,  0.3164,  0.3203,  0.3047,  0.3184,  0.3184,  0.3223,\n",
      "         0.2949,  0.3086,  0.3223,  0.3086,  0.3301,  0.3242,  0.0864,  0.2832,\n",
      "         0.3105,  0.3262,  0.2676,  0.2949,  0.2949,  0.3184,  0.3438,  0.3418,\n",
      "         0.2812,  0.2910,  0.4121,  0.3125,  0.3457,  0.2871,  0.3184,  0.3262,\n",
      "         0.2793,  0.3301,  0.2930,  0.3398,  0.3164,  0.3125,  0.2832,  0.3613,\n",
      "         0.2871,  0.2969,  0.3008,  0.3027,  0.2988,  0.3359,  0.3379,  0.2891,\n",
      "         0.3340,  0.3242,  0.3203, -0.1924,  0.2930,  0.3066,  0.3223,  0.2734,\n",
      "         0.3340,  0.3301,  0.3223,  0.2910,  0.2832,  0.3086,  0.3203,  0.3242,\n",
      "         0.3125,  0.3301,  0.2734,  0.3535,  0.3125,  0.2021,  0.3145,  0.3145,\n",
      "         0.2910,  0.3027,  0.2480,  0.2949,  0.3359,  0.3594,  0.3066,  0.3496,\n",
      "         0.3105,  0.3008,  0.3164,  0.3184,  0.3496,  0.2832,  0.3027,  0.3145,\n",
      "         0.3008,  0.3359,  0.3301,  0.2969,  0.3340,  0.3145,  0.3125,  0.3164,\n",
      "         0.3184,  0.3066,  0.2988,  0.3164,  0.3340,  0.3145,  0.3438,  0.3125,\n",
      "         0.2891, -0.2871,  0.3086,  0.2871,  0.3633,  0.3203,  0.2988,  0.2949,\n",
      "         0.3145,  0.2969,  0.2988,  0.3125,  0.2891,  0.3047,  0.3184,  0.3223,\n",
      "         0.3301,  0.3262,  0.3242,  0.3457,  0.3184,  0.2988,  0.3008,  0.3164,\n",
      "         0.3359,  0.3066,  0.3945,  0.3555,  0.0220,  0.2988,  0.3047, -0.1631,\n",
      "         0.3125,  0.3398, -0.2656,  0.3203,  0.3027,  0.2871,  0.3105,  0.3281,\n",
      "         0.2871,  0.3184,  0.3379,  0.3320,  0.2969,  0.3359,  0.3379,  0.3359,\n",
      "         0.3574,  0.3477,  0.3359,  0.3125,  0.3105,  0.2988,  0.2930,  0.2910,\n",
      "         0.0962,  0.3184,  0.2793,  0.3301,  0.3027,  0.3281,  0.3164,  0.2754,\n",
      "        -0.3359,  0.6055,  0.2832,  0.3125,  0.3262,  0.2988,  0.3184,  0.3281,\n",
      "         0.3066,  0.3184,  0.3027,  0.2832,  0.3105,  0.2871,  0.3008,  0.3086,\n",
      "         0.5273,  0.3164,  0.2871,  0.3281,  0.2891,  0.3340,  0.3301,  0.2930,\n",
      "         0.2930,  0.3203,  0.3145,  0.3125, -0.1445,  0.3027,  0.3203,  0.0417,\n",
      "         0.3105,  0.2695,  0.3047,  0.3379,  0.3008,  0.3301,  0.3223, -0.2139,\n",
      "         0.3066,  0.2988,  0.2930,  0.2891,  0.3203,  0.3750,  0.3066,  0.3008,\n",
      "         0.3184,  0.3047,  0.3223,  0.2891,  0.3418,  0.3262,  0.3086,  0.3105,\n",
      "         0.2969,  0.3184,  0.3262,  0.3223,  0.2910,  0.3027,  0.2910,  0.3086,\n",
      "         0.2969,  0.2832,  0.3145,  0.3672,  0.3223,  0.3359,  0.2852,  0.4434,\n",
      "         0.3320,  0.3262,  0.3184,  0.3242,  0.2871,  0.3145,  0.3184,  0.3203,\n",
      "         0.2910,  0.3027,  0.3027,  0.2910,  0.3184,  0.3145,  0.3047,  0.2793,\n",
      "         0.2891,  0.3125,  0.3262,  0.3145,  0.3086,  0.3184,  0.3379,  0.3418,\n",
      "         0.2930,  0.3809,  0.3105,  0.3047, -0.3066,  0.3125,  0.3438,  0.2910,\n",
      "         0.3223,  0.3242,  0.1523, -0.3105,  0.3027,  0.3867,  0.3281,  0.2988,\n",
      "         0.2969,  0.3262,  0.3086,  0.2969,  0.3066,  0.2988,  0.3066,  0.3145,\n",
      "         0.3203,  0.3418,  0.3496,  0.0388,  0.3633,  0.3301,  0.3008,  0.3125,\n",
      "         0.2910,  0.3066,  0.3047,  0.3008,  0.3145,  0.3125,  0.3027,  0.3145,\n",
      "         0.3848,  0.3203,  0.2852,  0.3145,  0.3926,  0.3184, -0.1484,  0.3359,\n",
      "         0.3086,  0.3184,  0.3008,  0.2988,  0.2988,  0.3047,  0.3281,  0.3770,\n",
      "         0.2949,  0.2988,  0.2930,  0.3164,  0.2891,  0.3398,  0.2812,  0.3027,\n",
      "         0.2871,  0.3613,  0.3457,  0.2969,  0.3086,  0.3145,  0.3301,  0.3047,\n",
      "         0.3027,  0.3184,  0.2793,  0.2754,  0.3105,  0.3184,  0.3203,  0.3105,\n",
      "         0.3613,  0.4434,  0.3242,  0.3203,  0.3164,  0.3086,  0.3223,  0.3008,\n",
      "         0.2969,  0.3457,  0.2988,  0.3301,  0.3555,  0.3066,  0.3477,  0.3086,\n",
      "         0.2930,  0.3242,  0.3047,  0.3242,  0.2949,  0.2891,  0.3438,  0.3594,\n",
      "         0.3184,  0.2754,  0.2793,  0.2910,  0.3379,  0.2754,  0.2988,  0.3672,\n",
      "         0.3066,  0.2891,  0.3223,  0.3125,  0.2988,  0.3184,  0.3203,  0.3145,\n",
      "         0.2969,  0.3105,  0.3125,  0.3398,  0.3086,  0.3184,  0.3301,  0.3086,\n",
      "         0.3125,  0.3145,  0.3457,  0.3145,  0.3047,  0.2871,  0.2949,  0.3008,\n",
      "         0.3105,  0.3262,  0.2969,  0.2871,  0.2930,  0.3125,  0.3027,  0.3008,\n",
      "         0.3281,  0.3477,  0.3145,  0.2910,  0.3047,  0.3223,  0.3008,  0.2539,\n",
      "         0.3633,  0.3281,  0.3047,  0.2754,  0.4023,  0.3203,  0.3301,  0.3145,\n",
      "         0.3008,  0.3242, -0.3203,  0.4219,  0.3125,  0.3574,  0.2949,  0.2715,\n",
      "         0.3047,  0.3164,  0.3145,  0.3027,  0.3125,  0.4062,  0.2832,  0.3105,\n",
      "         0.3105,  0.2852,  0.3242,  0.2988,  0.3301, -0.1855,  0.3750,  0.3047,\n",
      "         0.3008,  0.2852,  0.2969,  0.3281,  0.3418,  0.2793,  0.3145,  0.2969,\n",
      "         0.3340,  0.3145,  0.2930,  0.3281,  0.2910,  0.2637,  0.2715,  0.3027,\n",
      "         0.3027,  0.3242,  0.3301,  0.3633,  0.3066,  0.3008,  0.2793,  0.2988,\n",
      "         0.2910,  0.2988,  0.2793,  0.2891,  0.3203,  0.3008,  0.2773,  0.2793],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.10.layer.1.EncDecAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0391,  0.0040,  0.0598,  ...,  0.0012,  0.0879, -0.0085],\n",
      "        [ 0.0325, -0.0378,  0.0082,  ..., -0.0002,  0.0004,  0.0084],\n",
      "        [-0.0669,  0.0505,  0.0222,  ..., -0.0069, -0.0134, -0.0049],\n",
      "        ...,\n",
      "        [-0.0415, -0.0065, -0.0148,  ...,  0.0337,  0.0461, -0.0359],\n",
      "        [ 0.0330, -0.0388,  0.0052,  ...,  0.0327, -0.0596,  0.0669],\n",
      "        [-0.0393, -0.0569, -0.0059,  ...,  0.0280,  0.0391,  0.0167]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.10.layer.1.EncDecAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.6758, -0.5547, -0.1206,  ...,  0.3164, -0.1611,  0.4414],\n",
      "        [ 0.0850,  0.0835,  0.3262,  ..., -0.1660, -0.0664,  0.1436],\n",
      "        [-0.2891,  0.0079,  0.2129,  ...,  0.4492,  0.1553,  0.2695],\n",
      "        ...,\n",
      "        [-0.0757,  0.2617, -0.1553,  ...,  0.4551,  0.3750,  0.1494],\n",
      "        [ 0.3340,  0.0623,  0.3848,  ..., -0.0112, -0.3105, -0.1201],\n",
      "        [-0.1426, -0.3320, -0.2070,  ..., -0.0688,  0.3457,  0.3652]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.10.layer.1.EncDecAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 4.3750e-01, -4.1602e-01,  1.3828e+00,  ...,  3.0469e-01,\n",
      "         -2.0781e+00, -1.5859e+00],\n",
      "        [ 1.3086e-01, -1.1328e+00, -1.1139e-03,  ..., -1.9062e+00,\n",
      "         -3.4766e-01, -3.6328e-01],\n",
      "        [-3.8477e-01, -1.8848e-01, -1.5320e-02,  ..., -9.4922e-01,\n",
      "          2.1387e-01, -3.2471e-02],\n",
      "        ...,\n",
      "        [ 1.9238e-01, -4.3213e-02, -8.7500e-01,  ..., -2.5391e-01,\n",
      "          1.4648e-01, -3.4570e-01],\n",
      "        [ 1.9219e+00,  6.1328e-01,  4.0039e-01,  ...,  5.0000e-01,\n",
      "          7.4609e-01, -3.8086e-01],\n",
      "        [ 2.3906e+00, -4.3750e-01, -7.3047e-01,  ...,  7.0312e-01,\n",
      "         -7.2754e-02,  2.3730e-01]], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.10.layer.1.EncDecAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.4453, -1.1094,  1.8516,  ..., -1.1094, -0.0723, -0.6328],\n",
      "        [ 0.3301,  0.1021,  1.1797,  ...,  0.1250,  0.0596, -0.6328],\n",
      "        [-0.5898,  0.7656,  0.4062,  ..., -0.3516,  0.1357, -0.1738],\n",
      "        ...,\n",
      "        [ 0.6641, -0.5625, -0.5117,  ...,  0.6172, -0.3223,  1.0156],\n",
      "        [ 0.3379, -1.1094,  0.3945,  ...,  0.4824,  0.5273, -0.2969],\n",
      "        [ 0.0889, -0.4102,  0.2139,  ...,  1.7500, -0.4395,  0.5859]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.10.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 5.4932e-02,  8.2520e-02,  7.0801e-02, -4.1748e-02,  5.8350e-02,\n",
      "         5.8594e-02,  1.0791e-01,  6.7383e-02,  6.6895e-02,  7.5195e-02,\n",
      "         5.3223e-02,  8.3496e-02,  6.9336e-02,  4.8340e-02,  7.4707e-02,\n",
      "         6.6895e-02,  5.9814e-02,  7.2754e-02, -5.7617e-02,  8.4961e-02,\n",
      "         7.0312e-02,  6.3477e-02,  7.1777e-02,  6.4453e-02,  6.8848e-02,\n",
      "         8.2031e-02,  7.4219e-02,  5.3223e-02,  7.8613e-02,  7.5684e-02,\n",
      "         6.8359e-02,  8.2520e-02,  7.0312e-02,  8.4473e-02,  6.5430e-02,\n",
      "         7.8613e-02,  8.3984e-02,  5.7373e-02,  6.5918e-02,  7.7637e-02,\n",
      "         5.4443e-02,  6.5918e-02,  6.3965e-02,  6.3965e-02,  9.2773e-02,\n",
      "         6.8848e-02,  8.5938e-02,  9.0820e-02,  6.6895e-02,  5.9570e-02,\n",
      "         9.0332e-02,  5.9082e-02,  7.0801e-02,  6.6895e-02,  1.0596e-01,\n",
      "         5.3223e-02,  5.7373e-02,  6.6895e-02,  1.0010e-01,  6.4941e-02,\n",
      "         6.1035e-02,  6.5918e-02,  8.1055e-02,  7.4219e-02,  7.3242e-02,\n",
      "         7.5195e-02,  6.9824e-02,  6.8359e-02,  6.8359e-02,  6.0303e-02,\n",
      "         8.9844e-02,  8.6426e-02,  7.9590e-02,  6.0059e-02,  7.8125e-02,\n",
      "         6.6406e-02,  5.4443e-02,  6.2500e-02,  7.9102e-02,  8.5449e-02,\n",
      "         7.2266e-02,  6.1523e-02, -6.2988e-02,  6.5430e-02,  8.3008e-02,\n",
      "         7.9102e-02, -5.3406e-04,  6.2500e-02,  6.9824e-02,  7.0312e-02,\n",
      "         6.3965e-02,  7.7148e-02,  5.7129e-02,  5.2979e-02,  7.3242e-02,\n",
      "         8.0566e-02,  7.3242e-02,  7.6660e-02,  7.6172e-02,  6.0059e-02,\n",
      "        -5.4688e-02,  7.8125e-02,  5.8105e-02,  6.5430e-02,  7.0312e-02,\n",
      "         6.0303e-02,  6.5918e-02,  6.7383e-02,  9.3750e-02,  8.1543e-02,\n",
      "         8.0566e-02,  6.8848e-02, -4.7363e-02,  6.5918e-02,  6.8848e-02,\n",
      "         1.1084e-01,  6.2500e-02,  6.3477e-02,  1.2589e-04,  7.0801e-02,\n",
      "         6.6406e-02,  9.1797e-02,  8.4473e-02,  6.4453e-02,  7.3730e-02,\n",
      "         7.0801e-02,  6.5918e-02,  5.3955e-02,  6.2256e-02, -5.9814e-02,\n",
      "        -5.7861e-02,  6.7383e-02,  6.7383e-02,  7.0312e-02,  6.2988e-02,\n",
      "        -5.5908e-02, -6.0547e-02,  6.8359e-02,  5.6885e-02,  5.2979e-02,\n",
      "         6.9824e-02,  7.5684e-02,  5.1758e-02,  6.7383e-02,  6.2256e-02,\n",
      "         5.3223e-02,  8.6914e-02,  6.2012e-02,  7.7637e-02,  5.1758e-02,\n",
      "         1.0547e-01,  6.8848e-02,  1.1084e-01,  6.1279e-02,  8.1055e-02,\n",
      "         8.6426e-02,  5.9570e-02,  8.5938e-02,  1.0986e-01,  5.8594e-02,\n",
      "         6.1279e-02,  6.0547e-02,  5.7373e-02,  7.1777e-02,  8.1055e-02,\n",
      "         5.2002e-02,  7.8125e-02,  7.3730e-02,  5.2490e-02,  6.9824e-02,\n",
      "         5.5176e-02,  7.0312e-02,  9.2773e-02,  7.5684e-02,  7.6172e-02,\n",
      "         6.5918e-02,  6.4941e-02,  9.7168e-02,  5.2979e-02,  7.7148e-02,\n",
      "         7.5195e-02,  6.6895e-02,  7.2754e-02,  7.9590e-02,  6.4941e-02,\n",
      "         7.9590e-02,  6.4453e-02,  7.7148e-02,  6.5430e-02,  5.6396e-02,\n",
      "        -7.2266e-02,  6.8848e-02,  7.6172e-02,  5.8594e-02,  6.4453e-02,\n",
      "         6.4941e-02,  6.7383e-02,  8.2031e-02,  6.4453e-02,  6.7871e-02,\n",
      "         6.6895e-02,  5.5420e-02,  7.3242e-02,  8.1055e-02,  7.0312e-02,\n",
      "        -5.3223e-02,  6.1523e-02,  7.1289e-02,  7.6172e-02,  7.0312e-02,\n",
      "         8.9844e-02,  5.5176e-02,  6.3965e-02,  7.5195e-02,  7.0312e-02,\n",
      "         7.7148e-02,  6.0547e-02,  7.8125e-02,  6.4941e-02, -1.1063e-03,\n",
      "         5.6885e-02,  1.1377e-01,  7.6172e-02, -6.7871e-02,  6.7871e-02,\n",
      "         8.4961e-02,  2.7930e-01,  8.0566e-02,  6.0547e-02,  7.8613e-02,\n",
      "         7.7148e-02,  6.1035e-02,  6.2500e-02,  7.3242e-02,  7.3730e-02,\n",
      "         6.0791e-02, -5.6885e-02,  5.6396e-02,  5.6396e-02,  6.2988e-02,\n",
      "         9.6680e-02,  7.3730e-02,  6.7871e-02,  7.2754e-02,  6.5918e-02,\n",
      "         5.9082e-02,  9.6191e-02,  5.6641e-02,  7.1289e-02,  6.0791e-02,\n",
      "         5.6885e-02,  6.5918e-02,  7.9590e-02,  6.5918e-02,  7.0801e-02,\n",
      "         7.0312e-02,  7.0801e-02,  5.2734e-02,  7.4219e-02,  7.2266e-02,\n",
      "         6.0547e-02,  5.8594e-02,  7.4219e-02,  6.5430e-02,  7.6172e-02,\n",
      "         6.1035e-02,  6.7383e-02,  7.4707e-02,  6.4941e-02, -6.1279e-02,\n",
      "         6.5430e-02,  8.0566e-02,  6.7871e-02,  7.0801e-02,  5.8838e-02,\n",
      "         6.2256e-02,  8.2520e-02,  5.7617e-02,  7.6172e-02,  5.9082e-02,\n",
      "         1.5106e-03,  7.7148e-02,  7.3730e-02,  6.4941e-02,  5.8350e-02,\n",
      "         7.1289e-02,  6.4453e-02,  7.5195e-02,  5.6152e-02,  5.0537e-02,\n",
      "         7.1289e-02,  7.3242e-02,  6.4941e-02,  7.0312e-02,  6.7383e-02,\n",
      "         7.4707e-02,  6.3965e-02,  7.8613e-02,  7.7637e-02,  8.1543e-02,\n",
      "         8.7891e-02,  7.1289e-02,  2.9663e-02,  6.0059e-02,  6.3477e-02,\n",
      "         7.3730e-02,  5.4199e-02,  6.9824e-02,  6.0303e-02,  6.8848e-02,\n",
      "         1.0889e-01,  8.0078e-02,  6.4453e-02,  5.8594e-02,  2.9297e-01,\n",
      "        -6.4453e-02,  1.2305e-01,  6.7383e-02,  7.2754e-02,  8.2520e-02,\n",
      "         6.5918e-02,  7.9590e-02,  5.7129e-02,  8.2520e-02,  6.5430e-02,\n",
      "         6.7871e-02,  5.7617e-02,  9.5703e-02,  6.2500e-02,  6.2012e-02,\n",
      "         7.8125e-02,  6.5430e-02,  6.8848e-02,  8.2520e-02,  7.4707e-02,\n",
      "         6.4453e-02,  7.7637e-02,  9.9609e-02,  7.2754e-02, -3.8330e-02,\n",
      "         6.5430e-02,  8.0078e-02,  7.5195e-02,  5.8594e-02,  7.8613e-02,\n",
      "         7.7148e-02,  7.8613e-02,  5.3711e-02,  5.8594e-02,  5.4932e-02,\n",
      "         6.7383e-02,  7.4219e-02,  6.6406e-02,  8.4961e-02,  5.8594e-02,\n",
      "         8.9355e-02,  6.7383e-02,  1.9646e-04,  7.6172e-02,  6.4453e-02,\n",
      "         6.4941e-02,  6.1768e-02,  4.8340e-02,  6.3477e-02,  8.3496e-02,\n",
      "         9.1797e-02,  6.0059e-02,  6.5430e-02,  6.5430e-02,  5.8838e-02,\n",
      "         6.5918e-02,  7.6660e-02,  7.9590e-02,  6.4453e-02,  6.9824e-02,\n",
      "         6.7871e-02,  6.6406e-02,  1.0791e-01,  7.5195e-02,  5.9814e-02,\n",
      "         6.7383e-02,  5.9326e-02,  5.0781e-02,  7.1777e-02,  7.3730e-02,\n",
      "         7.2754e-02,  6.4453e-02,  6.0791e-02,  6.9824e-02,  6.6895e-02,\n",
      "         7.5195e-02,  6.1279e-02,  6.2012e-02,  5.4443e-02,  6.5918e-02,\n",
      "         6.3477e-02,  8.5449e-02,  7.1777e-02,  6.4941e-02,  5.8105e-02,\n",
      "         5.9570e-02,  6.2988e-02,  7.2266e-02,  7.5195e-02,  6.1523e-02,\n",
      "         7.0312e-02,  6.4941e-02,  6.7383e-02,  6.5918e-02,  6.9336e-02,\n",
      "         7.5684e-02,  8.2520e-02,  6.4941e-02,  6.4453e-02,  7.3242e-02,\n",
      "         8.1055e-02,  7.8613e-02,  6.8848e-02,  2.9102e-01,  7.9590e-02,\n",
      "        -2.3460e-04,  6.8359e-02,  7.4219e-02, -1.4587e-02,  6.9336e-02,\n",
      "         7.7637e-02,  4.8584e-02,  7.4219e-02,  5.6641e-02,  4.9561e-02,\n",
      "         7.5684e-02,  7.2266e-02,  5.9570e-02,  6.3477e-02,  7.1777e-02,\n",
      "         6.9824e-02,  6.7383e-02,  7.6660e-02,  6.6406e-02,  6.5918e-02,\n",
      "         9.2773e-02,  7.6660e-02,  7.2754e-02,  7.2754e-02,  7.1289e-02,\n",
      "         6.4453e-02,  7.6172e-02,  6.2988e-02, -1.7834e-04,  6.1279e-02,\n",
      "        -6.2012e-02,  7.7148e-02,  6.8848e-02,  7.7148e-02,  7.5684e-02,\n",
      "         5.2002e-02,  6.6406e-02,  1.7480e-01,  5.2979e-02,  7.0312e-02,\n",
      "         7.2754e-02,  7.7637e-02,  7.3730e-02,  7.1289e-02,  6.7383e-02,\n",
      "         7.2266e-02,  5.2246e-02,  6.2256e-02,  6.4941e-02,  6.2988e-02,\n",
      "         6.9824e-02,  7.4219e-02,  1.0742e-01,  7.5195e-02,  6.2500e-02,\n",
      "         8.2031e-02,  6.5430e-02,  8.1543e-02,  6.0303e-02,  6.3965e-02,\n",
      "         4.9561e-02,  6.3965e-02,  8.0566e-02,  6.0791e-02, -8.4400e-05,\n",
      "         6.9824e-02,  7.1289e-02,  5.1758e-02,  8.5449e-02, -6.0059e-02,\n",
      "         6.2256e-02,  9.7656e-02,  6.7383e-02,  8.8867e-02,  7.9102e-02,\n",
      "         1.6992e-01,  6.2256e-02,  7.2266e-02,  6.8359e-02,  6.4941e-02,\n",
      "         7.2754e-02,  1.3672e-01,  6.8848e-02,  5.9326e-02,  7.4219e-02,\n",
      "         6.8848e-02,  7.2266e-02,  5.7129e-02,  6.9336e-02,  7.1777e-02,\n",
      "         6.5918e-02,  7.6660e-02,  5.4688e-02,  8.1543e-02,  7.8613e-02,\n",
      "         7.4219e-02,  5.8105e-02,  6.2500e-02,  5.3711e-02,  7.1289e-02,\n",
      "         6.2988e-02,  6.6895e-02,  7.9590e-02,  7.9590e-02,  7.3730e-02,\n",
      "         6.7383e-02,  5.7373e-02,  9.5215e-02,  6.8848e-02,  6.4453e-02,\n",
      "         6.2012e-02,  7.1777e-02,  6.5430e-02,  6.9824e-02,  7.3242e-02,\n",
      "         7.5684e-02,  6.8359e-02,  6.3965e-02,  6.0059e-02,  6.5918e-02,\n",
      "         8.5449e-02,  7.5684e-02,  5.5420e-02,  6.1279e-02,  6.2012e-02,\n",
      "         7.1777e-02,  6.6406e-02,  6.6895e-02,  6.9824e-02,  6.7871e-02,\n",
      "         6.1035e-02,  1.1475e-01,  7.7148e-02,  9.3262e-02,  7.4707e-02,\n",
      "         8.0078e-02,  6.2500e-02,  6.5918e-02,  7.4707e-02,  6.1523e-02,\n",
      "         7.5195e-02,  6.7383e-02,  2.5940e-04,  5.6885e-02,  6.7871e-02,\n",
      "         9.5215e-02,  8.3496e-02,  6.2256e-02,  5.3711e-02,  7.7148e-02,\n",
      "         8.4961e-02,  6.5430e-02,  7.0312e-02,  6.2500e-02,  5.7373e-02,\n",
      "         6.5430e-02,  7.5195e-02,  7.8125e-02,  8.7891e-02,  7.1106e-03,\n",
      "         7.0312e-02,  8.1543e-02,  6.5430e-02,  6.6406e-02,  6.0547e-02,\n",
      "         7.2754e-02,  6.3477e-02,  6.2500e-02,  6.8359e-02,  7.3730e-02,\n",
      "         6.6895e-02,  7.8125e-02,  9.7656e-02,  6.6406e-02,  5.9570e-02,\n",
      "         6.9824e-02,  8.4961e-02,  7.5195e-02,  3.6133e-02,  6.9336e-02,\n",
      "         5.5420e-02,  7.3730e-02,  6.1523e-02,  6.5430e-02,  6.0303e-02,\n",
      "         6.7871e-02,  7.4707e-02,  5.6396e-02,  6.1279e-02,  6.4453e-02,\n",
      "         6.0303e-02,  6.7871e-02,  6.2500e-02,  9.6191e-02,  6.3477e-02,\n",
      "         7.8125e-02,  6.3965e-02,  1.9141e-01,  8.8867e-02,  6.4453e-02,\n",
      "         7.0312e-02,  6.7383e-02,  7.0801e-02,  4.5654e-02,  6.3965e-02,\n",
      "         6.2988e-02,  5.8594e-02,  5.5420e-02,  7.1289e-02,  6.8848e-02,\n",
      "         7.3730e-02,  7.4707e-02,  7.1289e-02,  1.1621e-01,  6.9824e-02,\n",
      "         5.9326e-02,  6.3477e-02,  7.0801e-02,  7.0312e-02,  6.8359e-02,\n",
      "         5.6885e-02,  7.3730e-02,  7.2266e-02,  7.1777e-02,  5.9082e-02,\n",
      "        -5.6885e-02,  5.7129e-02,  7.0801e-02,  5.6885e-02,  8.3008e-02,\n",
      "         6.6406e-02,  7.3242e-02, -5.7129e-02,  6.0059e-02,  8.1055e-02,\n",
      "         5.1514e-02,  6.3477e-02,  5.9570e-02,  6.0547e-02,  6.2256e-02,\n",
      "         9.6680e-02, -5.9326e-02,  6.0059e-02,  1.3867e-01,  6.5918e-02,\n",
      "         5.5664e-02,  8.1055e-02,  6.6895e-02,  7.1289e-02,  6.3965e-02,\n",
      "         7.7148e-02,  6.9824e-02,  6.4941e-02,  6.2988e-02,  7.3730e-02,\n",
      "         8.2520e-02,  6.5430e-02,  8.6914e-02,  6.4453e-02,  6.8359e-02,\n",
      "         7.3242e-02,  8.6914e-02,  6.1768e-02,  6.5430e-02,  6.2988e-02,\n",
      "         6.1035e-02,  7.5195e-02,  5.6641e-02,  7.7637e-02,  6.8359e-02,\n",
      "         6.7383e-02,  5.8838e-02, -5.7129e-02,  6.9336e-02,  6.9336e-02,\n",
      "         5.8838e-02,  8.5449e-02,  8.2031e-02,  6.7383e-02,  6.2988e-02,\n",
      "         6.6406e-02,  8.8867e-02,  6.3477e-02,  5.2979e-02,  9.4238e-02,\n",
      "         7.8613e-02,  6.6895e-02,  5.1514e-02,  8.4473e-02,  6.9824e-02,\n",
      "         7.0312e-02,  6.6406e-02,  7.2754e-02,  7.1777e-02,  5.4199e-02,\n",
      "         1.7188e-01,  6.2500e-02,  9.5215e-02, -6.1768e-02,  5.3955e-02,\n",
      "         8.1055e-02,  7.1777e-02,  5.8838e-02,  7.5195e-02,  6.9824e-02,\n",
      "         8.1055e-02,  5.9326e-02,  7.3730e-02,  6.4453e-02,  5.6641e-02,\n",
      "         7.0312e-02,  6.2012e-02,  8.3984e-02,  8.0566e-02,  1.2354e-01,\n",
      "         7.7148e-02,  7.6660e-02,  6.3965e-02,  6.9824e-02,  6.3965e-02,\n",
      "         6.8848e-02, -5.9326e-02,  6.7383e-02,  6.1523e-02,  7.8125e-02,\n",
      "         6.0059e-02,  5.6641e-02,  7.5684e-02,  7.0801e-02, -5.0781e-02,\n",
      "        -5.2979e-02,  6.3965e-02, -5.8105e-02,  7.0801e-02,  7.3242e-02,\n",
      "         9.3750e-02,  6.1768e-02,  6.0791e-02,  6.9824e-02,  6.6895e-02,\n",
      "         6.0791e-02,  6.0303e-02, -5.1025e-02, -5.5420e-02,  7.1777e-02,\n",
      "         6.7871e-02,  5.9570e-02,  5.4443e-02], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.decoder.block.10.layer.2.DenseReluDense.wi.weight\n",
      "Parameter containing:\n",
      "tensor([[ 3.9648e-01,  8.7500e-01, -4.2114e-03,  ..., -5.7129e-02,\n",
      "          1.8555e-01,  8.6426e-02],\n",
      "        [-5.4688e-01,  1.1875e+00, -4.9023e-01,  ...,  6.4941e-02,\n",
      "         -6.2500e-01, -1.2793e-01],\n",
      "        [ 1.0469e+00,  2.5781e-01, -3.4912e-02,  ..., -5.3906e-01,\n",
      "          3.8086e-01,  4.6875e-01],\n",
      "        ...,\n",
      "        [-3.9258e-01,  8.6975e-04, -4.3164e-01,  ...,  2.0605e-01,\n",
      "          5.1953e-01, -2.8125e-01],\n",
      "        [ 1.5820e-01, -1.5918e-01,  4.6094e-01,  ..., -1.2891e-01,\n",
      "         -6.5625e-01,  4.1016e-01],\n",
      "        [-2.9053e-02, -4.4141e-01, -1.4453e+00,  ...,  7.6562e-01,\n",
      "         -4.9414e-01, -6.6016e-01]], device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.10.layer.2.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.1387, -0.0718, -0.2129,  ...,  0.3652, -0.7383, -0.3574],\n",
      "        [-0.3438,  0.0072,  0.1787,  ..., -0.1914, -0.0378, -0.2559],\n",
      "        [-0.2012, -0.1533,  0.1514,  ...,  0.2539,  0.5117,  0.1768],\n",
      "        ...,\n",
      "        [-0.2090, -0.0610, -0.1475,  ..., -0.2715, -0.1167,  0.1172],\n",
      "        [-0.1924,  0.1196, -0.3281,  ...,  0.0713, -0.2471,  0.1826],\n",
      "        [-0.1245,  0.4531,  0.2617,  ..., -0.2383, -0.0752, -0.0771]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.10.layer.2.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 4.5625e+00,  5.4688e+00,  5.9062e+00,  2.5312e+00,  4.6562e+00,\n",
      "         4.6250e+00,  5.0938e+00,  4.8125e+00,  5.2500e+00,  4.7812e+00,\n",
      "         4.6875e+00,  4.9375e+00,  6.2812e+00,  5.1562e+00,  4.7812e+00,\n",
      "         4.3750e+00,  4.6250e+00,  5.0938e+00,  5.0625e+00,  4.5938e+00,\n",
      "         4.9062e+00,  4.5625e+00,  5.3438e+00,  4.2812e+00,  4.6250e+00,\n",
      "         5.2500e+00,  5.2188e+00,  5.9375e+00,  5.1562e+00,  4.7500e+00,\n",
      "         4.6250e+00,  6.1875e+00,  4.6562e+00,  4.7188e+00,  4.9062e+00,\n",
      "         5.5625e+00,  4.9375e+00,  7.7812e+00,  4.6562e+00,  4.8125e+00,\n",
      "         4.6562e+00,  5.1875e+00,  4.8125e+00,  4.9688e+00,  6.7500e+00,\n",
      "         4.6562e+00,  5.2500e+00,  4.9062e+00,  4.5938e+00,  4.8750e+00,\n",
      "         5.5312e+00,  4.5000e+00,  5.0312e+00,  4.5938e+00,  8.5000e+00,\n",
      "         4.8750e+00,  5.4688e+00,  4.6562e+00,  4.9062e+00,  4.8750e+00,\n",
      "         5.2188e+00,  4.8750e+00,  4.7188e+00,  5.1875e+00,  4.7500e+00,\n",
      "         1.3500e+01,  4.6875e+00,  5.2812e+00,  8.8125e+00,  4.6250e+00,\n",
      "         5.3750e+00,  4.8750e+00,  5.0938e+00,  4.6250e+00,  5.5625e+00,\n",
      "         4.5000e+00,  5.3750e+00,  4.3750e+00,  4.9062e+00,  5.8750e+00,\n",
      "         4.7188e+00,  4.8438e+00,  4.5625e+00,  5.8750e+00,  5.0312e+00,\n",
      "         5.1562e+00,  9.0820e-02,  4.5938e+00,  4.5938e+00,  4.5938e+00,\n",
      "         4.7500e+00,  4.6562e+00,  4.5000e+00,  4.5625e+00,  4.8438e+00,\n",
      "         4.6875e+00,  4.8750e+00,  4.7500e+00,  4.6875e+00,  4.5312e+00,\n",
      "         4.5000e+00,  4.6250e+00,  7.4062e+00,  4.9375e+00,  4.9062e+00,\n",
      "         5.0000e+00,  4.3438e+00,  5.0312e+00,  5.3125e+00,  5.0938e+00,\n",
      "         4.8750e+00,  4.7188e+00,  4.5938e+00,  4.5938e+00,  5.1875e+00,\n",
      "         5.5938e+00,  4.8125e+00,  4.6562e+00,  8.9111e-03,  4.9375e+00,\n",
      "         4.5938e+00,  4.9062e+00,  6.2500e+00,  4.5938e+00,  4.8750e+00,\n",
      "         4.7812e+00,  4.6875e+00,  4.5312e+00,  4.8438e+00,  5.5000e+00,\n",
      "         6.6250e+00,  5.0625e+00,  4.5625e+00,  4.5312e+00,  4.8125e+00,\n",
      "         4.9375e+00,  4.6875e+00,  6.0312e+00,  4.4375e+00,  4.9688e+00,\n",
      "         6.0938e+00,  4.9688e+00,  4.4375e+00,  5.3125e+00,  4.7500e+00,\n",
      "         4.8750e+00,  5.0625e+00,  5.7188e+00,  4.9688e+00,  4.4375e+00,\n",
      "         4.9688e+00,  5.1250e+00,  6.1250e+00,  5.4062e+00,  4.7188e+00,\n",
      "         4.8125e+00,  4.6875e+00,  5.0625e+00,  5.0312e+00,  4.7812e+00,\n",
      "         4.6875e+00,  4.8125e+00,  4.7812e+00,  5.7500e+00,  5.0625e+00,\n",
      "         6.0938e+00,  4.8750e+00,  5.4688e+00,  7.3750e+00,  4.8125e+00,\n",
      "         7.4688e+00,  4.8125e+00,  5.3125e+00,  5.0000e+00,  4.9688e+00,\n",
      "         4.6562e+00,  4.7812e+00,  5.0000e+00,  6.9375e+00,  5.4062e+00,\n",
      "         4.9062e+00,  4.6562e+00,  4.6562e+00,  4.8125e+00,  4.9688e+00,\n",
      "         5.6875e+00,  4.6875e+00,  3.3438e+00,  4.9688e+00,  4.7500e+00,\n",
      "         4.5625e+00,  4.5625e+00,  5.1562e+00,  4.5312e+00,  4.8438e+00,\n",
      "         5.1562e+00,  4.6250e+00,  4.7812e+00,  4.9062e+00,  4.8125e+00,\n",
      "         4.5312e+00,  4.4375e+00,  4.9688e+00,  5.2500e+00,  4.9688e+00,\n",
      "         4.5938e+00,  4.3438e+00,  4.7188e+00,  4.8750e+00,  4.6875e+00,\n",
      "         6.0938e+00,  4.6250e+00,  4.5312e+00,  5.2812e+00,  4.6875e+00,\n",
      "         4.9688e+00,  4.6562e+00,  5.0000e+00,  4.6875e+00, -2.9102e-01,\n",
      "         6.1875e+00,  5.0312e+00,  5.5312e+00,  4.5938e+00,  5.4688e+00,\n",
      "         4.6562e+00,  4.6562e+00,  4.9688e+00,  4.5625e+00,  5.3125e+00,\n",
      "         4.8750e+00,  4.2812e+00,  4.9062e+00,  4.6875e+00,  4.9688e+00,\n",
      "         4.8750e+00,  5.2500e+00,  4.5625e+00,  5.5312e+00,  4.5312e+00,\n",
      "         9.4375e+00,  4.7500e+00,  4.9062e+00,  5.0625e+00,  4.2188e+00,\n",
      "         4.4062e+00,  5.1250e+00,  4.3750e+00,  4.6562e+00,  4.7500e+00,\n",
      "         5.3438e+00,  4.6562e+00,  4.7500e+00,  5.0625e+00,  4.9062e+00,\n",
      "         5.3125e+00,  4.9062e+00,  5.8125e+00,  4.6875e+00,  4.9062e+00,\n",
      "         5.5312e+00,  5.7500e+00,  4.7500e+00,  4.6562e+00,  5.0625e+00,\n",
      "         4.7188e+00,  5.0625e+00,  4.8750e+00,  4.5312e+00,  4.9062e+00,\n",
      "         4.6875e+00,  1.1125e+01,  4.8125e+00,  5.0000e+00,  6.4062e+00,\n",
      "         4.6875e+00,  5.1250e+00,  4.7188e+00,  4.9688e+00,  4.7500e+00,\n",
      "         1.3594e+00,  7.8750e+00,  5.0312e+00,  5.0000e+00,  4.4062e+00,\n",
      "         4.8750e+00,  4.6250e+00,  5.1250e+00,  5.4062e+00,  5.3438e+00,\n",
      "         4.8750e+00,  4.7812e+00,  4.6875e+00,  4.9688e+00,  4.9688e+00,\n",
      "         4.8125e+00,  6.6250e+00,  4.8125e+00,  4.9375e+00,  5.1562e+00,\n",
      "         5.1562e+00,  4.9375e+00,  2.6406e+00,  4.6250e+00,  4.7812e+00,\n",
      "         4.7812e+00,  4.3750e+00,  4.5000e+00,  5.0625e+00,  4.7812e+00,\n",
      "         5.2500e+00,  5.2812e+00,  4.4062e+00,  4.6875e+00,  5.8125e+00,\n",
      "         4.5938e+00,  5.1562e+00,  5.6562e+00,  4.7812e+00,  4.8125e+00,\n",
      "         4.3750e+00,  5.3750e+00,  4.5000e+00,  5.0312e+00,  4.9688e+00,\n",
      "         5.1562e+00,  4.5938e+00,  6.3750e+00,  4.6875e+00,  4.5938e+00,\n",
      "         4.7188e+00,  6.3125e+00,  4.6250e+00,  4.7500e+00,  4.9688e+00,\n",
      "         4.5938e+00,  5.1250e+00,  4.9375e+00,  4.9375e+00,  4.5000e+00,\n",
      "         5.0312e+00,  5.3125e+00,  4.8750e+00,  4.4062e+00,  5.0938e+00,\n",
      "         5.1250e+00,  4.7812e+00,  4.7500e+00,  5.9062e+00,  5.1875e+00,\n",
      "         5.1875e+00,  4.8750e+00,  4.9062e+00,  4.9375e+00,  4.5312e+00,\n",
      "         5.0312e+00,  4.8438e+00, -1.4832e-02,  4.9688e+00,  4.5938e+00,\n",
      "         4.8750e+00,  5.0625e+00,  9.2500e+00,  4.5625e+00,  5.0625e+00,\n",
      "         4.8125e+00,  5.0625e+00,  4.7812e+00,  4.7812e+00,  4.6875e+00,\n",
      "         4.9062e+00,  4.8438e+00,  5.2812e+00,  4.7188e+00,  4.5625e+00,\n",
      "         4.9062e+00,  4.8125e+00,  4.8125e+00,  5.3125e+00,  5.9688e+00,\n",
      "         4.7500e+00,  5.0000e+00,  6.0000e+00,  4.7500e+00,  5.0625e+00,\n",
      "         4.5938e+00,  4.4062e+00,  4.7188e+00,  5.0000e+00,  4.8125e+00,\n",
      "         5.3125e+00,  4.7188e+00,  4.9375e+00,  5.0938e+00,  4.9688e+00,\n",
      "         4.5625e+00,  5.4375e+00,  5.0938e+00,  4.9062e+00,  4.5625e+00,\n",
      "         4.7188e+00,  4.9375e+00,  4.6250e+00,  4.8438e+00,  4.5938e+00,\n",
      "         4.7812e+00,  5.5938e+00,  4.7188e+00,  4.9688e+00,  4.5938e+00,\n",
      "         5.1250e+00,  5.1250e+00,  4.6875e+00,  4.5000e+00,  4.6875e+00,\n",
      "         4.9688e+00,  5.3125e+00,  4.4688e+00,  7.1562e+00,  5.4375e+00,\n",
      "        -3.6133e-02,  4.5625e+00,  4.5312e+00,  4.4688e+00,  4.6562e+00,\n",
      "         5.0000e+00,  4.4062e+00,  4.6250e+00,  5.2188e+00,  5.2500e+00,\n",
      "         4.9375e+00,  5.1562e+00,  4.3438e+00,  4.8750e+00,  5.1562e+00,\n",
      "         5.0625e+00,  4.4375e+00,  5.1562e+00,  5.2188e+00,  5.9375e+00,\n",
      "         5.6250e+00,  6.5938e+00,  5.1250e+00,  4.6250e+00,  4.7188e+00,\n",
      "         4.8750e+00,  4.6250e+00,  4.6250e+00, -1.9165e-02,  4.7812e+00,\n",
      "         4.5625e+00,  4.9062e+00,  4.7500e+00,  4.9688e+00,  5.1875e+00,\n",
      "         4.6562e+00,  6.6875e+00,  6.5625e+00,  4.7812e+00,  4.8750e+00,\n",
      "         4.9062e+00,  4.6875e+00,  4.8750e+00,  5.0312e+00,  4.9062e+00,\n",
      "         4.9375e+00,  5.3125e+00,  4.4375e+00,  4.6250e+00,  4.6875e+00,\n",
      "         4.5312e+00,  4.8438e+00,  1.0562e+01,  4.6250e+00,  4.6875e+00,\n",
      "         4.9688e+00,  4.5000e+00,  5.1562e+00,  5.0938e+00,  4.6875e+00,\n",
      "         4.5000e+00,  5.0625e+00,  4.9062e+00,  4.8125e+00,  2.9688e+00,\n",
      "         4.5938e+00,  5.0625e+00,  1.1406e+00,  5.3750e+00,  4.2500e+00,\n",
      "         4.5938e+00,  5.0000e+00,  4.6562e+00,  4.7188e+00,  5.0625e+00,\n",
      "         1.6250e+01,  5.2188e+00,  4.7188e+00,  4.4688e+00,  4.5000e+00,\n",
      "         4.8438e+00,  5.8125e+00,  4.9375e+00,  4.7500e+00,  4.8125e+00,\n",
      "         5.0625e+00,  5.1875e+00,  4.7812e+00,  4.6250e+00,  4.8750e+00,\n",
      "         4.7500e+00,  4.6250e+00,  5.0000e+00,  5.0000e+00,  5.2500e+00,\n",
      "         4.7188e+00,  4.8125e+00,  4.6250e+00,  5.5938e+00,  4.6250e+00,\n",
      "         4.4375e+00,  4.9062e+00,  4.6250e+00,  5.4062e+00,  4.9375e+00,\n",
      "         5.0938e+00,  4.5000e+00,  7.1875e+00,  5.1875e+00,  4.6875e+00,\n",
      "         5.2188e+00,  4.8125e+00,  4.5000e+00,  4.7812e+00,  5.0625e+00,\n",
      "         4.9688e+00,  4.5938e+00,  4.6562e+00,  4.8750e+00,  4.5000e+00,\n",
      "         4.8125e+00,  4.7188e+00,  4.5938e+00,  4.7500e+00,  4.6875e+00,\n",
      "         4.6562e+00,  5.2500e+00,  4.7812e+00,  4.9375e+00,  5.2812e+00,\n",
      "         5.5625e+00,  6.3125e+00,  4.5312e+00,  6.6250e+00,  4.5938e+00,\n",
      "         4.9688e+00,  5.5312e+00,  4.6875e+00,  5.1875e+00,  4.7188e+00,\n",
      "         5.1875e+00,  5.2812e+00,  1.8281e+00,  6.0625e+00,  4.5938e+00,\n",
      "         6.8125e+00,  5.0938e+00,  4.6250e+00,  4.8750e+00,  4.5625e+00,\n",
      "         4.7812e+00,  5.2188e+00,  5.0625e+00,  4.5625e+00,  5.2188e+00,\n",
      "         5.3125e+00,  4.6875e+00,  5.1250e+00,  5.5312e+00,  1.0859e+00,\n",
      "         5.4375e+00,  5.0312e+00,  4.7188e+00,  5.1250e+00,  4.5000e+00,\n",
      "         4.6875e+00,  5.0000e+00,  4.6562e+00,  4.6562e+00,  4.7812e+00,\n",
      "         4.6875e+00,  5.1562e+00,  5.7812e+00,  5.0312e+00,  4.2812e+00,\n",
      "         4.8750e+00,  7.2188e+00,  5.0000e+00,  3.4688e+00,  5.3438e+00,\n",
      "         5.0625e+00,  4.9688e+00,  4.4688e+00,  4.9688e+00,  4.6875e+00,\n",
      "         4.6562e+00,  4.7812e+00,  7.6250e+00,  4.4688e+00,  4.4375e+00,\n",
      "         4.7188e+00,  4.6250e+00,  4.3750e+00,  5.0938e+00,  4.6875e+00,\n",
      "         4.7500e+00,  4.5625e+00,  1.0125e+01,  5.1562e+00,  4.5000e+00,\n",
      "         4.6250e+00,  5.0625e+00,  4.9688e+00,  5.8438e+00,  4.5312e+00,\n",
      "         4.8125e+00,  4.4375e+00,  4.5312e+00,  4.9688e+00,  5.0625e+00,\n",
      "         5.1562e+00,  4.6875e+00,  5.7812e+00,  9.1250e+00,  5.0938e+00,\n",
      "         4.9375e+00,  5.0938e+00,  4.9375e+00,  5.0625e+00,  4.8750e+00,\n",
      "         5.1875e+00,  5.2500e+00,  4.5938e+00,  5.5938e+00,  6.5938e+00,\n",
      "         4.7188e+00,  8.0625e+00,  4.7188e+00,  4.2812e+00,  5.2812e+00,\n",
      "         4.9375e+00,  4.8750e+00,  4.5000e+00,  4.5312e+00,  5.2188e+00,\n",
      "         7.4375e+00,  4.8438e+00,  4.5625e+00,  4.5000e+00,  4.6562e+00,\n",
      "         5.0000e+00,  5.0000e+00,  5.0000e+00,  6.0625e+00,  4.8438e+00,\n",
      "         5.0312e+00,  4.8750e+00,  4.9062e+00,  4.6562e+00,  4.7188e+00,\n",
      "         4.9375e+00,  4.9062e+00,  4.8438e+00,  4.5938e+00,  4.7500e+00,\n",
      "         4.9062e+00,  5.0625e+00,  5.1875e+00,  4.9062e+00,  4.7188e+00,\n",
      "         4.8125e+00,  5.0312e+00,  6.6875e+00,  4.8438e+00,  4.8438e+00,\n",
      "         4.6875e+00,  4.6250e+00,  4.8438e+00,  4.8125e+00,  5.0000e+00,\n",
      "         4.5938e+00,  4.5625e+00,  5.1250e+00,  4.7500e+00,  4.7500e+00,\n",
      "         4.7500e+00,  5.0938e+00,  5.2812e+00,  5.0312e+00,  4.7812e+00,\n",
      "         4.6250e+00,  4.7188e+00,  5.0312e+00,  5.6875e+00,  5.1562e+00,\n",
      "         4.7500e+00,  4.9062e+00,  5.3125e+00,  6.5938e+00,  4.7500e+00,\n",
      "         5.1875e+00,  5.0312e+00,  4.9688e+00,  5.1250e+00,  6.7500e+00,\n",
      "         5.5000e+00,  5.0000e+00,  5.4062e+00,  4.5000e+00,  4.7188e+00,\n",
      "         4.7188e+00,  5.0312e+00,  6.7812e+00,  4.8125e+00,  4.6875e+00,\n",
      "         6.7188e+00,  5.0312e+00,  5.0000e+00,  4.7500e+00,  4.5625e+00,\n",
      "         4.5938e+00,  4.5000e+00,  5.4688e+00,  6.5312e+00,  5.2812e+00,\n",
      "         5.0625e+00,  4.8125e+00,  4.6250e+00,  4.5000e+00,  5.3125e+00,\n",
      "         5.1562e+00,  6.0938e+00,  4.9375e+00,  6.2188e+00,  4.7500e+00,\n",
      "         4.9375e+00,  5.5938e+00,  4.9375e+00,  5.0312e+00,  5.2500e+00,\n",
      "         4.3750e+00,  4.6875e+00,  4.9062e+00,  4.8750e+00,  5.0312e+00,\n",
      "         5.2188e+00,  4.7500e+00,  4.5938e+00,  5.5625e+00,  4.5938e+00,\n",
      "         5.5312e+00,  4.9688e+00,  4.4062e+00,  4.3125e+00,  4.7500e+00,\n",
      "         4.7188e+00,  4.3125e+00,  4.5625e+00], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.decoder.block.11.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0422,  0.0155,  0.0095,  ..., -0.0305, -0.0032, -0.0254],\n",
      "        [ 0.0034,  0.0444, -0.0369,  ..., -0.0332,  0.0381, -0.0095],\n",
      "        [ 0.0209, -0.0101,  0.0300,  ...,  0.0032, -0.0048, -0.0104],\n",
      "        ...,\n",
      "        [-0.0010, -0.0167, -0.0178,  ...,  0.0245, -0.0165, -0.0239],\n",
      "        [-0.0115,  0.0520, -0.0233,  ..., -0.0513, -0.0510,  0.0162],\n",
      "        [-0.0581, -0.0192,  0.0105,  ...,  0.0508, -0.0074, -0.0248]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.11.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.2578,  0.2031,  0.0038,  ..., -0.1318,  0.3633, -0.2480],\n",
      "        [ 0.0093, -0.0164, -0.1040,  ..., -0.0447, -0.3535,  0.2236],\n",
      "        [-0.1592,  0.1641,  0.0058,  ...,  0.4551,  0.1592, -0.2031],\n",
      "        ...,\n",
      "        [-0.5156,  0.1279, -0.0649,  ..., -0.4023,  0.0620,  0.0177],\n",
      "        [-0.0815, -0.2754, -0.2520,  ...,  0.6172, -0.3164, -0.3340],\n",
      "        [-0.1084, -0.1016,  0.1816,  ..., -0.3457, -0.5273,  0.2793]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.11.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[-1.1328,  0.4805, -1.1875,  ...,  0.3770, -0.0292,  0.3359],\n",
      "        [ 2.0000,  0.7148,  0.8281,  ...,  0.1807,  0.8750,  0.7617],\n",
      "        [-1.2734,  0.1748, -0.3809,  ..., -0.6172, -0.5234,  0.7109],\n",
      "        ...,\n",
      "        [-0.0703,  1.0234,  0.3066,  ..., -1.1641,  0.1040, -0.6680],\n",
      "        [ 0.1406, -2.4688, -0.5781,  ..., -1.1953, -0.3223, -0.0405],\n",
      "        [ 0.1494, -0.1367,  0.3535,  ...,  0.0579, -0.4297,  0.1216]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.11.layer.0.SelfAttention.o.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.7891, -0.3184, -1.1250,  ..., -1.0156,  0.3418,  0.9883],\n",
      "        [ 0.0255, -0.3301, -0.0742,  ..., -0.3633,  0.1689,  0.4473],\n",
      "        [-0.1230,  1.1875,  0.2090,  ...,  0.7539, -0.6367, -0.1426],\n",
      "        ...,\n",
      "        [-0.1357, -0.0898,  0.6523,  ...,  1.3438, -0.5586,  0.4648],\n",
      "        [-0.5977,  0.3359, -0.2109,  ..., -0.8359,  0.1250,  0.2969],\n",
      "        [-0.7852, -0.1226,  0.2969,  ...,  0.9258,  0.0732,  1.1719]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.11.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 3.1641e-01,  3.5352e-01,  6.1328e-01,  2.8711e-01,  3.2812e-01,\n",
      "         3.5742e-01,  4.0039e-01,  3.4961e-01,  4.5117e-01,  3.8086e-01,\n",
      "         3.3594e-01,  3.4961e-01,  3.6523e-01,  4.0430e-01,  3.3008e-01,\n",
      "         3.2617e-01,  3.8086e-01,  4.1211e-01,  4.5312e-01,  3.8867e-01,\n",
      "         3.1445e-01,  3.2227e-01,  4.8438e-01,  3.5742e-01,  3.1055e-01,\n",
      "         4.9219e-01,  3.1641e-01,  5.2344e-01,  3.6328e-01,  3.2617e-01,\n",
      "         3.3398e-01,  4.3750e-01,  3.7891e-01,  3.7109e-01,  3.4570e-01,\n",
      "         3.6719e-01,  3.4180e-01,  1.0625e+00,  3.2227e-01,  3.6328e-01,\n",
      "         4.1797e-01,  4.2773e-01,  3.9062e-01,  3.3789e-01,  6.4844e-01,\n",
      "         3.6719e-01,  4.6680e-01,  3.5352e-01,  3.7109e-01,  3.2617e-01,\n",
      "         5.0391e-01,  3.3594e-01,  4.1406e-01,  3.2031e-01,  8.8281e-01,\n",
      "         4.0039e-01,  4.8828e-01,  3.7695e-01,  3.4570e-01,  3.5156e-01,\n",
      "         3.4180e-01,  3.2812e-01,  3.5156e-01,  3.2422e-01,  3.1250e-01,\n",
      "         1.5859e+00,  3.2031e-01,  3.6719e-01,  5.0000e-01,  3.2227e-01,\n",
      "         3.7500e-01,  3.3984e-01,  3.1836e-01,  3.6133e-01,  4.1406e-01,\n",
      "         3.2617e-01,  4.4922e-01,  3.2617e-01,  3.2031e-01,  4.1016e-01,\n",
      "         3.4180e-01,  3.3008e-01,  3.3984e-01,  5.5859e-01,  4.6484e-01,\n",
      "         3.4961e-01, -1.0107e-01,  3.3789e-01,  2.9492e-01,  3.2422e-01,\n",
      "         3.4766e-01,  3.4375e-01,  3.5742e-01,  3.5352e-01,  3.3594e-01,\n",
      "         3.2422e-01,  3.2812e-01,  3.1445e-01,  3.3398e-01,  3.8477e-01,\n",
      "         3.9062e-01,  3.5938e-01,  7.4219e-01,  3.7109e-01,  3.3984e-01,\n",
      "         5.4297e-01,  3.6914e-01,  3.4766e-01,  3.3789e-01,  3.3398e-01,\n",
      "         3.6523e-01,  3.8867e-01,  3.1055e-01,  3.2422e-01,  3.5547e-01,\n",
      "         4.3945e-01,  4.1992e-01,  3.5156e-01,  8.3496e-02,  3.3984e-01,\n",
      "         3.0859e-01,  3.3594e-01,  6.1719e-01,  3.4180e-01,  3.8477e-01,\n",
      "         3.3984e-01,  3.4375e-01,  3.3008e-01,  3.3203e-01,  4.9219e-01,\n",
      "         5.2344e-01,  3.3984e-01,  3.6914e-01,  3.3008e-01,  4.2188e-01,\n",
      "         5.1953e-01,  3.5742e-01,  3.8867e-01,  3.2422e-01,  4.4336e-01,\n",
      "         4.6680e-01,  3.2031e-01,  4.0430e-01,  4.9609e-01,  4.0039e-01,\n",
      "         3.9258e-01,  3.5156e-01,  3.9648e-01,  3.8867e-01,  3.4180e-01,\n",
      "         4.1406e-01,  4.0625e-01,  4.1016e-01,  5.4297e-01,  3.5547e-01,\n",
      "         3.4180e-01,  3.6328e-01,  3.1250e-01, -2.0020e-01,  3.5742e-01,\n",
      "         2.9492e-01,  3.6328e-01,  3.6719e-01,  5.4688e-01,  3.5938e-01,\n",
      "         6.4844e-01,  3.3008e-01,  6.0547e-01,  8.5547e-01,  3.2031e-01,\n",
      "         4.9023e-01,  3.7109e-01,  3.3594e-01,  3.6719e-01,  3.4766e-01,\n",
      "         3.4961e-01,  3.1055e-01,  4.2773e-01,  4.0039e-01,  3.5156e-01,\n",
      "         3.4570e-01,  4.0234e-01,  3.7891e-01,  3.3984e-01,  4.0039e-01,\n",
      "         5.2734e-01,  3.9062e-01,  1.2695e-01,  4.0234e-01,  3.9844e-01,\n",
      "         3.1641e-01,  3.5938e-01,  3.4961e-01,  3.3789e-01,  3.4375e-01,\n",
      "         4.1406e-01,  3.2617e-01,  3.6719e-01,  4.0039e-01,  3.2422e-01,\n",
      "         3.3594e-01,  3.9062e-01,  3.9453e-01,  3.6523e-01,  3.4180e-01,\n",
      "         4.1211e-01,  3.6133e-01,  3.5547e-01,  3.9258e-01,  3.9453e-01,\n",
      "         7.2266e-01,  3.5742e-01,  3.3008e-01,  3.9648e-01,  3.2617e-01,\n",
      "         3.6133e-01,  3.5742e-01,  3.6719e-01,  4.2188e-01, -3.5156e-02,\n",
      "         5.7812e-01,  4.1016e-01,  3.5938e-01,  3.3203e-01,  4.0820e-01,\n",
      "         3.6328e-01,  1.8906e+00,  3.5352e-01,  3.1836e-01,  4.1406e-01,\n",
      "         3.4961e-01,  3.3789e-01,  3.7109e-01,  2.9688e-01,  3.0664e-01,\n",
      "         4.0234e-01, -4.5508e-01,  3.4961e-01,  4.6289e-01,  3.5742e-01,\n",
      "         8.1250e-01,  3.7500e-01,  3.3203e-01,  3.5742e-01,  3.0078e-01,\n",
      "         3.4180e-01,  3.5742e-01,  3.3398e-01,  3.2617e-01,  4.6484e-01,\n",
      "         3.7695e-01,  3.3398e-01,  3.2031e-01,  3.5156e-01,  3.6328e-01,\n",
      "         4.6484e-01,  3.1250e-01,  3.8281e-01,  3.8281e-01,  3.8477e-01,\n",
      "         3.3398e-01,  4.1797e-01,  3.1836e-01,  3.6133e-01,  3.9062e-01,\n",
      "         3.4375e-01,  3.7305e-01,  3.5156e-01,  3.2617e-01,  3.5352e-01,\n",
      "         4.0430e-01,  1.2188e+00,  3.7305e-01,  3.7109e-01, -3.7109e-01,\n",
      "         3.5742e-01,  3.3984e-01,  3.3789e-01,  3.3984e-01,  3.7109e-01,\n",
      "        -8.9355e-02,  6.4453e-01,  3.2422e-01,  3.5352e-01,  3.1641e-01,\n",
      "         3.5547e-01,  3.0859e-01,  3.3984e-01,  4.5508e-01,  5.6641e-01,\n",
      "         3.3008e-01,  3.3008e-01,  2.9688e-01,  3.8672e-01,  3.3398e-01,\n",
      "         3.1250e-01,  4.8633e-01,  3.3594e-01,  3.9648e-01,  3.0859e-01,\n",
      "         4.2188e-01,  3.2227e-01,  1.6016e-01,  3.6523e-01,  3.7305e-01,\n",
      "         3.3008e-01,  3.5547e-01,  3.3203e-01,  4.4922e-01,  5.0000e-01,\n",
      "         4.1602e-01,  4.0625e-01,  3.0859e-01,  4.3750e-01,  5.9375e-01,\n",
      "         3.5156e-01,  4.0820e-01,  5.5078e-01,  3.5742e-01,  3.3594e-01,\n",
      "         3.0273e-01,  3.5352e-01,  3.9062e-01,  3.6523e-01,  4.4922e-01,\n",
      "         3.9648e-01,  4.0430e-01,  5.7812e-01,  4.3945e-01,  3.0078e-01,\n",
      "         3.2227e-01,  4.5703e-01,  3.7695e-01,  3.5352e-01,  3.4766e-01,\n",
      "         2.9688e-01,  3.4375e-01,  3.7891e-01,  3.6133e-01, -2.6172e-01,\n",
      "         3.5352e-01,  3.7891e-01,  3.4961e-01,  3.1836e-01,  3.5352e-01,\n",
      "         4.0430e-01,  3.2617e-01,  3.8477e-01,  3.3008e-01,  3.6914e-01,\n",
      "         4.3555e-01,  3.6133e-01,  3.2617e-01,  3.2617e-01,  3.4375e-01,\n",
      "         4.4727e-01,  3.6719e-01,  1.3379e-01,  4.0039e-01,  3.9062e-01,\n",
      "         3.1250e-01,  4.2383e-01, -8.7109e-01,  3.4961e-01,  3.4570e-01,\n",
      "         4.3359e-01,  3.9648e-01,  3.7305e-01,  3.1641e-01,  3.3984e-01,\n",
      "         3.2812e-01,  3.2031e-01,  4.0625e-01,  3.9453e-01,  4.0625e-01,\n",
      "         3.2227e-01,  3.6523e-01,  3.9648e-01,  3.5742e-01,  5.1562e-01,\n",
      "         3.8086e-01,  3.1836e-01,  4.6875e-01,  4.0820e-01,  3.1836e-01,\n",
      "         3.4375e-01,  3.5938e-01,  4.0430e-01,  3.5547e-01,  3.6719e-01,\n",
      "         3.7500e-01,  3.8281e-01,  3.6719e-01,  4.9023e-01,  4.1016e-01,\n",
      "         3.1836e-01,  4.1016e-01,  3.7109e-01,  3.5547e-01,  3.2227e-01,\n",
      "         3.7500e-01,  3.6523e-01,  3.0664e-01,  3.3789e-01,  3.4180e-01,\n",
      "         3.5352e-01,  4.6484e-01,  3.4375e-01,  3.4375e-01,  3.0469e-01,\n",
      "         3.5742e-01,  3.7500e-01,  3.8477e-01,  3.9453e-01,  3.3984e-01,\n",
      "         3.2812e-01,  4.1016e-01,  3.4766e-01,  9.0625e-01,  3.6133e-01,\n",
      "         3.4424e-02,  3.7695e-01,  3.3008e-01, -2.5586e-01,  3.6914e-01,\n",
      "         3.6914e-01,  3.4766e-01,  3.5352e-01,  3.3789e-01,  4.2578e-01,\n",
      "         3.7891e-01,  3.4180e-01,  3.2617e-01,  3.4570e-01,  4.3164e-01,\n",
      "         3.5938e-01,  3.2031e-01,  3.3594e-01,  3.3398e-01,  5.5469e-01,\n",
      "         3.8477e-01,  4.4531e-01,  4.2578e-01,  3.5156e-01,  3.1445e-01,\n",
      "         3.6719e-01,  3.2617e-01,  3.4375e-01, -1.1914e-01,  4.4531e-01,\n",
      "         4.8438e-01,  3.2422e-01,  3.1836e-01,  3.6719e-01,  3.5742e-01,\n",
      "         3.4375e-01,  5.9766e-01,  1.1406e+00,  4.0625e-01,  3.8477e-01,\n",
      "         3.2812e-01,  3.3594e-01,  3.4766e-01,  3.4375e-01,  3.3984e-01,\n",
      "         3.2617e-01,  4.4531e-01,  3.0273e-01,  3.6523e-01,  2.9688e-01,\n",
      "         3.1445e-01,  3.1836e-01,  6.3672e-01,  3.6719e-01,  3.1836e-01,\n",
      "         3.4766e-01,  3.2031e-01,  3.5352e-01,  3.7109e-01,  3.1836e-01,\n",
      "         4.1797e-01,  3.6914e-01,  3.4375e-01,  3.1250e-01, -1.9141e-01,\n",
      "         3.4375e-01,  3.4570e-01,  7.5195e-02,  3.3789e-01,  3.0078e-01,\n",
      "         3.4961e-01,  4.0039e-01,  3.3984e-01,  3.2227e-01,  4.0039e-01,\n",
      "         6.0547e-01,  3.5156e-01,  3.4961e-01,  3.0664e-01,  3.1055e-01,\n",
      "         3.2617e-01,  6.4453e-01,  3.2227e-01,  3.4180e-01,  3.4375e-01,\n",
      "         4.3555e-01,  3.9844e-01,  3.7109e-01,  3.5938e-01,  3.8672e-01,\n",
      "         3.4570e-01,  3.1836e-01,  3.3789e-01,  3.5352e-01,  4.3164e-01,\n",
      "         3.3789e-01,  4.2969e-01,  3.6328e-01,  4.4727e-01,  3.1055e-01,\n",
      "         2.9492e-01,  3.4180e-01,  3.5352e-01,  4.7266e-01,  3.4961e-01,\n",
      "         3.6914e-01,  3.8281e-01,  7.4219e-01,  3.5742e-01,  4.0820e-01,\n",
      "         4.2383e-01,  3.2617e-01,  3.3789e-01,  3.1836e-01,  3.8672e-01,\n",
      "         3.3398e-01,  3.1641e-01,  3.4180e-01,  4.3164e-01,  4.0820e-01,\n",
      "         4.2383e-01,  3.8281e-01,  3.9844e-01,  3.8281e-01,  3.2031e-01,\n",
      "         3.3984e-01,  3.9258e-01,  3.1836e-01,  3.0273e-01,  3.8477e-01,\n",
      "         5.7031e-01,  6.2500e-01,  3.1445e-01,  6.3281e-01,  3.7109e-01,\n",
      "         3.4180e-01,  5.2344e-01,  3.1445e-01,  4.8438e-01,  4.0430e-01,\n",
      "         3.7695e-01,  4.4531e-01,  1.1683e-04,  5.4688e-01,  3.1055e-01,\n",
      "         6.1328e-01,  3.5547e-01,  3.4961e-01,  4.6875e-01,  3.1250e-01,\n",
      "         3.4570e-01,  3.9648e-01,  3.4570e-01,  4.1602e-01,  3.8086e-01,\n",
      "         4.5117e-01,  3.2422e-01,  4.6289e-01,  3.8281e-01, -5.2246e-02,\n",
      "         4.4531e-01,  3.3203e-01,  3.2812e-01,  3.6914e-01,  3.1250e-01,\n",
      "         3.3008e-01,  3.2031e-01,  3.3008e-01,  3.4180e-01,  3.1641e-01,\n",
      "         3.2422e-01,  3.5156e-01,  4.3945e-01,  3.5352e-01,  3.5547e-01,\n",
      "         3.4961e-01,  7.6562e-01,  3.2031e-01, -1.5625e-01,  3.6328e-01,\n",
      "         4.4922e-01,  3.8086e-01,  3.3984e-01,  3.4180e-01,  4.0234e-01,\n",
      "         3.0078e-01,  3.1055e-01,  1.0703e+00,  3.3203e-01,  3.5547e-01,\n",
      "         3.1641e-01,  4.3164e-01,  3.0664e-01,  3.6328e-01,  3.7891e-01,\n",
      "         3.5156e-01,  3.7695e-01,  1.4531e+00,  3.9453e-01,  3.7695e-01,\n",
      "         3.7891e-01,  3.7109e-01,  3.3008e-01,  4.9023e-01,  3.3789e-01,\n",
      "         3.2617e-01,  3.4961e-01,  3.3594e-01,  3.6719e-01,  4.1406e-01,\n",
      "         5.2344e-01,  3.2812e-01,  5.2734e-01,  8.7891e-01,  3.4375e-01,\n",
      "         3.7695e-01,  4.3555e-01,  3.3203e-01,  3.3789e-01,  3.4375e-01,\n",
      "         4.1992e-01,  3.6719e-01,  3.6133e-01,  3.5742e-01,  5.6250e-01,\n",
      "         4.0039e-01, -6.2891e-01,  3.0664e-01,  3.8281e-01,  3.9648e-01,\n",
      "         5.1953e-01,  3.8281e-01,  3.5742e-01,  3.7109e-01,  3.2617e-01,\n",
      "         5.4688e-01,  4.1211e-01,  4.0625e-01,  3.1055e-01,  3.9062e-01,\n",
      "         3.6719e-01,  4.3164e-01,  5.1953e-01,  5.5078e-01,  4.1211e-01,\n",
      "         4.1406e-01,  3.4766e-01,  3.5156e-01,  3.7500e-01,  3.0859e-01,\n",
      "         3.6133e-01,  3.5742e-01,  3.5352e-01,  3.2617e-01,  3.5742e-01,\n",
      "         3.2812e-01,  3.7891e-01,  3.9648e-01,  3.6328e-01,  3.1055e-01,\n",
      "         3.5938e-01,  3.7891e-01,  4.5312e-01,  3.5156e-01,  3.3203e-01,\n",
      "         3.8867e-01,  3.8086e-01,  3.7305e-01,  3.3008e-01,  3.5352e-01,\n",
      "         3.1445e-01,  3.4375e-01,  4.5703e-01,  3.3203e-01,  3.2812e-01,\n",
      "         3.8867e-01,  3.8477e-01,  3.7891e-01,  3.3203e-01,  3.5547e-01,\n",
      "         3.0273e-01,  3.1055e-01,  3.5938e-01,  4.3555e-01,  3.9453e-01,\n",
      "         3.3594e-01,  3.3594e-01,  4.8633e-01,  6.8750e-01,  3.3789e-01,\n",
      "         5.1172e-01,  3.3594e-01,  3.2422e-01,  3.4180e-01,  4.1797e-01,\n",
      "         5.7031e-01,  4.0039e-01,  4.7266e-01,  3.5547e-01,  4.1016e-01,\n",
      "         4.1797e-01,  3.4180e-01,  4.5898e-01,  3.2227e-01,  3.1445e-01,\n",
      "         8.1641e-01,  3.8477e-01,  3.9648e-01,  3.1836e-01,  3.6719e-01,\n",
      "         4.2188e-01,  3.0859e-01,  3.5742e-01,  3.4375e-01,  4.0234e-01,\n",
      "         3.2422e-01,  3.5156e-01,  3.2031e-01,  3.5352e-01,  4.1602e-01,\n",
      "         3.8281e-01,  4.4531e-01,  3.3789e-01,  5.5078e-01,  3.9258e-01,\n",
      "         4.8438e-01,  3.5156e-01,  3.2227e-01,  4.4922e-01,  3.5352e-01,\n",
      "         3.3594e-01,  3.3789e-01,  5.0391e-01,  3.3789e-01,  3.5156e-01,\n",
      "         3.7305e-01,  4.4727e-01,  3.1641e-01, -4.3555e-01,  3.6133e-01,\n",
      "         3.3008e-01,  3.3008e-01,  3.3008e-01,  3.0664e-01,  3.3398e-01,\n",
      "         3.3203e-01,  3.1250e-01,  3.9258e-01], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.decoder.block.11.layer.1.EncDecAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0311,  0.0115, -0.0100,  ...,  0.0635,  0.0806,  0.0854],\n",
      "        [ 0.0201, -0.0349,  0.0050,  ...,  0.0262,  0.0576,  0.0593],\n",
      "        [ 0.0552, -0.0125, -0.0205,  ..., -0.0242, -0.0457, -0.0679],\n",
      "        ...,\n",
      "        [ 0.0310,  0.0275, -0.0161,  ...,  0.0796, -0.0574,  0.0554],\n",
      "        [ 0.0723,  0.0192, -0.0410,  ...,  0.0072,  0.0056,  0.0052],\n",
      "        [-0.0035,  0.0073, -0.0254,  ..., -0.0139,  0.0112,  0.0098]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.11.layer.1.EncDecAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.5156, -0.0664,  0.2432,  ...,  0.2812, -0.1089, -0.2070],\n",
      "        [ 0.0457,  0.0510, -0.3887,  ...,  0.3184, -0.2012,  0.2109],\n",
      "        [-0.1484, -0.4551,  0.1455,  ...,  0.0537,  0.5547, -0.0513],\n",
      "        ...,\n",
      "        [-0.0057,  0.1055, -0.8125,  ...,  0.0801,  0.5742,  0.3320],\n",
      "        [ 0.3418,  0.6328, -0.0815,  ...,  0.4199, -0.0603,  0.1270],\n",
      "        [ 0.1211, -0.1514, -0.6680,  ..., -0.2812, -0.2676,  0.4766]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.11.layer.1.EncDecAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.3145,  0.4062, -0.6406,  ..., -1.7500, -0.6992, -0.5469],\n",
      "        [-0.9062, -0.2109,  0.2158,  ...,  1.4688, -1.5469, -1.1875],\n",
      "        [ 2.2656, -1.4609, -0.5820,  ...,  0.3926, -0.3262,  1.3047],\n",
      "        ...,\n",
      "        [-0.7461,  0.8594,  0.7070,  ..., -0.8359,  1.0547,  0.9492],\n",
      "        [-0.9688,  0.3242,  0.2910,  ...,  0.3457, -0.1484, -0.6953],\n",
      "        [ 0.2520, -1.2656,  0.5430,  ..., -1.6250, -1.4766, -1.9062]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.11.layer.1.EncDecAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1074, -0.4883,  1.1406,  ..., -1.3672,  0.5312,  0.2344],\n",
      "        [ 0.0075,  0.0786,  0.7969,  ..., -0.3184, -0.0060, -0.3809],\n",
      "        [-0.3574,  0.4102,  0.8789,  ...,  0.0493,  0.9023, -0.0251],\n",
      "        ...,\n",
      "        [-1.8828,  1.4531, -0.2480,  ..., -0.7305,  1.0625, -0.4883],\n",
      "        [-1.4219, -1.5312, -1.8203,  ..., -0.9258, -0.8516, -0.7539],\n",
      "        [-1.0078, -0.1367, -0.6367,  ...,  0.2295,  1.2734, -0.6836]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.11.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 7.0801e-02,  1.2891e-01,  1.4160e-01,  3.5400e-02,  7.6172e-02,\n",
      "        -7.0312e-02,  2.0508e-01,  8.3008e-02,  8.6426e-02,  1.0498e-01,\n",
      "        -6.3965e-02,  1.0547e-01,  7.5195e-02,  7.3730e-02,  8.3008e-02,\n",
      "         7.9590e-02,  7.0801e-02,  1.0156e-01,  7.0312e-02,  1.0352e-01,\n",
      "         9.1309e-02,  7.6660e-02,  8.5449e-02,  9.0332e-02,  9.5703e-02,\n",
      "         1.0938e-01,  9.4727e-02, -6.5918e-02,  9.6191e-02,  9.7656e-02,\n",
      "         9.1309e-02,  6.8848e-02,  1.0303e-01,  1.1719e-01,  7.9102e-02,\n",
      "         1.1719e-01,  1.1230e-01,  9.5703e-02,  8.1055e-02,  9.1797e-02,\n",
      "         7.1289e-02,  9.1797e-02,  8.2520e-02,  8.0078e-02,  2.9297e-01,\n",
      "         8.3008e-02,  1.0010e-01,  1.2793e-01,  8.6914e-02,  7.5684e-02,\n",
      "         1.4941e-01,  8.1543e-02,  7.6172e-02,  7.9102e-02,  1.1816e-01,\n",
      "         7.2266e-02,  6.4941e-02,  7.7637e-02,  1.5430e-01,  8.0566e-02,\n",
      "         8.5449e-02,  8.4473e-02,  1.0303e-01,  9.6191e-02,  9.8145e-02,\n",
      "         8.6426e-02,  8.6426e-02,  9.5215e-02,  8.0078e-02,  7.5195e-02,\n",
      "         1.2061e-01,  1.1182e-01,  1.1475e-01,  8.3984e-02,  1.0205e-01,\n",
      "         7.7637e-02,  7.1289e-02,  7.3242e-02,  1.0791e-01,  1.2354e-01,\n",
      "         9.3262e-02,  8.6914e-02,  8.6426e-02,  8.0566e-02,  1.0352e-01,\n",
      "         1.2158e-01,  5.3024e-04,  7.8613e-02,  8.8867e-02,  9.0820e-02,\n",
      "         8.8379e-02,  1.0449e-01,  7.1777e-02,  6.2012e-02,  9.5215e-02,\n",
      "         9.3262e-02,  9.2773e-02,  9.4238e-02,  1.2793e-01,  7.0312e-02,\n",
      "         6.1768e-02,  1.0254e-01,  8.1055e-02,  8.3984e-02,  9.1797e-02,\n",
      "        -6.9824e-02,  8.5449e-02,  8.7891e-02,  1.2988e-01,  1.1230e-01,\n",
      "         1.3672e-01,  9.0332e-02,  6.1523e-02,  8.4961e-02,  1.0986e-01,\n",
      "         1.8750e-01,  7.4707e-02, -7.1777e-02, -3.7598e-02,  9.3262e-02,\n",
      "         7.8125e-02,  1.2695e-01,  1.0742e-01,  7.9102e-02,  1.0449e-01,\n",
      "         8.8867e-02,  8.3496e-02,  6.4453e-02,  7.9102e-02, -7.0801e-02,\n",
      "         6.9824e-02,  1.0693e-01,  7.9590e-02,  9.6680e-02,  1.0938e-01,\n",
      "        -6.9824e-02,  7.9102e-02,  8.1543e-02,  6.4941e-02,  7.5684e-02,\n",
      "         9.1309e-02,  1.1475e-01,  6.5430e-02,  9.2773e-02,  7.8613e-02,\n",
      "        -6.8359e-02,  1.3379e-01,  8.2520e-02,  9.8145e-02, -6.8359e-02,\n",
      "         1.6113e-01,  1.0010e-01,  1.6602e-01,  1.0205e-01,  1.0596e-01,\n",
      "         1.2012e-01,  7.7637e-02,  1.1572e-01,  1.4258e-01,  7.7148e-02,\n",
      "         8.2520e-02,  8.2520e-02,  7.6172e-02,  8.7891e-02,  9.8145e-02,\n",
      "         1.3965e-01,  9.3262e-02,  9.8633e-02,  1.2891e-01,  9.4238e-02,\n",
      "        -7.3730e-02,  8.8379e-02,  1.3379e-01,  1.0010e-01,  1.0742e-01,\n",
      "         9.0820e-02,  8.3008e-02,  1.4258e-01, -8.0566e-02,  1.0547e-01,\n",
      "         9.6191e-02,  7.9102e-02, -9.7656e-02,  1.2256e-01,  7.4219e-02,\n",
      "         1.3281e-01,  8.3984e-02,  1.1279e-01,  7.9102e-02,  7.2266e-02,\n",
      "         9.4727e-02,  8.8379e-02,  1.1035e-01, -7.8125e-02,  8.2520e-02,\n",
      "         9.7656e-02,  9.0820e-02,  1.1230e-01,  8.7402e-02,  9.1309e-02,\n",
      "         9.4727e-02, -7.2754e-02,  9.1309e-02,  1.1816e-01,  8.8867e-02,\n",
      "         7.2754e-02,  8.2031e-02,  1.0889e-01,  9.3262e-02,  1.0059e-01,\n",
      "         1.6699e-01, -6.3965e-02, -7.2266e-02,  1.0742e-01,  9.0820e-02,\n",
      "         1.1621e-01,  7.5684e-02,  1.0645e-01,  7.6660e-02,  3.0884e-02,\n",
      "        -7.6660e-02,  1.7090e-01,  1.4062e-01, -7.6660e-02,  8.9355e-02,\n",
      "         1.0791e-01,  5.3516e-01,  1.1719e-01,  7.5195e-02,  1.0840e-01,\n",
      "         1.1377e-01,  7.2266e-02,  9.0332e-02,  9.2285e-02,  9.0820e-02,\n",
      "         8.3496e-02,  6.7871e-02,  7.3730e-02,  7.7637e-02,  7.2754e-02,\n",
      "         1.0156e-01,  9.8633e-02,  8.7402e-02,  1.0352e-01,  8.3008e-02,\n",
      "         7.5684e-02,  1.2891e-01,  7.7148e-02,  9.2773e-02,  9.7168e-02,\n",
      "         7.4219e-02,  8.3984e-02,  1.0449e-01,  9.0332e-02,  9.0332e-02,\n",
      "         1.2793e-01,  8.6914e-02,  7.2266e-02,  1.1621e-01,  1.1426e-01,\n",
      "         6.5918e-02,  7.3242e-02,  9.7168e-02,  8.3008e-02,  1.1377e-01,\n",
      "         7.5195e-02,  8.7891e-02,  8.9844e-02,  8.2520e-02,  7.3730e-02,\n",
      "         8.3984e-02,  1.3184e-01,  9.1797e-02,  8.7891e-02, -7.8125e-02,\n",
      "         8.1543e-02,  1.0791e-01,  7.3242e-02,  1.0254e-01,  6.8848e-02,\n",
      "        -1.3199e-03,  1.1816e-01,  8.7891e-02,  8.5938e-02,  7.8125e-02,\n",
      "         8.4961e-02,  8.0566e-02,  1.0010e-01,  8.5449e-02,  6.9824e-02,\n",
      "         7.8613e-02,  9.5703e-02,  8.0566e-02,  9.5703e-02,  9.0820e-02,\n",
      "         9.9609e-02,  8.9355e-02,  9.2773e-02,  9.0332e-02,  9.2773e-02,\n",
      "         1.4355e-01,  9.5703e-02, -5.0781e-02,  7.8125e-02,  9.2285e-02,\n",
      "         9.4238e-02,  6.3477e-02,  1.0645e-01,  7.3242e-02,  1.1621e-01,\n",
      "         1.6895e-01,  1.1084e-01,  7.8613e-02, -7.6172e-02,  7.0703e-01,\n",
      "         7.1289e-02,  2.0020e-01,  6.9824e-02,  1.0791e-01,  1.1816e-01,\n",
      "         9.0820e-02,  1.0791e-01,  6.8848e-02,  1.2598e-01,  8.3008e-02,\n",
      "         8.1543e-02,  6.9824e-02,  1.2500e-01,  7.8125e-02,  7.9102e-02,\n",
      "         1.1523e-01,  8.7891e-02,  9.2773e-02,  1.0156e-01,  1.0400e-01,\n",
      "         8.0566e-02,  1.0254e-01,  1.3770e-01,  9.4238e-02, -4.4189e-02,\n",
      "         8.3984e-02,  1.1279e-01,  1.1182e-01,  6.9824e-02,  1.1865e-01,\n",
      "         1.0986e-01,  1.1035e-01,  6.4941e-02,  6.6895e-02,  7.2266e-02,\n",
      "         9.0332e-02,  1.0986e-01,  9.9121e-02,  1.1133e-01, -7.9102e-02,\n",
      "         1.3672e-01,  8.4473e-02, -5.7602e-04,  1.0156e-01,  7.7148e-02,\n",
      "         8.8379e-02,  8.0078e-02, -5.6152e-02,  8.5449e-02,  1.1914e-01,\n",
      "         1.2109e-01,  7.2266e-02,  1.0059e-01,  8.1543e-02,  7.4707e-02,\n",
      "         7.9590e-02,  1.0352e-01,  1.1377e-01,  7.6660e-02,  1.0938e-01,\n",
      "         8.5449e-02,  8.2520e-02,  1.5820e-01,  1.0303e-01,  8.0078e-02,\n",
      "         9.0332e-02,  7.6660e-02,  6.0791e-02,  1.1719e-01,  9.1309e-02,\n",
      "         9.8633e-02,  8.3496e-02,  8.9355e-02,  9.6680e-02,  8.8379e-02,\n",
      "         9.8145e-02,  8.6914e-02, -7.1777e-02, -6.3965e-02,  8.3496e-02,\n",
      "        -7.4219e-02,  1.2109e-01,  8.8379e-02,  9.2773e-02,  7.2754e-02,\n",
      "        -8.2520e-02,  9.6680e-02,  8.8867e-02,  8.9844e-02,  8.1543e-02,\n",
      "         9.8633e-02,  8.6914e-02,  9.6191e-02,  8.7402e-02,  9.5215e-02,\n",
      "         1.0156e-01,  1.1279e-01,  8.0078e-02,  8.9844e-02,  1.0156e-01,\n",
      "         1.1133e-01,  1.0059e-01,  8.0566e-02,  6.1719e-01,  1.2109e-01,\n",
      "        -5.6076e-04,  8.2520e-02,  9.2285e-02, -9.2316e-04,  9.0332e-02,\n",
      "         1.0791e-01,  6.8359e-02,  9.7168e-02,  7.9590e-02,  7.8125e-02,\n",
      "         9.1309e-02,  1.0156e-01,  8.0078e-02,  9.0332e-02,  1.0693e-01,\n",
      "         9.9609e-02,  8.8867e-02,  1.0010e-01,  8.7402e-02,  8.3008e-02,\n",
      "         1.4160e-01,  1.0596e-01,  1.0254e-01,  1.0596e-01,  1.0645e-01,\n",
      "         8.6426e-02,  9.5703e-02,  7.5195e-02,  2.0981e-04,  8.1543e-02,\n",
      "         7.3242e-02,  1.1426e-01,  8.8379e-02,  9.4238e-02,  9.6680e-02,\n",
      "         7.9102e-02,  6.5918e-02,  3.4375e-01,  7.3242e-02,  8.4961e-02,\n",
      "         9.4238e-02,  1.1523e-01,  1.0889e-01,  8.6914e-02,  7.8613e-02,\n",
      "         9.0820e-02, -6.3965e-02,  7.6660e-02,  7.8125e-02,  8.9844e-02,\n",
      "         7.9590e-02,  9.4238e-02,  2.2168e-01,  1.0400e-01,  8.8867e-02,\n",
      "         1.0596e-01,  9.0332e-02,  1.2109e-01,  8.4961e-02,  7.6660e-02,\n",
      "         6.8359e-02,  9.6191e-02,  1.0352e-01,  7.8613e-02,  2.6512e-04,\n",
      "         8.7402e-02,  9.6680e-02,  5.2002e-02,  1.1328e-01,  7.2754e-02,\n",
      "         8.3496e-02,  1.1523e-01,  9.8633e-02,  1.2109e-01,  1.1865e-01,\n",
      "         1.9922e-01,  7.5195e-02,  8.9355e-02,  7.3730e-02,  8.1543e-02,\n",
      "         1.0791e-01,  2.0215e-01,  9.1309e-02,  7.5684e-02,  9.1797e-02,\n",
      "         9.0332e-02,  8.7402e-02, -7.3730e-02,  9.2773e-02,  9.8633e-02,\n",
      "         8.7402e-02,  9.8145e-02, -7.9590e-02,  1.0010e-01,  1.1182e-01,\n",
      "         9.7168e-02,  7.0312e-02,  9.5215e-02,  5.7861e-02,  9.2773e-02,\n",
      "         7.9590e-02,  8.5449e-02,  1.1377e-01,  1.0840e-01,  8.6914e-02,\n",
      "         9.1797e-02,  7.2266e-02,  2.2363e-01,  9.2773e-02,  9.0820e-02,\n",
      "         8.8379e-02,  9.2285e-02,  8.8867e-02,  1.0352e-01,  9.5703e-02,\n",
      "         1.0254e-01,  7.7637e-02,  9.3262e-02, -9.1309e-02,  7.5195e-02,\n",
      "         1.3184e-01,  8.9355e-02,  7.2266e-02,  7.1777e-02,  8.9355e-02,\n",
      "         9.2773e-02,  9.2773e-02,  8.0078e-02,  9.0820e-02,  8.4473e-02,\n",
      "         8.3984e-02,  1.5234e-01,  8.3008e-02,  1.1133e-01,  8.6914e-02,\n",
      "         9.9121e-02,  7.8613e-02,  9.2285e-02,  1.0693e-01,  7.2754e-02,\n",
      "         1.0791e-01,  8.3496e-02, -1.3542e-04, -6.6406e-02,  9.1309e-02,\n",
      "         1.2354e-01,  8.8867e-02,  8.4473e-02,  7.5195e-02,  1.0205e-01,\n",
      "         1.1035e-01,  7.6172e-02,  1.0156e-01,  8.4473e-02,  7.4219e-02,\n",
      "         8.8379e-02,  1.1133e-01,  9.7656e-02,  1.2207e-01,  3.2471e-02,\n",
      "         9.7168e-02,  1.1377e-01,  8.4961e-02,  9.4727e-02,  7.8125e-02,\n",
      "         9.7656e-02,  8.3008e-02,  8.2031e-02,  9.2773e-02,  1.0156e-01,\n",
      "         8.9355e-02,  1.0303e-01,  1.3086e-01,  9.0332e-02, -8.3008e-02,\n",
      "         9.7656e-02,  1.3770e-01,  9.8633e-02,  3.5400e-02,  7.8613e-02,\n",
      "        -8.1055e-02,  9.1797e-02,  7.8125e-02,  8.7402e-02,  7.8125e-02,\n",
      "         8.3008e-02,  9.4238e-02,  1.3379e-01,  8.0566e-02,  8.3008e-02,\n",
      "         7.8613e-02,  8.2031e-02,  7.8613e-02,  1.3184e-01,  7.6660e-02,\n",
      "         9.7656e-02,  7.2754e-02,  2.4414e-01,  1.7676e-01,  8.1543e-02,\n",
      "         9.5215e-02,  9.4727e-02,  8.7891e-02,  6.5918e-02,  7.2754e-02,\n",
      "         9.7168e-02, -6.6406e-02,  7.8125e-02,  8.6426e-02,  9.1309e-02,\n",
      "         1.0645e-01,  1.0156e-01,  8.8379e-02,  9.8633e-02,  8.6426e-02,\n",
      "         8.3496e-02,  8.0566e-02,  9.4727e-02,  9.1797e-02,  8.3496e-02,\n",
      "         7.5195e-02,  1.0693e-01,  9.6680e-02,  9.0332e-02,  8.6914e-02,\n",
      "         7.6660e-02,  6.7871e-02,  8.9844e-02,  7.2266e-02,  1.1816e-01,\n",
      "         1.1426e-01,  9.6191e-02, -7.1289e-02,  8.3008e-02,  1.1035e-01,\n",
      "         7.2754e-02,  8.3008e-02, -7.4219e-02,  7.5684e-02,  7.1289e-02,\n",
      "         1.6113e-01,  6.2988e-02,  7.8125e-02,  2.0996e-01,  8.6426e-02,\n",
      "         7.2266e-02,  1.2012e-01,  9.1797e-02,  1.0205e-01,  8.7891e-02,\n",
      "         1.0254e-01,  9.2773e-02,  8.4961e-02,  8.1055e-02, -8.6914e-02,\n",
      "         1.1914e-01,  8.3496e-02,  1.2695e-01,  8.9355e-02,  9.0820e-02,\n",
      "         1.0303e-01,  1.2988e-01,  8.3008e-02,  8.2520e-02,  7.8125e-02,\n",
      "         7.5684e-02,  9.7656e-02,  8.1055e-02,  1.0596e-01,  8.7891e-02,\n",
      "         9.0332e-02,  7.1289e-02,  7.8125e-02,  9.1309e-02,  8.6914e-02,\n",
      "         8.2031e-02,  1.1816e-01,  1.1377e-01,  8.4473e-02,  7.5195e-02,\n",
      "         8.2031e-02,  1.0645e-01,  8.3984e-02,  8.3008e-02,  1.4453e-01,\n",
      "         1.0840e-01,  8.9844e-02, -7.3242e-02,  1.6992e-01,  8.6914e-02,\n",
      "         9.5215e-02,  9.6191e-02,  8.7891e-02,  1.0156e-01,  5.2002e-02,\n",
      "         4.0039e-01,  9.1309e-02,  1.3477e-01,  7.5195e-02,  6.8848e-02,\n",
      "         9.4727e-02,  9.0820e-02,  7.8613e-02,  9.3750e-02,  8.8379e-02,\n",
      "         1.2402e-01,  7.2266e-02,  1.0205e-01,  9.0332e-02,  7.5684e-02,\n",
      "         9.1797e-02,  7.9102e-02,  1.2109e-01, -9.8145e-02,  2.1680e-01,\n",
      "         9.9121e-02,  9.4238e-02,  8.4473e-02,  9.5703e-02,  7.9102e-02,\n",
      "         8.2520e-02,  8.1055e-02,  8.8379e-02,  7.7637e-02,  9.6191e-02,\n",
      "         8.6914e-02,  6.9336e-02,  1.1084e-01,  7.8125e-02, -6.2500e-02,\n",
      "         6.2500e-02,  8.4961e-02, -7.1289e-02,  9.9121e-02,  9.7656e-02,\n",
      "         1.4551e-01,  8.9844e-02,  7.8125e-02,  1.2256e-01,  8.1055e-02,\n",
      "         7.0312e-02,  8.1055e-02,  7.4219e-02,  7.1289e-02,  8.4961e-02,\n",
      "         8.1543e-02,  7.5684e-02,  7.2266e-02], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.decoder.block.11.layer.2.DenseReluDense.wi.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.4023, -0.0117, -0.0041,  ..., -0.4473, -0.3555, -1.0781],\n",
      "        [ 0.9805,  1.0234,  0.7070,  ..., -0.2949, -0.6211,  0.3145],\n",
      "        [-0.2148,  0.9648, -0.1455,  ..., -1.1641,  0.1001,  0.9727],\n",
      "        ...,\n",
      "        [ 0.7383,  0.8320, -0.3242,  ..., -0.4414,  0.2676, -0.7344],\n",
      "        [-0.5312,  0.3809,  0.1572,  ..., -0.4277, -0.5820, -0.2695],\n",
      "        [ 0.0962,  0.7422, -0.7812,  ...,  0.2910,  0.8828,  0.1172]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.11.layer.2.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0659, -0.3398, -0.1514,  ..., -0.3555,  0.1689, -0.4082],\n",
      "        [ 0.1914,  0.1064, -0.1104,  ..., -0.4902,  0.0757,  0.0664],\n",
      "        [-0.1187,  0.1494, -0.2031,  ..., -0.3105,  0.1045, -0.1504],\n",
      "        ...,\n",
      "        [ 0.1680,  0.0684, -0.2119,  ...,  0.4316, -0.2295,  0.2236],\n",
      "        [ 0.5391, -0.2812,  0.1758,  ...,  0.4961, -0.2676, -0.2949],\n",
      "        [ 0.1318, -0.4316, -0.2002,  ...,  0.0996,  0.2949,  0.1816]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "prompt_model.plm.decoder.block.11.layer.2.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 4.4375e+00,  5.4375e+00,  6.1250e+00,  2.0938e+00,  4.5938e+00,\n",
      "         4.5312e+00,  5.0312e+00,  4.2812e+00,  5.5000e+00,  4.5625e+00,\n",
      "         4.4375e+00,  4.7500e+00,  5.0625e+00,  5.0312e+00,  4.3438e+00,\n",
      "         4.3125e+00,  4.6562e+00,  5.1875e+00,  5.1562e+00,  4.3438e+00,\n",
      "         4.3750e+00,  4.3438e+00,  5.5312e+00,  4.3125e+00,  4.5000e+00,\n",
      "         5.3125e+00,  4.6875e+00,  6.3750e+00,  5.0312e+00,  4.5312e+00,\n",
      "         4.4688e+00,  5.4375e+00,  4.5000e+00,  4.4688e+00,  4.7500e+00,\n",
      "         5.1875e+00,  4.8438e+00,  1.0000e+01,  4.3750e+00,  4.5000e+00,\n",
      "         4.9375e+00,  5.1875e+00,  4.6562e+00,  4.4688e+00,  7.0312e+00,\n",
      "         4.6250e+00,  5.3125e+00,  4.6250e+00,  4.3438e+00,  4.7500e+00,\n",
      "         5.7812e+00,  4.2188e+00,  4.8438e+00,  4.5625e+00,  9.1250e+00,\n",
      "         4.8125e+00,  5.3438e+00,  4.4375e+00,  4.7188e+00,  4.7812e+00,\n",
      "         4.7188e+00,  4.4688e+00,  4.3750e+00,  4.7500e+00,  4.6250e+00,\n",
      "         1.8375e+01,  4.5000e+00,  4.8438e+00,  9.5625e+00,  4.4375e+00,\n",
      "         4.9688e+00,  4.9375e+00,  4.7812e+00,  4.7188e+00,  5.2188e+00,\n",
      "         4.3750e+00,  5.5000e+00,  4.2812e+00,  4.7812e+00,  5.5625e+00,\n",
      "         4.5312e+00,  4.6875e+00,  4.3750e+00,  6.1562e+00,  4.9375e+00,\n",
      "         4.9375e+00,  9.0625e-01,  4.4062e+00,  4.4688e+00,  4.3750e+00,\n",
      "         4.6562e+00,  4.4375e+00,  4.4375e+00,  4.5000e+00,  4.7188e+00,\n",
      "         4.4375e+00,  4.4062e+00,  4.6250e+00,  4.8125e+00,  4.5312e+00,\n",
      "         4.4688e+00,  4.5938e+00,  7.3750e+00,  4.5312e+00,  4.8125e+00,\n",
      "         5.8125e+00,  4.4688e+00,  4.8438e+00,  5.2188e+00,  4.8438e+00,\n",
      "         4.6250e+00,  4.7500e+00,  4.4062e+00,  4.4688e+00,  5.0625e+00,\n",
      "         5.2188e+00,  4.8750e+00,  4.5938e+00,  1.2422e+00,  4.6250e+00,\n",
      "         4.4062e+00,  4.9062e+00,  6.4062e+00,  4.5000e+00,  4.8125e+00,\n",
      "         4.6250e+00,  4.6250e+00,  4.4062e+00,  4.5938e+00,  5.5000e+00,\n",
      "         6.1562e+00,  5.0000e+00,  4.3125e+00,  4.2812e+00,  4.9688e+00,\n",
      "         5.1562e+00,  4.4688e+00,  5.7500e+00,  4.1562e+00,  5.1250e+00,\n",
      "         5.8125e+00,  4.7812e+00,  4.4688e+00,  5.4688e+00,  4.7188e+00,\n",
      "         4.7500e+00,  4.8750e+00,  5.2812e+00,  4.7500e+00,  4.5000e+00,\n",
      "         4.9375e+00,  5.2188e+00,  5.5312e+00,  6.0312e+00,  4.5312e+00,\n",
      "         4.7188e+00,  4.5625e+00,  4.6562e+00,  5.0938e+00,  4.9375e+00,\n",
      "         4.3750e+00,  4.5312e+00,  4.6875e+00,  5.4062e+00,  5.0000e+00,\n",
      "         6.5938e+00,  4.4688e+00,  5.6562e+00,  8.0625e+00,  4.6562e+00,\n",
      "         7.5312e+00,  4.6562e+00,  4.9375e+00,  4.8438e+00,  4.7500e+00,\n",
      "         4.7188e+00,  4.8125e+00,  4.9688e+00,  6.5312e+00,  5.2500e+00,\n",
      "         4.6875e+00,  4.4062e+00,  4.4062e+00,  4.5938e+00,  5.0625e+00,\n",
      "         5.9062e+00,  4.4688e+00,  3.6562e+00,  4.8125e+00,  4.8750e+00,\n",
      "         4.5938e+00,  4.5312e+00,  5.0625e+00,  4.4375e+00,  4.7500e+00,\n",
      "         5.5000e+00,  4.4688e+00,  4.5312e+00,  4.7188e+00,  4.7500e+00,\n",
      "         4.3438e+00,  4.3750e+00,  4.7188e+00,  4.8750e+00,  4.8438e+00,\n",
      "         4.6250e+00,  4.3125e+00,  4.4375e+00,  5.0000e+00,  4.2812e+00,\n",
      "         6.6250e+00,  4.4688e+00,  4.5938e+00,  4.8750e+00,  4.4375e+00,\n",
      "         5.0000e+00,  4.5938e+00,  5.0938e+00,  4.5938e+00,  1.1172e+00,\n",
      "         5.8125e+00,  5.0000e+00,  5.5625e+00,  4.5625e+00,  5.1875e+00,\n",
      "         4.4688e+00,  4.9375e+00,  4.9375e+00,  4.4688e+00,  5.0938e+00,\n",
      "         4.9688e+00,  4.2188e+00,  4.8750e+00,  4.4688e+00,  4.7188e+00,\n",
      "         4.9375e+00,  5.4375e+00,  4.3125e+00,  5.4062e+00,  4.4375e+00,\n",
      "         9.6250e+00,  4.7812e+00,  4.7812e+00,  4.7500e+00,  4.2812e+00,\n",
      "         4.4375e+00,  4.9688e+00,  4.3125e+00,  4.6562e+00,  4.8438e+00,\n",
      "         5.0938e+00,  4.5938e+00,  4.4688e+00,  5.0000e+00,  4.8438e+00,\n",
      "         5.2812e+00,  4.5938e+00,  5.2500e+00,  4.8750e+00,  4.8750e+00,\n",
      "         4.9688e+00,  5.4062e+00,  4.4062e+00,  4.3750e+00,  4.9062e+00,\n",
      "         4.4688e+00,  4.9375e+00,  4.8125e+00,  4.4062e+00,  4.4688e+00,\n",
      "         4.7188e+00,  1.3188e+01,  4.6250e+00,  4.9688e+00,  6.0000e+00,\n",
      "         4.5625e+00,  4.9062e+00,  4.5938e+00,  5.0312e+00,  4.6250e+00,\n",
      "        -1.5312e+00,  7.5938e+00,  4.7500e+00,  4.9375e+00,  4.5938e+00,\n",
      "         4.6875e+00,  4.4375e+00,  4.5938e+00,  4.8750e+00,  5.4375e+00,\n",
      "         4.6250e+00,  4.5938e+00,  4.4062e+00,  4.6875e+00,  4.7500e+00,\n",
      "         4.4375e+00,  6.8125e+00,  4.5000e+00,  4.7188e+00,  4.7812e+00,\n",
      "         4.7812e+00,  4.9688e+00,  3.8281e+00,  4.5625e+00,  4.7500e+00,\n",
      "         4.5625e+00,  4.3125e+00,  4.3750e+00,  4.9062e+00,  4.7812e+00,\n",
      "         4.9062e+00,  5.0625e+00,  4.1875e+00,  4.8438e+00,  5.4375e+00,\n",
      "         4.5938e+00,  4.9062e+00,  6.1250e+00,  4.5000e+00,  4.6562e+00,\n",
      "         4.3438e+00,  5.0000e+00,  4.6250e+00,  4.9375e+00,  5.0938e+00,\n",
      "         5.0312e+00,  4.6250e+00,  6.4062e+00,  4.7188e+00,  4.5312e+00,\n",
      "         4.5625e+00,  5.8438e+00,  4.5625e+00,  4.7188e+00,  4.7188e+00,\n",
      "         4.6562e+00,  5.1250e+00,  4.8438e+00,  4.7500e+00,  4.3438e+00,\n",
      "         4.7500e+00,  4.8750e+00,  4.7500e+00,  4.2500e+00,  4.9062e+00,\n",
      "         5.0312e+00,  4.5312e+00,  4.3438e+00,  5.2812e+00,  5.1875e+00,\n",
      "         5.5312e+00,  4.9688e+00,  4.9375e+00,  4.7812e+00,  4.6562e+00,\n",
      "         4.9062e+00,  4.8125e+00,  8.3618e-03,  4.8750e+00,  4.5000e+00,\n",
      "         4.4688e+00,  5.0312e+00,  1.3312e+01,  4.8125e+00,  4.9375e+00,\n",
      "         4.6250e+00,  4.9688e+00,  4.8750e+00,  4.6562e+00,  4.4688e+00,\n",
      "         4.6250e+00,  4.6250e+00,  5.3750e+00,  4.6562e+00,  4.5938e+00,\n",
      "         4.7812e+00,  4.8438e+00,  4.9062e+00,  5.0000e+00,  5.9062e+00,\n",
      "         4.8125e+00,  4.9062e+00,  5.8125e+00,  4.8438e+00,  4.7500e+00,\n",
      "         4.5000e+00,  4.2500e+00,  4.6875e+00,  4.8438e+00,  4.6250e+00,\n",
      "         5.0312e+00,  4.5938e+00,  4.7188e+00,  5.3750e+00,  4.9062e+00,\n",
      "         4.4062e+00,  5.1250e+00,  4.9688e+00,  4.7188e+00,  4.5000e+00,\n",
      "         4.8750e+00,  4.8750e+00,  4.3125e+00,  4.6875e+00,  4.6562e+00,\n",
      "         4.7188e+00,  5.7188e+00,  4.7188e+00,  4.8438e+00,  4.4375e+00,\n",
      "         4.8125e+00,  5.1875e+00,  4.7812e+00,  4.6250e+00,  4.5625e+00,\n",
      "         4.8438e+00,  5.2812e+00,  4.3125e+00,  1.1250e+01,  5.0938e+00,\n",
      "        -1.0391e+00,  4.5938e+00,  4.6250e+00,  4.0312e+00,  4.7812e+00,\n",
      "         4.8438e+00,  4.4375e+00,  4.6875e+00,  5.0312e+00,  4.8438e+00,\n",
      "         4.8438e+00,  4.7812e+00,  4.2812e+00,  4.7500e+00,  5.0938e+00,\n",
      "         4.8750e+00,  4.2188e+00,  5.0000e+00,  4.9688e+00,  6.4062e+00,\n",
      "         5.2812e+00,  6.5312e+00,  5.2188e+00,  4.7188e+00,  4.6562e+00,\n",
      "         4.8125e+00,  4.5938e+00,  4.5000e+00, -4.6082e-03,  4.5312e+00,\n",
      "         4.5625e+00,  4.6250e+00,  4.5938e+00,  5.0000e+00,  5.0938e+00,\n",
      "         4.4688e+00,  7.1250e+00,  7.9062e+00,  4.6875e+00,  4.9375e+00,\n",
      "         4.7500e+00,  4.5000e+00,  4.6875e+00,  4.7812e+00,  4.8438e+00,\n",
      "         4.5625e+00,  5.1562e+00,  4.5000e+00,  4.4375e+00,  4.4688e+00,\n",
      "         4.3125e+00,  4.6875e+00,  1.0438e+01,  4.6562e+00,  4.5938e+00,\n",
      "         5.0938e+00,  4.3125e+00,  5.2500e+00,  4.8750e+00,  4.5938e+00,\n",
      "         4.5625e+00,  4.9062e+00,  4.7188e+00,  4.6875e+00,  2.3906e+00,\n",
      "         4.6562e+00,  4.6250e+00,  1.7344e+00,  5.0000e+00,  4.3438e+00,\n",
      "         4.5312e+00,  4.7188e+00,  4.4688e+00,  4.6250e+00,  5.0000e+00,\n",
      "         2.1000e+01,  5.1250e+00,  4.4688e+00,  4.2500e+00,  4.3125e+00,\n",
      "         4.7188e+00,  6.2500e+00,  4.9062e+00,  4.6250e+00,  4.7188e+00,\n",
      "         5.1875e+00,  5.0625e+00,  4.6250e+00,  4.6562e+00,  4.6250e+00,\n",
      "         4.6250e+00,  4.5312e+00,  4.5000e+00,  4.7500e+00,  5.0625e+00,\n",
      "         4.5938e+00,  4.5938e+00,  4.7188e+00,  4.9375e+00,  4.5312e+00,\n",
      "         4.3125e+00,  4.5938e+00,  4.6250e+00,  5.4688e+00,  4.5938e+00,\n",
      "         5.0312e+00,  4.4375e+00,  7.8438e+00,  4.8750e+00,  4.6875e+00,\n",
      "         5.3438e+00,  4.7188e+00,  4.4062e+00,  4.6875e+00,  5.1250e+00,\n",
      "         4.9375e+00,  4.4375e+00,  4.5625e+00,  4.6875e+00,  4.4375e+00,\n",
      "         5.0000e+00,  4.8750e+00,  4.7188e+00,  4.5938e+00,  4.4688e+00,\n",
      "         4.7188e+00,  5.1562e+00,  4.5312e+00,  4.5938e+00,  5.1875e+00,\n",
      "         5.8125e+00,  6.6562e+00,  4.4688e+00,  7.1562e+00,  4.5000e+00,\n",
      "         4.9375e+00,  5.8125e+00,  4.6250e+00,  5.2812e+00,  4.5000e+00,\n",
      "         5.0000e+00,  5.0938e+00,  9.9487e-03,  6.0625e+00,  4.4062e+00,\n",
      "         6.7812e+00,  5.0312e+00,  4.7188e+00,  5.0938e+00,  4.5312e+00,\n",
      "         4.6875e+00,  5.1562e+00,  4.7812e+00,  4.3750e+00,  4.9688e+00,\n",
      "         5.5312e+00,  4.6562e+00,  5.1875e+00,  5.0625e+00,  1.4375e+00,\n",
      "         5.5625e+00,  4.6875e+00,  4.5312e+00,  4.8125e+00,  4.1875e+00,\n",
      "         4.4375e+00,  4.7500e+00,  4.4688e+00,  4.5312e+00,  4.7812e+00,\n",
      "         4.6562e+00,  4.8125e+00,  5.6250e+00,  4.9062e+00,  4.4062e+00,\n",
      "         4.6562e+00,  7.3438e+00,  4.5625e+00,  2.3125e+00,  5.0938e+00,\n",
      "         5.2500e+00,  4.9062e+00,  4.5000e+00,  5.0000e+00,  4.4375e+00,\n",
      "         4.3750e+00,  4.5938e+00,  1.1438e+01,  4.4375e+00,  4.2500e+00,\n",
      "         4.5938e+00,  4.4688e+00,  4.5000e+00,  4.7812e+00,  4.3438e+00,\n",
      "         4.9062e+00,  4.4062e+00,  1.3375e+01,  5.0938e+00,  4.5625e+00,\n",
      "         4.6562e+00,  4.6250e+00,  4.9375e+00,  5.6875e+00,  4.4375e+00,\n",
      "         4.7812e+00,  4.4688e+00,  4.4062e+00,  5.0625e+00,  5.1250e+00,\n",
      "         5.3750e+00,  4.5000e+00,  5.8125e+00,  1.1188e+01,  5.0625e+00,\n",
      "         4.8750e+00,  4.8125e+00,  4.7812e+00,  4.9375e+00,  4.8125e+00,\n",
      "         5.2500e+00,  5.3750e+00,  4.3750e+00,  5.3125e+00,  6.8438e+00,\n",
      "         4.5625e+00,  6.4688e+00,  4.4688e+00,  4.2500e+00,  5.0000e+00,\n",
      "         4.8438e+00,  4.7812e+00,  4.5938e+00,  4.6250e+00,  4.9375e+00,\n",
      "         7.7812e+00,  4.6562e+00,  4.6875e+00,  4.5000e+00,  4.5312e+00,\n",
      "         5.0312e+00,  5.0000e+00,  5.0625e+00,  6.4688e+00,  4.9062e+00,\n",
      "         4.6875e+00,  4.7188e+00,  4.6250e+00,  4.5938e+00,  4.4375e+00,\n",
      "         4.8438e+00,  4.7812e+00,  4.5000e+00,  4.6562e+00,  4.7188e+00,\n",
      "         4.8438e+00,  4.7500e+00,  4.9062e+00,  4.8438e+00,  4.6875e+00,\n",
      "         4.7188e+00,  4.8125e+00,  6.0000e+00,  4.9375e+00,  4.6875e+00,\n",
      "         4.6250e+00,  4.5000e+00,  4.9375e+00,  4.5625e+00,  4.8125e+00,\n",
      "         4.5312e+00,  4.3438e+00,  5.5312e+00,  4.8125e+00,  4.7812e+00,\n",
      "         4.8438e+00,  4.9688e+00,  5.0938e+00,  4.7812e+00,  4.6250e+00,\n",
      "         4.5625e+00,  4.6875e+00,  4.8438e+00,  5.9375e+00,  5.0938e+00,\n",
      "         4.7812e+00,  4.6250e+00,  5.1875e+00,  7.0000e+00,  4.5312e+00,\n",
      "         5.3750e+00,  4.7812e+00,  4.6562e+00,  4.9375e+00,  6.0938e+00,\n",
      "         5.4062e+00,  4.8125e+00,  5.4062e+00,  4.6250e+00,  4.5312e+00,\n",
      "         4.6250e+00,  4.9375e+00,  6.0625e+00,  4.9062e+00,  4.3750e+00,\n",
      "         7.6875e+00,  4.8750e+00,  4.8438e+00,  4.6250e+00,  4.6875e+00,\n",
      "         4.7812e+00,  4.2188e+00,  5.0625e+00,  7.0312e+00,  4.9688e+00,\n",
      "         4.6250e+00,  4.7188e+00,  4.5312e+00,  4.8125e+00,  5.1562e+00,\n",
      "         4.9062e+00,  6.2812e+00,  4.9375e+00,  5.8750e+00,  4.9688e+00,\n",
      "         5.0625e+00,  5.2812e+00,  4.8125e+00,  4.9688e+00,  4.4375e+00,\n",
      "         4.1562e+00,  4.6875e+00,  5.1875e+00,  4.8438e+00,  5.0938e+00,\n",
      "         4.8438e+00,  4.7500e+00,  4.5625e+00,  6.1562e+00,  4.3438e+00,\n",
      "         5.2812e+00,  4.8438e+00,  4.5312e+00,  4.1250e+00,  4.9375e+00,\n",
      "         4.5938e+00,  4.2500e+00,  4.5625e+00], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "prompt_model.plm.decoder.final_layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([0.1953, 0.3652, 0.3438, 0.0242, 0.1582, 0.2031, 0.1992, 0.1836, 0.3105,\n",
      "        0.1826, 0.2061, 0.1865, 0.4160, 0.2949, 0.1484, 0.1660, 0.2256, 0.2637,\n",
      "        0.2949, 0.1592, 0.1611, 0.1748, 0.3184, 0.1689, 0.1543, 0.2617, 0.1631,\n",
      "        0.3340, 0.1885, 0.1650, 0.1689, 0.6406, 0.1758, 0.1670, 0.2139, 0.2148,\n",
      "        0.1953, 0.5312, 0.1621, 0.2109, 0.2891, 0.2041, 0.2139, 0.1758, 0.5156,\n",
      "        0.1904, 0.2773, 0.1709, 0.1797, 0.1943, 0.2891, 0.1904, 0.2139, 0.1592,\n",
      "        4.5312, 0.2148, 0.3906, 0.1787, 0.1592, 0.1719, 0.2031, 0.1484, 0.1729,\n",
      "        0.1699, 0.1602, 0.8203, 0.1709, 0.1738, 1.5234, 0.1562, 0.1855, 0.2061,\n",
      "        0.1680, 0.1875, 0.1641, 0.1631, 0.2910, 0.1865, 0.1523, 0.2109, 0.1621,\n",
      "        0.2002, 0.1826, 0.3848, 0.1895, 0.1807, 0.5391, 0.1855, 0.1748, 0.1621,\n",
      "        0.2041, 0.1592, 0.1797, 0.2129, 0.1572, 0.1553, 0.1738, 0.1572, 0.1523,\n",
      "        0.1729, 0.2178, 0.1973, 0.3340, 0.1572, 0.1807, 0.4980, 0.1748, 0.1797,\n",
      "        0.1748, 0.2129, 0.1865, 0.1963, 0.1895, 0.1729, 0.1787, 0.1797, 0.2051,\n",
      "        0.1719, 0.0231, 0.1719, 0.1475, 0.1709, 0.4082, 0.1719, 0.2168, 0.1982,\n",
      "        0.1729, 0.2031, 0.1982, 0.3789, 0.5352, 0.2168, 0.1582, 0.1533, 0.2197,\n",
      "        0.2617, 0.2080, 0.2656, 0.1836, 0.2236, 0.2227, 0.2188, 0.2324, 0.3672,\n",
      "        0.2402, 0.2480, 0.1777, 0.3594, 0.1777, 0.1963, 0.1699, 0.1924, 0.2080,\n",
      "        0.3398, 0.1582, 0.1523, 0.2109, 0.1553, 0.3418, 0.2266, 0.1592, 0.1953,\n",
      "        0.2266, 0.3789, 0.1797, 0.2432, 0.1729, 0.3281, 0.3438, 0.2100, 0.4434,\n",
      "        0.1797, 0.1631, 0.2275, 0.1816, 0.2041, 0.2012, 0.2793, 0.3906, 0.2432,\n",
      "        0.1611, 0.1582, 0.1768, 0.1562, 0.2969, 0.3027, 0.1611, 0.2422, 0.1924,\n",
      "        0.2334, 0.1738, 0.1572, 0.1719, 0.1660, 0.1738, 0.1865, 0.1719, 0.1934,\n",
      "        0.1924, 0.1807, 0.1455, 0.1895, 0.1807, 0.1895, 0.1729, 0.2754, 0.1904,\n",
      "        0.1777, 0.1973, 0.1650, 0.2910, 0.2021, 0.2051, 0.2119, 0.1670, 0.2002,\n",
      "        0.1816, 0.1846, 0.2070, 0.6641, 0.2598, 0.1953, 0.1846, 0.1982, 0.2266,\n",
      "        0.1758, 0.0417, 0.2266, 0.1934, 0.2217, 0.2090, 0.1855, 0.2285, 0.1523,\n",
      "        0.1621, 0.1855, 0.2812, 0.2041, 0.2656, 0.2061, 4.5938, 0.1973, 0.1738,\n",
      "        0.1914, 0.1660, 0.1738, 0.1904, 0.1973, 0.1562, 0.2363, 0.2236, 0.1650,\n",
      "        0.1680, 0.1631, 0.1582, 0.2314, 0.1621, 0.2480, 0.2012, 0.1729, 0.3535,\n",
      "        0.2930, 0.1533, 0.1582, 0.1797, 0.1875, 0.1787, 0.1924, 0.1748, 0.1660,\n",
      "        0.1709, 0.8594, 0.2158, 0.1934, 0.5664, 0.1895, 0.1689, 0.2197, 0.1836,\n",
      "        0.2256, 0.9727, 0.4707, 0.1748, 0.2051, 0.1934, 0.2236, 0.1533, 0.1621,\n",
      "        0.2832, 0.3789, 0.1582, 0.1885, 0.1631, 0.1543, 0.1748, 0.1504, 0.4746,\n",
      "        0.1660, 0.1631, 0.1836, 0.1934, 0.1709, 0.3594, 0.2002, 0.1914, 0.1543,\n",
      "        0.1924, 0.1592, 0.2246, 0.2334, 0.1807, 0.2207, 0.1621, 0.2539, 0.2227,\n",
      "        0.1787, 0.1904, 1.4531, 0.1787, 0.1670, 0.1582, 0.2148, 0.2012, 0.1543,\n",
      "        0.2734, 0.1885, 0.2344, 0.3184, 0.1953, 0.1768, 0.1611, 0.2891, 0.2021,\n",
      "        0.1934, 0.1807, 0.1660, 0.1797, 0.1592, 0.2002, 0.6484, 0.1943, 0.1924,\n",
      "        0.1904, 0.1699, 0.1914, 0.2061, 0.2012, 0.1973, 0.3594, 0.2217, 0.2852,\n",
      "        0.1807, 0.2021, 0.1777, 0.2324, 0.1816, 0.1865, 0.0130, 0.1543, 0.2041,\n",
      "        0.1650, 0.2949, 0.5938, 0.1670, 0.1621, 0.1729, 0.2393, 0.2061, 0.1680,\n",
      "        0.1826, 0.1826, 0.2266, 0.2539, 0.1934, 0.2012, 0.1562, 0.2432, 0.1572,\n",
      "        0.2256, 0.4688, 0.1963, 0.2109, 0.5430, 0.1777, 0.1660, 0.2119, 0.1660,\n",
      "        0.2070, 0.1670, 0.1992, 0.1875, 0.2402, 0.2080, 0.5000, 0.2314, 0.1777,\n",
      "        0.1934, 0.1670, 0.1973, 0.1553, 0.1836, 0.2090, 0.1523, 0.1670, 0.1768,\n",
      "        0.1855, 0.3691, 0.1777, 0.1699, 0.1660, 0.1875, 0.1689, 0.2100, 0.2178,\n",
      "        0.1572, 0.1826, 0.1934, 0.1514, 0.1943, 0.2275, 0.5078, 0.1855, 0.1504,\n",
      "        0.5625, 0.1934, 0.1709, 0.2139, 0.1631, 0.2041, 0.2109, 0.2227, 0.1748,\n",
      "        0.2119, 0.1748, 0.2949, 0.2012, 0.1807, 0.1719, 0.2070, 0.3184, 0.1924,\n",
      "        0.2715, 0.3125, 0.2100, 0.1660, 0.2188, 0.1650, 0.2021, 0.0317, 0.1934,\n",
      "        0.1865, 0.1699, 0.1787, 0.1631, 0.2080, 0.2539, 0.7773, 0.1641, 0.2393,\n",
      "        0.1631, 0.1611, 0.1543, 0.1689, 0.1768, 0.1816, 0.1777, 0.2695, 0.1865,\n",
      "        0.1660, 0.1582, 0.1611, 0.1543, 0.4121, 0.1455, 0.1846, 0.1738, 0.1553,\n",
      "        0.1836, 0.2021, 0.1875, 0.2275, 0.2344, 0.1699, 0.1836, 0.4336, 0.1650,\n",
      "        0.1729, 0.1748, 0.1592, 0.1865, 0.1631, 0.1992, 0.1816, 0.1650, 0.1904,\n",
      "        3.7031, 0.2080, 0.1885, 0.1680, 0.1553, 0.1592, 0.3887, 0.1982, 0.2148,\n",
      "        0.1787, 0.2520, 0.2148, 0.2314, 0.1543, 0.2207, 0.2070, 0.1504, 0.1836,\n",
      "        0.1729, 0.2412, 0.1543, 0.1943, 0.2178, 0.5547, 0.1807, 0.1543, 0.1875,\n",
      "        0.1484, 0.2227, 0.1807, 0.2383, 0.2109, 0.4648, 0.1875, 0.2393, 0.3164,\n",
      "        0.1719, 0.2129, 0.1836, 0.2266, 0.1904, 0.1855, 0.1729, 0.2383, 0.2031,\n",
      "        0.2227, 0.1641, 0.2227, 0.2363, 0.1738, 0.1660, 0.2441, 0.1719, 0.1729,\n",
      "        0.2021, 0.3652, 0.4199, 0.1562, 0.6953, 0.1641, 0.1787, 0.2598, 0.1602,\n",
      "        0.3027, 0.2080, 0.2109, 0.2178, 0.0090, 2.9062, 0.1748, 0.3438, 0.1885,\n",
      "        0.2139, 0.2559, 0.1748, 0.2021, 0.2275, 0.1621, 0.1787, 0.2109, 0.2773,\n",
      "        0.1650, 0.2500, 0.1670, 0.1416, 0.2852, 0.1533, 0.1631, 0.1914, 0.1484,\n",
      "        0.1445, 0.1689, 0.1641, 0.1514, 0.1729, 0.1777, 0.2051, 0.1797, 0.2002,\n",
      "        0.1934, 0.1699, 0.3848, 0.1816, 0.5508, 0.1865, 0.3418, 0.1709, 0.1807,\n",
      "        0.2207, 0.1992, 0.1543, 0.1660, 0.6562, 0.2002, 0.1797, 0.1875, 0.1699,\n",
      "        0.1943, 0.1670, 0.1875, 0.1943, 0.1846, 0.3516, 0.1963, 0.1895, 0.1895,\n",
      "        0.1855, 0.1748, 0.3281, 0.1758, 0.1768, 0.1924, 0.2227, 0.2207, 0.2314,\n",
      "        0.2559, 0.1768, 0.3340, 3.5938, 0.1709, 0.2480, 0.2178, 0.1895, 0.2139,\n",
      "        0.1826, 0.2871, 0.2090, 0.1631, 0.1855, 0.3555, 0.2041, 0.4043, 0.1572,\n",
      "        0.1670, 0.1631, 0.2041, 0.1738, 0.1914, 0.2100, 0.2021, 0.6250, 0.1914,\n",
      "        0.2266, 0.1914, 0.2207, 0.2100, 0.2275, 0.2314, 0.3516, 0.1963, 0.2168,\n",
      "        0.1807, 0.1582, 0.1826, 0.1621, 0.2002, 0.2012, 0.1826, 0.2012, 0.1855,\n",
      "        0.1611, 0.2041, 0.1738, 0.1758, 0.1768, 0.1934, 0.1621, 0.2178, 0.1719,\n",
      "        0.2266, 0.2031, 0.1836, 0.2334, 0.1514, 0.2070, 0.1572, 0.1953, 0.4082,\n",
      "        0.1729, 0.1748, 0.2109, 0.2021, 0.1660, 0.1914, 0.2070, 0.1982, 0.1631,\n",
      "        0.2100, 0.2324, 0.2031, 0.1582, 0.1709, 0.3438, 0.6250, 0.1680, 0.2559,\n",
      "        0.1992, 0.1797, 0.1836, 0.7500, 0.2041, 0.1982, 0.2471, 0.2012, 0.2441,\n",
      "        0.1709, 0.1953, 0.3828, 0.1582, 0.2051, 0.4863, 0.2236, 0.1875, 0.1709,\n",
      "        0.2188, 0.1895, 0.1631, 0.1729, 0.2773, 0.1982, 0.1797, 0.1787, 0.1846,\n",
      "        0.1787, 0.2383, 0.1973, 0.3340, 0.2119, 0.3535, 0.1660, 0.3320, 0.2559,\n",
      "        0.1729, 0.2559, 0.2930, 0.1885, 0.1992, 0.2988, 0.2109, 0.1992, 0.1719,\n",
      "        0.2422, 0.1738, 0.4219, 0.2041, 0.2617, 0.1768, 0.1914, 0.1475, 0.2090,\n",
      "        0.2129, 0.1523, 0.2373], device='cuda:0', requires_grad=True)\n",
      "verbalizer.label_words_ids\n",
      "Parameter containing:\n",
      "tensor([[[4273]],\n",
      "\n",
      "        [[ 150]],\n",
      "\n",
      "        [[2087]]], device='cuda:0')\n",
      "verbalizer.words_ids_mask\n",
      "Parameter containing:\n",
      "tensor([[[1]],\n",
      "\n",
      "        [[1]],\n",
      "\n",
      "        [[1]]], device='cuda:0')\n",
      "verbalizer.label_words_mask\n",
      "Parameter containing:\n",
      "tensor([[1],\n",
      "        [1],\n",
      "        [1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 查看prompt_model里的参数\n",
    "for n,p in prompt_model.named_parameters():\n",
    "    print(n)\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d132a421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, average loss: 0.8800722360610962\n",
      "Epoch 1, average loss: 0.031409017741680145\n",
      "Epoch 2, average loss: 0.0015051163209136575\n",
      "Epoch 3, average loss: 0.0034735492590698414\n",
      "Epoch 4, average loss: 0.00035082946124020964\n",
      "Epoch 5, average loss: 0.00021342094260035083\n",
      "Epoch 6, average loss: 0.0001429102776455693\n",
      "Epoch 7, average loss: 0.0002875708451028913\n",
      "Epoch 8, average loss: 0.00012032450831611641\n",
      "Epoch 9, average loss: 5.591935769189149e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 56it [00:00, 274.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9464285714285714\n"
     ]
    }
   ],
   "source": [
    "# Now the training is standard\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "# it's always good practice to set no decay to biase and LayerNorm parameters\n",
    "# 避免过拟合\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-4)\n",
    "\n",
    "for epoch in range(10):\n",
    "    tot_loss = 0\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "        tot_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if step %100 ==1:\n",
    "            print(\"Epoch {}, average loss: {}\".format(epoch, tot_loss/(step+1)), flush=True)\n",
    "\n",
    "# Evaluate\n",
    "validation_dataloader = PromptDataLoader(dataset=dataset[\"validation\"], template=mytemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
    "    batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")\n",
    "\n",
    "allpreds = []\n",
    "alllabels = []\n",
    "for step, inputs in enumerate(validation_dataloader):\n",
    "    if use_cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    logits = prompt_model(inputs)\n",
    "    labels = inputs['label']\n",
    "    alllabels.extend(labels.cpu().tolist())\n",
    "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5daea4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 0, 1, 1, 0, 0, 0, 2, 1, 1, 1, 1, 0, 1, 2, 1, 1, 0, 0, 0, 1, 0, 0, 1, 2, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 2, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(allpreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "75237b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:  torch.Size([4, 256])\n",
      "['▁B', ':', '▁U', 'h', '-', 'huh', '.', '▁It', ',', '▁I', '▁mean', ',', '▁I', '▁don', \"'\", 't', '▁know', ',', '▁I', '▁don', \"'\", 't', '▁think', '▁George', '▁Bush', '▁will', '▁make', '▁the', '▁American', '▁people', '▁happy', '▁with', '▁nine', 't', 'y', '-', 's', 'even', '▁cent', 's', '▁', 'a', '▁week', '.', '▁A', ':', '▁No', ',', '▁no', ',', '▁not', '▁at', '▁all', '.', '▁B', ':', '▁I', '▁just', '▁don', \"'\", 't', '▁think', '▁it', '▁was', '▁', 'a', '▁well', '▁thought', '▁out', '▁incentive', '.', '▁Question', ':', '▁it', '▁was', '▁', 'a', '▁well', '▁thought', '▁out', '▁incentive', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.3274e+01, -2.3842e-06, -1.4195e+01],\n",
      "        [-1.1892e+01, -9.4176e-06, -1.2850e+01],\n",
      "        [-1.3240e+01, -2.1458e-06, -1.4808e+01],\n",
      "        [-1.4599e+01, -7.1526e-07, -1.5262e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([1, 1, 1, 1], device='cuda:0')\n",
      "pred: tensor(1, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁Anna', '▁looked', '▁at', '▁Peter', '▁again', '▁and', '▁said', '▁to', '▁herself', '▁in', '▁', 'a', '▁guilty', '▁whisper', ',', '▁', '<unk>', 'W', 'ill', '▁', 'he', '▁become', '▁even', '▁more', '▁difficult', '?', \"'\", \"'\", '▁She', '▁wondered', '▁', 'if', '▁', 'a', '▁stranger', '▁could', '▁tell', '▁that', '▁', 'he', '▁was', '▁difficult', ',', '▁just', '▁by', '▁looking', '▁at', '▁him', '.', '▁Would', '▁such', '▁', 'a', '▁person', '▁watching', '▁Peter', '▁now', '▁reading', '▁the', '▁prayers', '▁of', '▁Rit', 'e', '▁B', '▁in', '▁his', '▁level', '▁pleasant', '▁voice', '▁notice', '▁that', '▁', 're', 'sent', 'ment', '▁lay', '▁like', '▁his', '▁blood', '▁just', '▁under', '▁his', '▁skin', '▁because', '▁the', '▁life', '▁', 'he', '▁had', '▁chosen', '▁had', '▁not', '▁turned', '▁out', '▁as', '▁', 'he', '▁had', '▁expected', '▁it', '▁to', '?', '▁Question', ':', '▁', 're', 'sent', 'ment', '▁lay', '▁just', '▁under', '▁Peter', \"'\", 's', '▁skin', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.0491e-04, -1.0350e+01, -9.5250e+00],\n",
      "        [-3.2187e-05, -1.2037e+01, -1.0548e+01],\n",
      "        [-1.4526e+01, -5.9605e-07, -1.6453e+01],\n",
      "        [-1.4130e+01, -1.4305e-06, -1.4117e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 0, 1, 1], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁', 'Nevertheless', ',', '▁her', '▁heart', '▁', 's', 'ank', '▁at', '▁the', '▁thought', '▁of', '▁spending', '▁an', '▁evening', '▁with', '▁him', '▁in', '▁his', '▁present', '▁state', '▁of', '▁mind', ',', '▁and', '▁she', '▁was', '▁', 'tempted', '▁to', '▁in', 'vent', '▁', 'a', '▁so', 're', '▁throat', '▁in', '▁order', '▁to', '▁get', '▁out', '▁of', '▁it', '.', '▁But', '▁', 'he', '▁had', '▁been', '▁very', '▁helpful', '▁over', '▁the', '▁Pu', 'd', 'de', 'p', 'hat', '▁business', ',', '▁she', '▁admitted', '▁to', '▁herself', ',', '▁and', '▁his', '▁mood', 's', '▁were', '▁unpredictable', '▁', '-', '▁', 'he', '▁might', '▁be', '▁on', '▁top', '▁of', '▁the', '▁world', '▁by', '▁the', '▁time', '▁', 'he', '▁arrived', '▁at', '▁the', '▁cinema', '.', '▁', '<unk>', 'I', \"'\", 'm', '▁still', '▁keen', '▁', 'if', '▁you', '▁are', '▁', \"'\", \"'\", '▁she', '▁said', '▁bright', 'ly', '▁pretend', 'ing', '▁she', '▁had', '▁not', '▁noticed', '▁anything', '▁was', '▁amis', 's', '.', '▁Question', ':', '▁something', '▁was', '▁amis', 's', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.4747e-04, -1.0603e+01, -9.0064e+00],\n",
      "        [-1.2272e+01, -1.1563e-05, -1.1887e+01],\n",
      "        [-2.1339e-05, -1.2645e+01, -1.0916e+01],\n",
      "        [-1.1729e+01, -1.0252e-05, -1.3012e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 1, 0, 1], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁Joseph', '▁spa', 't', '▁and', '▁', 's', 'plu', 't', 'tered', '▁blood', '.', '▁He', '▁had', '▁lost', '▁the', '▁two', '▁centre', '▁top', '▁teeth', '▁and', '▁with', '▁the', '▁tip', '▁of', '▁his', '▁tongue', '▁', 'he', '▁could', '▁feel', '▁that', '▁the', '▁two', '▁on', '▁either', '▁side', '▁were', '▁also', '▁loose', '.', '▁Question', ':', '▁the', '▁two', '▁teeth', '▁on', '▁either', '▁side', '▁were', '▁also', '▁loose', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:  tensor([[-3.6956e-05, -1.1279e+01, -1.0627e+01],\n",
      "        [-1.2712e+01, -5.2452e-06, -1.2995e+01],\n",
      "        [-1.1394e+01, -1.6213e-05, -1.2220e+01],\n",
      "        [-4.6492e-06, -1.3113e+01, -1.2839e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 1, 1, 0], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁It', '▁seemed', '▁impossible', '▁that', '▁anyone', '▁could', '▁endure', '▁such', '▁pain', '▁for', '▁so', '▁long', ',', '▁but', '▁at', '▁last', '▁the', '▁doors', '▁of', '▁the', '▁Renault', '▁', 's', 'l', 'am', 'med', '▁and', '▁there', '▁was', '▁comparativ', 'e', '▁silence', '.', '▁The', '▁engine', '▁was', '▁started', '▁up', ',', '▁rev', 'ving', '▁violent', 'ly', '▁as', '▁the', '▁car', '▁was', '▁turned', '▁round', '▁on', '▁the', '▁narrow', '▁road', '.', '▁John', '▁could', '▁tell', '▁that', '▁it', '▁was', '▁being', '▁driven', '▁back', '▁up', '▁the', '▁hill', '▁towards', '▁Put', 'n', 'a', '.', '▁Question', ':', '▁the', '▁car', '▁was', '▁being', '▁driven', '▁back', '▁up', '▁the', '▁hill', '▁towards', '▁Put', 'n', 'a', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-4.7684e-06, -1.3735e+01, -1.2497e+01],\n",
      "        [-1.1490e+01, -1.6570e-05, -1.1976e+01],\n",
      "        [-1.0419e-04, -1.0016e+01, -9.7306e+00],\n",
      "        [-2.3246e-05, -1.1952e+01, -1.0996e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 1, 0, 0], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁Country', '▁churches', '▁were', '▁never', '▁locked', '.', '▁You', '▁could', '▁wander', '▁in', '▁at', '▁any', '▁time', '.', '▁Perhaps', '▁Cro', 'm', 'well', '▁when', '▁', 'he', '▁passed', '▁also', '▁found', '▁the', '▁door', '▁of', '▁Cold', 'ingham', '▁Prior', 'y', '▁locked', '▁and', '▁decided', '▁that', '▁', 'he', '▁would', '▁get', '▁in', '▁anyway', '▁even', '▁', 'if', '▁it', '▁meant', '▁', 'removing', '▁', 'a', '▁whole', '▁wall', '▁in', '▁order', '▁to', '▁do', '▁so', '.', '▁Question', ':', '▁Cro', 'm', 'well', '▁would', '▁get', '▁in', '▁anyway', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.0932e-04, -9.3000e+00, -1.0932e+01],\n",
      "        [-7.7485e+00, -9.4352e+00, -5.1130e-04],\n",
      "        [-8.7384e-05, -1.1484e+01, -9.4699e+00],\n",
      "        [-3.0701e-04, -1.0818e+01, -8.1561e+00]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 2, 0, 0], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁B', ':', '▁Right', '.', '▁And', '▁I', \"'\", 'm', '▁sure', '▁that', '▁would', '▁make', '▁', 'a', '▁big', '▁difference', ',', '▁too', '.', '▁You', '▁know', ',', '▁you', \"'\", 've', '▁got', ',', '▁A', ':', '▁Yeah', '.', '▁Well', ',', '▁what', '▁about', '▁', 'a', '▁voluntary', '▁program', '?', '▁Do', '▁you', '▁think', '▁that', '▁would', '▁be', '▁', 'a', '▁good', '▁idea', '?', '▁Question', ':', '▁', 'a', '▁voluntary', '▁program', '▁would', '▁be', '▁', 'a', '▁good', '▁idea', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-9.2971e+00, -1.0820e+01, -1.1171e-04],\n",
      "        [-1.2899e-04, -9.6597e+00, -9.6375e+00],\n",
      "        [-8.9888e-05, -9.9139e+00, -1.0115e+01],\n",
      "        [-1.3605e+01, -1.4305e-06, -1.5689e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([2, 0, 0, 1], device='cuda:0')\n",
      "pred: tensor(2, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁A', ':', '▁They', '▁might', '▁be', ',', '▁but', '▁not', '▁at', '▁the', '▁human', '▁factors', '▁level', '.', '▁they', \"'\", 're', ',', '▁B', ':', '▁Well', ',', '▁I', '▁heard', '▁it', '▁on', '▁the', '▁news', '▁today', ',', '▁I', '▁could', '▁swear', '▁it', '▁was', '▁IBM', '.', '▁Question', ':', '▁it', '▁was', '▁IBM', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.1563e-05, -1.1802e+01, -1.2420e+01],\n",
      "        [-2.1728e-04, -1.0455e+01, -8.5771e+00],\n",
      "        [-1.2184e-04, -1.1289e+01, -9.1218e+00],\n",
      "        [-1.3841e+01, -1.3113e-06, -1.4916e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 0, 0, 1], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁Je', 'd', '▁wondered', '.', '▁He', '▁', \"'\", 'd', '▁scarce', 'ly', '▁set', '▁eyes', '▁on', '▁him', '▁since', '▁the', '▁night', '▁they', '▁', \"'\", 'd', '▁had', '▁dinner', '▁together', '▁at', '▁the', '▁house', '▁in', '▁West', 'wood', '.', '▁Nobody', '▁had', '▁mentioned', '▁him', '▁either', '▁and', '▁Je', 'd', '▁didn', \"'\", 't', '▁feel', '▁', 'he', '▁should', '▁ask', '.', '▁Question', ':', '▁Je', 'd', '▁should', '▁ask', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.1549e+01, -1.1087e-05, -1.3422e+01],\n",
      "        [-4.9831e-05, -1.0181e+01, -1.1341e+01],\n",
      "        [-1.2796e+01, -2.9802e-06, -1.5362e+01],\n",
      "        [-1.1731e-04, -9.1149e+00, -1.1830e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([1, 0, 1, 0], device='cuda:0')\n",
      "pred: tensor(1, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁', '<unk>', 'Oh', ',', '▁very', '▁well', ',', \"'\", \"'\", '▁', 'he', '▁said', '▁wear', 'ily', '.', '▁He', '▁might', '▁have', '▁known', '▁that', '▁it', '▁was', '▁useless', '▁to', '▁argue', '▁with', '▁Mc', 'All', 'ister', '▁', '-', '▁her', '▁tongue', '▁was', '▁as', '▁long', '▁as', '▁her', '▁will', '▁was', '▁strong', '.', '▁Question', ':', '▁it', '▁was', '▁useless', '▁to', '▁argue', '▁with', '▁Mc', 'All', 'ister', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:  tensor([[-4.6254e-05, -1.0225e+01, -1.1517e+01],\n",
      "        [-1.3861e+01, -1.0729e-06, -1.5554e+01],\n",
      "        [-2.0981e-05, -1.2154e+01, -1.1062e+01],\n",
      "        [-7.9158e-05, -1.1160e+01, -9.6419e+00]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 1, 0, 0], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁Just', '▁when', '▁you', '▁think', '▁you', '▁', \"'\", 've', '▁got', '▁it', '▁straight', ',', '▁along', '▁comes', '▁the', '▁Fool', '▁with', '▁his', '▁', 'pig', \"'\", 's', '▁bladder', '▁and', '▁who', 'p', 's', '▁you', '▁on', '▁the', '▁nose', '.', '▁By', '▁the', '▁way', ',', '▁I', \"'\", 'm', '▁no', '▁idiot', '.', '▁I', '▁could', '▁tell', '▁G', 'illian', '▁and', '▁Stuart', '▁weren', \"'\", 't', '▁thrilled', '▁to', '▁see', '▁me', '▁at', '▁the', '▁airport', '.', '▁Question', ':', '▁G', 'illian', '▁and', '▁Stuart', '▁weren', \"'\", 't', '▁thrilled', '▁to', '▁see', '▁her', '▁at', '▁the', '▁airport', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.2184e-04, -9.4221e+00, -1.0104e+01],\n",
      "        [-1.1398e+01, -3.0160e-05, -1.0875e+01],\n",
      "        [-1.3716e+01, -1.1921e-06, -1.5856e+01],\n",
      "        [-1.2191e+01, -6.6757e-06, -1.3330e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 1, 1, 1], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁A', ':', '▁So', ',', '▁we', \"'\", 're', '▁comparable', '.', '▁B', ':', '▁Yeah', '.', '▁A', ':', '▁As', '▁', 'a', '▁matter', '▁of', '▁fact', ',', '▁I', '▁just', '▁paid', '▁my', '▁Richard', 'son', '▁taxes', '▁because', '▁I', '▁live', '▁in', '▁Richard', 'son', '▁and', '▁supplement', 'e', 'd', '▁the', '▁Robin', '▁Hood', 's', '▁very', '▁thoroughly', ',', '▁I', '▁think', '.', '▁B', ':', '▁Yeah', ',', '▁I', '▁think', '▁Yeah', ',', '▁we', '▁have', '▁got', '▁it', '▁on', '▁the', '▁line', ',', '▁don', \"'\", 't', '▁we', '.', '▁Question', ':', '▁they', '▁have', '▁got', '▁it', '▁on', '▁the', '▁line', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-2.5794e-04, -8.9946e+00, -8.9188e+00],\n",
      "        [-8.8338e-05, -9.7622e+00, -1.0388e+01],\n",
      "        [-9.2963e+00, -1.2244e-04, -1.0394e+01],\n",
      "        [-1.3238e+01, -1.9074e-06, -1.6391e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 0, 1, 1], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁', '<unk>', 'C', 'le', 'ver', \"'\", \"'\", '.', '▁K', 'lug', '▁means', '▁', '<unk>', 'cle', 'ver', \"'\", \"'\", '.', '▁Would', '▁you', '▁say', '▁that', '▁A', 'bie', '▁was', '▁clever', '?', '▁Question', ':', '▁A', 'bie', '▁was', '▁clever', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-8.4691e+00, -9.8290e+00, -2.6379e-04],\n",
      "        [-6.7713e-05, -1.1150e+01, -9.8396e+00],\n",
      "        [-1.3656e+01, -1.5497e-06, -1.4842e+01],\n",
      "        [-1.2697e+01, -4.2915e-06, -1.3573e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([2, 0, 1, 1], device='cuda:0')\n",
      "pred: tensor(2, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁A', ':', '▁I', ',', '▁that', '▁would', '▁have', '▁been', '▁stupid', ',', '▁B', ':', '▁Yeah', '.', '▁A', ':', '▁and', '▁I', '▁don', \"'\", 't', '▁think', '▁we', '▁did', '▁it', '.', '▁Everything', '▁else', '▁we', '▁handled', '▁in', '▁this', '▁seemed', '▁to', '▁be', '▁perfectly', '▁right', '.', '▁I', '▁don', \"'\", 't', '▁think', '▁they', '▁would', '▁have', '▁done', '▁that', '.', '▁Question', ':', '▁they', '▁would', '▁have', '▁done', '▁that', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.3797e+01, -1.3113e-06, -1.5224e+01],\n",
      "        [-1.3736e+01, -1.4305e-06, -1.4770e+01],\n",
      "        [-1.0729e+01, -2.3961e-05, -1.3080e+01],\n",
      "        [-4.3035e-05, -1.2787e+01, -1.0118e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([1, 1, 1, 0], device='cuda:0')\n",
      "pred: tensor(1, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁A', ':', '▁The', '▁one', '▁thing', '▁I', '▁sometimes', '▁wonder', '▁about', ',', '▁um', ',', '▁in', '▁civil', '▁cases', '▁is', ',', '▁', 'u', 'h', ',', '▁whether', ',', '▁especially', '▁sort', '▁of', '▁in', ',', '▁', 'u', 'h', ',', '▁maybe', '▁like', '▁product', '▁liability', ',', '▁or', '▁medical', '▁malpractice', ',', '▁where', '▁there', \"'\", 's', ',', '▁um', ',', '▁sort', '▁of', '▁', 'a', '▁very', '▁technical', '▁decision', '▁to', '▁be', '▁made', '▁sometimes', '▁B', ':', '▁Yes', '.', '▁A', ':', '▁you', '▁know', ',', '▁it', \"'\", 's', '▁not', '▁just', '▁', 'a', '▁matter', '▁um', ',', '▁of', ',', '▁you', '▁know', ',', '▁did', '▁this', '▁guy', '▁', 'rip', '▁off', '▁this', '▁guy', ',', '▁and', '▁it', \"'\", 's', '▁just', '▁', 'a', '▁matter', '▁of', '▁', 'interpreting', '▁', 'a', '▁contract', ',', '▁it', \"'\", 's', '▁sort', '▁of', '▁', 'a', '▁matter', '▁of', ',', '▁um', ',', '▁you', '▁know', ',', '▁sometimes', '▁getting', '▁into', '▁very', '▁technical', '▁issues', ',', '▁and', '▁I', '▁wonder', '▁um', ',', '▁', 'if', '▁the', '▁system', '▁works', '▁adequately', '▁in', '▁', 'educating', '▁the', '▁jur', 'or', 's', '▁about', ',', '▁', 'u', 'h', ',', '▁whatever', ',', '▁um', ',', '▁you', '▁know', ',', '▁issue', '▁is', '▁under', '▁discussion', '.', '▁B', ':', '▁I', '▁don', \"'\", 't', '▁think', '▁that', '▁they', '▁educate', '▁them', '▁enough', '▁to', '▁really', '▁know', '▁what', \"'\", 's', '▁going', '▁on', '.', '▁Question', ':', '▁they', '▁educate', '▁the', '▁jur', 'or', 's', '▁enough', '▁to', '▁really', '▁know', '▁what', \"'\", 's', '▁going', '▁on', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.5064e+01, -5.9605e-07, -1.5025e+01],\n",
      "        [-2.5273e-05, -1.2088e+01, -1.0836e+01],\n",
      "        [-1.0214e+01, -6.7594e-05, -1.0383e+01],\n",
      "        [-1.3697e+01, -2.2650e-06, -1.3681e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([1, 0, 1, 1], device='cuda:0')\n",
      "pred: tensor(1, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁I', '▁spent', '▁just', '▁over', '▁an', '▁hour', '▁with', '▁Patterson', '▁which', ',', '▁I', '▁found', '▁out', '▁later', ',', '▁turned', '▁out', '▁to', '▁be', '▁another', '▁first', '.', '▁Time', '▁is', '▁money', '▁in', '▁the', '▁City', '▁and', '▁few', '▁people', '▁are', '▁worth', '▁an', '▁hour', '▁', 'unless', '▁it', \"'\", 's', '▁over', '▁lunch', '▁and', '▁only', '▁then', '▁', 'if', '▁you', \"'\", 're', '▁involved', '▁in', '▁', 'a', '▁take', 'over', '▁bid', '.', '▁It', '▁was', '▁also', '▁I', '▁learned', '▁one', '▁of', '▁the', '▁few', '▁occasions', '▁anyone', '▁at', '▁P', 'KB', '▁could', '▁remember', '▁that', '▁Patterson', '▁had', '▁', 'a', '▁meeting', '▁with', '▁his', '▁door', '▁shut', '▁and', '▁nobody', '▁got', '▁fired', '.', '▁Question', ':', '▁Patterson', '▁had', '▁', 'a', '▁meeting', '▁with', '▁his', '▁door', '▁shut', '▁and', '▁nobody', '▁got', '▁fired', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:  tensor([[-1.7047e-05, -1.2677e+01, -1.1182e+01],\n",
      "        [-1.2875e-05, -1.2688e+01, -1.1532e+01],\n",
      "        [-1.4245e+01, -1.6689e-06, -1.3844e+01],\n",
      "        [-3.8267e-05, -1.1655e+01, -1.0428e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 0, 1, 0], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁I', '▁can', \"'\", 't', '▁afford', '▁to', '▁get', '▁bo', 'gged', '▁down', '▁in', '▁the', '▁', 'weed', 's', '.', '▁But', '▁at', '▁least', '▁you', '▁know', '▁she', '▁did', '▁leave', '.', '▁Maybe', '▁', 'a', '▁coincidence', '▁maybe', '▁the', '▁two', '▁girls', '▁talked', '▁on', '▁the', '▁phone', '▁decided', '▁they', '▁', \"'\", 'd', '▁both', '▁had', '▁enough', '.', '▁Question', ':', '▁the', '▁two', '▁girls', '▁had', '▁both', '▁had', '▁enough', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-7.2350e+00, -9.6017e+00, -7.8888e-04],\n",
      "        [-9.3396e+00, -1.0351e+01, -1.1981e-04],\n",
      "        [-1.4038e+01, -1.1921e-06, -1.4657e+01],\n",
      "        [-2.9803e-05, -1.1378e+01, -1.0903e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([2, 2, 1, 0], device='cuda:0')\n",
      "pred: tensor(2, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁B', ':', '▁Oh', ',', '▁I', '▁see', '.', '▁A', ':', '▁um', ',', '▁and', '▁I', '▁think', '▁I', \"'\", 'm', '▁getting', '▁', 'a', '▁better', '▁ca', 'liber', '▁of', '▁student', '▁at', '▁the', '▁private', '▁school', ',', '▁because', '▁I', '▁think', '▁their', '▁parents', '▁pay', '▁more', ',', '▁and', '▁I', '▁think', '▁the', '▁kids', '▁are', '▁', 'a', '▁little', '▁bit', '▁more', '▁challenged', ',', '▁because', '▁their', '▁parents', '▁are', '▁probably', '▁college', '▁educate', 'd', ',', '▁where', '▁at', '▁the', '▁public', '▁school', ',', '▁I', '▁don', \"'\", 't', '▁think', '▁as', '▁many', '▁parents', '▁are', '▁college', '▁educate', 'd', ',', '▁Question', ':', '▁as', '▁many', '▁parents', '▁are', '▁college', '▁educate', 'd', '▁at', '▁the', '▁public', '▁school', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.0831e+01, -3.8028e-05, -1.0912e+01],\n",
      "        [-5.1500e-05, -1.1452e+01, -1.0104e+01],\n",
      "        [-1.1655e+01, -1.0490e-05, -1.3192e+01],\n",
      "        [-1.2658e+01, -3.5763e-06, -1.4703e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([1, 0, 1, 1], device='cuda:0')\n",
      "pred: tensor(1, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁And', '▁why', '▁bother', '▁to', '▁write', '▁anyway', '?', '▁What', '▁was', '▁there', '▁to', '▁say', '?', '▁Mary', '▁had', '▁some', '▁vague', '▁idea', '▁that', '▁Adam', \"'\", 's', '▁parents', '▁might', '▁suspect', '▁', 'he', '▁was', '▁down', '▁here', '▁and', '▁come', '▁to', '▁see', '▁him', '.', '▁Question', ':', '▁Adam', '▁was', '▁down', '▁here', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-7.4270e-05, -1.0250e+01, -1.0154e+01],\n",
      "        [-6.3181e-06, -1.2863e+01, -1.2522e+01],\n",
      "        [-1.3928e+01, -1.1921e-06, -1.5190e+01],\n",
      "        [-2.1696e-05, -1.2674e+01, -1.0893e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 0, 1, 0], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁B', ':', '▁So', ',', '▁let', \"'\", 's', '▁talk', '▁about', '▁the', ',', '▁', 'u', 'h', ',', '▁wonderful', '▁abuse', 's', '▁in', '▁the', '▁State', '▁of', '▁Pennsylvania', '▁of', '▁personal', '▁property', '▁taxes', '▁where', 'by', '▁you', '▁can', '▁purchase', '▁something', '▁mail', '▁order', '▁and', '▁after', '▁the', '▁fact', ',', '▁the', '▁State', '▁of', '▁Pennsylvania', '▁can', '▁find', '▁out', '▁about', '▁it', '▁and', '▁send', '▁you', '▁', 'a', '▁bill', '▁for', '▁the', '▁sales', '▁tax', '▁appropriate', '▁to', '▁that', '▁item', '▁that', '▁you', '▁purchased', '▁as', '▁well', '▁as', '▁interest', '▁and', '▁penalties', '▁from', '▁the', '▁time', '▁that', '▁you', '▁bought', '▁it', '.', '▁What', '▁do', '▁you', '▁think', '?', '▁I', 's', '▁Pennsylvania', '▁kind', '▁of', '▁out', '▁of', '▁line', '▁there', '?', '▁A', ':', '▁Well', ',', '▁actually', ',', '▁I', '▁do', '▁', 'n', \"'\", 't', '▁think', '▁they', \"'\", 're', '▁out', '▁of', '▁line', '.', '▁Question', ':', '▁they', \"'\", 're', '▁out', '▁of', '▁line', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.1659e+01, -1.3828e-05, -1.2160e+01],\n",
      "        [-1.8597e-05, -1.1114e+01, -1.2521e+01],\n",
      "        [-7.8920e-05, -1.2843e+01, -9.4810e+00],\n",
      "        [-1.3321e+01, -2.2650e-06, -1.4267e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([1, 0, 0, 1], device='cuda:0')\n",
      "pred: tensor(1, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁B', ':', '▁All', '▁right', ',', '▁well', '.', '▁A', ':', '▁Um', ',', '▁short', '▁term', ',', '▁I', '▁don', \"'\", 't', '▁think', '▁anything', \"'\", 's', '▁going', '▁to', '▁be', '▁done', '▁about', '▁it', '▁or', '▁probably', '▁should', '▁be', '▁done', '▁about', '▁it', '.', '▁Question', ':', '▁something', \"'\", 's', '▁going', '▁to', '▁be', '▁done', '▁about', '▁it', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.3923e+01, -3.2187e-06, -1.2993e+01],\n",
      "        [-1.0954e+01, -4.1843e-05, -1.0625e+01],\n",
      "        [-1.3500e+01, -3.5763e-06, -1.3028e+01],\n",
      "        [-4.3870e-05, -1.0313e+01, -1.1448e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([1, 1, 1, 0], device='cuda:0')\n",
      "pred: tensor(1, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁A', ':', '▁And', ',', '▁', 'u', 'h', ',', '▁I', '▁got', '▁to', '▁stay', '▁home', '▁with', '▁my', '▁kids', ',', '▁which', '▁I', '▁really', '▁wanted', '▁to', '▁do', ',', '▁but', '▁now', '▁I', '▁could', '▁not', '▁go', '▁back', '▁and', '▁do', '▁it', '.', '▁B', ':', '▁Yeah', '.', '▁A', ':', '▁I', '▁really', '▁couldn', \"'\", 't', ',', '▁I', '▁don', \"'\", 't', '▁think', '▁I', '▁could', '▁stay', '▁home', '▁all', '▁the', '▁time', '▁and', '▁do', '▁nothing', '.', '▁Question', ':', '▁', 'he', '▁could', '▁stay', '▁home', '▁all', '▁the', '▁time', '▁and', '▁do', '▁nothing', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:  tensor([[-1.3585e+01, -1.5497e-06, -1.5157e+01],\n",
      "        [-1.1540e-04, -9.9900e+00, -9.5738e+00],\n",
      "        [-2.1577e-05, -1.1003e+01, -1.2222e+01],\n",
      "        [-1.2656e+01, -5.1260e-06, -1.3124e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([1, 0, 0, 1], device='cuda:0')\n",
      "pred: tensor(1, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁B', ':', '▁I', '▁did', ',', '▁too', '.', '▁A', ':', '▁I', '▁mean', ',', '▁it', '▁was', '▁just', '▁more', '▁for', '▁my', '▁money', '.', '▁B', ':', '▁Yeah', '.', '▁I', '▁didn', \"'\", 't', '▁think', '▁it', '▁was', '▁too', '▁long', '▁at', '▁all', '.', '▁Question', ':', '▁it', '▁was', '▁too', '▁long', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.5025e+01, -3.5763e-07, -1.6329e+01],\n",
      "        [-4.4466e-05, -1.0443e+01, -1.1087e+01],\n",
      "        [-1.5391e-04, -9.4168e+00, -9.5311e+00],\n",
      "        [-1.3640e+01, -2.2650e-06, -1.3722e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([1, 0, 0, 1], device='cuda:0')\n",
      "pred: tensor(1, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁B', ':', '▁That', \"'\", 's', '▁true', '.', '▁A', ':', '▁So', '.', '▁B', ':', '▁U', 'h', ',', '▁the', '▁other', '▁argument', '▁is', '▁that', '▁the', '▁death', '▁penalty', '▁is', '▁', 'a', '▁de', 'ter', 'rent', '▁and', '▁I', '▁really', '▁don', \"'\", 't', ',', '▁', 'u', 'h', ',', '▁agree', '▁with', '▁that', '.', '▁I', '▁don', \"'\", 't', '▁think', '▁anyone', '▁who', '▁would', '▁commit', '▁', 'u', 'h', ',', '▁', 'a', '▁crime', '▁that', '▁would', '▁get', '▁them', '▁the', '▁death', '▁penalty', '▁would', '▁stop', '▁at', '▁the', '▁moment', '▁and', '▁say', ',', '▁well', ',', '▁I', '▁was', '▁about', '▁to', '▁kill', '▁and', '▁dis', 'member', '▁this', '▁person', '▁but', ',', '▁', 'o', 'h', ',', '▁', 'if', '▁they', '▁catch', '▁me', '▁they', \"'\", 're', '▁going', '▁to', '▁kill', '▁me', '▁so', '▁I', '▁better', '▁not', '▁do', '▁it', '.', '▁I', '▁just', ',', '▁don', \"'\", 't', '▁think', '▁', 'u', 'h', ',', '▁that', '▁it', '▁works', '▁that', '▁way', '.', '▁A', ':', '▁Yeah', '.', '▁I', '▁don', \"'\", 't', '▁think', '▁it', \"'\", 's', '▁done', '.', '▁Question', ':', '▁it', \"'\", 's', '▁done', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.2034e+01, -6.9142e-06, -1.3813e+01],\n",
      "        [-1.0501e+01, -1.7996e-04, -8.7884e+00],\n",
      "        [-1.4719e+01, -1.1921e-06, -1.4014e+01],\n",
      "        [-1.4153e+01, -7.1526e-07, -1.6760e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([1, 1, 1, 1], device='cuda:0')\n",
      "pred: tensor(1, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁Your', '▁honesty', '▁shine', 's', '▁out', '▁of', '▁your', '▁face', ',', '▁my', '▁dar', 'ling', '.', '▁It', '▁is', 'n', \"'\", 't', '▁your', '▁fault', '▁that', '▁', 'c', 'y', 'n', 'ical', '▁men', '▁like', '▁myself', '▁won', \"'\", 't', '▁let', '▁themselves', '▁believe', '▁what', '▁they', '▁see', '!', '▁I', '▁just', '▁wish', '▁you', '▁could', '▁believe', '▁that', '▁Eddie', \"'\", 's', '▁death', '▁was', '▁an', '▁accident', '▁and', '▁nothing', '▁to', '▁do', '▁with', '▁me', '.', '▁Question', ':', '▁Eddie', \"'\", 's', '▁death', '▁was', '▁an', '▁accident', '▁and', '▁nothing', '▁to', '▁do', '▁with', '▁him', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-6.8071e-05, -9.9299e+00, -1.0856e+01],\n",
      "        [-1.3150e+01, -2.3842e-06, -1.4631e+01],\n",
      "        [-1.4329e+01, -1.1921e-06, -1.4308e+01],\n",
      "        [-1.2864e-04, -9.3460e+00, -1.0097e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 1, 1, 0], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁Robert', '▁Er', 'win', ',', '▁president', '▁of', '▁Bio', 'source', ',', '▁called', '▁Plant', '▁Genetic', \"'\", 's', '▁approach', '▁', '<unk>', 'interest', 'ing', \"'\", \"'\", '▁and', '▁', '<unk>', 'nov', 'e', 'l', ',', \"'\", \"'\", '▁and', '▁', '<unk>', 'comp', 'lement', 'ary', '▁rather', '▁than', '▁competitive', '.', \"'\", \"'\", '▁', '<unk>', 'There', '▁is', '▁', 'a', '▁large', '▁market', '▁out', '▁there', '▁hungry', '▁for', '▁hybrid', '▁seeds', ',', \"'\", \"'\", '▁', 'he', '▁said', '.', '▁Mr', '.', '▁Robinson', '▁of', '▁Delta', '▁', '&', '▁Pine', ',', '▁the', '▁seed', '▁producer', '▁in', '▁Scott', ',', '▁Miss', '.', ',', '▁said', '▁Plant', '▁Genetic', \"'\", 's', '▁success', '▁in', '▁creating', '▁genetic', 'ally', '▁engine', 'ered', '▁male', '▁', 'ster', 'ile', 's', '▁doesn', \"'\", 't', '▁automatically', '▁mean', '▁it', '▁would', '▁be', '▁simple', '▁to', '▁create', '▁hybrid', 's', '▁in', '▁all', '▁crops', '.', '▁Question', ':', '▁it', '▁would', '▁be', '▁simple', '▁to', '▁create', '▁hybrid', 's', '▁in', '▁all', '▁crops', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.3479e+01, -2.3842e-06, -1.3829e+01],\n",
      "        [-5.9605e-06, -1.2881e+01, -1.2565e+01],\n",
      "        [-1.5236e+01, -3.5763e-07, -1.5610e+01],\n",
      "        [-5.0068e-06, -1.3180e+01, -1.2699e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([1, 0, 1, 0], device='cuda:0')\n",
      "pred: tensor(1, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁', '<unk>', 'These', '▁', '<unk>', '▁', 'er', 'e', '▁', 's', 'mug', 'gler', 's', '▁is', '▁', 'a', '▁dangerous', '▁bunch', '▁from', '▁wo', 't', '▁I', '▁', \"'\", 've', '▁', '<unk>', '▁', 'e', 'ard', '!', \"'\", \"'\", '▁', '<unk>', 'If', '▁only', '▁we', '▁could', '▁devis', 'e', '▁', 'a', '▁safe', '▁way', '▁of', '▁', 'laying', '▁our', '▁hands', '▁on', '▁all', '▁that', '▁money', ',', \"'\", \"'\", '▁mur', 'm', 'ure', 'd', '▁Pu', 'g', 'wash', ',', '▁', 'whose', '▁greed', '▁was', '▁as', '▁pro', 'verb', 'i', 'al', '▁as', '▁his', '▁co', 'ward', 'ice', '.', '▁And', '▁the', '▁pirate', 's', '▁were', '▁so', '▁busy', '▁discussing', '▁the', '▁problem', '▁and', '▁what', '▁they', '▁would', '▁do', '▁with', '▁the', '▁reward', '▁', 'if', '▁they', '▁won', '▁it', '▁that', '▁they', '▁didn', \"'\", 't', '▁notice', '▁that', '▁they', '▁were', '▁being', '▁observed', '▁from', '▁the', '▁window', '▁above', '▁by', '▁none', '▁other', '▁than', '▁the', '▁new', '▁Mayor', '▁and', '▁his', '▁entour', 'age', '.', '▁Question', ':', '▁the', '▁pirate', 's', '▁were', '▁being', '▁observed', '▁from', '▁the', '▁window', '▁above', '▁by', '▁none', '▁other', '▁than', '▁the', '▁new', '▁Mayor', '▁and', '▁his', '▁entour', 'age', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-6.6757e-06, -1.2413e+01, -1.2839e+01],\n",
      "        [-8.0992e+00, -1.0248e+01, -3.3921e-04],\n",
      "        [-1.3562e+01, -1.7881e-06, -1.4612e+01],\n",
      "        [-1.2811e+01, -3.5763e-06, -1.4037e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 2, 1, 1], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁It', '▁was', '▁Alan', \"'\", 's', '▁idea', '.', '▁He', '▁made', '▁', 'a', '▁', 's', 'our', '▁kind', '▁of', '▁joke', '▁out', '▁of', '▁it', ',', '▁that', '▁they', '▁must', '▁wait', '▁until', '▁their', '▁wedding', '▁night', '.', '▁Carolyn', '▁agreed', '▁because', '▁she', '▁could', '▁see', '▁', 'he', '▁meant', '▁it', '▁although', '▁she', '▁didn', \"'\", 't', '▁understand', '▁why', '.', '▁Question', ':', '▁Alan', '▁meant', '▁it', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:  tensor([[-1.5378e-05, -1.2125e+01, -1.1529e+01],\n",
      "        [-1.2241e+01, -7.0334e-06, -1.3033e+01],\n",
      "        [-1.3784e+01, -1.1921e-06, -1.5892e+01],\n",
      "        [-3.1949e-05, -1.0845e+01, -1.1297e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 1, 1, 0], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁This', '▁was', '▁', 'a', '▁sheer', '▁waste', '▁of', '▁time', '.', '▁He', '▁would', '▁probably', '▁land', '▁and', '▁then', '▁tell', '▁them', '▁to', '▁walk', '▁back', '.', '▁When', '▁she', '▁glance', 'd', '▁at', '▁him', '▁again', '▁', 'he', '▁looked', '▁very', '▁grim', '▁and', '▁she', '▁wondered', '▁', 'if', '▁she', '▁should', '▁have', '▁told', '▁Mit', 'ch', '▁that', '▁', 'he', '▁might', '▁well', '▁have', '▁', 'a', '▁lot', '▁of', '▁opportunity', '▁to', '▁photograph', '▁Spain', '▁', '-', '▁on', '▁foot', '▁as', '▁', 'he', '▁', 'walked', '▁back', '▁to', '▁Mal', 'aga', '.', '▁Question', ':', '▁Mit', 'ch', '▁might', '▁well', '▁have', '▁', 'a', '▁lot', '▁of', '▁opportunity', '▁to', '▁photograph', '▁Spain', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-8.6073e-05, -1.1163e+01, -9.5405e+00],\n",
      "        [-9.7041e-05, -1.0932e+01, -9.4435e+00],\n",
      "        [-1.5178e+01, -2.3842e-07, -1.7245e+01],\n",
      "        [-3.0160e-05, -1.1136e+01, -1.1067e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 0, 1, 0], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁B', ':', '▁She', '▁says', '▁that', '▁when', '▁her', '▁husband', '▁died', '▁', 'o', 'h', ',', '▁that', '▁my', '▁uncle', '▁had', '▁said', '▁that', '▁', 'he', '▁would', '▁never', '▁put', '▁her', '▁in', '▁', 'a', '▁rest', '▁home', '.', '▁So', '▁it', \"'\", 's', '▁kind', '▁of', ',', '▁', 'u', 'h', ',', '▁I', '▁don', \"'\", 't', '▁know', '.', '▁I', '▁mean', ',', '▁I', '▁don', \"'\", 't', '▁think', '▁my', '▁parents', '▁would', '▁but', '▁she', '▁is', '▁getting', '▁pretty', '▁bad', '▁like', '▁she', '▁has', '▁to', '▁have', '▁like', '▁', 'a', '▁little', '▁toilet', '▁right', '▁by', '▁her', '▁bed', '▁and', ',', '▁it', \"'\", 's', ',', '▁A', ':', '▁U', 'h', '-', 'huh', '.', '▁B', ':', '▁and', '▁my', '▁mom', '▁has', '▁to', '▁take', '▁care', '▁of', '▁her', '▁pretty', '▁much', '▁so', '▁it', '▁gets', ',', '▁I', '▁don', \"'\", 't', '▁know', '.', '▁it', \"'\", 's', '▁', 'a', '▁hard', '▁decision', ',', '▁but', '▁I', '▁don', \"'\", 't', '▁think', '▁I', '▁would', '▁do', '▁it', '▁to', '▁my', '▁parents', '▁personally', '.', '▁Question', ':', '▁she', '▁would', '▁do', '▁it', '▁to', '▁her', '▁parents', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.3945e+01, -1.0729e-06, -1.5281e+01],\n",
      "        [-7.5105e-05, -1.1606e+01, -9.6255e+00],\n",
      "        [-1.2786e+01, -3.2187e-06, -1.4661e+01],\n",
      "        [-1.7882e-05, -1.2211e+01, -1.1263e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([1, 0, 1, 0], device='cuda:0')\n",
      "pred: tensor(1, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁B', ':', '▁and', '▁both', '▁of', '▁those', '▁seem', '▁very', '▁easy', '▁to', '▁use', '▁', 'compared', '▁to', '▁D', '▁Base', '.', '▁A', ':', '▁U', 'h', '-', 'huh', '.', '▁Do', '▁you', '▁think', '▁D', '▁Base', '▁is', '▁more', '▁flexible', '▁or', '▁allows', '▁you', '▁to', '▁do', '▁more', '.', '▁Or', '▁do', '▁you', '▁think', '▁the', '▁others', '▁are', '▁pretty', '▁much', '▁compatible', '▁these', '▁days', '?', '▁Question', ':', '▁the', '▁others', '▁are', '▁pretty', '▁much', '▁compatible', '▁these', '▁days', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-8.9156e+00, -1.0843e+01, -1.5391e-04],\n",
      "        [-1.1844e+01, -7.8678e-06, -1.4224e+01],\n",
      "        [-5.2692e-05, -1.2133e+01, -9.9590e+00],\n",
      "        [-4.7804e-05, -1.0229e+01, -1.1362e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([2, 1, 0, 0], device='cuda:0')\n",
      "pred: tensor(2, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁', '<unk>', 'Yes', '?', \"'\", \"'\", '▁', '<unk>', 'N', 'a', 'than', '?', \"'\", \"'\", '▁He', '▁could', '▁tell', '▁it', '▁was', '▁long', '-', 'd', 'i', 'stance', '▁the', '▁line', '▁was', '▁so', '▁gravel', 'ly', '▁and', '▁hollow', '▁but', '▁', 'he', '▁didn', \"'\", 't', '▁recognise', '▁the', '▁voice', '.', '▁Question', ':', '▁the', '▁call', '▁was', '▁long', '-', 'd', 'i', 'stance', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.7519e-04, -8.9596e+00, -9.9715e+00],\n",
      "        [-6.4328e-04, -9.9700e+00, -7.4248e+00],\n",
      "        [-1.3102e+01, -2.3842e-06, -1.4774e+01],\n",
      "        [-1.0873e+01, -2.8491e-05, -1.1555e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 0, 1, 1], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁', '<unk>', 'You', '▁don', \"'\", 't', '▁need', '▁to', '▁worry', '.', '▁We', \"'\", 're', '▁quite', '▁adequately', '▁chap', 'er', 'one', 'd', '.', '▁Rosa', '▁is', '▁', 'a', '▁woman', '▁of', '▁strict', '▁moral', '▁principles', '.', \"'\", \"'\", '▁If', '▁she', '▁knows', '▁I', \"'\", 'm', '▁in', '▁here', '▁she', \"'\", 's', '▁probably', '▁hover', 'ing', '▁outside', '▁the', '▁door', '▁right', '▁now', '.', '▁Question', ':', '▁', 'he', \"'\", 's', '▁in', '▁here', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-4.3274e-05, -1.0475e+01, -1.1110e+01],\n",
      "        [-8.7503e-05, -9.4566e+00, -1.1590e+01],\n",
      "        [-1.1944e+01, -1.2875e-05, -1.1959e+01],\n",
      "        [-1.1103e+01, -3.1710e-05, -1.1003e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 0, 1, 1], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁B', ':', '▁I', '▁understand', '▁we', '▁are', '▁doing', '▁care', '▁of', '▁the', '▁elderly', ',', '▁right', '?', '▁A', ':', '▁Yes', '.', '▁B', ':', '▁And', '▁how', '▁do', '▁you', '▁feel', '▁about', '▁', 'putting', '▁someone', '▁in', '▁the', '▁nursing', '▁home', '?', '▁A', ':', '▁Well', ',', '▁I', '▁don', \"'\", 't', '▁think', '▁that', '▁', 'u', 'h', ',', '▁any', '▁of', '▁my', '▁relatives', '▁would', '▁really', '▁like', '▁to', '▁go', '▁there', '.', '▁Question', ':', '▁some', '▁of', '▁her', '▁relatives', '▁would', '▁really', '▁like', '▁to', '▁go', '▁there', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:  tensor([[-1.2699e+01, -4.1723e-06, -1.3653e+01],\n",
      "        [-1.2912e+01, -2.8610e-06, -1.4903e+01],\n",
      "        [-9.3716e+00, -9.4909e+00, -1.6071e-04],\n",
      "        [-1.1087e-05, -1.2281e+01, -1.1951e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([1, 1, 2, 0], device='cuda:0')\n",
      "pred: tensor(1, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁B', ':', '▁But', ',', '▁', 'u', 'h', ',', '▁', 'if', '▁the', '▁wind', '▁comes', '▁basically', '▁from', '▁the', '▁south', '▁it', '▁can', '▁be', '▁really', '▁bad', '.', '▁A', ':', '▁U', 'h', '-', 'huh', '.', '▁B', ':', '▁U', 'h', ',', '▁the', '▁State', '▁of', '▁Wisconsin', ',', '▁as', '▁', 'a', '▁matter', '▁of', '▁fact', ',', '▁', 'u', 'h', ',', '▁started', '▁some', '▁litigation', '▁against', '▁Illinois', '▁because', '▁of', '▁the', '▁air', '▁pollution', '▁we', '▁were', '▁getting', '.', '▁A', ':', '▁U', 'h', '-', 'huh', '.', '▁B', ':', '▁U', 'h', ',', '▁I', '▁don', \"'\", 't', '▁think', '▁it', \"'\", 's', '▁going', '▁to', '▁go', '▁very', '▁far', ',', '▁Question', ':', '▁it', \"'\", 's', '▁going', '▁to', '▁go', '▁very', '▁far', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.4545e+01, -7.1526e-07, -1.5250e+01],\n",
      "        [-5.2197e-04, -8.7836e+00, -7.9056e+00],\n",
      "        [-1.2022e+01, -7.1526e-06, -1.3685e+01],\n",
      "        [-1.7001e-04, -8.8435e+00, -1.0574e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([1, 0, 1, 0], device='cuda:0')\n",
      "pred: tensor(1, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁Oh', ',', '▁I', '▁did', ',', '▁I', '▁did', '!', '▁I', '▁was', '▁lucky', '.', '▁I', '▁would', '▁have', '▁liked', '▁brothers', '▁and', '▁sisters', '▁but', '▁I', '▁don', \"'\", 't', '▁remember', '▁that', '▁I', '▁was', '▁ever', '▁lonely', '.', '▁Question', ':', '▁she', '▁was', '▁ever', '▁lonely', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-9.8572e+00, -5.2811e-05, -1.4568e+01],\n",
      "        [-1.0455e+01, -3.3737e-05, -1.2214e+01],\n",
      "        [-1.3936e+01, -1.7881e-06, -1.3906e+01],\n",
      "        [-3.5883e-05, -1.1112e+01, -1.0772e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([1, 1, 1, 0], device='cuda:0')\n",
      "pred: tensor(1, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁A', ':', '▁Well', '▁I', ',', '▁', 'u', 'h', ',', '▁when', '▁is', '▁your', '▁next', '▁one', ',', '▁', 'u', 'h', ',', '▁scheduled', '▁now', '.', '▁B', ':', '▁Well', '▁it', \"'\", 's', '▁like', ',', '▁the', '▁last', '▁one', '▁was', '▁my', '▁high', '▁school', '▁graduation', '▁the', '▁next', '▁one', '▁was', '▁when', '▁I', '▁graduated', '▁from', '▁college', ',', '▁so', '▁I', '▁guess', '▁about', '▁two', '▁more', '▁years', '.', '▁A', ':', '▁Yes', ',', '▁well', ',', '▁and', '▁do', '▁you', '▁think', '▁you', \"'\", 'll', '▁have', '▁', 'a', '▁baby', '▁to', '▁take', '▁back', '▁with', '▁you', '.', '▁Question', ':', '▁speaker', '▁B', '▁will', '▁have', '▁', 'a', '▁baby', '▁to', '▁take', '▁back', '▁with', '▁her', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-8.0762e+00, -7.9445e+00, -6.6571e-04],\n",
      "        [-4.2082e-05, -1.0896e+01, -1.0655e+01],\n",
      "        [-1.3355e+01, -1.9074e-06, -1.4949e+01],\n",
      "        [-2.1458e-06, -1.4282e+01, -1.3404e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([2, 0, 1, 0], device='cuda:0')\n",
      "pred: tensor(2, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁', 'GM', '▁confirmed', '▁it', '▁received', '▁U', '.', 'S', '.', '▁anti', 'trust', '▁clearance', '▁to', '▁boost', '▁its', '▁holding', '.', '▁San', 's', 'u', 'i', '▁Electric', '▁agreed', '▁to', '▁sell', '▁', 'a', '▁5', '1%', '▁stake', '▁to', '▁Poll', 'y', '▁P', 'eck', '▁of', '▁Britain', '▁for', '▁$1', '10', '▁million', '.', '▁Still', ',', '▁analysts', '▁said', '▁the', '▁accord', '▁doesn', \"'\", 't', '▁suggest', '▁Japan', '▁is', '▁opening', '▁up', '▁to', '▁more', '▁foreign', '▁take', 'over', 's', '.', '▁Question', ':', '▁Japan', '▁is', '▁opening', '▁up', '▁to', '▁more', '▁foreign', '▁take', 'over', 's', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.3137e+01, -3.0994e-06, -1.3742e+01],\n",
      "        [-6.4137e-05, -1.0360e+01, -1.0337e+01],\n",
      "        [-8.7058e+00, -8.9687e+00, -2.9300e-04],\n",
      "        [-2.2766e-04, -1.1052e+01, -8.4601e+00]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([1, 0, 2, 0], device='cuda:0')\n",
      "pred: tensor(1, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁It', \"'\", 's', '▁where', '▁the', '▁bands', '▁practise', '.', '▁I', '▁can', \"'\", 't', '▁remember', '▁what', '▁band', '▁Pe', 'tra', \"'\", 's', '▁in', ',', '▁but', '▁I', '▁seen', '▁them', '▁practise', '▁once', '.', '▁They', '▁were', '▁OK', '▁but', '▁I', '▁didn', \"'\", 't', '▁think', '▁they', '▁was', '▁brilliant', '.', '▁Question', ':', '▁Pe', 'tra', \"'\", 's', '▁band', '▁was', '▁brilliant', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.3278e+01, -3.4571e-06, -1.3271e+01],\n",
      "        [-5.4480e-05, -1.1259e+01, -1.0087e+01],\n",
      "        [-1.4335e+01, -1.5497e-06, -1.3921e+01],\n",
      "        [-1.2005e-04, -9.4833e+00, -1.0034e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([1, 0, 1, 0], device='cuda:0')\n",
      "pred: tensor(1, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁Nick', 'y', '▁approached', '▁her', '▁with', '▁the', '▁assumption', '▁that', '▁men', '▁are', '▁naturally', '▁right', '▁and', '▁it', '▁is', '▁the', '▁role', '▁of', '▁women', '▁to', '▁follow', '▁their', '▁lead', '.', '▁Con', 'stance', ',', '▁', 'whose', '▁confidence', '▁was', '▁growing', '▁daily', ',', '▁was', '▁not', '▁prepared', '▁to', '▁give', '▁in', '▁to', '▁Nick', 'y', \"'\", 's', '▁wishes', '▁', 'merely', '▁because', '▁of', '▁his', '▁', 's', 'ex', '.', '▁If', '▁she', '▁felt', '▁', 'he', '▁was', '▁right', '▁then', '▁she', '▁agreed', '▁with', '▁him', '.', '▁Question', ':', '▁Nick', 'y', '▁was', '▁right', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:  tensor([[-8.8164e+00, -9.6814e+00, -2.1072e-04],\n",
      "        [-1.2239e+01, -1.3828e-05, -1.1620e+01],\n",
      "        [-1.4133e+01, -8.3447e-07, -1.5847e+01],\n",
      "        [-1.2320e+01, -5.3644e-06, -1.3920e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([2, 1, 1, 1], device='cuda:0')\n",
      "pred: tensor(2, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁A', ':', '▁That', '▁is', '▁the', '▁reason', ',', '▁I', '▁don', \"'\", 't', '▁play', '▁over', '▁there', '.', '▁B', ':', '▁Yeah', '.', '▁A', ':', '▁I', '▁like', '▁the', '▁course', ',', '▁but', '▁I', '▁don', \"'\", 't', '▁play', '▁over', '▁there', '▁because', ',', '▁they', '▁don', \"'\", 't', ',', '▁', 'u', 'h', ',', '▁you', '▁know', '▁don', \"'\", 't', '▁allow', '▁you', '▁to', '▁pull', '▁', 'a', '▁cart', '.', '▁B', ':', '▁Right', '.', '▁A', ':', '▁And', ',', '▁I', '▁don', \"'\", 't', '▁think', '▁', 'a', '▁cart', '▁damages', '▁the', '▁turf', '.', '▁Question', ':', '▁', 'a', '▁cart', '▁damages', '▁the', '▁turf', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.2464e+01, -4.5300e-06, -1.4276e+01],\n",
      "        [-7.3555e-05, -1.0837e+01, -9.8277e+00],\n",
      "        [-7.3555e-05, -9.7274e+00, -1.1181e+01],\n",
      "        [-1.9999e-04, -9.8874e+00, -8.8099e+00]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([1, 0, 0, 0], device='cuda:0')\n",
      "pred: tensor(1, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁The', '▁South', '▁Korean', '▁government', '▁is', '▁signing', '▁', 'a', '▁protocol', '▁today', '▁', 'establishing', '▁formal', '▁diplomatic', '▁relations', '▁with', '▁Poland', '.', '▁The', '▁two', '▁are', '▁also', '▁signing', '▁', 'a', '▁trade', '▁agreement', '.', '▁South', '▁Korean', '▁government', '▁officials', '▁said', '▁they', '▁don', \"'\", 't', '▁expect', '▁that', '▁Seoul', '▁can', '▁loan', '▁money', '▁to', '▁War', 'saw', ',', '▁but', '▁it', '▁can', '▁', '<unk>', 'off', 'er', '▁experience', '.', \"'\", \"'\", '▁Question', ':', '▁Seoul', '▁can', '▁loan', '▁money', '▁to', '▁War', 'saw', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.2280e+01, -1.0490e-05, -1.2045e+01],\n",
      "        [-1.6357e-04, -9.7093e+00, -9.1821e+00],\n",
      "        [-1.5355e-04, -9.0834e+00, -1.0128e+01],\n",
      "        [-1.1800e+01, -7.8678e-06, -1.4986e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([1, 0, 0, 1], device='cuda:0')\n",
      "pred: tensor(1, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁A', ':', '▁Yeah', ',', '▁it', \"'\", 's', '▁interesting', '▁because', ',', '▁', 'u', 'h', ',', '▁we', \"'\", 're', '▁just', '▁having', '▁conversation', '▁on', '▁this', ',', '▁', 'u', 'h', ',', '▁with', '▁', 'a', '▁couple', '▁of', '▁people', '▁yesterday', '.', '▁And', '▁I', '▁was', '▁', 'expressing', '▁my', '▁frustration', 's', '▁that', ',', '▁', 'u', 'h', ',', '▁so', '▁many', '▁problems', ',', '▁I', '▁work', '▁in', '▁', 'a', '▁high', '▁school', ',', '▁are', '▁that', '▁kids', '▁don', \"'\", 't', '▁have', '▁', 'a', '▁degree', '▁of', '▁self', '-', 'd', 'is', 'cip', 'line', '▁which', '▁may', '▁be', '▁', 'reflected', '▁in', '▁society', '▁at', '▁large', '.', '▁U', 'h', ',', '▁and', '▁you', '▁can', \"'\", 't', '▁expect', '▁in', '▁', 'a', '▁classroom', '▁for', '▁', 'a', '▁particular', '▁course', '▁an', '▁hour', '▁', 'a', '▁day', '▁to', '▁counter', 'act', ',', '▁', 'u', 'h', ',', '▁sixteen', '▁or', '▁seventeen', '▁years', '▁of', '▁influence', '▁at', '▁home', '.', '▁B', ':', '▁Right', '.', '▁A', ':', '▁Um', ',', '▁and', ',', '▁it', \"'\", 's', '▁seen', '▁more', '▁so', '▁because', '▁when', '▁you', '▁call', '▁parents', '▁up', ',', '▁many', '▁parents', '▁won', \"'\", 't', '▁even', '▁recognize', '▁that', '▁there', '▁is', '▁', 'a', '▁problem', '▁and', '▁they', \"'\", 'll', '▁say', ',', '▁', 'o', 'h', ',', '▁well', ',', '▁my', '▁kid', ',', '▁I', \"'\", 've', '▁never', '▁heard', '▁anything', '▁about', '▁this', '▁before', '.', '▁This', '▁is', '▁the', '▁first', '▁time', '▁there', '▁have', '▁been', '▁problems', '.', '▁and', ',', '▁you', '▁wonder', ',', '▁don', \"'\", 't', '▁these', '▁parents', '▁know', '▁that', '▁teachers', '▁talk', ',', '▁Question', ':', '▁teachers', '▁talk', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-6.0679e-05, -1.0573e+01, -1.0260e+01],\n",
      "        [-1.2728e+01, -4.7684e-06, -1.3251e+01],\n",
      "        [-1.1714e+01, -9.5368e-06, -1.3520e+01],\n",
      "        [-1.6928e-05, -1.1235e+01, -1.2512e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 1, 1, 0], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁B', ':', '▁but', ',', '▁', 'u', 'h', ',', '▁I', '▁can', '▁definitely', ',', '▁', 'u', 'h', ',', '▁see', '▁on', '▁down', '▁the', '▁road', ',', '▁you', '▁know', ',', '▁where', '▁we', '▁do', '▁have', '▁kids', '▁and', '▁are', '▁getting', '▁to', '▁that', '▁age', ',', '▁that', \"'\", 's', '▁going', '▁to', '▁be', '▁', 'a', '▁', 'definite', '▁concern', '.', '▁A', ':', '▁Yeah', ',', '▁you', '▁talked', '▁before', ',', '▁about', '▁the', '▁school', '▁funding', '.', '▁I', '▁think', '▁there', \"'\", 's', '▁only', '▁going', '▁to', '▁be', '▁one', '▁solution', '▁to', '▁school', '▁funding', '▁which', '▁I', '▁don', \"'\", 't', '▁think', '▁will', '▁be', '▁necessarily', '▁the', '▁best', '▁way', '▁Question', ':', '▁the', '▁one', '▁solution', '▁to', '▁school', '▁funding', '▁will', '▁be', '▁necessarily', '▁the', '▁best', '▁way', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.3538e+01, -1.2636e-05, -1.1386e+01],\n",
      "        [-1.7841e-04, -8.8255e+00, -1.0368e+01],\n",
      "        [-2.2181e-04, -8.7965e+00, -9.5588e+00],\n",
      "        [-9.1792e-06, -1.3713e+01, -1.1729e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([1, 0, 0, 0], device='cuda:0')\n",
      "pred: tensor(1, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁B', ':', '▁Yeah', '.', '▁How', '▁about', '▁M', 'ister', '▁Roger', 's', ',', '▁is', '▁', 'he', '▁still', '▁around', '?', '▁A', ':', '▁Yes', '.', '▁Yeah', '.', '▁They', '▁still', '▁show', '▁M', 'ister', '▁Roger', 's', '.', '▁I', '▁don', \"'\", 't', '▁think', '▁', 'he', \"'\", 's', '▁making', '▁new', '▁ones', ',', '▁Question', ':', '▁M', 'ister', '▁Roger', 's', '▁is', '▁making', '▁new', '▁M', 'ister', '▁Roger', 's', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.3356e+01, -2.0266e-06, -1.4735e+01],\n",
      "        [-1.3515e+01, -1.9074e-06, -1.4316e+01],\n",
      "        [-1.3471e-05, -1.2205e+01, -1.1684e+01],\n",
      "        [-1.4831e-04, -8.8764e+00, -1.1664e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([1, 1, 0, 0], device='cuda:0')\n",
      "pred: tensor(1, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁A', ':', '▁How', '▁do', '▁you', '▁feel', '▁about', '▁that', '.', '▁B', ':', '▁I', '▁don', \"'\", 't', '▁really', ',', '▁I', '▁more', ',', '▁I', '▁don', \"'\", 't', '▁know', '▁about', '▁the', '▁government', '▁as', '▁much', '▁as', ',', '▁', 'u', 'h', ',', '▁the', '▁people', ',', '▁', 'u', 'h', ',', '▁I', '▁wouldn', \"'\", 't', '▁consider', '▁to', '▁be', '▁', 'a', '▁threat', '▁at', '▁all', '▁and', '▁I', '▁really', '▁don', \"'\", 't', '▁feel', '▁much', '▁like', '▁the', '▁Soviet', '▁Union', '▁itself', '▁is', '▁', 'a', '▁threat', '▁anymore', '.', '▁Question', ':', '▁the', '▁Soviet', '▁Union', '▁itself', '▁is', '▁', 'a', '▁threat', '▁still', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:  tensor([[-1.1765e+01, -8.5831e-06, -1.4028e+01],\n",
      "        [-7.4732e+00, -5.7786e-04, -1.1552e+01],\n",
      "        [-6.6044e-05, -1.0101e+01, -1.0597e+01],\n",
      "        [-1.2281e+01, -4.8876e-06, -1.5094e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([1, 1, 0, 1], device='cuda:0')\n",
      "pred: tensor(1, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁Jean', '▁was', '▁tough', '▁and', '▁liked', '▁to', '▁drink', '.', '▁She', '▁would', '▁endure', '▁for', '▁', 'a', '▁long', '▁while', '▁yet', '.', '▁But', '▁what', '▁would', '▁she', '▁do', '▁when', '▁she', '▁realized', '▁that', '▁with', '▁things', '▁as', '▁they', '▁were', '▁she', '▁was', '▁on', '▁', 'a', '▁life', '▁sentence', '▁not', '▁just', '▁', 'a', '▁temporary', '▁suspension', '▁of', '▁essential', '▁pleasure', '?', '▁Question', ':', '▁Jean', '▁was', '▁on', '▁', 'a', '▁life', '▁sentence', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.5427e-04, -9.1796e+00, -9.8818e+00],\n",
      "        [-1.5974e-05, -1.1360e+01, -1.2349e+01],\n",
      "        [-1.1102e-03, -8.8057e+00, -6.9490e+00],\n",
      "        [-8.4639e-06, -1.2855e+01, -1.2057e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁There', '▁was', '▁no', '▁answer', '.', '▁Moving', '▁carefully', ',', '▁Ben', 'n', 'y', '▁', 'stepped', '▁around', '▁the', '▁edges', '▁of', '▁the', '▁room', ',', '▁and', '▁opened', '▁the', '▁window', '▁shutter', 's', '.', '▁Her', '▁moment', 'ary', '▁horror', '▁at', '▁seeing', '▁the', '▁un', 'm', 'ist', 'ak', 'able', '▁form', '▁of', '▁General', '▁Et', 'ienne', '▁was', '▁only', '▁slightly', '▁dull', 'e', 'd', '▁by', '▁the', '▁realization', '▁that', '▁the', '▁stiff', '▁posture', '▁', 'he', '▁was', '▁in', '▁could', '▁only', '▁mean', '▁', 'he', '▁was', '▁dead', '.', '▁Question', ':', '▁General', '▁Et', 'ienne', '▁was', '▁dead', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.3221e-04, -9.0641e+00, -1.1017e+01],\n",
      "        [-1.5065e+01, -5.9605e-07, -1.4884e+01],\n",
      "        [-1.2870e+01, -5.4836e-06, -1.2737e+01],\n",
      "        [-9.9783e-05, -1.0143e+01, -9.7143e+00]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 1, 1, 0], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁A', ':', '▁No', ',', '▁not', '▁really', '.', '▁I', '▁spend', 's', '▁', 'a', '▁lot', '▁of', '▁time', '▁with', '▁our', '▁income', '▁tax', ',', '▁though', '.', '▁especially', ',', '▁this', '▁year', '▁and', '▁last', '▁year', '.', '▁Um', ',', '▁I', '▁have', '▁been', '▁married', '▁for', '▁just', '▁', 'a', '▁few', '▁years', ',', '▁so', '▁I', \"'\", 've', '▁had', '▁to', '▁really', '▁switch', '▁around', '▁from', '▁the', '▁', 'EZ', '▁form', '▁to', '▁the', ',', '▁', 'u', 'h', ',', '▁B', ':', '▁Schedule', '▁A', '.', '▁A', ':', '▁Right', '.', '▁B', ':', '▁Well', ',', '▁yeah', '.', '▁A', ':', '▁All', '▁the', '▁deduction', 's', '▁and', '▁all', '▁that', '.', '▁B', ':', '▁Did', '▁you', '▁notice', '▁that', '▁when', '▁they', '▁passed', '▁the', '▁new', '▁simplified', '▁tax', '▁act', ',', '▁it', '▁seemed', '▁like', '▁it', '▁made', '▁everything', '▁harder', '?', '▁Question', ':', '▁when', '▁they', '▁passed', '▁the', '▁new', '▁simplified', '▁tax', '▁act', '▁it', '▁seemed', '▁like', '▁it', '▁made', '▁everything', '▁harder', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-9.1676e-05, -1.0804e+01, -9.5477e+00],\n",
      "        [-1.0919e+01, -2.2054e-05, -1.2457e+01],\n",
      "        [-1.2755e-05, -1.2431e+01, -1.1645e+01],\n",
      "        [-7.3436e-05, -9.7576e+00, -1.1064e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 1, 0, 0], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁A', ':', '▁Oh', ',', '▁yes', '.', '▁Animal', 's', '▁have', '▁', 'a', '▁way', '▁of', '▁talking', '.', '▁B', ':', '▁Al', 'fie', '▁did', '.', '▁I', '▁tell', '▁you', '▁', 'if', '▁I', '▁could', '▁have', '▁', 'gotten', '▁', 'a', '▁hold', '▁of', '▁that', '▁cat', '▁that', '▁day', '.', '▁A', ':', '▁I', '▁don', \"'\", 't', '▁know', '▁', 'u', 'h', ',', '▁that', '▁I', \"'\", 'd', '▁trade', '▁my', '▁dog', '▁in', '▁for', '▁the', '▁world', '.', '▁Question', ':', '▁', 'he', '▁would', '▁trade', '▁his', '▁dog', '▁in', '▁for', '▁the', '▁world', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.2186e+01, -5.3644e-06, -1.5343e+01],\n",
      "        [-1.0583e+01, -2.8253e-05, -1.2735e+01],\n",
      "        [-1.9312e-05, -1.1778e+01, -1.1357e+01],\n",
      "        [-8.5621e+00, -2.8167e-04, -9.3109e+00]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([1, 1, 0, 1], device='cuda:0')\n",
      "pred: tensor(1, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁I', '▁', 'd', 'ucked', '▁so', '▁fast', '▁I', '▁wasn', \"'\", 't', '▁sure', '▁whether', '▁', 'he', '▁', \"'\", 'd', '▁seen', '▁me', '▁or', '▁not', ',', '▁but', '▁it', '▁gave', '▁me', '▁', 'a', '▁', 'p', 'rick', 'ly', '▁feeling', '▁just', '▁to', '▁imagine', '▁it', ',', '▁so', '▁I', '▁', 's', 'cut', 't', 'led', '▁for', '▁the', '▁door', '▁and', '▁le', 'gged', '▁it', '▁up', '▁the', '▁spiral', '▁', 'stair', 'way', '▁three', '▁steps', '▁at', '▁', 'a', '▁time', ',', '▁just', '▁in', '▁case', '.', '▁As', '▁I', '▁ran', ',', '▁I', '▁remember', '▁thinking', '▁stupid', '▁thoughts', '▁like', '.', '▁How', '▁did', '▁', 'he', '▁know', '▁I', '▁was', '▁up', '▁here', '▁looking', '▁down', '?', '▁Question', ':', '▁', 'he', '▁was', '▁up', '▁there', '▁looking', '▁down', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.1373e-04, -9.9488e+00, -9.6276e+00],\n",
      "        [-2.1776e-04, -1.0699e+01, -8.5413e+00],\n",
      "        [-7.5946e+00, -7.5556e+00, -1.0268e-03],\n",
      "        [-1.3685e+01, -1.4305e-06, -1.4865e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 0, 2, 1], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁A', ':', '▁it', '▁was', '▁so', '▁fat', 'ten', 'ing', '.', '▁B', ':', '▁That', '▁sounds', '▁good', '.', '▁A', ':', '▁But', ',', '▁I', '▁don', \"'\", 't', '▁think', '▁we', '▁gained', '▁any', '▁weight', '▁from', '▁it', '▁Question', ':', '▁they', '▁gained', '▁any', '▁weight', '▁from', '▁it', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:  tensor([[-1.1275e+01, -1.3113e-05, -1.4811e+01],\n",
      "        [-1.3518e+01, -1.6689e-06, -1.4950e+01],\n",
      "        [-1.3544e+01, -1.6689e-06, -1.4895e+01],\n",
      "        [-3.4571e-06, -1.3214e+01, -1.3310e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([1, 1, 1, 0], device='cuda:0')\n",
      "pred: tensor(1, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁A', ':', '▁Well', ',', '▁I', '▁don', \"'\", 't', '▁know', ',', '▁', 'u', 'h', ',', '▁I', '▁have', '▁', 'a', '▁hard', '▁time', '▁getting', ',', '▁', 'u', 'h', ',', '▁people', '▁on', '▁the', '▁telephone', '.', '▁B', ':', '▁Oh', '▁really', '.', '▁A', ':', '▁U', 'h', '-', 'huh', ',', '▁getting', '▁through', '▁to', '▁anybody', '.', '▁Sometimes', '▁I', '▁call', '▁off', '▁and', '▁on', '▁all', '▁day', ',', '▁B', ':', '▁Hu', 'h', '.', '▁A', ':', '▁but', '▁anyway', ',', '▁', 'u', 'h', ',', '▁I', '▁guess', '▁we', \"'\", 're', '▁supposed', '▁to', '▁be', '▁talking', '▁about', '▁family', '▁reunion', 's', '▁are', 'n', \"'\", 't', '▁we', '.', '▁Question', ':', '▁they', \"'\", 're', '▁supposed', '▁to', '▁be', '▁talking', '▁about', '▁family', '▁reunion', 's', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-3.3074e-04, -9.5557e+00, -8.2550e+00],\n",
      "        [-2.0476e-04, -9.2669e+00, -9.1128e+00],\n",
      "        [-3.5745e-04, -8.7456e+00, -8.5265e+00],\n",
      "        [-9.9664e-05, -9.5712e+00, -1.0417e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 0, 0, 0], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁A', ':', '▁Do', '▁you', '▁go', '▁to', '▁museums', '▁in', '▁Europe', '?', '▁B', ':', '▁U', 'h', ',', '▁actually', ',', '▁no', ',', '▁I', '▁don', \"'\", 't', '▁think', '▁I', '▁went', '▁to', '▁any', '▁of', '▁them', '.', '▁Question', ':', '▁she', '▁went', '▁to', '▁some', '▁of', '▁them', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.2176e+01, -5.7221e-06, -1.4469e+01],\n",
      "        [-7.7339e+00, -8.6164e+00, -6.1901e-04],\n",
      "        [-4.6373e-05, -1.1743e+01, -1.0168e+01],\n",
      "        [-7.3722e+00, -9.8220e+00, -6.8294e-04]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([1, 2, 0, 2], device='cuda:0')\n",
      "pred: tensor(1, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁But', '▁the', '▁horror', '▁of', '▁losing', '▁was', '▁as', '▁much', '▁to', '▁do', '▁with', '▁money', '▁as', '▁with', '▁pride', '.', '▁Bi', 'ddy', '▁had', '▁never', '▁let', '▁them', '▁down', ',', '▁come', '▁without', '▁fail', '▁all', '▁through', '▁the', '▁bad', '▁weather', ',', '▁and', '▁now', '▁was', '▁giving', '▁Na', 'ils', '▁an', '▁intensive', '▁course', '▁on', '▁her', '▁own', '▁horse', '▁which', '▁', '-', '▁in', '▁terms', '▁of', '▁money', '▁', '-', '▁was', '▁worth', '▁another', '▁couple', '▁of', '▁hundred', '▁pounds', '.', '▁Yet', '▁surely', '▁she', '▁knew', '▁they', '▁had', '▁no', '▁way', '▁of', '▁paying', '▁should', '▁she', '▁demand', '▁it', '?', '▁Question', ':', '▁they', '▁had', '▁no', '▁way', '▁of', '▁paying', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.8533e-04, -8.9344e+00, -9.8348e+00],\n",
      "        [-1.2616e+01, -4.0531e-06, -1.4149e+01],\n",
      "        [-3.8505e-05, -1.2380e+01, -1.0279e+01],\n",
      "        [-1.3183e+01, -2.0266e-06, -1.5495e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 1, 0, 1], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁Richard', '▁Breed', 'en', '▁had', '▁', 'n', \"'\", 't', '▁noticed', '▁that', '▁his', '▁new', '▁desk', '▁had', '▁just', '▁four', '▁telephone', '▁lines', '▁and', '▁one', '▁phone', '▁', '.', '▁Question', ':', '▁Richard', '▁Breed', 'en', \"'\", 's', '▁new', '▁desk', '▁had', '▁just', '▁four', '▁telephone', '▁lines', '▁and', '▁one', '▁phone', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.7047e-05, -1.1207e+01, -1.2567e+01],\n",
      "        [-1.2750e+01, -3.4571e-06, -1.4448e+01],\n",
      "        [-1.3472e+01, -1.6689e-06, -1.5348e+01],\n",
      "        [-1.4479e+01, -1.1921e-06, -1.4134e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 1, 1, 1], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁B', ':', '▁That', '▁was', '▁kind', '▁of', '▁', 'a', '▁funny', '▁movie', '▁with', ',', '▁', 'u', 'h', ',', '▁Richard', '▁Dr', 'e', 'y', 'f', 'us', 's', '▁and', '▁Bill', '▁Murray', '.', '▁A', ':', '▁U', 'h', '-', 'huh', '.', '▁B', ':', '▁That', '▁was', '▁fun', '.', '▁A', ':', '▁Go', 'lly', ',', '▁I', '▁don', \"'\", 't', '▁think', '▁that', '▁I', \"'\", 've', '▁ever', '▁heard', '▁of', '▁that', '▁movie', '.', '▁Question', ':', '▁', 'he', '▁has', '▁heard', '▁of', '▁that', '▁movie', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.2368e+01, -4.8876e-06, -1.4208e+01],\n",
      "        [-2.0357e-04, -8.8476e+00, -9.7246e+00],\n",
      "        [-9.1318e-05, -1.0164e+01, -9.8494e+00],\n",
      "        [-1.0133e-04, -9.8872e+00, -9.8928e+00]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([1, 0, 0, 0], device='cuda:0')\n",
      "pred: tensor(1, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁A', ':', '▁Boy', '▁that', \"'\", 's', '▁scary', ',', '▁is', 'n', \"'\", 't', '▁it', '.', '▁B', ':', '▁Oh', ',', '▁can', '▁you', '▁imagine', ',', '▁because', '▁it', '▁happens', '▁in', '▁the', '▁middle', '▁of', '▁the', '▁night', ',', '▁so', '▁you', '▁know', ',', '▁these', '▁parents', '▁didn', \"'\", 't', '▁know', '▁the', '▁kid', '▁was', '▁gone', '▁until', '▁the', '▁kid', '▁is', '▁knock', 'ing', '▁on', '▁the', '▁door', '▁screaming', ',', '▁let', '▁me', '▁in', '.', '▁Question', ':', '▁the', '▁kid', '▁was', '▁gone', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:  tensor([[-6.2229e-05, -1.0044e+01, -1.0884e+01],\n",
      "        [-1.0772e+01, -2.6584e-05, -1.2087e+01],\n",
      "        [-1.2323e+01, -8.9407e-06, -1.2322e+01],\n",
      "        [-7.1409e-05, -1.0639e+01, -9.9551e+00]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 1, 1, 0], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁A', ':', '▁and', '▁', 'if', '▁they', '▁weren', \"'\", 't', '▁spending', '▁all', '▁the', '▁money', '▁on', '▁drug', '▁testing', ',', '▁people', '▁could', '▁have', '▁got', '▁', 'a', '▁raise', '.', '▁So', ',', '▁see', ',', '▁you', '▁know', ',', '▁there', \"'\", 's', '▁different', ',', '▁I', '▁think', '▁that', \"'\", 's', '▁more', '▁of', '▁', 'a', '▁personal', '▁view', '▁of', '▁mine', '▁other', '▁than', '▁', 'a', '▁yes', ',', '▁si', 'r', ',', '▁we', '▁should', '▁have', '▁drug', '▁testing', '▁because', '▁there', \"'\", 's', '▁really', '▁', 'a', '▁problem', '▁B', ':', '▁U', 'h', '-', 'huh', '.', '▁A', ':', '▁and', '▁I', '▁know', '▁that', '.', '▁But', '▁then', ',', '▁I', '▁have', '▁other', '▁views', '▁to', '▁it', '.', '▁B', ':', '▁I', '▁didn', \"'\", 't', '▁think', '▁it', '▁was', '▁that', '▁expensive', '▁because', '▁my', '▁son', '▁was', '▁in', '▁probably', '▁', 'a', '▁week', '▁and', '▁', 'a', '▁half', '▁period', '▁Question', ':', '▁it', '▁was', '▁that', '▁expensive', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.3522e+01, -1.6689e-06, -1.4878e+01],\n",
      "        [-1.2714e+01, -1.0133e-05, -1.1845e+01],\n",
      "        [-9.6445e+00, -7.2601e-05, -1.1760e+01],\n",
      "        [-1.0419e-04, -9.8473e+00, -9.8782e+00]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([1, 1, 1, 0], device='cuda:0')\n",
      "pred: tensor(1, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁Matthew', '▁rod', 'e', '▁on', '▁feeling', '▁', 'a', '▁little', '▁more', '▁at', '▁peace', '▁with', '▁himself', '.', '▁He', '▁skirt', 'e', 'd', '▁the', '▁', 's', 'pru', 'ce', '▁plant', 'ation', '▁and', '▁supposed', '▁that', '▁at', '▁some', '▁point', '▁', 'he', '▁should', '▁tell', '▁Sara', '▁about', '▁it', '.', '▁He', '▁could', '▁imagine', '▁that', '▁she', '▁might', '▁be', '▁interested', '▁in', '▁its', '▁money', '-', 'making', '▁pro', 'pens', 'ity', '▁at', '▁the', '▁end', '▁of', '▁the', '▁year', '.', '▁Question', ':', '▁Sara', '▁might', '▁be', '▁interested', '▁in', '▁its', '▁money', '-', 'making', '▁pro', 'pens', 'ity', '▁at', '▁the', '▁end', '▁of', '▁the', '▁year', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.0169e-04, -1.2886e+01, -9.2188e+00],\n",
      "        [-1.4667e+01, -2.6226e-06, -1.3052e+01],\n",
      "        [-1.1029e+01, -1.9670e-05, -1.2595e+01],\n",
      "        [-1.2535e+01, -4.4108e-06, -1.3965e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 1, 1, 1], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁A', ':', '▁Yeah', '.', '▁The', '▁radio', '▁doesn', \"'\", 't', '▁really', '▁have', '▁much', '▁news', '▁sometimes', '.', '▁The', '▁stations', '▁I', '▁listen', '▁to', '▁are', '▁just', '▁', 'mainly', '▁music', '.', '▁B', ':', '▁Yeah', ',', '▁I', '▁think', '▁you', '▁pretty', '▁much', '▁have', '▁to', '▁listen', '▁to', '▁all', '▁news', '▁station', '▁to', '▁get', '▁any', '▁news', '▁at', '▁all', '.', '▁A', ':', '▁Yeah', '.', '▁Do', '▁you', '▁think', '▁that', '▁TV', '▁is', ',', '▁', 'u', 'h', ',', '▁pretty', '▁accurate', '.', '▁Question', ':', '▁TV', '▁is', '▁pretty', '▁accurate', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-8.6779e+00, -1.0452e+01, -1.9916e-04],\n",
      "        [-1.1760e+01, -1.2040e-05, -1.2368e+01],\n",
      "        [-9.5612e+00, -1.7013e-04, -9.2139e+00],\n",
      "        [-1.2066e+01, -7.3910e-06, -1.3304e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([2, 1, 1, 1], device='cuda:0')\n",
      "pred: tensor(2, device='cuda:0') \n",
      "inputs:  torch.Size([4, 256])\n",
      "['▁Jane', '▁', 'ate', '▁without', '▁pa', 'using', '.', '▁Hunger', '▁was', '▁an', '▁unknown', '▁experience', '.', '▁She', '▁had', '▁never', '▁imagined', '▁it', '▁could', '▁actually', '▁hurt', '.', '▁Question', ':', '▁hunger', '▁could', '▁actually', '▁hurt', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-1.7770e-04, -9.9959e+00, -8.9309e+00],\n",
      "        [-3.4571e-05, -1.2707e+01, -1.0364e+01],\n",
      "        [-9.5837e+00, -9.0991e+00, -1.8068e-04],\n",
      "        [-1.3989e+01, -1.1921e-06, -1.4874e+01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 0, 2, 1], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n",
      "inputs:  torch.Size([2, 256])\n",
      "['▁', '<unk>', 'Hi', 's', '▁name', '▁is', '▁Matthew', '▁Blake', ',', \"'\", \"'\", '▁Man', 'd', 'y', '▁informed', '▁Charity', '▁as', '▁they', '▁de', 'scended', '▁the', '▁steps', '▁from', '▁their', '▁cabin', '▁on', '▁to', '▁the', '▁', 'paved', '▁pathway', '▁that', '▁led', '▁to', '▁the', '▁lodge', '.', '▁', 'Thankfully', '▁she', '▁hadn', \"'\", 't', '▁even', '▁noticed', '▁that', '▁Charity', '▁had', '▁changed', '▁from', '▁the', '▁blue', '▁wrap', '-', 'around', '▁skirt', '▁and', '▁was', '▁now', '▁wearing', '▁red', '▁short', 's', '▁with', '▁her', '▁white', '▁silk', '▁blouse', '.', '▁Question', ':', '▁Charity', '▁had', '▁changed', '▁from', '▁the', '▁blue', '▁wrap', '-', 'around', '▁skirt', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "logits:  tensor([[-6.3181e-06, -1.2500e+01, -1.2839e+01],\n",
      "        [-9.3465e+00, -4.2716e-04, -7.9873e+00]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "labels:  tensor([0, 1], device='cuda:0')\n",
      "pred: tensor(0, device='cuda:0') \n"
     ]
    }
   ],
   "source": [
    "for step, inputs in enumerate(train_dataloader):\n",
    "    if use_cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    print(\"inputs: \", inputs[\"input_ids\"].shape)\n",
    "    print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]))\n",
    "    logits = prompt_model(inputs)\n",
    "    labels = inputs['label']\n",
    "    print(\"logits: \",logits)\n",
    "    print(\"labels: \",labels)\n",
    "    print(\"pred:\", torch.argmax(logits[0], dim=-1), \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cf5b926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/cryptography/hazmat/backends/openssl/x509.py:17: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  utils.DeprecatedIn35,\n",
      "Reusing dataset super_glue (../datasets/.cache/huggingface_datasets/super_glue/cb/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa097bb757e34f28aa3013974dfd6aa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'premise': 'It was a complex language. Not written down but handed down. One might say it was peeled down.',\n",
       " 'hypothesis': 'the language was peeled down',\n",
       " 'idx': 0,\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.1 mixed_template  url:https://github.com/thunlp/OpenPrompt/blob/main/tutorial/1.1_mixed_template.py\n",
    "\n",
    "from datasets import load_dataset\n",
    "raw_dataset = load_dataset('super_glue', 'cb', cache_dir=\"../datasets/.cache/huggingface_datasets\")\n",
    "raw_dataset['train'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82176139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"guid\": 0,\n",
      "  \"label\": 0,\n",
      "  \"meta\": {},\n",
      "  \"text_a\": \"It was a complex language. Not written down but handed down. One might say it was peeled down.\",\n",
      "  \"text_b\": \"the language was peeled down\",\n",
      "  \"tgt_text\": null\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/cryptography/hazmat/backends/openssl/x509.py:17: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  utils.DeprecatedIn35,\n"
     ]
    }
   ],
   "source": [
    "from openprompt.data_utils import InputExample\n",
    "\n",
    "dataset = {}\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    dataset[split] = []\n",
    "    for data in raw_dataset[split]:\n",
    "        input_example = InputExample(text_a = data['premise'], text_b = data['hypothesis'], label=int(data['label']), guid=data['idx'])\n",
    "        dataset[split].append(input_example)\n",
    "print(dataset['train'][0])\n",
    "\n",
    "from openprompt.plms import load_plm\n",
    "\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"t5\", \"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ecc939b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'text': 'It was a complex language. Not written down but handed down. One might say it was peeled down.', 'soft_token_ids': 0, 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '', 'soft_token_ids': 1, 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '', 'soft_token_ids': 2, 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '', 'soft_token_ids': 3, 'loss_ids': 0, 'shortenable_ids': 0}, {'text': ' the language was peeled down', 'soft_token_ids': 0, 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '', 'soft_token_ids': 4, 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'soft_token_ids': 0, 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '.', 'soft_token_ids': 0, 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'label': 0}]\n"
     ]
    }
   ],
   "source": [
    "# 使用MixedTemplate，可以用{soft}来表示一个可调的template 具体相关概念可以自行搜索\n",
    "from openprompt.prompts import MixedTemplate\n",
    "\n",
    "mytemplate1 = MixedTemplate(model=plm, tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} {\"soft\": \"Question:\"} {\"placeholder\":\"text_b\"}? Is it correct? {\"mask\"}.')\n",
    "\n",
    "mytemplate = MixedTemplate(model=plm, tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} {\"soft\"} {\"soft\"} {\"soft\"} {\"placeholder\":\"text_b\"} {\"soft\"} {\"mask\"}.')\n",
    "\n",
    "\n",
    "wrapped_example = mytemplate.wrap_one_example(dataset['train'][0])\n",
    "print(wrapped_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0bd8d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 250it [00:00, 343.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[4273]],\n",
      "\n",
      "        [[ 150]],\n",
      "\n",
      "        [[2087]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6852, -1.4578, -0.5415],\n",
       "        [-2.2715, -1.1212, -0.5605]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_t5tokenizer = WrapperClass(max_seq_length=128, decoder_max_length=3, tokenizer=tokenizer,truncate_method=\"head\")\n",
    "\n",
    "from openprompt import PromptDataLoader\n",
    "\n",
    "train_dataloader = PromptDataLoader(dataset=dataset[\"train\"], template=mytemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
    "    batch_size=4,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")\n",
    "\n",
    "from openprompt.prompts import ManualVerbalizer\n",
    "import torch\n",
    "\n",
    "# for example the verbalizer contains multiple label words in each class\n",
    "myverbalizer = ManualVerbalizer(tokenizer, num_classes=3,\n",
    "                        label_words=[[\"yes\"], [\"no\"], [\"maybe\"]])\n",
    "\n",
    "print(myverbalizer.label_words_ids)\n",
    "logits = torch.randn(2,len(tokenizer)) # creating a pseudo output from the plm\n",
    "myverbalizer.process_logits(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45b885b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.413600206375122\n",
      "1.0612258911132812\n",
      "0.9850967725118002\n",
      "0.8783328384160995\n",
      "0.9662164092063904\n",
      "0.8626028199990591\n",
      "0.9454956821032933\n",
      "0.9703408256173134\n",
      "0.9662239419089423\n",
      "0.9114614963531494\n",
      "0.90981140461835\n",
      "0.8689070468147596\n",
      "0.852993939931576\n",
      "0.805360204407147\n",
      "0.7953401148319245\n",
      "0.7784849796444178\n",
      "0.7391081965144943\n",
      "0.7070387274854713\n",
      "0.6926711268330875\n",
      "0.7425214562565088\n",
      "0.711621394824414\n",
      "0.7300253263251348\n",
      "0.7242054884200511\n",
      "0.7027535975600282\n",
      "0.7184759750962257\n",
      "0.7274311497234381\n",
      "0.7077179642187225\n",
      "0.6892181499195951\n",
      "0.6964782304291067\n",
      "0.7047956181069215\n",
      "0.6896308028409558\n",
      "0.6769438742194325\n",
      "0.6646323034709151\n",
      "0.6552177661043757\n",
      "0.6772534466215543\n",
      "0.6588325233509144\n",
      "0.6463159014647072\n",
      "0.640822395486267\n",
      "0.6251856757280154\n",
      "0.61884994097054\n",
      "0.6257700327692962\n",
      "0.6146987985287394\n",
      "0.6026702864572059\n",
      "0.5908679230646654\n",
      "0.5786888536479738\n",
      "0.5664924554850744\n",
      "0.5552989737467563\n",
      "0.5444123907169948\n",
      "0.534322806538976\n",
      "0.5239627437293529\n",
      "0.5141876362556336\n",
      "0.5330581286062415\n",
      "0.5259003191483471\n",
      "0.5162624637941243\n",
      "0.5353353227912026\n",
      "0.5265935254782173\n",
      "0.5190525465203744\n",
      "0.5116923631104673\n",
      "0.5036826219441275\n",
      "0.496138293404753\n",
      "0.4901430510198239\n",
      "0.48303707903851906\n",
      "0.47602100006585557\n",
      "0.03436437249183655\n",
      "0.02440208662301302\n",
      "0.030822014436125755\n",
      "0.035118677187711\n",
      "0.0924038652330637\n",
      "0.08460118031750123\n",
      "0.07665289459483964\n",
      "0.06900168559513986\n",
      "0.06281731257008182\n",
      "0.057594220619648695\n",
      "0.05453533518382094\n",
      "0.05035493829442809\n",
      "0.04784595590228072\n",
      "0.054796918927292736\n",
      "0.0517867867834866\n",
      "0.05803998975898139\n",
      "0.055151030612999905\n",
      "0.05308722350228992\n",
      "0.05047701807741664\n",
      "0.04828025080496445\n",
      "0.04642846068323013\n",
      "0.04472328494937921\n",
      "0.05677926193153405\n",
      "0.05533082605688833\n",
      "0.053314818693324925\n",
      "0.05185932267564707\n",
      "0.05002084803870983\n",
      "0.04878617824787008\n",
      "0.048027368342696594\n",
      "0.047031887884562217\n",
      "0.04571600701479662\n",
      "0.044615591104957275\n",
      "0.0533013029278002\n",
      "0.05228167896925965\n",
      "0.051524492552770036\n",
      "0.05014893515423561\n",
      "0.04891212171925282\n",
      "0.04767506985961901\n",
      "0.04645614318015317\n",
      "0.04542006176561699\n",
      "0.044440894561575545\n",
      "0.043636769758276864\n",
      "0.04286364781423905\n",
      "0.042182851723654574\n",
      "0.041626722710336454\n",
      "0.04077881181159603\n",
      "0.04338679949448325\n",
      "0.042506033499497185\n",
      "0.04175718872135562\n",
      "0.047177052929182534\n",
      "0.046272794410755276\n",
      "0.04586008603543563\n",
      "0.0452489706051738\n",
      "0.0444578872007292\n",
      "0.0436521885241911\n",
      "0.04291775911640018\n",
      "0.04367599998250514\n",
      "0.043059531077672325\n",
      "0.042346167207697516\n",
      "0.0416467405402121\n",
      "0.0413238915061811\n",
      "0.045909557687991764\n",
      "0.04529701368555256\n",
      "0.0037362356670200825\n",
      "0.011735271895304322\n",
      "0.012145476260532936\n",
      "0.010400702594779432\n",
      "0.010346246231347323\n",
      "0.009733745129778981\n",
      "0.009420204708086593\n",
      "0.00930736301233992\n",
      "0.009588423236790631\n",
      "0.009850058285519481\n",
      "0.017565338068049063\n",
      "0.022541503383157153\n",
      "0.0211610603862657\n",
      "0.022677789209410548\n",
      "0.022120397817343474\n",
      "0.02109292714158073\n",
      "0.020213673576055205\n",
      "0.01957717933692038\n",
      "0.018564608629707147\n",
      "0.017978761340782513\n",
      "0.017553147101785897\n",
      "0.016806169737140986\n",
      "0.01632374750435069\n",
      "0.01617075536463138\n",
      "0.016480436275014654\n",
      "0.015927231903053604\n",
      "0.015453653691414123\n",
      "0.015130718343633427\n",
      "0.01463810906540763\n",
      "0.014200297057201775\n",
      "0.014330455910361882\n",
      "0.013885260013239531\n",
      "0.013506167814546532\n",
      "0.013197122640977957\n",
      "0.013006996648826836\n",
      "0.012728548330010704\n",
      "0.012398214517766924\n",
      "0.01211445835992651\n",
      "0.011806393877822511\n",
      "0.011517921119775565\n",
      "0.011252242112610342\n",
      "0.011045882307024883\n",
      "0.010796817889143334\n",
      "0.010565296974825535\n",
      "0.010381387723060066\n",
      "0.010778067968045512\n",
      "0.010594501363657206\n",
      "0.010389454472867024\n",
      "0.010215853340181342\n",
      "0.010017922881961567\n",
      "0.009822814201042467\n",
      "0.009846948020752349\n",
      "0.009672914748766486\n",
      "0.009546515396693765\n",
      "0.009407967535001014\n",
      "0.009250927226535819\n",
      "0.00912354230176629\n",
      "0.008982952667730418\n",
      "0.008839901120986956\n",
      "0.008695772154048124\n",
      "0.008557984447626772\n",
      "0.00844223773721399\n",
      "0.008314638457834787\n",
      "0.0006769357132725418\n",
      "0.0010077145125251263\n",
      "0.0020407046928691366\n",
      "0.0015663435951864813\n",
      "0.001533366719377227\n",
      "0.001512328921914256\n",
      "0.0013045879552789433\n",
      "0.0012702774029094144\n",
      "0.0011445253160976183\n",
      "0.001066109633393353\n",
      "0.0010686523660710504\n",
      "0.0009901159604244942\n",
      "0.0009353400656926589\n",
      "0.000911183362664555\n",
      "0.0008626232627041948\n",
      "0.0008443656483905215\n",
      "0.0008364525860730175\n",
      "0.0007985199687633818\n",
      "0.0008419340956606902\n",
      "0.0008693474603205687\n",
      "0.0008707605825509832\n",
      "0.0008460564445571931\n",
      "0.0008422793544013985\n",
      "0.0008135092390754531\n",
      "0.0007974056559032761\n",
      "0.0007908241719773146\n",
      "0.0007642230169343142\n",
      "0.0007493975891910461\n",
      "0.0007963668430075532\n",
      "0.0008081989428319503\n",
      "0.0007870717679672394\n",
      "0.0007905059853783314\n",
      "0.0007807431770329872\n",
      "0.0007691273946393802\n",
      "0.0007620296073063011\n",
      "0.0007560614158137469\n",
      "0.0007689551423174782\n",
      "0.0007729209884066796\n",
      "0.000765929416304108\n",
      "0.0007604339376484859\n",
      "0.0007504507387837781\n",
      "0.0007340275206799907\n",
      "0.0007423216943713013\n",
      "0.0007309941480674421\n",
      "0.0007242794392772744\n",
      "0.0007142864214193137\n",
      "0.000707119607776552\n",
      "0.0006967639412020313\n",
      "0.0006865407891122491\n",
      "0.0006792183568177279\n",
      "0.0006677958025487469\n",
      "0.0006901857973739075\n",
      "0.0008105336488576009\n",
      "0.0007996500620113997\n",
      "0.0007936848324871707\n",
      "0.0007830163426011236\n",
      "0.000773342009204881\n",
      "0.0007766700748169538\n",
      "0.0007659774105916968\n",
      "0.000757841066418526\n",
      "0.0007466832423949095\n",
      "0.0007506842469616283\n",
      "0.0007394384448407292\n",
      "0.00015054471441544592\n",
      "0.00020516631775535643\n",
      "0.00021170715141731003\n",
      "0.0002539218621677719\n",
      "0.0002741237811278552\n",
      "0.0002688099048100412\n",
      "0.0002684939494689128\n",
      "0.00029729976449743845\n",
      "0.0003632710205339309\n",
      "0.0003705356182763353\n",
      "0.00035480519926006144\n",
      "0.00035449985928911093\n",
      "0.0003603545908565418\n",
      "0.00034245666808731457\n",
      "0.0003832442089333199\n",
      "0.000375827685729746\n",
      "0.0003562708073278533\n",
      "0.0003440002480298669\n",
      "0.0003369917584151201\n",
      "0.0003219660235117772\n",
      "0.0003240377002276502\n",
      "0.00036099791048565584\n",
      "0.0003654512827621226\n",
      "0.0003618995938268199\n",
      "0.00035539810443879105\n",
      "0.00034923376887700916\n",
      "0.00035940138507242037\n",
      "0.0003539884997475643\n",
      "0.0003495785671914526\n",
      "0.00035123851375828963\n",
      "0.00034699230673140846\n",
      "0.0003436945454495799\n",
      "0.0003441337578282621\n",
      "0.0003462787496092086\n",
      "0.00034760399648803286\n",
      "0.0003515453323012076\n",
      "0.00036901899708224135\n",
      "0.00036463947244077356\n",
      "0.00036997359347878955\n",
      "0.00036920108041158526\n",
      "0.0003831595426074672\n",
      "0.0003752628142267765\n",
      "0.0003687727853397265\n",
      "0.0003652520404516095\n",
      "0.0003682226823002566\n",
      "0.00036291392090155887\n",
      "0.00035882204719336127\n",
      "0.0003559478907391167\n",
      "0.0003566104789978435\n",
      "0.00035478128360409754\n",
      "0.00035064906264511487\n",
      "0.00034965034900289116\n",
      "0.00034373748767736615\n",
      "0.0003427442881258213\n",
      "0.00034332004440577955\n",
      "0.0003495975800953082\n",
      "0.0003481890698981659\n",
      "0.00034767404984700865\n",
      "0.0003427242835163794\n",
      "0.0003396012714195725\n",
      "0.00033446103067244173\n",
      "0.00033096263359766454\n",
      "0.00032887346894731596\n",
      "0.0003244451654609293\n",
      "0.00021465322060976177\n",
      "0.00023026825510896742\n",
      "0.00030445228185271844\n",
      "0.0002682413760339841\n",
      "0.00025713557018510375\n",
      "0.0002276647173857782\n",
      "0.0002287103156959347\n",
      "0.0002280430776914323\n",
      "0.00021492701816896442\n",
      "0.0002087264375321948\n",
      "0.00020125520950387\n",
      "0.00021925800767628691\n",
      "0.0002157543112843996\n",
      "0.00021193220770025314\n",
      "0.00020608207091754593\n",
      "0.00021169404948630151\n",
      "0.0002085606742588829\n",
      "0.00021315648738987205\n",
      "0.00025756071790965506\n",
      "0.000255019880222833\n",
      "0.0002474280847143911\n",
      "0.00024127897837467532\n",
      "0.0002370907230518545\n",
      "0.0002354324703628663\n",
      "0.00023221014677032005\n",
      "0.00022664487793614777\n",
      "0.00022393952440324938\n",
      "0.00021748763839272653\n",
      "0.00022018839978651765\n",
      "0.00022025634614457828\n",
      "0.00021615495688820374\n",
      "0.00021253607040059498\n",
      "0.00021039437535150414\n",
      "0.0002064895918010734\n",
      "0.0002045853852905566\n",
      "0.0002000314520587012\n",
      "0.00020112593579381754\n",
      "0.00020403302597771518\n",
      "0.00020725175290863261\n",
      "0.0002034099225278535\n",
      "0.0002013314964036302\n",
      "0.00020182970860501925\n",
      "0.00020055574796887495\n",
      "0.00020299116295063868\n",
      "0.00020647342661623438\n",
      "0.0002043280237171046\n",
      "0.00020623642285499955\n",
      "0.00020275734975572904\n",
      "0.0002115580957615748\n",
      "0.00020804683698966698\n",
      "0.0002054591132036876\n",
      "0.0002023607228453192\n",
      "0.000200612067163564\n",
      "0.0002040336139378434\n",
      "0.0002111735238291398\n",
      "0.0002090256593395392\n",
      "0.0002077362492729725\n",
      "0.0002065050866187479\n",
      "0.00020594006909959716\n",
      "0.00020939291658339884\n",
      "0.0002120684440019578\n",
      "0.00021034364729689153\n",
      "0.0002644508786033839\n",
      "0.0001821948608267121\n",
      "0.00012965864031381594\n",
      "0.00014951469256629935\n",
      "0.0001386104580888059\n",
      "0.0001722678395405334\n",
      "0.00016140887445154867\n",
      "0.00016803044081825647\n",
      "0.00016076046085800044\n",
      "0.00018556895483925472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00018176948486308737\n",
      "0.00018554589981552758\n",
      "0.00017689396386581042\n",
      "0.00016976114394700353\n",
      "0.00017831943284060496\n",
      "0.00017092569737542362\n",
      "0.00016163558029802516\n",
      "0.00016304835420063077\n",
      "0.00015548325618989397\n",
      "0.00015201828800854856\n",
      "0.00015284216325588723\n",
      "0.0001498038715329709\n",
      "0.00014956789219349562\n",
      "0.0001459471675389068\n",
      "0.00014307973040558864\n",
      "0.0001465957427801466\n",
      "0.00014672870862274\n",
      "0.00014594501362807932\n",
      "0.00014639515706974259\n",
      "0.00014241830655616164\n",
      "0.00014201326033094853\n",
      "0.0001430023199304742\n",
      "0.00014438112113125635\n",
      "0.0001475645609817173\n",
      "0.00014459156419824076\n",
      "0.00014104864981668975\n",
      "0.00014095789879934887\n",
      "0.00014842931580604177\n",
      "0.0001450651156912313\n",
      "0.0001416992552776719\n",
      "0.0001434205883126152\n",
      "0.00014460303960038887\n",
      "0.00014172809496389968\n",
      "0.0001402530534629808\n",
      "0.00014273148184050418\n",
      "0.0001430776384515174\n",
      "0.00014695941681179162\n",
      "0.00015202984243008663\n",
      "0.00015062522320198585\n",
      "0.0001480728526621533\n",
      "0.000152251768685899\n",
      "0.00014963275839363632\n",
      "0.00014882734287208942\n",
      "0.0001486930839039107\n",
      "0.0001484340434299983\n",
      "0.00014620012905197655\n",
      "0.0001471386269035416\n",
      "0.0001500108800553727\n",
      "0.00014869212736812187\n",
      "0.000148654750970915\n",
      "0.00015018102485406476\n",
      "0.00014891472248505563\n",
      "0.00014766451159442705\n",
      "0.0001226244930876419\n",
      "0.0001025988967739977\n",
      "0.00010642025639147808\n",
      "0.000133693127281731\n",
      "0.00011435704800533131\n",
      "0.00011952189985701504\n",
      "0.00011830917563721803\n",
      "0.00010509630840260797\n",
      "9.834938434020539e-05\n",
      "9.183133852275205e-05\n",
      "9.556281665109881e-05\n",
      "0.00010042215740213578\n",
      "9.994067470240855e-05\n",
      "9.937921881178877e-05\n",
      "0.00010004453730895572\n",
      "0.00011396694009135899\n",
      "0.00012557984909482699\n",
      "0.0001213780373190174\n",
      "0.00011890140036772017\n",
      "0.0001186614763810212\n",
      "0.00011811941922566067\n",
      "0.00011441108807213658\n",
      "0.00011064819682360394\n",
      "0.00011058580688919999\n",
      "0.00011065337388572515\n",
      "0.00010860734538744928\n",
      "0.00011093762013506084\n",
      "0.0001109633875720257\n",
      "0.00010836711530357897\n",
      "0.0001080964383011936\n",
      "0.00010629269218043191\n",
      "0.00010777911748505176\n",
      "0.00010665509864448916\n",
      "0.00010695279934225255\n",
      "0.000115931092217839\n",
      "0.00011404851397855964\n",
      "0.00011998989806228122\n",
      "0.0001180509762697942\n",
      "0.00011560629940131315\n",
      "0.00011423448802361236\n",
      "0.00011632509771205638\n",
      "0.00011911838328160229\n",
      "0.00011735728360711851\n",
      "0.00011511678194024584\n",
      "0.00011482874860020173\n",
      "0.00011307555211675682\n",
      "0.00011313093987252053\n",
      "0.00011101929129608834\n",
      "0.00011078538303694104\n",
      "0.00011283497065960547\n",
      "0.00011771800387470969\n",
      "0.00011881445888880429\n",
      "0.00011730028249728075\n",
      "0.00011537530195305913\n",
      "0.0001136086398136368\n",
      "0.00011388770909499206\n",
      "0.00011304036772280437\n",
      "0.00011162011787746451\n",
      "0.00011079245979413765\n",
      "0.00011012991270339019\n",
      "0.00011000208797056865\n",
      "0.00011102074012664924\n",
      "0.00011177303576192889\n",
      "8.77016645972617e-05\n",
      "6.550111538672354e-05\n",
      "9.361116948033062e-05\n",
      "0.00012207449526613345\n",
      "0.00011724904397851788\n",
      "0.00012374926579165427\n",
      "0.00011846724217840736\n",
      "0.00010599083043416613\n",
      "9.838950386943502e-05\n",
      "9.376555681228638e-05\n",
      "8.743862483904444e-05\n",
      "8.223073155022576e-05\n",
      "9.153304465424019e-05\n",
      "8.879023451819583e-05\n",
      "8.799059226779112e-05\n",
      "8.367393604657991e-05\n",
      "8.021748373507494e-05\n",
      "7.890330531760507e-05\n",
      "7.677380764155379e-05\n",
      "7.855627400203957e-05\n",
      "7.687603638693136e-05\n",
      "7.861554720214123e-05\n",
      "7.679251820777806e-05\n",
      "7.523687077082286e-05\n",
      "7.903741170594003e-05\n",
      "7.83529514172607e-05\n",
      "8.155076776111619e-05\n",
      "8.004418168638949e-05\n",
      "7.936078722634883e-05\n",
      "7.771278536286749e-05\n",
      "7.601345650102538e-05\n",
      "7.65397729196593e-05\n",
      "7.503676451134115e-05\n",
      "7.957657678215422e-05\n",
      "8.250159875647764e-05\n",
      "8.110642036424704e-05\n",
      "8.125239583023358e-05\n",
      "8.469689577108703e-05\n",
      "8.52763816323997e-05\n",
      "8.422400374001882e-05\n",
      "8.5039959985141e-05\n",
      "8.416248684148914e-05\n",
      "8.594808262619894e-05\n",
      "8.551185663401901e-05\n",
      "8.429041291593522e-05\n",
      "8.4147478921058e-05\n",
      "8.326510412208587e-05\n",
      "8.266720484092123e-05\n",
      "8.290371710134014e-05\n",
      "8.231782103393926e-05\n",
      "8.543906487295103e-05\n",
      "8.708805549199147e-05\n",
      "8.60392243202165e-05\n",
      "8.597618664309805e-05\n",
      "8.585034708878745e-05\n",
      "8.641053711601541e-05\n",
      "8.792557762940864e-05\n",
      "8.796066814629112e-05\n",
      "8.720422430103052e-05\n",
      "8.78169104604846e-05\n",
      "8.76821384616826e-05\n",
      "8.707926401491991e-05\n",
      "8.785105129636433e-05\n",
      "9.961413888959214e-05\n",
      "0.00010475884118932299\n",
      "7.144854847259315e-05\n",
      "7.018503163180867e-05\n",
      "6.480829570136848e-05\n",
      "7.741546346551331e-05\n",
      "8.057440644344232e-05\n",
      "7.730462692734363e-05\n",
      "7.23973775065032e-05\n",
      "7.20267702035926e-05\n",
      "6.779528116980642e-05\n",
      "6.33973511791434e-05\n",
      "6.645431813012692e-05\n",
      "6.477067872115836e-05\n",
      "6.205597004130445e-05\n",
      "6.344086099829838e-05\n",
      "6.557977188576307e-05\n",
      "6.472949429634254e-05\n",
      "7.022738327577827e-05\n",
      "7.259832025283686e-05\n",
      "7.547131279005996e-05\n",
      "7.575207140772281e-05\n",
      "7.715782582992055e-05\n",
      "7.548635575934289e-05\n",
      "7.649224737178884e-05\n",
      "7.526494669736376e-05\n",
      "7.28095945945555e-05\n",
      "7.131403723243628e-05\n",
      "7.045695184419385e-05\n",
      "6.905806853865215e-05\n",
      "7.418403685087762e-05\n",
      "7.276635376740614e-05\n",
      "7.250728745158554e-05\n",
      "7.143003672889095e-05\n",
      "7.13355682949311e-05\n",
      "6.974890141009382e-05\n",
      "6.838814807771407e-05\n",
      "6.83294174091555e-05\n",
      "7.030275680965412e-05\n",
      "6.969552806594948e-05\n",
      "7.273286265200756e-05\n",
      "7.311763893301846e-05\n",
      "7.193564286269044e-05\n",
      "7.10728682468636e-05\n",
      "7.097422297495845e-05\n",
      "7.141935721531057e-05\n",
      "7.187974719311163e-05\n",
      "7.168975477611639e-05\n",
      "7.080509423609996e-05\n",
      "7.210775461317099e-05\n",
      "7.289264649622127e-05\n",
      "7.456584151336407e-05\n",
      "7.437570368738478e-05\n",
      "7.342111426724942e-05\n",
      "7.300455410421075e-05\n",
      "7.406883915044611e-05\n",
      "7.396972277044704e-05\n",
      "7.287422306882067e-05\n",
      "7.176534692485278e-05\n",
      "7.151540860377281e-05\n",
      "7.069917797170852e-05\n",
      "7.117045406483803e-05\n",
      "7.01902479574658e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 56it [00:00, 316.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9285714285714286\n"
     ]
    }
   ],
   "source": [
    "from openprompt import PromptForClassification\n",
    "\n",
    "use_cuda = True\n",
    "prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\n",
    "if use_cuda:\n",
    "    prompt_model=  prompt_model.cuda()\n",
    "\n",
    "# ## below is standard training\n",
    "\n",
    "\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "# it's always good practice to set no decay to biase and LayerNorm parameters\n",
    "optimizer_grouped_parameters1 = [\n",
    "    {'params': [p for n, p in prompt_model.plm.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# Using different optimizer for prompt parameters and model parameters\n",
    "optimizer_grouped_parameters2 = [\n",
    "    {'params': [p for n,p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n",
    "]\n",
    "\n",
    "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=1e-4)\n",
    "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=1e-3)\n",
    "\n",
    "for epoch in range(10):\n",
    "    tot_loss = 0\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "        tot_loss += loss.item()\n",
    "        optimizer1.step()\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.step()\n",
    "        optimizer2.zero_grad()\n",
    "        print(tot_loss/(step+1))\n",
    "\n",
    "# ## evaluate\n",
    "\n",
    "# %%\n",
    "validation_dataloader = PromptDataLoader(dataset=dataset[\"validation\"], template=mytemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
    "    batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")\n",
    "\n",
    "\n",
    "allpreds = []\n",
    "alllabels = []\n",
    "for step, inputs in enumerate(validation_dataloader):\n",
    "    if use_cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    logits = prompt_model(inputs)\n",
    "    labels = inputs['label']\n",
    "    alllabels.extend(labels.cpu().tolist())\n",
    "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "419ccb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 conditonal_generation\n",
    "# https://github.com/thunlp/OpenPrompt/blob/main/tutorial/2.1_conditional_generation.py\n",
    "from openprompt.data_utils.conditional_generation_dataset import WebNLGProcessor\n",
    "dataset = {}\n",
    "dataset['train'] = WebNLGProcessor().get_train_examples(\"/root/datasets/CondGen/webnlg_2017/\")\n",
    "dataset['validation'] = WebNLGProcessor().get_dev_examples(\"/root/datasets/CondGen/webnlg_2017/\")\n",
    "dataset['test'] = WebNLGProcessor().get_test_examples(\"/root/datasets/CondGen/webnlg_2017/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05d20d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/cryptography/hazmat/backends/openssl/x509.py:17: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  utils.DeprecatedIn35,\n"
     ]
    }
   ],
   "source": [
    "from openprompt.plms import load_plm\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm('t5', 't5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0386681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用prefix_tuning_template\n",
    "from openprompt.prompts.prefix_tuning_template import PrefixTuningTemplate\n",
    "mytemplate = PrefixTuningTemplate(model=plm,  tokenizer=tokenizer, text=' {\"placeholder\":\"text_a\"} {\"special\": \"<eos>\"} {\"mask\"} ', using_decoder_past_key_values=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6987e536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'text': '  | Aarhus_Airport : cityServed : \"Aarhus, Denmark\"', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<eos>', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}], {'guid': '0', 'tgt_text': 'The Aarhus is the airport of Aarhus, Denmark.'}]\n"
     ]
    }
   ],
   "source": [
    "# 取一个训练数据作为样例观察template如何wrap\n",
    "wrapped_example = mytemplate.wrap_one_example(dataset['train'][0])\n",
    "print(wrapped_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29cc95f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 18025it [00:38, 468.04it/s]\n",
      "tokenizing: 872it [00:01, 579.13it/s]\n",
      "tokenizing: 1862it [00:03, 578.81it/s]\n"
     ]
    }
   ],
   "source": [
    "from openprompt import PromptDataLoader\n",
    "train_dataloader = PromptDataLoader(dataset=dataset[\"train\"], template=mytemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=256,\n",
    "    batch_size=5,shuffle=True, teacher_forcing=True, predict_eos_token=True, # be sure to pass predict_eos_token=True if your template doesn't contain one, or you model may fail to stop generation.\n",
    "    truncate_method=\"head\")\n",
    "\n",
    "validation_dataloader = PromptDataLoader(dataset=dataset[\"validation\"], template=mytemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=256,\n",
    "    batch_size=5,shuffle=False, teacher_forcing=False, predict_eos_token=True,\n",
    "    truncate_method=\"head\")\n",
    "\n",
    "test_dataloader = PromptDataLoader(dataset=dataset[\"test\"], template=mytemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=256,\n",
    "    batch_size=5,shuffle=False, teacher_forcing=False, predict_eos_token=True,\n",
    "    truncate_method=\"head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e162a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pipeline model PromptForGeneration.\n",
    "from openprompt import PromptForGeneration\n",
    "use_cuda = True\n",
    "prompt_model = PromptForGeneration(plm=plm,template=mytemplate, freeze_plm=True,tokenizer=tokenizer, plm_eval_mode=False)\n",
    "if use_cuda:\n",
    "    prompt_model=  prompt_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d704bf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "# Follow PrefixTuning（https://github.com/XiangLi1999/PrefixTuning), we also fix the language model\n",
    "# only include the template's parameters in training.\n",
    "\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "{\n",
    "    \"params\": [p for n, p in mytemplate.named_parameters() if (not any(nd in n for nd in no_decay)) and p.requires_grad],\n",
    "    \"weight_decay\": 0.0,\n",
    "},\n",
    "{\n",
    "    \"params\": [p for n, p in mytemplate.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "    \"weight_decay\": 0.0,\n",
    "},\n",
    "]\n",
    "\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78d91015",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "where(): argument 'other' (position 3) must be Tensor, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9012ee052d0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mtot_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/openprompt/pipeline_base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputFeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/openprompt/pipeline_base.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshift_logits_and_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/openprompt/pipeline_base.py\u001b[0m in \u001b[0;36mshift_logits_and_labels\u001b[0;34m(self, logits, loss_ids, reference_ids)\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0mshift_loss_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0mshift_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreference_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0mshift_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshift_loss_ids\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mshift_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_input_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: where(): argument 'other' (position 3) must be Tensor, not int"
     ]
    }
   ],
   "source": [
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "\n",
    "tot_step  = len(train_dataloader)*5\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, 0, tot_step)\n",
    "\n",
    "# We provide generation a generation metric, you can also define your own. Note that it's not directly comparable to WebNLG's scripts evaluation.\n",
    "from openprompt.utils.metrics import generation_metric\n",
    "# Define evaluate function\n",
    "def evaluate(prompt_model, dataloader):\n",
    "    generated_sentence = []\n",
    "    groundtruth_sentence = []\n",
    "    prompt_model.eval()\n",
    "\n",
    "    for step, inputs in enumerate(dataloader):\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        _, output_sentence = prompt_model.generate(inputs, **generation_arguments)\n",
    "        generated_sentence.extend(output_sentence)\n",
    "        groundtruth_sentence.extend(inputs['tgt_text'])\n",
    "    score = generation_metric(generated_sentence, groundtruth_sentence, \"sentence_bleu\")\n",
    "    print(\"test_score\", score, flush=True)\n",
    "    return generated_sentence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "generation_arguments = {\n",
    "    \"max_length\": 512,\n",
    "    \"max_new_tokens\": None,\n",
    "    \"min_length\": 5,\n",
    "    \"temperature\": 1.0,\n",
    "    \"do_sample\": False,\n",
    "    \"top_k\": 0,\n",
    "    \"top_p\": 0.9,\n",
    "    \"repetition_penalty\": 1.0,\n",
    "    \"num_beams\": 5,\n",
    "    \"bad_words_ids\": [[628], [198]]\n",
    "}\n",
    "\n",
    "# training and generation.\n",
    "global_step = 0\n",
    "tot_loss = 0\n",
    "log_loss = 0\n",
    "for epoch in range(5):\n",
    "    prompt_model.train()\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        global_step +=1\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        loss = prompt_model(inputs)\n",
    "        loss.backward()\n",
    "        tot_loss += loss.item()\n",
    "        torch.nn.utils.clip_grad_norm_(mytemplate.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        if global_step %5 ==0:\n",
    "            print(\"Epoch {}, global_step {} average loss: {} lr: {}\".format(epoch, global_step, (tot_loss-log_loss)/500, scheduler.get_last_lr()[0]), flush=True)\n",
    "            log_loss = tot_loss\n",
    "\n",
    "generated_sentence = evaluate(prompt_model, test_dataloader)\n",
    "with open(f\"Generated_sentence_webnlg_gpt2_False.txt\",'w') as f:\n",
    "    for i in generated_sentence:\n",
    "        f.write(i+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8f6b455",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-623fa5b0b93e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
