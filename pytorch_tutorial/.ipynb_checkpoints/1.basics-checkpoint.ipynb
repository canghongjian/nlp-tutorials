{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8efd22cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c1d59e",
   "metadata": {},
   "source": [
    "# 目录：\n",
    "## 1. Basic autograd example 1\n",
    "## 2. Basic autograd example 2\n",
    "## 3. Loading Data from Numpy\n",
    "## 4. Input pipline\n",
    "## 5. Input pipline for custom dataset\n",
    "## 6. Pretrained model\n",
    "## 7. Save and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "879377ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad: tensor(20.)  w.grad: tensor(10.)  b.grad tensor(10.)\n"
     ]
    }
   ],
   "source": [
    "# 1. Basic autograd example 1\n",
    "# create tensors\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "# Build a computational graph. 建立计算图\n",
    "y = (w * x + b) ** 2\n",
    "\n",
    "# 计算梯度\n",
    "y.backward()\n",
    "\n",
    "# 输出梯度\n",
    "print(\"x.grad:\", x.grad, \" w.grad:\", w.grad, \" b.grad\", b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b5efe9",
   "metadata": {},
   "source": [
    "x的梯度即为y对x求偏导，再代入x的值的结果。\n",
    "\n",
    "dy/dx = w, dy/dw = x, dy/db = 1\n",
    "\n",
    "代入x、w、b的值由此可求出x、w、b的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "373154d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  Parameter containing:\n",
      "tensor([[-0.0146, -0.2289, -0.2957,  0.4126,  0.2726],\n",
      "        [ 0.0514,  0.0362, -0.3249, -0.3307,  0.0771],\n",
      "        [-0.3290,  0.1781, -0.1378,  0.3778,  0.1981]], requires_grad=True)\n",
      "b:  Parameter containing:\n",
      "tensor([-0.4289,  0.1331,  0.4228], requires_grad=True)\n",
      "loss:  1.2896790504455566\n",
      "loss2: 1.2896790504455566\n",
      "theory_grad:dL/dw = 2*(pred - y) * x / n, where n means the length of y(here is 20 * 3): \n",
      " tensor([[ 0.0173, -0.1220, -0.2418,  0.2516,  0.1707],\n",
      "        [-0.0826, -0.0309, -0.2013, -0.0494, -0.0718],\n",
      "        [-0.1695,  0.2735, -0.3993,  0.1538,  0.2530]], grad_fn=<DivBackward0>)\n",
      "theory_grad:dL/db = 2*(pred - y) / n: \n",
      " tensor([-0.1025,  0.0395,  0.2876], grad_fn=<DivBackward0>)\n",
      "dL/dw:  tensor([[ 0.0173, -0.1220, -0.2418,  0.2516,  0.1707],\n",
      "        [-0.0826, -0.0309, -0.2013, -0.0494, -0.0718],\n",
      "        [-0.1695,  0.2735, -0.3993,  0.1538,  0.2530]])\n",
      "dL/db:  tensor([-0.1025,  0.0395,  0.2876])\n",
      "wnew: tensor([[-0.0148, -0.2277, -0.2933,  0.4101,  0.2709],\n",
      "        [ 0.0522,  0.0365, -0.3229, -0.3302,  0.0778],\n",
      "        [-0.3273,  0.1754, -0.1338,  0.3762,  0.1956]], grad_fn=<SubBackward0>)\n",
      "bnew: tensor([-0.4279,  0.1327,  0.4199], grad_fn=<SubBackward0>)\n",
      "loss after 1 step optimization:  1.2830297946929932\n",
      "w:  Parameter containing:\n",
      "tensor([[-0.0148, -0.2277, -0.2933,  0.4101,  0.2709],\n",
      "        [ 0.0522,  0.0365, -0.3229, -0.3302,  0.0778],\n",
      "        [-0.3273,  0.1754, -0.1338,  0.3762,  0.1956]], requires_grad=True)  b: Parameter containing:\n",
      "tensor([-0.4279,  0.1327,  0.4199], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 2. Basic autograd example 2\n",
    "# 创建张量\n",
    "x = torch.randn(45, 5)  # 创建10*3的随机张量 其中填充了标准正态分布的值，即均值为0，方差为1\n",
    "y = torch.randn(45, 3)  # 创建10*2的随机张量\n",
    "\n",
    "# 创建全连接层\n",
    "linear = nn.Linear(5, 3)\n",
    "print(\"w: \", linear.weight)\n",
    "print(\"b: \", linear.bias)\n",
    "\n",
    "# 创建损失函数和optimizer\n",
    "criterion =  nn.MSELoss()  # 使用MSE（均方误差）损失函数\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01) # 使用SGD（随机梯度下降）作为优化器，学习率为0.01\n",
    "\n",
    "# 前向传递\n",
    "pred = linear(x) # 等价于pred = torch.mm(x, linear.weight.T) + linear.bias 这里相加使用了广播机制\n",
    "#print(\"pred:\", pred, \"\\nwx+b:\", torch.mm(x, linear.weight.T))\n",
    "# 计算误差\n",
    "loss = criterion(pred, y)\n",
    "print(\"loss: \", loss.item())\n",
    "loss2 = torch.mean((pred-y)**2) # \n",
    "print(\"loss2:\", loss2.item())\n",
    "# 反向传递\n",
    "loss.backward()\n",
    "\n",
    "# 理论计算梯度： 需要注意向量对标量求导、向量对向量求导的区别 https://wenku.baidu.com/view/a3cba05602f69e3143323968011ca300a6c3f6a2.html\n",
    "w_theory_grad = torch.div(2*torch.mm((pred - y).T, x), y.shape[0] * y.shape[1])\n",
    "b_theory_grad = torch.div(2*torch.sum((pred - y), dim=0), y.shape[0] * y.shape[1])\n",
    "print(\"theory_grad:dL/dw = 2*(pred - y) * x / n, where n means the length of y(here is 20 * 3): \\n\", w_theory_grad)\n",
    "print(\"theory_grad:dL/db = 2*(pred - y) / n: \\n\", b_theory_grad)\n",
    "# 打印梯度\n",
    "print ('dL/dw: ', linear.weight.grad) \n",
    "print ('dL/db: ', linear.bias.grad)\n",
    "\n",
    "print(\"wnew:\", linear.weight - 0.01 * linear.weight.grad)\n",
    "print(\"bnew:\", linear.bias - 0.01 * linear.bias.grad)\n",
    "# 进行一步的梯度下降\n",
    "optimizer.step()   # wnew = wold - lr * dL/dw\n",
    "\n",
    "# You can also perform gradient descent at the low level.\n",
    "# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
    "# linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
    "\n",
    "# 再打印更新参数后的损失\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print(\"loss after 1 step optimization: \", loss.item())\n",
    "print(\"w: \", linear.weight, \" b:\", linear.bias) # linear.weight == wnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55a9814e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3333, 2.0000, 3.3333])\n",
      "tensor([ 2.6667,  6.0000, 16.6667], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "p = torch.tensor([2.0, 3.0, 5.0], requires_grad=True)\n",
    "q = p**2+1\n",
    "z = torch.mean(q)\n",
    "z.backward()\n",
    "print(p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ca7103d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5398, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
